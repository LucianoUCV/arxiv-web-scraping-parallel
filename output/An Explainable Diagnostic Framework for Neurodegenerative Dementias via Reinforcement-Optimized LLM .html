<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning</title>
<!--Generated on Mon May 26 13:09:21 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2505.19954v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S1" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S1.SS0.SSS0.Px1" title="In 1 Introduction ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Synthetic radiology report generation from neuroimaging.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S1.SS0.SSS0.Px2" title="In 1 Introduction ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Reinforcement learning–optimized reasoning.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S2" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S2.SS0.SSS0.Px1" title="In 2 Related work ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Neuroimaging-based Diagnosis and Post-hoc Explainability Limitations.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S2.SS0.SSS0.Px2" title="In 2 Related work ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">LLMs in medicine.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Approach</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS1" title="In 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS1.SSS0.Px1" title="In 3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">(1) MRI segmentation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS1.SSS0.Px2" title="In 3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">(2) Volume ratio computation.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS1.SSS0.Px3" title="In 3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">(3) Atrophy Estimation via Normative Modeling.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS1.SSS0.Px4" title="In 3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">(4) Radiology Report generation.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS2" title="In 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Prompting strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS3" title="In 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Incentivizing diagnostic reasoning with GRPO</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS3.SSS0.Px1" title="In 3.3 Incentivizing diagnostic reasoning with GRPO ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Format reward.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS3.SSS0.Px2" title="In 3.3 Incentivizing diagnostic reasoning with GRPO ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Accuracy reward.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS1" title="In 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Experimental setup</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS1.SSS0.Px1" title="In 4.1 Experimental setup ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Datasets.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS1.SSS0.Px2" title="In 4.1 Experimental setup ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Models and training details.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS2" title="In 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS2.SSS0.Px1" title="In 4.2 Results ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Off-the-shelf LLMs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS2.SSS0.Px2" title="In 4.2 Results ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">GRPO Fine-Tuned LLMs.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS2.SSS0.Px3" title="In 4.2 Results ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Comparison with classification-only approaches.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S5" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A1" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Example of a Complete Synthetic Report and Human Comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A2" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Insight into GRPO Training Dynamics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A3" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Full Diagnostic Reasoning Examples</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A4" title="In An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Training Details for Classification-Only Baselines</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A4.SS0.SSS0.Px1" title="In Appendix D Training Details for Classification-Only Baselines ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Vision-transformer (ViT).</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A4.SS0.SSS0.Px2" title="In Appendix D Training Details for Classification-Only Baselines ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_title">Support Vector Machine (SVM).</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"><span class="ltx_text ltx_font_bold" id="id1.1.1">Andrew Zamai<sup class="ltx_sup" id="id1.1.1.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id1.1.1.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id2.2.2">Nathanaël Fijalkow<sup class="ltx_sup" id="id2.2.2.1"><span class="ltx_text ltx_font_medium" id="id2.2.2.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id3.3.3">Boris Mansencal<sup class="ltx_sup" id="id3.3.3.1"><span class="ltx_text ltx_font_medium" id="id3.3.3.1.1">1</span></sup></span>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id4.4.4">Laurent Simon<sup class="ltx_sup" id="id4.4.4.1"><span class="ltx_text ltx_font_medium" id="id4.4.4.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id5.5.5">Eloi Navet<sup class="ltx_sup" id="id5.5.5.1"><span class="ltx_text ltx_font_medium" id="id5.5.5.1.1">1</span></sup></span>  <span class="ltx_text ltx_font_bold" id="id6.6.6">Pierrick Coupé<sup class="ltx_sup" id="id6.6.6.1"><span class="ltx_text ltx_font_medium" id="id6.6.6.1.1">1</span></sup></span>
<br class="ltx_break"/>
<sup class="ltx_sup" id="id8.8.id1">1</sup>Univ. Bordeaux
</span><span class="ltx_author_notes">Correspondence to andrew.zamai@u-bordeaux.fr
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> CNRS
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> Bordeaux INP
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> LaBRI
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> UMR 5800
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> F-33400 Talence
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname"> France
<br class="ltx_break"/>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation">
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id9.id1">The differential diagnosis of neurodegenerative dementias is a challenging clinical task, mainly because of the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging. To improve diagnostic efficiency and accuracy, deep learning–based methods such as Convolutional Neural Networks and Vision Transformers have been proposed for the automatic classification of brain MRIs. However, despite their strong predictive performance, these models find limited clinical utility due to their opaque decision making.
In this work, we propose a framework that integrates two core components to enhance diagnostic transparency. First, we introduce a modular pipeline for converting 3D T1-weighted brain MRIs into textual radiology reports. Second, we explore the potential of modern Large Language Models (LLMs) to assist clinicians in the differential diagnosis between Frontotemporal dementia subtypes, Alzheimer’s disease, and normal aging based on the generated reports.
To bridge the gap between predictive accuracy and explainability, we employ reinforcement learning to incentivize diagnostic reasoning in LLMs. Without requiring supervised reasoning traces or distillation from larger models, our approach enables the emergence of structured diagnostic rationales grounded in neuroimaging findings. Unlike post-hoc explainability methods that retrospectively justify model decisions, our framework generates diagnostic rationales as part of the inference process—producing causally grounded explanations that inform and guide the model’s decision-making process. In doing so, our framework matches the diagnostic performance of existing deep learning methods while offering rationales that support its diagnostic conclusions.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Neurodegenerative dementias denote a group of disorders characterized by progressive loss of neuronal structure and function, resulting in cognitive, motor, and behavioral impairments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib1" title="">1</a>]</cite>. These conditions typically develop insidiously and worsen over time <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib2" title="">2</a>]</cite>. Early and accurate diagnosis is therefore critical to slow disease progression and improve patients’ quality of life. However, the differential diagnosis of neurodegenerative dementias—particularly between Alzheimer’s disease (AD), subtypes of Frontotemporal Dementia (FTD), and cognitively normal aging (CN)—remains an open clinical challenge, due to the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib4" title="">4</a>]</cite>.
In this work, following <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib5" title="">5</a>]</cite>, we focus on structural Magnetic Resonance Imaging (MRI) due to its widespread availability, non-invasive nature, and ability to detect region-specific patterns of cerebral atrophy that are indicative of neurodegeneration.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Existing deep learning approaches, employing either Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib8" title="">8</a>]</cite> or Vision Transformers (ViTs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib10" title="">10</a>]</cite>, have demonstrated strong performance distinguishing between AD, FTD, and healthy controls from 3D MRI scans. However, despite advances in diagnostic performance, a significant limitation lies in their limited interpretability—the ability to understand the internal mechanics of the model—and their insufficient explainability, that is the capacity to provide human-understandable justifications, or rationales, that clarify why a specific prediction was made.
Post-hoc explainability methods have been applied to medical imaging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib13" title="">13</a>]</cite>.
However, such tools often only indicate <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">where</span> the model focused, not <span class="ltx_text ltx_font_italic" id="S1.p2.1.2">why</span> it reached a particular diagnosis. Methods like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib13" title="">13</a>]</cite> generate visual heatmaps over MRI scans to indicate which regions influenced the model’s prediction, but they lack semantic attribution—they neither explicitly identify the anatomical regions (e.g., "the hippocampus") nor explain their clinical relevance (e.g., how hippocampal atrophy is indicative of Alzheimer’s disease). Furthermore, these methods operate post hoc and play no role in the model’s decision-making process, thereby failing to provide causally grounded justifications.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="126" id="S1.F1.g1" src="extracted/6479989/pipeline.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Overview of the proposed framework for the automated differential diagnosis of neurodegenerative dementias. 3D T1-weighted brain MRIs are converted into radiology reports and used to prompt an LLM for detailed diagnostic reasoning and a final ranked list of candidate diagnoses.
</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Our aim in this work is to leverage reasoning-capable Large Language Models (LLMs) to generate rationales that inform and support the diagnostic process, directly addressing key limitations of prior methods.
We present the first comprehensive framework for detailed diagnostic reasoning based on neuroimaging evidence in the differential diagnosis of neurodegenerative dementias<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>The framework will be available soon at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.volbrain.net/" title="">https://www.volbrain.net/</a>.</span></span></span>. While vision-to-text models in the medical domain exist <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib14" title="">14</a>]</cite>, they typically require massive datasets for training and often fail to capture the fine-grained anatomical details critical for neurological differential diagnosis.
To address this, we introduce a framework that integrates high-resolution segmentation and statistical analysis with the text-based reasoning strengths of LLMs, as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Synthetic radiology report generation from neuroimaging.</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">To enable diagnostic reasoning grounded in neuroimaging evidence, we develop a modular pipeline that transforms T1-weighted 3D MRI scans into textual radiology reports. These reports capture clinically relevant features such as spatial atrophy patterns and anatomical asymmetries, expressed in semantically meaningful terms (e.g., hippocampal atrophy). Unlike end-to-end deep learning solutions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib17" title="">17</a>]</cite>, our approach allows each intermediate output to be verified and adjusted for clinical fidelity. We then employ a prompting strategy that guides LLMs to perform differential diagnosis based on these synthetic reports.</p>
</div>
</section>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Reinforcement learning–optimized reasoning.</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p1.1">To address the lack of training data, we apply Group Relative Policy Optimization (GRPO), a reinforcement learning paradigm recently introduced by DeepSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib19" title="">19</a>]</cite>, to fine-tune lightweight open-source LLMs, optimizing them to generate diagnostic rationales based on image findings. In the absence of labeled reasoning traces for supervised training, GRPO enables emergent reasoning capabilities without relying on explicit supervision.</p>
</div>
<div class="ltx_para" id="S1.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S1.SS0.SSS0.Px2.p2.1">Our experiments first evaluate the zero-shot diagnostic capabilities of existing LLMs, providing not only a comparative benchmark of their performance, but also intrinsic validation of our synthetic report generation pipeline. Their zero-shot performance suggests that the generated reports effectively capture clinically relevant information and align with the distributional characteristics of real-world radiology reports likely encountered during pretraining.
Building on this, we apply GRPO-based fine-tuning to lightweight 8B-scale models, enabling them to not only match but often surpass the diagnostic accuracy of much larger models like GPT-4o. Beyond improved diagnostic accuracy, fine-tuned models exhibit detailed and nuanced reasoning grounded in neuroanatomical evidence. Their outputs demonstrate sophisticated behaviors such as hypothesis testing, iterative refinement, and ranked differential diagnoses. Finally, compared to conventional classification-only deep learning solutions, our LLM-based framework achieves competitive diagnostic accuracy while providing transparent, causally grounded rationales that inform and support its diagnostic conclusions.
</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related work</h2>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Neuroimaging-based Diagnosis and Post-hoc Explainability Limitations.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.1">Deep learning models—particularly CNNs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib8" title="">8</a>]</cite> and, more recently, ViTs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib10" title="">10</a>]</cite>—have achieved strong performance in structural MRI-based diagnosis. However, these studies primarily focus on distinguishing cognitively normal individuals, AD, and FTD, without explicitly addressing the more challenging task of differentiating between FTD subtypes.
Post-hoc explainability techniques have been utilized in medical imaging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib13" title="">13</a>]</cite>, but these methods have inherent limitations in this specific context <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib20" title="">20</a>]</cite>. These visualizations typically highlight <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.1">where</span> the model focused, without providing insight into <span class="ltx_text ltx_font_italic" id="S2.SS0.SSS0.Px1.p1.1.2">why</span> a particular diagnosis was made. This is particularly problematic in disorders with overlapping atrophy patterns, where accurate diagnosis depends not just on the presence of atrophy, but on its severity, distribution, and clinical relevance.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">LLMs in medicine.</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.1">LLMs have proven effective in encoding medical knowledge <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib21" title="">21</a>]</cite> and supporting various clinical tasks, including medical question answering <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib22" title="">22</a>]</cite>, discharging summaries generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib24" title="">24</a>]</cite>, electronic health record (EHR) analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib25" title="">25</a>]</cite>, and text-based differential diagnosis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib26" title="">26</a>]</cite>. Domain-adapted models fine-tuned on biomedical corpora—such as PMC-LLaMA, MedAlpaca, BioBERT, and BioGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib30" title="">30</a>]</cite>—along with multimodal architectures (e.g., Med-Flamingo, LLaVA-Med, Gemini)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib32" title="">32</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib33" title="">33</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib34" title="">34</a>]</cite>, are increasingly capable of assisting clinical decision-making tasks.
LLMs have shown promise in clinical reasoning and explainability. For instance, Savage et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib35" title="">35</a>]</cite> recently demonstrated that GPT-4 can be prompted to produce structured, step-by-step diagnostic reasoning. This approach, along with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib37" title="">37</a>]</cite>, offers physicians a way to assess the plausibility and trustworthiness of LLM-generated predictions.
Recent work by DeepSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib18" title="">18</a>]</cite> introduced Group Relative Policy Optimization (GRPO), a reinforcement learning method that has powered a new family of LLMs with emergent reasoning capabilities. GRPO shapes the reward signal without relying on labeled preference data or supervised reasoning traces, estimating a group-wise relative advantage across candidate responses, enabling reasoning behaviors to emerge from diverse prompt-response examples alone. This recent work opens the path to the development of reasoning-powered models in medical domain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib38" title="">38</a>]</cite>. To our knowledge, no prior work has used GRPO to optimize LLMs for generating diagnostic rationales from radiological reports, especially in the context of differential diagnosis of neurodegenerative dementias.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Approach</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the overall architecture of our automated diagnostic framework. The pipeline begins with the processing of a 3D T1-weighted brain MRI using AssemblyNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib39" title="">39</a>]</cite>, which generates detailed anatomical segmentations organized into a quantitative volumetric report. This data is then translated into a qualitative radiology report, which serves as input for the LLM. In this section, we first describe the process of generating these synthetic radiology reports. We then detail the prompting strategy employed to guide the LLM in interpreting these reports effectively. Finally, we describe the training procedure, with a particular focus on the strategies employed to encourage nuanced and detailed rationale generation through reinforcement-optimized reasoning.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We propose a modular pipeline capable of converting a 3D T1-weighted brain MRI into a textual radiology report through a series of interpretable intermediate steps. Unlike end-to-end vision-to-text models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib17" title="">17</a>]</cite>, our approach preserves the clinical detail essential for the diagnosis of neurodegenerative diseases and offers transparency into each intermediate output.
The pipeline consists of four main stages: (1) fine-grained brain segmentation, (2) volume ratio computation of each anatomical structure, (3) atrophy estimation via normative modeling, and (4) textual report generation. We describe each step in detail below.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">(1) MRI segmentation.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">Fine-grained whole-brain segmentation is obtained using AssemblyNet <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib39" title="">39</a>]</cite>, a state-of-the-art deep learning framework designed for high-resolution brain segmentation, employing a multiscale ensemble of 3D U-Net models.
Two assemblies operating at different spatial resolutions enable the model to progressively refine anatomical boundaries and capture detailed structural information. The final output is a detailed segmentation map that includes over 132 brain structures, with specific identification of bilateral elements and detailed left/right segmentation, and a particular focus on cortical, subcortical, and lobar areas—key regions for the diagnosis of neurodegenerative diseases.
</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">(2) Volume ratio computation.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">Volume ratios are computed by first measuring the absolute volume of each anatomical region based on the voxel-wise segmentation output; each regional volume is then normalized by the subject’s total intracranial volume (ICV) to produce a relative volume ratio. This normalization facilitates the comparison of brain structure sizes between subjects with different brain volumes, accounting for inter-subject variability in brain size.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="137" id="S3.F2.g1" src="extracted/6479989/atrophy_estimation.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F2.8.3.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S3.F2.4.2" style="font-size:90%;">Atrophy estimation via normative modeling. <span class="ltx_text ltx_font_bold" id="S3.F2.4.2.1">Left:</span> Lifespan curve of left hippocampal volume ratio with normative mean <math alttext="\mu_{\text{norm}}(a,s)" class="ltx_Math" display="inline" id="S3.F2.3.1.m1.2"><semantics id="S3.F2.3.1.m1.2b"><mrow id="S3.F2.3.1.m1.2.3" xref="S3.F2.3.1.m1.2.3.cmml"><msub id="S3.F2.3.1.m1.2.3.2" xref="S3.F2.3.1.m1.2.3.2.cmml"><mi id="S3.F2.3.1.m1.2.3.2.2" xref="S3.F2.3.1.m1.2.3.2.2.cmml">μ</mi><mtext id="S3.F2.3.1.m1.2.3.2.3" xref="S3.F2.3.1.m1.2.3.2.3a.cmml">norm</mtext></msub><mo id="S3.F2.3.1.m1.2.3.1" xref="S3.F2.3.1.m1.2.3.1.cmml">⁢</mo><mrow id="S3.F2.3.1.m1.2.3.3.2" xref="S3.F2.3.1.m1.2.3.3.1.cmml"><mo id="S3.F2.3.1.m1.2.3.3.2.1" stretchy="false" xref="S3.F2.3.1.m1.2.3.3.1.cmml">(</mo><mi id="S3.F2.3.1.m1.1.1" xref="S3.F2.3.1.m1.1.1.cmml">a</mi><mo id="S3.F2.3.1.m1.2.3.3.2.2" xref="S3.F2.3.1.m1.2.3.3.1.cmml">,</mo><mi id="S3.F2.3.1.m1.2.2" xref="S3.F2.3.1.m1.2.2.cmml">s</mi><mo id="S3.F2.3.1.m1.2.3.3.2.3" stretchy="false" xref="S3.F2.3.1.m1.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.3.1.m1.2c"><apply id="S3.F2.3.1.m1.2.3.cmml" xref="S3.F2.3.1.m1.2.3"><times id="S3.F2.3.1.m1.2.3.1.cmml" xref="S3.F2.3.1.m1.2.3.1"></times><apply id="S3.F2.3.1.m1.2.3.2.cmml" xref="S3.F2.3.1.m1.2.3.2"><csymbol cd="ambiguous" id="S3.F2.3.1.m1.2.3.2.1.cmml" xref="S3.F2.3.1.m1.2.3.2">subscript</csymbol><ci id="S3.F2.3.1.m1.2.3.2.2.cmml" xref="S3.F2.3.1.m1.2.3.2.2">𝜇</ci><ci id="S3.F2.3.1.m1.2.3.2.3a.cmml" xref="S3.F2.3.1.m1.2.3.2.3"><mtext id="S3.F2.3.1.m1.2.3.2.3.cmml" mathsize="70%" xref="S3.F2.3.1.m1.2.3.2.3">norm</mtext></ci></apply><interval closure="open" id="S3.F2.3.1.m1.2.3.3.1.cmml" xref="S3.F2.3.1.m1.2.3.3.2"><ci id="S3.F2.3.1.m1.1.1.cmml" xref="S3.F2.3.1.m1.1.1">𝑎</ci><ci id="S3.F2.3.1.m1.2.2.cmml" xref="S3.F2.3.1.m1.2.2">𝑠</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.1.m1.2d">\mu_{\text{norm}}(a,s)</annotation><annotation encoding="application/x-llamapun" id="S3.F2.3.1.m1.2e">italic_μ start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_a , italic_s )</annotation></semantics></math> (black) and confidence bounds <math alttext="\pm\sigma_{\text{norm}}(a,s)" class="ltx_Math" display="inline" id="S3.F2.4.2.m2.2"><semantics id="S3.F2.4.2.m2.2b"><mrow id="S3.F2.4.2.m2.2.3" xref="S3.F2.4.2.m2.2.3.cmml"><mo id="S3.F2.4.2.m2.2.3b" xref="S3.F2.4.2.m2.2.3.cmml">±</mo><mrow id="S3.F2.4.2.m2.2.3.2" xref="S3.F2.4.2.m2.2.3.2.cmml"><msub id="S3.F2.4.2.m2.2.3.2.2" xref="S3.F2.4.2.m2.2.3.2.2.cmml"><mi id="S3.F2.4.2.m2.2.3.2.2.2" xref="S3.F2.4.2.m2.2.3.2.2.2.cmml">σ</mi><mtext id="S3.F2.4.2.m2.2.3.2.2.3" xref="S3.F2.4.2.m2.2.3.2.2.3a.cmml">norm</mtext></msub><mo id="S3.F2.4.2.m2.2.3.2.1" xref="S3.F2.4.2.m2.2.3.2.1.cmml">⁢</mo><mrow id="S3.F2.4.2.m2.2.3.2.3.2" xref="S3.F2.4.2.m2.2.3.2.3.1.cmml"><mo id="S3.F2.4.2.m2.2.3.2.3.2.1" stretchy="false" xref="S3.F2.4.2.m2.2.3.2.3.1.cmml">(</mo><mi id="S3.F2.4.2.m2.1.1" xref="S3.F2.4.2.m2.1.1.cmml">a</mi><mo id="S3.F2.4.2.m2.2.3.2.3.2.2" xref="S3.F2.4.2.m2.2.3.2.3.1.cmml">,</mo><mi id="S3.F2.4.2.m2.2.2" xref="S3.F2.4.2.m2.2.2.cmml">s</mi><mo id="S3.F2.4.2.m2.2.3.2.3.2.3" stretchy="false" xref="S3.F2.4.2.m2.2.3.2.3.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.F2.4.2.m2.2c"><apply id="S3.F2.4.2.m2.2.3.cmml" xref="S3.F2.4.2.m2.2.3"><csymbol cd="latexml" id="S3.F2.4.2.m2.2.3.1.cmml" xref="S3.F2.4.2.m2.2.3">plus-or-minus</csymbol><apply id="S3.F2.4.2.m2.2.3.2.cmml" xref="S3.F2.4.2.m2.2.3.2"><times id="S3.F2.4.2.m2.2.3.2.1.cmml" xref="S3.F2.4.2.m2.2.3.2.1"></times><apply id="S3.F2.4.2.m2.2.3.2.2.cmml" xref="S3.F2.4.2.m2.2.3.2.2"><csymbol cd="ambiguous" id="S3.F2.4.2.m2.2.3.2.2.1.cmml" xref="S3.F2.4.2.m2.2.3.2.2">subscript</csymbol><ci id="S3.F2.4.2.m2.2.3.2.2.2.cmml" xref="S3.F2.4.2.m2.2.3.2.2.2">𝜎</ci><ci id="S3.F2.4.2.m2.2.3.2.2.3a.cmml" xref="S3.F2.4.2.m2.2.3.2.2.3"><mtext id="S3.F2.4.2.m2.2.3.2.2.3.cmml" mathsize="70%" xref="S3.F2.4.2.m2.2.3.2.2.3">norm</mtext></ci></apply><interval closure="open" id="S3.F2.4.2.m2.2.3.2.3.1.cmml" xref="S3.F2.4.2.m2.2.3.2.3.2"><ci id="S3.F2.4.2.m2.1.1.cmml" xref="S3.F2.4.2.m2.1.1">𝑎</ci><ci id="S3.F2.4.2.m2.2.2.cmml" xref="S3.F2.4.2.m2.2.2">𝑠</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.2.m2.2d">\pm\sigma_{\text{norm}}(a,s)</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.2.m2.2e">± italic_σ start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_a , italic_s )</annotation></semantics></math> (blue/red). <span class="ltx_text ltx_font_bold" id="S3.F2.4.2.2">Right:</span> SDS distributions across diagnostic groups, reflecting condition-specific structural deviations.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">(3) Atrophy Estimation via Normative Modeling.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.6">To assess the clinical relevance of volumetric changes in brain structures, our pipeline estimates structural atrophy by comparing an individual’s measured brain volume ratios to normative models that account for age and sex differences. These normative trajectories are derived from the lifespan analysis conducted in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib40" title="">40</a>]</cite>, which provides robust, data-driven volumetric reference curves based on 2,944 high-quality T1-weighted MRI scans from healthy individuals aged 9 months to 94 years.
As depicted in Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.F2" title="Figure 2 ‣ (2) Volume ratio computation. ‣ 3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">2</span></a>, for each brain structure, we compute its Structural Deviation Score (SDS) as <math alttext="\frac{r_{\text{subject}}-\mu_{\text{norm}}(a,s)}{\sigma_{\text{norm}}(a,s)}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.1.m1.4"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.4a"><mfrac id="S3.SS1.SSS0.Px3.p1.1.m1.4.4" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.cmml"><mrow id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.cmml"><msub id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.cmml"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.2.cmml">r</mi><mtext id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.3a.cmml">subject</mtext></msub><mo id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.3.cmml">−</mo><mrow id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.cmml"><msub id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.2.cmml">μ</mi><mtext id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.3a.cmml">norm</mtext></msub><mo id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.1.cmml">⁢</mo><mrow id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.1.cmml"><mo id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.2.1" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.cmml">a</mi><mo id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.2.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.cmml">s</mi><mo id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.2.3" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.1.cmml">)</mo></mrow></mrow></mrow><mrow id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.cmml"><msub id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.cmml"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.2.cmml">σ</mi><mtext id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.3a.cmml">norm</mtext></msub><mo id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.3.cmml">⁢</mo><mrow id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.1.cmml"><mo id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.2.1" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px3.p1.1.m1.3.3.3.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.3.3.3.1.cmml">a</mi><mo id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.2.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.2.cmml">s</mi><mo id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.2.3" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.1.cmml">)</mo></mrow></mrow></mfrac><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.4b"><apply id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4"><divide id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.5.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4"></divide><apply id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2"><minus id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.3.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.3"></minus><apply id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.2">𝑟</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.3a.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.3"><mtext id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.3.cmml" mathsize="50%" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.4.3">subject</mtext></ci></apply><apply id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5"><times id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.1"></times><apply id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.2">𝜇</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.3a.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.3"><mtext id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.3.cmml" mathsize="50%" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.2.3">norm</mtext></ci></apply><interval closure="open" id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.5.3.2"><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.1">𝑎</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.2.2.2.2">𝑠</ci></interval></apply></apply><apply id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4"><times id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.3.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.3"></times><apply id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.2">𝜎</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.3a.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.3"><mtext id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.3.cmml" mathsize="50%" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.4.3">norm</mtext></ci></apply><interval closure="open" id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.5.2"><ci id="S3.SS1.SSS0.Px3.p1.1.m1.3.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.3.3.3.1">𝑎</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.4.4.4.2">𝑠</ci></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.4c">\frac{r_{\text{subject}}-\mu_{\text{norm}}(a,s)}{\sigma_{\text{norm}}(a,s)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.1.m1.4d">divide start_ARG italic_r start_POSTSUBSCRIPT subject end_POSTSUBSCRIPT - italic_μ start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_a , italic_s ) end_ARG start_ARG italic_σ start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_a , italic_s ) end_ARG</annotation></semantics></math>, where <math alttext="r_{\text{subject}}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px3.p1.2.m2.1a"><msub id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml">r</mi><mtext id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3a.cmml">subject</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.2">𝑟</ci><ci id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3a.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3"><mtext id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.3">subject</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.1c">r_{\text{subject}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.2.m2.1d">italic_r start_POSTSUBSCRIPT subject end_POSTSUBSCRIPT</annotation></semantics></math> is the subject’s measured volume ratio, <math alttext="\mu_{\text{norm}}(a,s)" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.3.m3.2"><semantics id="S3.SS1.SSS0.Px3.p1.3.m3.2a"><mrow id="S3.SS1.SSS0.Px3.p1.3.m3.2.3" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.cmml"><msub id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.2" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.2.cmml">μ</mi><mtext id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.3" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.3a.cmml">norm</mtext></msub><mo id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.1" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.1.cmml">⁢</mo><mrow id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.2" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.1.cmml"><mo id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.2.1" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml">a</mi><mo id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.2.2" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.3.m3.2.2" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.2.cmml">s</mi><mo id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.2.3" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.3.m3.2b"><apply id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3"><times id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.1"></times><apply id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.2">𝜇</ci><ci id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.3a.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.3"><mtext id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.3.cmml" mathsize="70%" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.2.3">norm</mtext></ci></apply><interval closure="open" id="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.3.3.2"><ci id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1">𝑎</ci><ci id="S3.SS1.SSS0.Px3.p1.3.m3.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.2.2">𝑠</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.3.m3.2c">\mu_{\text{norm}}(a,s)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.3.m3.2d">italic_μ start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_a , italic_s )</annotation></semantics></math> is the expected normative volume ratio for the subject’s age <math alttext="a" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.4.m4.1"><semantics id="S3.SS1.SSS0.Px3.p1.4.m4.1a"><mi id="S3.SS1.SSS0.Px3.p1.4.m4.1.1" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1.cmml">a</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.4.m4.1b"><ci id="S3.SS1.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.4.m4.1.1">𝑎</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.4.m4.1c">a</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.4.m4.1d">italic_a</annotation></semantics></math> and sex <math alttext="s" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.5.m5.1"><semantics id="S3.SS1.SSS0.Px3.p1.5.m5.1a"><mi id="S3.SS1.SSS0.Px3.p1.5.m5.1.1" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.5.m5.1b"><ci id="S3.SS1.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.5.m5.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.5.m5.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.5.m5.1d">italic_s</annotation></semantics></math>, and <math alttext="\sigma_{\text{norm}}(a,s)" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.6.m6.2"><semantics id="S3.SS1.SSS0.Px3.p1.6.m6.2a"><mrow id="S3.SS1.SSS0.Px3.p1.6.m6.2.3" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.cmml"><msub id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.cmml"><mi id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.2.cmml">σ</mi><mtext id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.3" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.3a.cmml">norm</mtext></msub><mo id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.1" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.1.cmml">⁢</mo><mrow id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.1.cmml"><mo id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.2.1" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.1.cmml">(</mo><mi id="S3.SS1.SSS0.Px3.p1.6.m6.1.1" xref="S3.SS1.SSS0.Px3.p1.6.m6.1.1.cmml">a</mi><mo id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.2.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.1.cmml">,</mo><mi id="S3.SS1.SSS0.Px3.p1.6.m6.2.2" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2.cmml">s</mi><mo id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.2.3" stretchy="false" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.6.m6.2b"><apply id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3"><times id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.1"></times><apply id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.2">𝜎</ci><ci id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.3a.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.3"><mtext id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.3.cmml" mathsize="70%" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.2.3">norm</mtext></ci></apply><interval closure="open" id="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.3.3.2"><ci id="S3.SS1.SSS0.Px3.p1.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.1.1">𝑎</ci><ci id="S3.SS1.SSS0.Px3.p1.6.m6.2.2.cmml" xref="S3.SS1.SSS0.Px3.p1.6.m6.2.2">𝑠</ci></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.6.m6.2c">\sigma_{\text{norm}}(a,s)</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.6.m6.2d">italic_σ start_POSTSUBSCRIPT norm end_POSTSUBSCRIPT ( italic_a , italic_s )</annotation></semantics></math> is the corresponding standard deviation from the normative distribution’s 95% confidence interval. This computation provides a measure of how many standard deviations the subject’s volume ratio deviates from the expected value, where negative SDS indicate smaller-than-expected volumes (atrophy) and positive SDS suggest larger-than-expected volumes (enlargements). As suggested on the right of Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.F2" title="Figure 2 ‣ (2) Volume ratio computation. ‣ 3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">2</span></a>, the magnitude of the score is key for categorizing the severity of these deviations, as it helps distinguish between neurodegenerative diseases with overlapping affected structures.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">(4) Radiology Report generation.</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px4.p1.1">The conversion from quantitative volumetric measures to clinically interpretable qualitative descriptions represents a critical component of our pipeline. While the above SDS score provides standardized measurements of deviation from normative reference trajectories, clinicians typically rely on categorical severity assessments—such as mild, moderate, or severe atrophy.
Our pipeline’s final stage translates these SDS scores into a descriptive report using a mapping consisting of a seven-point severity scale ranging from <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px4.p1.1.1">normal</span> to <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px4.p1.1.2">severe</span>, with intermediate gradations (e.g. <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px4.p1.1.3">normal-to-mild</span>). As illustrated on the left of Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.F3" title="Figure 3 ‣ (4) Radiology Report generation. ‣ 3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">3</span></a>, the severity thresholds can be visualized on a Gaussian distribution representing the normative population data, with vertical demarcation lines indicating the boundaries between severity categories.
The central region of the curve represents volumes within normal limits, while progressively leftward regions correspond to increasing degrees of atrophy severity. Conversely, the right tail of the distribution represents structural enlargement or hypertrophy, which may be particularly relevant for ventricular assessment. This mapping provides an intuitive severity assessments while preserving some of the granularity necessary for differential diagnosis of neurodegenerative conditions with overlapping atrophy patterns.
Thresholds were chosen based on the statistical meaning of SDS magnitudes, rather than tuning to specific dataset distributions, to prevent overfitting and maintain interpretability. Preliminary zero-shot tests with existing LLMs showed promising results, supporting our decision to retain general-purpose, interpretable thresholds.
The radiology report itself is hierarchically structured, grouping findings by anatomical domain (cortical, subcortical, ventricular). Within each group, regions are sorted by severity, and cortical findings are further differentiated between diffuse lobar atrophy and focal subregional losses—important cues for differential diagnosis. For bilateral structures, we assess both overall and asymmetric volume changes, explicitly noting hemisphere-specific atrophy when present, which is especially relevant for syndromes with lateralized presentations.
By standardizing atrophy descriptions across brain regions using consistent severity terminology, the system enhances clinical communication and supports diagnostic reasoning. In Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A1" title="Appendix A Example of a Complete Synthetic Report and Human Comparison ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">A</span></a>, we provide a full synthetic report alongside a comparison with a human-generated one.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="161" id="S3.F3.g1" src="extracted/6479989/report.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.4.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.5.2" style="font-size:90%;">Mapping structural deviation to qualitative severity. <span class="ltx_text ltx_font_bold" id="S3.F3.5.2.1">Left:</span> Quantitative-to-qualitative conversion of Structural Deviation Scores (SDS) using a seven-point severity scale ranging from severe atrophy to severe enlargement. <span class="ltx_text ltx_font_bold" id="S3.F3.5.2.2">Right:</span> Example of a (truncated) generated radiology report summarizing anatomical findings by region and hemisphere, using standardized severity descriptors.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Prompting strategy</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.F4" title="Figure 4 ‣ 3.2 Prompting strategy ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">4</span></a> illustrates the adopted prompting strategy, designed to elicit an open-ended and thorough diagnostic reasoning based on the neuroimaging findings. The model is instructed to act as a neurologist with expertise in neurodegenerative diseases, tasked with interpreting T1-weighted MRI radiology reports. To encourage deeper engagement with the imaging features, the prompt explicitly requires the model to think exhaustively within <span class="ltx_text ltx_font_typewriter" id="S3.SS2.p1.1.1">&lt;think&gt;</span> tags before committing to a final diagnosis. This intermediate reasoning step encourages detailed examination of regional atrophy patterns, asymmetries, and structural deviations described in the report.
Finally, the output is structured as a ranked list of differential diagnoses. This format reflects the clinical reasoning process where multiple possibilities are considered and prioritized based on their fit to the observed data.
</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text" id="S3.SS2.p2.1.1" style="color:#000000;">To enhance diagnostic stability and consensus at inference, we employ a dual-sampling strategy. First, multiple linguistically varied radiology reports are generated for each brain MRI using sentence templates. Second, the model produces multiple diagnostic predictions for each report via non-deterministic sampling. This approach captures a wider range of interpretations, reducing sensitivity to report phrasing and mitigating LLM inference stochasticity. Final diagnoses are determined by majority vote on the top-ranked differential diagnosis from all aggregated samples, with a supporting reasoning randomly selected from those aligning with the consensus.</span></p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="321" id="S3.F4.g1" src="extracted/6479989/prompt_template.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Prompt used to elicit open-ended diagnostic reasoning from MRI reports, ending in a ranked list of differential diagnoses.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Incentivizing diagnostic reasoning with GRPO</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.16">In the absence of labeled reasoning traces for supervised training (SFT), GRPO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib18" title="">18</a>]</cite> has been shown to foster emergent reasoning capabilities without relying on explicit supervision or distillation from larger teacher models.
Chosen an LLM as our policy model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><msub id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.p1.1.m1.1.1.2" xref="S3.SS3.p1.1.m1.1.1.2.cmml">π</mi><mi id="S3.SS3.p1.1.m1.1.1.3" xref="S3.SS3.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><apply id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.p1.1.m1.1.1.2">𝜋</ci><ci id="S3.SS3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\pi_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, where <math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><mi id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml">θ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><ci id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">𝜃</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">italic_θ</annotation></semantics></math> represents its trainable parameters, and a training dataset consisting of tuples <math alttext="(r,d)" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.2"><semantics id="S3.SS3.p1.3.m3.2a"><mrow id="S3.SS3.p1.3.m3.2.3.2" xref="S3.SS3.p1.3.m3.2.3.1.cmml"><mo id="S3.SS3.p1.3.m3.2.3.2.1" stretchy="false" xref="S3.SS3.p1.3.m3.2.3.1.cmml">(</mo><mi id="S3.SS3.p1.3.m3.1.1" xref="S3.SS3.p1.3.m3.1.1.cmml">r</mi><mo id="S3.SS3.p1.3.m3.2.3.2.2" xref="S3.SS3.p1.3.m3.2.3.1.cmml">,</mo><mi id="S3.SS3.p1.3.m3.2.2" xref="S3.SS3.p1.3.m3.2.2.cmml">d</mi><mo id="S3.SS3.p1.3.m3.2.3.2.3" stretchy="false" xref="S3.SS3.p1.3.m3.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.2b"><interval closure="open" id="S3.SS3.p1.3.m3.2.3.1.cmml" xref="S3.SS3.p1.3.m3.2.3.2"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">𝑟</ci><ci id="S3.SS3.p1.3.m3.2.2.cmml" xref="S3.SS3.p1.3.m3.2.2">𝑑</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.2c">(r,d)</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.2d">( italic_r , italic_d )</annotation></semantics></math>, each comprising a properly converted MRI radiology report <math alttext="r" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" xref="S3.SS3.p1.4.m4.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">r</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_r</annotation></semantics></math> and a gold diagnosis <math alttext="d" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><mi id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml">d</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><ci id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">𝑑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">d</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">italic_d</annotation></semantics></math>. Each report <math alttext="r" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><mi id="S3.SS3.p1.6.m6.1.1" xref="S3.SS3.p1.6.m6.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">𝑟</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">r</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">italic_r</annotation></semantics></math> is formatted into a prompting query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m7.1"><semantics id="S3.SS3.p1.7.m7.1a"><mi id="S3.SS3.p1.7.m7.1.1" xref="S3.SS3.p1.7.m7.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><ci id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m7.1d">italic_q</annotation></semantics></math>, using the template in Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.F4" title="Figure 4 ‣ 3.2 Prompting strategy ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">4</span></a>.
The goal of GRPO is to optimize <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m8.1"><semantics id="S3.SS3.p1.8.m8.1a"><msub id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml"><mi id="S3.SS3.p1.8.m8.1.1.2" xref="S3.SS3.p1.8.m8.1.1.2.cmml">π</mi><mi id="S3.SS3.p1.8.m8.1.1.3" xref="S3.SS3.p1.8.m8.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.1b"><apply id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.8.m8.1.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS3.p1.8.m8.1.1.2.cmml" xref="S3.SS3.p1.8.m8.1.1.2">𝜋</ci><ci id="S3.SS3.p1.8.m8.1.1.3.cmml" xref="S3.SS3.p1.8.m8.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.1c">\pi_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m8.1d">italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> such that the generated outputs exhibit human-understandable reasoning that end with a diagnostically accurate prediction. To do so, at each training iteration, for a query <math alttext="q" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m9.1"><semantics id="S3.SS3.p1.9.m9.1a"><mi id="S3.SS3.p1.9.m9.1.1" xref="S3.SS3.p1.9.m9.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.1b"><ci id="S3.SS3.p1.9.m9.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.1c">q</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.9.m9.1d">italic_q</annotation></semantics></math> we let the model generate a group of <math alttext="G" class="ltx_Math" display="inline" id="S3.SS3.p1.10.m10.1"><semantics id="S3.SS3.p1.10.m10.1a"><mi id="S3.SS3.p1.10.m10.1.1" xref="S3.SS3.p1.10.m10.1.1.cmml">G</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m10.1b"><ci id="S3.SS3.p1.10.m10.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1">𝐺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m10.1c">G</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.10.m10.1d">italic_G</annotation></semantics></math> candidate outputs <math alttext="\{o_{i}\}_{i=1}^{G}" class="ltx_Math" display="inline" id="S3.SS3.p1.11.m11.1"><semantics id="S3.SS3.p1.11.m11.1a"><msubsup id="S3.SS3.p1.11.m11.1.1" xref="S3.SS3.p1.11.m11.1.1.cmml"><mrow id="S3.SS3.p1.11.m11.1.1.1.1.1" xref="S3.SS3.p1.11.m11.1.1.1.1.2.cmml"><mo id="S3.SS3.p1.11.m11.1.1.1.1.1.2" stretchy="false" xref="S3.SS3.p1.11.m11.1.1.1.1.2.cmml">{</mo><msub id="S3.SS3.p1.11.m11.1.1.1.1.1.1" xref="S3.SS3.p1.11.m11.1.1.1.1.1.1.cmml"><mi id="S3.SS3.p1.11.m11.1.1.1.1.1.1.2" xref="S3.SS3.p1.11.m11.1.1.1.1.1.1.2.cmml">o</mi><mi id="S3.SS3.p1.11.m11.1.1.1.1.1.1.3" xref="S3.SS3.p1.11.m11.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S3.SS3.p1.11.m11.1.1.1.1.1.3" stretchy="false" xref="S3.SS3.p1.11.m11.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S3.SS3.p1.11.m11.1.1.1.3" xref="S3.SS3.p1.11.m11.1.1.1.3.cmml"><mi id="S3.SS3.p1.11.m11.1.1.1.3.2" xref="S3.SS3.p1.11.m11.1.1.1.3.2.cmml">i</mi><mo id="S3.SS3.p1.11.m11.1.1.1.3.1" xref="S3.SS3.p1.11.m11.1.1.1.3.1.cmml">=</mo><mn id="S3.SS3.p1.11.m11.1.1.1.3.3" xref="S3.SS3.p1.11.m11.1.1.1.3.3.cmml">1</mn></mrow><mi id="S3.SS3.p1.11.m11.1.1.3" xref="S3.SS3.p1.11.m11.1.1.3.cmml">G</mi></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m11.1b"><apply id="S3.SS3.p1.11.m11.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m11.1.1.2.cmml" xref="S3.SS3.p1.11.m11.1.1">superscript</csymbol><apply id="S3.SS3.p1.11.m11.1.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m11.1.1.1.2.cmml" xref="S3.SS3.p1.11.m11.1.1">subscript</csymbol><set id="S3.SS3.p1.11.m11.1.1.1.1.2.cmml" xref="S3.SS3.p1.11.m11.1.1.1.1.1"><apply id="S3.SS3.p1.11.m11.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m11.1.1.1.1.1.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.11.m11.1.1.1.1.1.1.2.cmml" xref="S3.SS3.p1.11.m11.1.1.1.1.1.1.2">𝑜</ci><ci id="S3.SS3.p1.11.m11.1.1.1.1.1.1.3.cmml" xref="S3.SS3.p1.11.m11.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S3.SS3.p1.11.m11.1.1.1.3.cmml" xref="S3.SS3.p1.11.m11.1.1.1.3"><eq id="S3.SS3.p1.11.m11.1.1.1.3.1.cmml" xref="S3.SS3.p1.11.m11.1.1.1.3.1"></eq><ci id="S3.SS3.p1.11.m11.1.1.1.3.2.cmml" xref="S3.SS3.p1.11.m11.1.1.1.3.2">𝑖</ci><cn id="S3.SS3.p1.11.m11.1.1.1.3.3.cmml" type="integer" xref="S3.SS3.p1.11.m11.1.1.1.3.3">1</cn></apply></apply><ci id="S3.SS3.p1.11.m11.1.1.3.cmml" xref="S3.SS3.p1.11.m11.1.1.3">𝐺</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m11.1c">\{o_{i}\}_{i=1}^{G}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.11.m11.1d">{ italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_G end_POSTSUPERSCRIPT</annotation></semantics></math> from the current policy <math alttext="\pi_{\theta_{\text{old}}}" class="ltx_Math" display="inline" id="S3.SS3.p1.12.m12.1"><semantics id="S3.SS3.p1.12.m12.1a"><msub id="S3.SS3.p1.12.m12.1.1" xref="S3.SS3.p1.12.m12.1.1.cmml"><mi id="S3.SS3.p1.12.m12.1.1.2" xref="S3.SS3.p1.12.m12.1.1.2.cmml">π</mi><msub id="S3.SS3.p1.12.m12.1.1.3" xref="S3.SS3.p1.12.m12.1.1.3.cmml"><mi id="S3.SS3.p1.12.m12.1.1.3.2" xref="S3.SS3.p1.12.m12.1.1.3.2.cmml">θ</mi><mtext id="S3.SS3.p1.12.m12.1.1.3.3" xref="S3.SS3.p1.12.m12.1.1.3.3a.cmml">old</mtext></msub></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.12.m12.1b"><apply id="S3.SS3.p1.12.m12.1.1.cmml" xref="S3.SS3.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.12.m12.1.1.1.cmml" xref="S3.SS3.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS3.p1.12.m12.1.1.2.cmml" xref="S3.SS3.p1.12.m12.1.1.2">𝜋</ci><apply id="S3.SS3.p1.12.m12.1.1.3.cmml" xref="S3.SS3.p1.12.m12.1.1.3"><csymbol cd="ambiguous" id="S3.SS3.p1.12.m12.1.1.3.1.cmml" xref="S3.SS3.p1.12.m12.1.1.3">subscript</csymbol><ci id="S3.SS3.p1.12.m12.1.1.3.2.cmml" xref="S3.SS3.p1.12.m12.1.1.3.2">𝜃</ci><ci id="S3.SS3.p1.12.m12.1.1.3.3a.cmml" xref="S3.SS3.p1.12.m12.1.1.3.3"><mtext id="S3.SS3.p1.12.m12.1.1.3.3.cmml" mathsize="50%" xref="S3.SS3.p1.12.m12.1.1.3.3">old</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.12.m12.1c">\pi_{\theta_{\text{old}}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.12.m12.1d">italic_π start_POSTSUBSCRIPT italic_θ start_POSTSUBSCRIPT old end_POSTSUBSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, i.e., the model parameters before the update. Each output <math alttext="o_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.13.m13.1"><semantics id="S3.SS3.p1.13.m13.1a"><msub id="S3.SS3.p1.13.m13.1.1" xref="S3.SS3.p1.13.m13.1.1.cmml"><mi id="S3.SS3.p1.13.m13.1.1.2" xref="S3.SS3.p1.13.m13.1.1.2.cmml">o</mi><mi id="S3.SS3.p1.13.m13.1.1.3" xref="S3.SS3.p1.13.m13.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.13.m13.1b"><apply id="S3.SS3.p1.13.m13.1.1.cmml" xref="S3.SS3.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.13.m13.1.1.1.cmml" xref="S3.SS3.p1.13.m13.1.1">subscript</csymbol><ci id="S3.SS3.p1.13.m13.1.1.2.cmml" xref="S3.SS3.p1.13.m13.1.1.2">𝑜</ci><ci id="S3.SS3.p1.13.m13.1.1.3.cmml" xref="S3.SS3.p1.13.m13.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.13.m13.1c">o_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.13.m13.1d">italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> contains (1) a diagnostic reasoning trace enclosed between <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.16.1">&lt;think&gt;</span>…<span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.16.2">&lt;/think&gt;</span> tags, and (2) a ranked differential diagnosis list in <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.16.3">JSON</span> format enclosed in triple backticks <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.16.4">‘‘‘json</span> … <span class="ltx_text ltx_font_typewriter" id="S3.SS3.p1.16.5">‘‘‘</span>. For each output <math alttext="o_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.14.m14.1"><semantics id="S3.SS3.p1.14.m14.1a"><msub id="S3.SS3.p1.14.m14.1.1" xref="S3.SS3.p1.14.m14.1.1.cmml"><mi id="S3.SS3.p1.14.m14.1.1.2" xref="S3.SS3.p1.14.m14.1.1.2.cmml">o</mi><mi id="S3.SS3.p1.14.m14.1.1.3" xref="S3.SS3.p1.14.m14.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.14.m14.1b"><apply id="S3.SS3.p1.14.m14.1.1.cmml" xref="S3.SS3.p1.14.m14.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.14.m14.1.1.1.cmml" xref="S3.SS3.p1.14.m14.1.1">subscript</csymbol><ci id="S3.SS3.p1.14.m14.1.1.2.cmml" xref="S3.SS3.p1.14.m14.1.1.2">𝑜</ci><ci id="S3.SS3.p1.14.m14.1.1.3.cmml" xref="S3.SS3.p1.14.m14.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.14.m14.1c">o_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.14.m14.1d">italic_o start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, GRPO computes a scalar reward <math alttext="r_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.15.m15.1"><semantics id="S3.SS3.p1.15.m15.1a"><msub id="S3.SS3.p1.15.m15.1.1" xref="S3.SS3.p1.15.m15.1.1.cmml"><mi id="S3.SS3.p1.15.m15.1.1.2" xref="S3.SS3.p1.15.m15.1.1.2.cmml">r</mi><mi id="S3.SS3.p1.15.m15.1.1.3" xref="S3.SS3.p1.15.m15.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.15.m15.1b"><apply id="S3.SS3.p1.15.m15.1.1.cmml" xref="S3.SS3.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.15.m15.1.1.1.cmml" xref="S3.SS3.p1.15.m15.1.1">subscript</csymbol><ci id="S3.SS3.p1.15.m15.1.1.2.cmml" xref="S3.SS3.p1.15.m15.1.1.2">𝑟</ci><ci id="S3.SS3.p1.15.m15.1.1.3.cmml" xref="S3.SS3.p1.15.m15.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.15.m15.1c">r_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.15.m15.1d">italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> using a task-specific reward function (detailed below). It then calculates the <span class="ltx_text ltx_font_italic" id="S3.SS3.p1.16.6">group-relative advantage</span> <math alttext="A_{i}" class="ltx_Math" display="inline" id="S3.SS3.p1.16.m16.1"><semantics id="S3.SS3.p1.16.m16.1a"><msub id="S3.SS3.p1.16.m16.1.1" xref="S3.SS3.p1.16.m16.1.1.cmml"><mi id="S3.SS3.p1.16.m16.1.1.2" xref="S3.SS3.p1.16.m16.1.1.2.cmml">A</mi><mi id="S3.SS3.p1.16.m16.1.1.3" xref="S3.SS3.p1.16.m16.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.16.m16.1b"><apply id="S3.SS3.p1.16.m16.1.1.cmml" xref="S3.SS3.p1.16.m16.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.16.m16.1.1.1.cmml" xref="S3.SS3.p1.16.m16.1.1">subscript</csymbol><ci id="S3.SS3.p1.16.m16.1.1.2.cmml" xref="S3.SS3.p1.16.m16.1.1.2">𝐴</ci><ci id="S3.SS3.p1.16.m16.1.1.3.cmml" xref="S3.SS3.p1.16.m16.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.16.m16.1c">A_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.16.m16.1d">italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> for each output as follows:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="A_{i}=\frac{r_{i}-\mu}{\sigma},\quad\text{where}\quad\mu=\text{mean}(\{r_{1},r%
_{2},\dots,r_{G}\}),\quad\sigma=\text{std}(\{r_{1},r_{2},\dots,r_{G}\})." class="ltx_Math" display="block" id="S3.Ex1.m1.5"><semantics id="S3.Ex1.m1.5a"><mrow id="S3.Ex1.m1.5.5.1"><mrow id="S3.Ex1.m1.5.5.1.1.2" xref="S3.Ex1.m1.5.5.1.1.3.cmml"><mrow id="S3.Ex1.m1.5.5.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.cmml"><msub id="S3.Ex1.m1.5.5.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.cmml"><mi id="S3.Ex1.m1.5.5.1.1.1.1.2.2" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2.cmml">A</mi><mi id="S3.Ex1.m1.5.5.1.1.1.1.2.3" xref="S3.Ex1.m1.5.5.1.1.1.1.2.3.cmml">i</mi></msub><mo id="S3.Ex1.m1.5.5.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.1.1.1.cmml">=</mo><mrow id="S3.Ex1.m1.5.5.1.1.1.1.3.2" xref="S3.Ex1.m1.5.5.1.1.1.1.3.1.cmml"><mfrac id="S3.Ex1.m1.3.3" xref="S3.Ex1.m1.3.3.cmml"><mrow id="S3.Ex1.m1.3.3.2" xref="S3.Ex1.m1.3.3.2.cmml"><msub id="S3.Ex1.m1.3.3.2.2" xref="S3.Ex1.m1.3.3.2.2.cmml"><mi id="S3.Ex1.m1.3.3.2.2.2" xref="S3.Ex1.m1.3.3.2.2.2.cmml">r</mi><mi id="S3.Ex1.m1.3.3.2.2.3" xref="S3.Ex1.m1.3.3.2.2.3.cmml">i</mi></msub><mo id="S3.Ex1.m1.3.3.2.1" xref="S3.Ex1.m1.3.3.2.1.cmml">−</mo><mi id="S3.Ex1.m1.3.3.2.3" xref="S3.Ex1.m1.3.3.2.3.cmml">μ</mi></mrow><mi id="S3.Ex1.m1.3.3.3" xref="S3.Ex1.m1.3.3.3.cmml">σ</mi></mfrac><mo id="S3.Ex1.m1.5.5.1.1.1.1.3.2.1" rspace="1.167em" xref="S3.Ex1.m1.5.5.1.1.1.1.3.1.cmml">,</mo><mtext id="S3.Ex1.m1.4.4" xref="S3.Ex1.m1.4.4a.cmml">where</mtext></mrow></mrow><mspace id="S3.Ex1.m1.5.5.1.1.2.3" width="1em" xref="S3.Ex1.m1.5.5.1.1.3a.cmml"></mspace><mrow id="S3.Ex1.m1.5.5.1.1.2.2.2" xref="S3.Ex1.m1.5.5.1.1.2.2.3.cmml"><mrow id="S3.Ex1.m1.5.5.1.1.2.2.1.1" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.cmml"><mi id="S3.Ex1.m1.5.5.1.1.2.2.1.1.3" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.3.cmml">μ</mi><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.2" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.2.cmml">=</mo><mrow id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.cmml"><mtext id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.3a.cmml">mean</mtext><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.cmml"><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.cmml">(</mo><mrow id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.4.cmml"><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.4" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.4.cmml">{</mo><msub id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.2.cmml">r</mi><mn id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.5" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.cmml"><mi id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.2" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.2.cmml">r</mi><mn id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.3" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.6" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.4.cmml">,</mo><mi id="S3.Ex1.m1.1.1" mathvariant="normal" xref="S3.Ex1.m1.1.1.cmml">…</mi><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.7" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.4.cmml">,</mo><msub id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.cmml"><mi id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.2" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.2.cmml">r</mi><mi id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.3" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.3.cmml">G</mi></msub><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.8" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.4.cmml">}</mo></mrow><mo id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.cmml">)</mo></mrow></mrow></mrow><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.3" rspace="1.167em" xref="S3.Ex1.m1.5.5.1.1.2.2.3a.cmml">,</mo><mrow id="S3.Ex1.m1.5.5.1.1.2.2.2.2" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.cmml"><mi id="S3.Ex1.m1.5.5.1.1.2.2.2.2.3" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.3.cmml">σ</mi><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.2" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.2.cmml">=</mo><mrow id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.cmml"><mtext id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.3" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.3a.cmml">std</mtext><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.2" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.2.cmml">⁢</mo><mrow id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.cmml"><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.2" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.cmml">(</mo><mrow id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.4.cmml"><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.4" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.4.cmml">{</mo><msub id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.cmml"><mi id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.2" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.2.cmml">r</mi><mn id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.3" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.5" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.4.cmml">,</mo><msub id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.cmml"><mi id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.2" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.2.cmml">r</mi><mn id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.3" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.3.cmml">2</mn></msub><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.6" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.4.cmml">,</mo><mi id="S3.Ex1.m1.2.2" mathvariant="normal" xref="S3.Ex1.m1.2.2.cmml">…</mi><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.7" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.4.cmml">,</mo><msub id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.cmml"><mi id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.2" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.2.cmml">r</mi><mi id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.3" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.3.cmml">G</mi></msub><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.8" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.4.cmml">}</mo></mrow><mo id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.3" stretchy="false" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.cmml">)</mo></mrow></mrow></mrow></mrow></mrow><mo id="S3.Ex1.m1.5.5.1.2" lspace="0em">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.Ex1.m1.5b"><apply id="S3.Ex1.m1.5.5.1.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.3a.cmml" xref="S3.Ex1.m1.5.5.1.1.2.3">formulae-sequence</csymbol><apply id="S3.Ex1.m1.5.5.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1"><eq id="S3.Ex1.m1.5.5.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.1"></eq><apply id="S3.Ex1.m1.5.5.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.1.1.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.2">𝐴</ci><ci id="S3.Ex1.m1.5.5.1.1.1.1.2.3.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.2.3">𝑖</ci></apply><list id="S3.Ex1.m1.5.5.1.1.1.1.3.1.cmml" xref="S3.Ex1.m1.5.5.1.1.1.1.3.2"><apply id="S3.Ex1.m1.3.3.cmml" xref="S3.Ex1.m1.3.3"><divide id="S3.Ex1.m1.3.3.1.cmml" xref="S3.Ex1.m1.3.3"></divide><apply id="S3.Ex1.m1.3.3.2.cmml" xref="S3.Ex1.m1.3.3.2"><minus id="S3.Ex1.m1.3.3.2.1.cmml" xref="S3.Ex1.m1.3.3.2.1"></minus><apply id="S3.Ex1.m1.3.3.2.2.cmml" xref="S3.Ex1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.3.3.2.2.1.cmml" xref="S3.Ex1.m1.3.3.2.2">subscript</csymbol><ci id="S3.Ex1.m1.3.3.2.2.2.cmml" xref="S3.Ex1.m1.3.3.2.2.2">𝑟</ci><ci id="S3.Ex1.m1.3.3.2.2.3.cmml" xref="S3.Ex1.m1.3.3.2.2.3">𝑖</ci></apply><ci id="S3.Ex1.m1.3.3.2.3.cmml" xref="S3.Ex1.m1.3.3.2.3">𝜇</ci></apply><ci id="S3.Ex1.m1.3.3.3.cmml" xref="S3.Ex1.m1.3.3.3">𝜎</ci></apply><ci id="S3.Ex1.m1.4.4a.cmml" xref="S3.Ex1.m1.4.4"><mtext id="S3.Ex1.m1.4.4.cmml" xref="S3.Ex1.m1.4.4">where</mtext></ci></list></apply><apply id="S3.Ex1.m1.5.5.1.1.2.2.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.2.2.3a.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.3">formulae-sequence</csymbol><apply id="S3.Ex1.m1.5.5.1.1.2.2.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1"><eq id="S3.Ex1.m1.5.5.1.1.2.2.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.2"></eq><ci id="S3.Ex1.m1.5.5.1.1.2.2.1.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.3">𝜇</ci><apply id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1"><times id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.2"></times><ci id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.3a.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.3"><mtext id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.3">mean</mtext></ci><set id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.4.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3"><apply id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.2">𝑟</ci><cn id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.2">𝑟</ci><cn id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.3.cmml" type="integer" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.2.2.3">2</cn></apply><ci id="S3.Ex1.m1.1.1.cmml" xref="S3.Ex1.m1.1.1">…</ci><apply id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.2">𝑟</ci><ci id="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.1.1.1.1.1.1.3.3.3">𝐺</ci></apply></set></apply></apply><apply id="S3.Ex1.m1.5.5.1.1.2.2.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2"><eq id="S3.Ex1.m1.5.5.1.1.2.2.2.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.2"></eq><ci id="S3.Ex1.m1.5.5.1.1.2.2.2.2.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.3">𝜎</ci><apply id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1"><times id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.2"></times><ci id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.3a.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.3"><mtext id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.3">std</mtext></ci><set id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.4.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3"><apply id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.2">𝑟</ci><cn id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.1.1.3">1</cn></apply><apply id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.2">𝑟</ci><cn id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.3.cmml" type="integer" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.2.2.3">2</cn></apply><ci id="S3.Ex1.m1.2.2.cmml" xref="S3.Ex1.m1.2.2">…</ci><apply id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.1.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3">subscript</csymbol><ci id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.2.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.2">𝑟</ci><ci id="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.3.cmml" xref="S3.Ex1.m1.5.5.1.1.2.2.2.2.1.1.1.1.3.3.3">𝐺</ci></apply></set></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.Ex1.m1.5c">A_{i}=\frac{r_{i}-\mu}{\sigma},\quad\text{where}\quad\mu=\text{mean}(\{r_{1},r%
_{2},\dots,r_{G}\}),\quad\sigma=\text{std}(\{r_{1},r_{2},\dots,r_{G}\}).</annotation><annotation encoding="application/x-llamapun" id="S3.Ex1.m1.5d">italic_A start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = divide start_ARG italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - italic_μ end_ARG start_ARG italic_σ end_ARG , where italic_μ = mean ( { italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT } ) , italic_σ = std ( { italic_r start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_r start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_r start_POSTSUBSCRIPT italic_G end_POSTSUBSCRIPT } ) .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.2">Completions with above-average rewards receive amplified policy gradient updates, thereby reinforcing desirable behaviors such as coherent diagnostic reasoning and accurate differential ranking.
The model <math alttext="\pi_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.p2.1.m1.1"><semantics id="S3.SS3.p2.1.m1.1a"><msub id="S3.SS3.p2.1.m1.1.1" xref="S3.SS3.p2.1.m1.1.1.cmml"><mi id="S3.SS3.p2.1.m1.1.1.2" xref="S3.SS3.p2.1.m1.1.1.2.cmml">π</mi><mi id="S3.SS3.p2.1.m1.1.1.3" xref="S3.SS3.p2.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.1.m1.1b"><apply id="S3.SS3.p2.1.m1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.1.m1.1.1.1.cmml" xref="S3.SS3.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.p2.1.m1.1.1.2.cmml" xref="S3.SS3.p2.1.m1.1.1.2">𝜋</ci><ci id="S3.SS3.p2.1.m1.1.1.3.cmml" xref="S3.SS3.p2.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.1.m1.1c">\pi_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.1.m1.1d">italic_π start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is updated by maximizing <math alttext="\mathcal{J}_{\text{GRPO}}" class="ltx_Math" display="inline" id="S3.SS3.p2.2.m2.1"><semantics id="S3.SS3.p2.2.m2.1a"><msub id="S3.SS3.p2.2.m2.1.1" xref="S3.SS3.p2.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p2.2.m2.1.1.2" xref="S3.SS3.p2.2.m2.1.1.2.cmml">𝒥</mi><mtext id="S3.SS3.p2.2.m2.1.1.3" xref="S3.SS3.p2.2.m2.1.1.3a.cmml">GRPO</mtext></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p2.2.m2.1b"><apply id="S3.SS3.p2.2.m2.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p2.2.m2.1.1.1.cmml" xref="S3.SS3.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p2.2.m2.1.1.2.cmml" xref="S3.SS3.p2.2.m2.1.1.2">𝒥</ci><ci id="S3.SS3.p2.2.m2.1.1.3a.cmml" xref="S3.SS3.p2.2.m2.1.1.3"><mtext id="S3.SS3.p2.2.m2.1.1.3.cmml" mathsize="70%" xref="S3.SS3.p2.2.m2.1.1.3">GRPO</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p2.2.m2.1c">\mathcal{J}_{\text{GRPO}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p2.2.m2.1d">caligraphic_J start_POSTSUBSCRIPT GRPO end_POSTSUBSCRIPT</annotation></semantics></math>, as further detailed in the original work by DeepSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib18" title="">18</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">We design our task-specific reward function to consist of two terms, a format and accuracy reward.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Format reward.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">Each completion is scored on a 1.0-point scale, composed of four equally weighted components (0.25 points each): (1) the presence of a &lt;think&gt;…&lt;/think&gt; reasoning block followed immediately by a JSON output ensures proper structural delimitation; (2) a single well-formed JSON block enclosed in triple backticks guarantees parseability; (3) the ability to extract the top-ranked diagnosis from the JSON confirms correct output formatting; and (4) the inclusion of all <math alttext="K" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S3.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S3.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px1.p1.1.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px1.p1.1.m1.1c">K</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px1.p1.1.m1.1d">italic_K</annotation></semantics></math> expected diagnostic categories, matched via regular expressions, checks for class coverage and no other diseases considered. If multiple diagnoses are assigned the top rank, the total reward is capped at 0.25 to penalize ambiguity.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Accuracy reward.</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">We assign a binary reward by comparing the model’s top-ranked diagnosis to the ground truth. The predicted diagnosis is extracted from the JSON output, mapped to a class ID using a regex-based scheme, and compared against the gold label. A reward of 1.0 is given for a correct match, and 0.0 otherwise, incentivizing clinically accurate predictions.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experiments</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Our experiments aim to assess: (1) the zero-shot diagnostic performance of existing LLMs in interpreting our synthetic reports—and, conversely, how well these reports align with their pretrained knowledge and typical report distributions; (2) the impact of GRPO fine-tuning on the emergence of diagnostic reasoning; and (3) how our framework compares to established deep learning classifiers trained directly on brain MRI data.</p>
</div>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Experimental setup</h3>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Datasets.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px1.p1.1">Table <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.T1" title="Table 1 ‣ Datasets. ‣ 4.1 Experimental setup ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">1</span></a> summarizes the diagnostic distribution across training, validation, and test sets for all 615 participants included in this study. The data were aggregated from two major sources: the Alzheimer’s Disease Neuroimaging Initiative (ADNI) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib41" title="">41</a>]</cite> and the Neuroimaging Initiative for Frontotemporal Lobar Degeneration (NIFD)<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>Available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ida.loni.usc.edu/" title="">https://ida.loni.usc.edu/</a></span></span></span>. Cases from NIFD included behavioral variant frontotemporal dementia (bvFTD), non-fluent variant primary progressive aphasia (nfvPPA), and semantic variant primary progressive aphasia (svPPA). Alzheimer’s disease (AD) cases were drawn from ADNI, and cognitively normal (CN) controls from both datasets.
<span class="ltx_text" id="S4.SS1.SSS0.Px1.p1.1.1" style="color:#000000;"> Differentiating between FTD subtypes using only structural neuroimaging is challenging, due to the heterogeneity in atrophy patterns within each subtype, the potential for overlapping regions of neurodegeneration across subtypes (e.g., anterior temporal involvement in both svPPA and some bvFTD, or fronto-insular atrophy in both nfvPPA and some bvFTD), particularly in early stages, and the tendency for atrophy to become more widespread with disease progression, further obscuring initial distinctions. Limited samples—particularly for nfvPPA and svPPA—exacerbate these challenges for robust model training and evaluation. To mitigate this, we applied stratified splitting to preserve class distributions and deliberately allocated more data to the test set—at the expense of training set size—in order to retain sufficient samples for reliable evaluation.</span></p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T1.2.1.1" style="font-size:90%;">Table 1</span>: </span><span class="ltx_text" id="S4.T1.3.2" style="font-size:90%;">Diagnostic distribution across splits for all 615 participants included in this study.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.4.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="S4.T1.4.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.1.1">Split</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.2.1">CN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.3.1">AD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.4.1">bvFTD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.5"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.5.1">nfvPPA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T1.4.1.1.6"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.6.1">svPPA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T1.4.1.1.7"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1.7.1">Total</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.4.2.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r ltx_border_t" id="S4.T1.4.2.1.1">Train</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.2.1.2">160</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.2.1.3">75</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.2.1.4">38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.2.1.5">20</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T1.4.2.1.6">17</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T1.4.2.1.7">310</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.3.2">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_r" id="S4.T1.4.3.2.1">Validation</th>
<td class="ltx_td ltx_align_center" id="S4.T1.4.3.2.2">62</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.3.2.3">30</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.3.2.4">14</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.3.2.5">8</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T1.4.3.2.6">8</td>
<td class="ltx_td ltx_align_center" id="S4.T1.4.3.2.7">122</td>
</tr>
<tr class="ltx_tr" id="S4.T1.4.4.3">
<th class="ltx_td ltx_align_right ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="S4.T1.4.4.3.1">Test</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.3.2">94</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.3.3">44</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.3.4">22</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.3.5">12</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T1.4.4.3.6">11</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T1.4.4.3.7">183</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Models and training details.</h4>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p1.1">We evaluate a diverse set of LLMs encompassing a range of model families, parameter scales, and reasoning capabilities. Specifically, we consider the GPT-4o model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib42" title="">42</a>]</cite>, renowned for its strong general-purpose performance, as well ranked highly on the Open Medical LLM Leaderboard <span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard" title="">https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard</a></span></span></span>. From the top of this leaderboard, we also include Llama3-OpenBioLLM-70B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib43" title="">43</a>]</cite>, a domain-specialized model based on the LLaMA-3 70B architecture and fine-tuned for medical tasks.
In addition to these, we assess several open-source generalist models across different model families. From the LLaMA family, we include Llama-3.3-Instruct models at 70B and 8B parameter scales <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib44" title="">44</a>]</cite>. From the Qwen family, we evaluate Qwen-2.5-Instruct-7B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib45" title="">45</a>]</cite> and the recent natively reasoner Qwen-3-8B <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib46" title="">46</a>]</cite>.
Finally, we evaluate reasoning-augmented models trained via GRPO as introduced by DeepSeek <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib18" title="">18</a>]</cite>. Specifically, we consider two distilled variants of these reasoner models aligned to the LLaMA architecture, with parameter sizes of 70B and 8B.</p>
</div>
<div class="ltx_para" id="S4.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS1.SSS0.Px2.p2.7">Resource constraints limited our fine-tuning efforts to the smaller 8B variants. Notably, GRPO training is computationally intensive; our setup utilized four NVIDIA H100 80GB GPUs, allocating three for training and one for completions generation using the vLLM framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib47" title="">47</a>]</cite>.
To enable efficient fine-tuning, we employed Low-Rank Adaptation (LoRA) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib48" title="">48</a>]</cite>, using a rank <math alttext="r=16" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.1.m1.1"><semantics id="S4.SS1.SSS0.Px2.p2.1.m1.1a"><mrow id="S4.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.2" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml">r</mi><mo id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.1" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.3" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.1.m1.1b"><apply id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1"><eq id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.1"></eq><ci id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.2">𝑟</ci><cn id="S4.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p2.1.m1.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.1.m1.1c">r=16</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p2.1.m1.1d">italic_r = 16</annotation></semantics></math> and scaling factor <math alttext="\alpha=32" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.2.m2.1"><semantics id="S4.SS1.SSS0.Px2.p2.2.m2.1a"><mrow id="S4.SS1.SSS0.Px2.p2.2.m2.1.1" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.2" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.2.cmml">α</mi><mo id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.1" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.3" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.3.cmml">32</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.2.m2.1b"><apply id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1"><eq id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.1"></eq><ci id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.2">𝛼</ci><cn id="S4.SS1.SSS0.Px2.p2.2.m2.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p2.2.m2.1.1.3">32</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.2.m2.1c">\alpha=32</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p2.2.m2.1d">italic_α = 32</annotation></semantics></math>, targeting the query, key, and value projection weights in the attention layers.
Specific to GRPO training, we followed established configurations from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib49" title="">49</a>]</cite> and <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib18" title="">18</a>]</cite>. We set the maximum generation length to 3000 tokens and generated <math alttext="G=6" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.3.m3.1"><semantics id="S4.SS1.SSS0.Px2.p2.3.m3.1a"><mrow id="S4.SS1.SSS0.Px2.p2.3.m3.1.1" xref="S4.SS1.SSS0.Px2.p2.3.m3.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p2.3.m3.1.1.2" xref="S4.SS1.SSS0.Px2.p2.3.m3.1.1.2.cmml">G</mi><mo id="S4.SS1.SSS0.Px2.p2.3.m3.1.1.1" xref="S4.SS1.SSS0.Px2.p2.3.m3.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px2.p2.3.m3.1.1.3" xref="S4.SS1.SSS0.Px2.p2.3.m3.1.1.3.cmml">6</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.3.m3.1b"><apply id="S4.SS1.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.3.m3.1.1"><eq id="S4.SS1.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.3.m3.1.1.1"></eq><ci id="S4.SS1.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p2.3.m3.1.1.2">𝐺</ci><cn id="S4.SS1.SSS0.Px2.p2.3.m3.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p2.3.m3.1.1.3">6</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.3.m3.1c">G=6</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p2.3.m3.1d">italic_G = 6</annotation></semantics></math> completions per query, with a sampling temperature of 0.9 to encourage exploration. In accordance with <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib49" title="">49</a>]</cite>, we adopted a max-completion-length averaging scheme for the loss computation, mitigating the bias on completions length.
However, contrary to their recommendation, we retained deviation scaling in the advantage estimation, as removing it consistently degraded training performance in our setting. To ensure stable updates and accommodate a diverse query set across multiple classes, we used a gradient accumulation step size of 64 and a conservative learning rate of <math alttext="5\times 10^{-5}" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.4.m4.1"><semantics id="S4.SS1.SSS0.Px2.p2.4.m4.1a"><mrow id="S4.SS1.SSS0.Px2.p2.4.m4.1.1" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.cmml"><mn id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.2" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.2.cmml">5</mn><mo id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.1" lspace="0.222em" rspace="0.222em" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.1.cmml">×</mo><msup id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.cmml"><mn id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.2" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.2.cmml">10</mn><mrow id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3.cmml"><mo id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3a" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3.cmml">−</mo><mn id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3.2" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.4.m4.1b"><apply id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1"><times id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.1"></times><cn id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.2.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.2">5</cn><apply id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.cmml" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3"><csymbol cd="ambiguous" id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.1.cmml" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3">superscript</csymbol><cn id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.2.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.2">10</cn><apply id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3.cmml" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3"><minus id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3.1.cmml" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3"></minus><cn id="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3.2.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p2.4.m4.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.4.m4.1c">5\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p2.4.m4.1d">5 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math>, preventing abrupt shifts in model behavior.
Finally, we preserved the default GRPO <math alttext="\epsilon" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.5.m5.1"><semantics id="S4.SS1.SSS0.Px2.p2.5.m5.1a"><mi id="S4.SS1.SSS0.Px2.p2.5.m5.1.1" xref="S4.SS1.SSS0.Px2.p2.5.m5.1.1.cmml">ϵ</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.5.m5.1b"><ci id="S4.SS1.SSS0.Px2.p2.5.m5.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.5.m5.1.1">italic-ϵ</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.5.m5.1c">\epsilon</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p2.5.m5.1d">italic_ϵ</annotation></semantics></math> parameter at 0.2, as in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib18" title="">18</a>]</cite>, but found that lowering <math alttext="\beta" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.6.m6.1"><semantics id="S4.SS1.SSS0.Px2.p2.6.m6.1a"><mi id="S4.SS1.SSS0.Px2.p2.6.m6.1.1" xref="S4.SS1.SSS0.Px2.p2.6.m6.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.6.m6.1b"><ci id="S4.SS1.SSS0.Px2.p2.6.m6.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.6.m6.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.6.m6.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p2.6.m6.1d">italic_β</annotation></semantics></math> to 0.005 was crucial. Setting <math alttext="\beta=0" class="ltx_Math" display="inline" id="S4.SS1.SSS0.Px2.p2.7.m7.1"><semantics id="S4.SS1.SSS0.Px2.p2.7.m7.1a"><mrow id="S4.SS1.SSS0.Px2.p2.7.m7.1.1" xref="S4.SS1.SSS0.Px2.p2.7.m7.1.1.cmml"><mi id="S4.SS1.SSS0.Px2.p2.7.m7.1.1.2" xref="S4.SS1.SSS0.Px2.p2.7.m7.1.1.2.cmml">β</mi><mo id="S4.SS1.SSS0.Px2.p2.7.m7.1.1.1" xref="S4.SS1.SSS0.Px2.p2.7.m7.1.1.1.cmml">=</mo><mn id="S4.SS1.SSS0.Px2.p2.7.m7.1.1.3" xref="S4.SS1.SSS0.Px2.p2.7.m7.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.SS1.SSS0.Px2.p2.7.m7.1b"><apply id="S4.SS1.SSS0.Px2.p2.7.m7.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.7.m7.1.1"><eq id="S4.SS1.SSS0.Px2.p2.7.m7.1.1.1.cmml" xref="S4.SS1.SSS0.Px2.p2.7.m7.1.1.1"></eq><ci id="S4.SS1.SSS0.Px2.p2.7.m7.1.1.2.cmml" xref="S4.SS1.SSS0.Px2.p2.7.m7.1.1.2">𝛽</ci><cn id="S4.SS1.SSS0.Px2.p2.7.m7.1.1.3.cmml" type="integer" xref="S4.SS1.SSS0.Px2.p2.7.m7.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.SSS0.Px2.p2.7.m7.1c">\beta=0</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.SSS0.Px2.p2.7.m7.1d">italic_β = 0</annotation></semantics></math> caused the model to produce incoherent outputs and deviate from its pre-trained domain knowledge, whereas larger values excessively suppressed the advantage estimation term, limiting effective learning.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Results</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Off-the-shelf LLMs.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.3">Table <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.T2" title="Table 2 ‣ Off-the-shelf LLMs. ‣ 4.2 Results ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">2</span></a> presents the zero-shot diagnostic performance of the models selected in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS1.SSS0.Px2" title="Models and training details. ‣ 4.1 Experimental setup ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">4.1</span></a>.
Using the inference prompting strategy outlined in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS2" title="3.2 Prompting strategy ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">3.2</span></a>, we generate <math alttext="3" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px1.p1.1.m1.1a"><mn id="S4.SS2.SSS0.Px1.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.1.m1.1b"><cn id="S4.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="S4.SS2.SSS0.Px1.p1.1.m1.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.1.m1.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p1.1.m1.1d">3</annotation></semantics></math> synthetic reports per brain MRI, each followed by <math alttext="3" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px1.p1.2.m2.1a"><mn id="S4.SS2.SSS0.Px1.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">3</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.2.m2.1b"><cn id="S4.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" type="integer" xref="S4.SS2.SSS0.Px1.p1.2.m2.1.1">3</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.2.m2.1c">3</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p1.2.m2.1d">3</annotation></semantics></math> independent diagnostic predictions, yielding a total of <math alttext="9" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px1.p1.3.m3.1"><semantics id="S4.SS2.SSS0.Px1.p1.3.m3.1a"><mn id="S4.SS2.SSS0.Px1.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">9</mn><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px1.p1.3.m3.1b"><cn id="S4.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" type="integer" xref="S4.SS2.SSS0.Px1.p1.3.m3.1.1">9</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px1.p1.3.m3.1c">9</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px1.p1.3.m3.1d">9</annotation></semantics></math> candidate outputs per case. The final diagnosis is obtained via majority voting, promoting a more stable and consensus-driven decision. In terms of M-F1, GPT-4o demonstrates the strongest overall zero-shot performance, confirming to be a top-tier generalist model. Among the open-source 70B models, the DeepSeek-R1-Distill-Llama achieves the best results, highlighting the effectiveness of GRPO-style reasoning also for clinical tasks. Notably, Llama3-OpenBioLLM, despite being domain-specialized, performs worse than its base model, likely due to fine-tuning on radiology reports predominantly associated with pathological cases, introducing a bias toward dementia prediction.
Among the 8B models, DeepSeek-R1-Distill-Llama and LLaMA-3.1-Instruct demonstrate strong zero-shot performance, even surpassing some 70B models, while their 3B variants, lacking sufficient pre-trained domain knowledge at that scale, were excluded for poor performance.
These results not only provide a comparative benchmark of the diagnostic capabilities of each model, but also offer an intrinsic validation of the proposed synthetic report generation pipeline, indicating that the generated reports effectively capture clinically relevant information and exhibit distributional characteristics consistent with real-world radiology reports likely encountered during pre-training.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.3.1.1" style="font-size:90%;">Table 2</span>: </span><span class="ltx_text" id="S4.T2.4.2" style="font-size:90%;">Diagnostic performance of off-the-shelf LLMs and GRPO fine-tuned 8B variants.
Models marked with † are reasoning models or distilled from them.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T2.1" style="width:433.6pt;height:273.5pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.2pt,7.7pt) scale(0.946540865001252,0.946540865001252) ;">
<table class="ltx_tabular ltx_align_middle" id="S4.T2.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1" style="font-size:90%;">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.1.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1" style="font-size:90%;">Params</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" colspan="5" id="S4.T2.1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T2.1.1.1.4.1" style="font-size:90%;">class-wise<span class="ltx_text ltx_font_upright" id="S4.T2.1.1.1.4.1.1"> F1</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.1" style="font-size:90%;">BACC</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T2.1.1.1.1.1" style="font-size:90%;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T2.1.1.1.1.1.1">M</span><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1.2">-F1</span></span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.2.1">
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.1"><span class="ltx_text" id="S4.T2.1.1.2.1.1.1" style="font-size:90%;">CN</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.2"><span class="ltx_text" id="S4.T2.1.1.2.1.2.1" style="font-size:90%;">AD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.3"><span class="ltx_text" id="S4.T2.1.1.2.1.3.1" style="font-size:90%;">bvFTD</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.2.1.4"><span class="ltx_text" id="S4.T2.1.1.2.1.4.1" style="font-size:90%;">nfvPPA</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.2.1.5"><span class="ltx_text" id="S4.T2.1.1.2.1.5.1" style="font-size:90%;">svPPA</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.3.2">
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="9" id="S4.T2.1.1.3.2.1">
<span class="ltx_text" id="S4.T2.1.1.3.2.1.1" style="font-size:90%;">                                  </span><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.3.2.1.2" style="font-size:90%;">Zero-shot</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.4.3">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.1.1.4.3.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.4.3.1.1" style="font-size:90%;">gpt-4o</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4.3.2"><span class="ltx_text" id="S4.T2.1.1.4.3.2.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.3.3"><span class="ltx_text" id="S4.T2.1.1.4.3.3.1" style="font-size:90%;">70.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.3.4"><span class="ltx_text" id="S4.T2.1.1.4.3.4.1" style="font-size:90%;">51.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.3.5"><span class="ltx_text" id="S4.T2.1.1.4.3.5.1" style="font-size:90%;">51.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.3.6"><span class="ltx_text" id="S4.T2.1.1.4.3.6.1" style="font-size:90%;">19.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.4.3.7"><span class="ltx_text" id="S4.T2.1.1.4.3.7.1" style="font-size:90%;">48.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.3.8"><span class="ltx_text" id="S4.T2.1.1.4.3.8.1" style="font-size:90%;">55.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.4.3.9"><span class="ltx_text" id="S4.T2.1.1.4.3.9.1" style="font-size:90%;">48.32</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.5.4">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.1.1.5.4.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.5.4.1.1" style="font-size:90%;">Llama-3.3-Instruct</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5.4.2"><span class="ltx_text" id="S4.T2.1.1.5.4.2.1" style="font-size:90%;">70B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.4.3"><span class="ltx_text" id="S4.T2.1.1.5.4.3.1" style="font-size:90%;">59.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.4.4"><span class="ltx_text" id="S4.T2.1.1.5.4.4.1" style="font-size:90%;">43.37</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.4.5"><span class="ltx_text" id="S4.T2.1.1.5.4.5.1" style="font-size:90%;">44.71</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.4.6"><span class="ltx_text" id="S4.T2.1.1.5.4.6.1" style="font-size:90%;">0.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.5.4.7"><span class="ltx_text" id="S4.T2.1.1.5.4.7.1" style="font-size:90%;">42.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.4.8"><span class="ltx_text" id="S4.T2.1.1.5.4.8.1" style="font-size:90%;">48.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.5.4.9"><span class="ltx_text" id="S4.T2.1.1.5.4.9.1" style="font-size:90%;">37.95</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.6.5">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.1.1.6.5.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.6.5.1.1" style="font-size:90%;">Llama3-OpenBioLLM</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.6.5.2"><span class="ltx_text" id="S4.T2.1.1.6.5.2.1" style="font-size:90%;">70B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.3"><span class="ltx_text" id="S4.T2.1.1.6.5.3.1" style="font-size:90%;">43.55</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.4"><span class="ltx_text" id="S4.T2.1.1.6.5.4.1" style="font-size:90%;">44.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.5"><span class="ltx_text" id="S4.T2.1.1.6.5.5.1" style="font-size:90%;">44.74</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.6"><span class="ltx_text" id="S4.T2.1.1.6.5.6.1" style="font-size:90%;">0.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.6.5.7"><span class="ltx_text" id="S4.T2.1.1.6.5.7.1" style="font-size:90%;">36.36</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.8"><span class="ltx_text" id="S4.T2.1.1.6.5.8.1" style="font-size:90%;">49.38</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.6.5.9"><span class="ltx_text" id="S4.T2.1.1.6.5.9.1" style="font-size:90%;">33.82</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.7.6">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.1.1.7.6.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.7.6.1.1" style="font-size:90%;">DeepSeek-R1-Distill-Llama †</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.7.6.2"><span class="ltx_text" id="S4.T2.1.1.7.6.2.1" style="font-size:90%;">70B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.3"><span class="ltx_text" id="S4.T2.1.1.7.6.3.1" style="font-size:90%;">64.38</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.4"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.7.6.4.1" style="font-size:90%;">59.93</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.5"><span class="ltx_text" id="S4.T2.1.1.7.6.5.1" style="font-size:90%;">48.19</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.6"><span class="ltx_text" id="S4.T2.1.1.7.6.6.1" style="font-size:90%;">0.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.7.6.7"><span class="ltx_text" id="S4.T2.1.1.7.6.7.1" style="font-size:90%;">31.25</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.8"><span class="ltx_text" id="S4.T2.1.1.7.6.8.1" style="font-size:90%;">48.18</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.7.6.9"><span class="ltx_text" id="S4.T2.1.1.7.6.9.1" style="font-size:90%;">39.55</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.8.7">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.1.1.8.7.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.8.7.1.1" style="font-size:90%;">DeepSeek-R1-Distill-Llama †</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.8.7.2"><span class="ltx_text" id="S4.T2.1.1.8.7.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.7.3"><span class="ltx_text" id="S4.T2.1.1.8.7.3.1" style="font-size:90%;">72.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.7.4"><span class="ltx_text" id="S4.T2.1.1.8.7.4.1" style="font-size:90%;">53.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.7.5"><span class="ltx_text" id="S4.T2.1.1.8.7.5.1" style="font-size:90%;">38.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.7.6"><span class="ltx_text" id="S4.T2.1.1.8.7.6.1" style="font-size:90%;">11.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.8.7.7"><span class="ltx_text" id="S4.T2.1.1.8.7.7.1" style="font-size:90%;">25.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.7.8"><span class="ltx_text" id="S4.T2.1.1.8.7.8.1" style="font-size:90%;">42.64</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.8.7.9"><span class="ltx_text" id="S4.T2.1.1.8.7.9.1" style="font-size:90%;">40.09</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.9.8">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.1.1.9.8.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.9.8.1.1" style="font-size:90%;">LlaMA-3.1-Instruct</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.9.8.2"><span class="ltx_text" id="S4.T2.1.1.9.8.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.3"><span class="ltx_text" id="S4.T2.1.1.9.8.3.1" style="font-size:90%;">64.94</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.4"><span class="ltx_text" id="S4.T2.1.1.9.8.4.1" style="font-size:90%;">31.75</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.5"><span class="ltx_text" id="S4.T2.1.1.9.8.5.1" style="font-size:90%;">54.05</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.6"><span class="ltx_text" id="S4.T2.1.1.9.8.6.1" style="font-size:90%;">15.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.9.8.7"><span class="ltx_text" id="S4.T2.1.1.9.8.7.1" style="font-size:90%;">36.73</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.8"><span class="ltx_text" id="S4.T2.1.1.9.8.8.1" style="font-size:90%;">53.06</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.9.8.9"><span class="ltx_text" id="S4.T2.1.1.9.8.9.1" style="font-size:90%;">40.57</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.10.9">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.1.1.10.9.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.10.9.1.1" style="font-size:90%;">Qwen-2.5-Instruct</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.10.9.2"><span class="ltx_text" id="S4.T2.1.1.10.9.2.1" style="font-size:90%;">7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.3"><span class="ltx_text" id="S4.T2.1.1.10.9.3.1" style="font-size:90%;">47.24</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.4"><span class="ltx_text" id="S4.T2.1.1.10.9.4.1" style="font-size:90%;">47.41</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.5"><span class="ltx_text" id="S4.T2.1.1.10.9.5.1" style="font-size:90%;">36.84</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.6"><span class="ltx_text" id="S4.T2.1.1.10.9.6.1" style="font-size:90%;">0.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.10.9.7"><span class="ltx_text" id="S4.T2.1.1.10.9.7.1" style="font-size:90%;">0.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.8"><span class="ltx_text" id="S4.T2.1.1.10.9.8.1" style="font-size:90%;">33.66</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.10.9.9"><span class="ltx_text" id="S4.T2.1.1.10.9.9.1" style="font-size:90%;">26.23</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.11.10">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.1.1.11.10.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.11.10.1.1" style="font-size:90%;">Qwen-3 †</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.11.10.2"><span class="ltx_text" id="S4.T2.1.1.11.10.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.3"><span class="ltx_text" id="S4.T2.1.1.11.10.3.1" style="font-size:90%;">60.56</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.4"><span class="ltx_text" id="S4.T2.1.1.11.10.4.1" style="font-size:90%;">54.95</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.5"><span class="ltx_text" id="S4.T2.1.1.11.10.5.1" style="font-size:90%;">39.13</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.6"><span class="ltx_text" id="S4.T2.1.1.11.10.6.1" style="font-size:90%;">19.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.11.10.7"><span class="ltx_text" id="S4.T2.1.1.11.10.7.1" style="font-size:90%;">10.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.8"><span class="ltx_text" id="S4.T2.1.1.11.10.8.1" style="font-size:90%;">42.03</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.11.10.9"><span class="ltx_text" id="S4.T2.1.1.11.10.9.1" style="font-size:90%;">36.74</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.12.11">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="9" id="S4.T2.1.1.12.11.1">
<span class="ltx_text" id="S4.T2.1.1.12.11.1.1" style="font-size:90%;">                                  </span><span class="ltx_text ltx_font_italic" id="S4.T2.1.1.12.11.1.2" style="font-size:90%;">Our GRPO fine-tuned models</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.13.12">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.1.1.13.12.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.13.12.1.1" style="font-size:90%;">DeepSeek-R1-Distill-Llama GRPO †</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.13.12.2"><span class="ltx_text" id="S4.T2.1.1.13.12.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.12.3"><span class="ltx_text" id="S4.T2.1.1.13.12.3.1" style="font-size:90%;">84.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.12.4"><span class="ltx_text" id="S4.T2.1.1.13.12.4.1" style="font-size:90%;">51.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.12.5"><span class="ltx_text" id="S4.T2.1.1.13.12.5.1" style="font-size:90%;">70.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.12.6"><span class="ltx_text" id="S4.T2.1.1.13.12.6.1" style="font-size:90%;">11.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.1.1.13.12.7"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.13.12.7.1" style="font-size:90%;">80.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.12.8"><span class="ltx_text" id="S4.T2.1.1.13.12.8.1" style="font-size:90%;">62.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.1.13.12.9"><span class="ltx_text" id="S4.T2.1.1.13.12.9.1" style="font-size:90%;">59.55</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.14.13">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.1.1.14.13.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.14.13.1.1" style="font-size:90%;">LlaMA-3.1-Instruct GRPO †</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.14.13.2"><span class="ltx_text" id="S4.T2.1.1.14.13.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.14.13.3"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.14.13.3.1" style="font-size:90%;">85.86</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.14.13.4"><span class="ltx_text" id="S4.T2.1.1.14.13.4.1" style="font-size:90%;">53.33</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.14.13.5"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.14.13.5.1" style="font-size:90%;">73.17</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.14.13.6"><span class="ltx_text" id="S4.T2.1.1.14.13.6.1" style="font-size:90%;">41.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.14.13.7"><span class="ltx_text" id="S4.T2.1.1.14.13.7.1" style="font-size:90%;">71.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.14.13.8"><span class="ltx_text" id="S4.T2.1.1.14.13.8.1" style="font-size:90%;">67.33</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.14.13.9"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.14.13.9.1" style="font-size:90%;">65.09</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.15.14">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.1.1.15.14.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.15.14.1.1" style="font-size:90%;">Qwen-2.5-Instruct GRPO †</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.15.14.2"><span class="ltx_text" id="S4.T2.1.1.15.14.2.1" style="font-size:90%;">7B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.15.14.3"><span class="ltx_text" id="S4.T2.1.1.15.14.3.1" style="font-size:90%;">78.36</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.15.14.4"><span class="ltx_text" id="S4.T2.1.1.15.14.4.1" style="font-size:90%;">49.44</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.15.14.5"><span class="ltx_text" id="S4.T2.1.1.15.14.5.1" style="font-size:90%;">58.06</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.15.14.6"><span class="ltx_text" id="S4.T2.1.1.15.14.6.1" style="font-size:90%;">0.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.1.1.15.14.7"><span class="ltx_text" id="S4.T2.1.1.15.14.7.1" style="font-size:90%;">42.42</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.15.14.8"><span class="ltx_text" id="S4.T2.1.1.15.14.8.1" style="font-size:90%;">52.89</span></td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.1.15.14.9"><span class="ltx_text" id="S4.T2.1.1.15.14.9.1" style="font-size:90%;">45.66</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.1.16.15">
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T2.1.1.16.15.1"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.15.1.1" style="font-size:90%;">Qwen-3 GRPO †</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.1.1.16.15.2"><span class="ltx_text" id="S4.T2.1.1.16.15.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.16.15.3"><span class="ltx_text" id="S4.T2.1.1.16.15.3.1" style="font-size:90%;">83.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.16.15.4"><span class="ltx_text" id="S4.T2.1.1.16.15.4.1" style="font-size:90%;">46.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.16.15.5"><span class="ltx_text" id="S4.T2.1.1.16.15.5.1" style="font-size:90%;">66.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.16.15.6"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.15.6.1" style="font-size:90%;">48.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T2.1.1.16.15.7"><span class="ltx_text" id="S4.T2.1.1.16.15.7.1" style="font-size:90%;">64.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.16.15.8"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.16.15.8.1" style="font-size:90%;">68.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.1.16.15.9"><span class="ltx_text" id="S4.T2.1.1.16.15.9.1" style="font-size:90%;">61.88</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">GRPO Fine-Tuned LLMs.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">We extend Table <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.T2" title="Table 2 ‣ Off-the-shelf LLMs. ‣ 4.2 Results ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">2</span></a> by reporting the diagnostic performance of the 8B models fine-tuned via GRPO, as detailed in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.SS1.SSS0.Px2" title="Models and training details. ‣ 4.1 Experimental setup ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">4.1</span></a>. Remarkably, without any supervised reasoning traces or distillation from larger models, GRPO enables the emergence of detailed, evidence-based diagnostic reasoning that contributes to improved diagnostic accuracy.
Due to the length of the generated outputs, in Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.F5" title="Figure 5 ‣ GRPO Fine-Tuned LLMs. ‣ 4.2 Results ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">5</span></a> we present only excerpts from the DeepSeek-R1-Distill-Llama-8B model. Qualitative analysis of these outputs reveals several key reasoning behaviors. First, the models engage in explicit “hypothesis testing”, systematically evaluating each candidate diagnosis by weighing supporting and opposing imaging features. This promotes balanced consideration across differential diagnoses rather than premature commitment. Second, the models demonstrate “non-linear reasoning”, often revisiting and refining earlier conclusions as additional evidence is considered. Third, responses typically conclude with a ranked list of differential diagnoses, reflecting varying degrees of confidence rather than a single-label decision.
The rationales exhibit a high degree of anatomical specificity, referencing expected neuroanatomical atrophies and capturing distribution patterns that reflect known disease profiles, including severity and asymmetry. Finally, we observe that output length and detail correlate with case complexity. For straightforward cases (e.g., cognitively normal scans or reports with hallmark disease features), the model produces concise justifications. In contrast, challenging cases elicit significantly longer and more elaborate reasoning—sometimes up to three times longer.
Further insights into training dynamics and full diagnostic reasoning examples are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A2" title="Appendix B Insight into GRPO Training Dynamics ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">B</span></a> and Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A3" title="Appendix C Full Diagnostic Reasoning Examples ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">C</span></a>, respectively.</p>
</div>
<figure class="ltx_figure" id="S4.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="147" id="S4.F5.g1" src="extracted/6479989/output_extracts.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S4.F5.3.2" style="font-size:90%;">Excerpts from the DeepSeek-R1-Distill-Llama-8B-GRPO model. The responses exhibit properties such as evidence-based hypothesis testing, non-linear reasoning, and detailed understanding of expected anatomical regions and atrophy severity.</span></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Comparison with classification-only approaches.</h4>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.1">We compare our LLM-based diagnostic framework with existing deep-learning classification methods training directly on brain MRIs. Nguyen <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px3.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib10" title="">10</a>]</cite> demonstrated that 3D transformer-based models achieve strong predictive performance, outperforming previous approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib6" title="">6</a>]</cite>. While these prior works focus on distinguishing between AD, FTD, and CN cases, we extended Nguyen <span class="ltx_text ltx_font_italic" id="S4.SS2.SSS0.Px3.p1.1.2">et al.</span>’s framework to include FTD subtypes.
In line with their setup, we also evaluate a Support Vector Machine (SVM) classifier trained on the Structural Deviation Scores computed as detailed in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS1" title="3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Further implementation and training details are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A4" title="Appendix D Training Details for Classification-Only Baselines ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">D</span></a>.
Table <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S4.T3" title="Table 3 ‣ Comparison with classification-only approaches. ‣ 4.2 Results ‣ 4 Experiments ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">3</span></a> presents the diagnostic performance of the ViT and SVM classifiers, compared with our best-performing zero-shot LLM and best GRPO fine-tuned variants.
<span class="ltx_text" id="S4.SS2.SSS0.Px3.p1.1.3" style="color:#000000;"> While GradCAM-based post hoc visualizations in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib10" title="">10</a>]</cite> offer some interpretability by highlighting image regions that influence model predictions, they lack semantic attribution and clinical contextualization. In contrast, our LLM-based framework achieves comparable diagnostic performance while producing transparent, human-readable rationales that explicitly reference neuroanatomical structures and articulate their relevance to the differential diagnosis.</span></p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.3.1.1" style="font-size:90%;">Table 3</span>: </span><span class="ltx_text" id="S4.T3.4.2" style="font-size:90%;">Diagnostic performance comparison between our LLM-based framework and existing classification-only deep learning approaches.</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S4.T3.1" style="width:433.6pt;height:171.3pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-12.2pt,4.8pt) scale(0.946540865001252,0.946540865001252) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S4.T3.1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.1.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.2" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.2.1" style="font-size:90%;">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.1.1.1.3" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.3.1" style="font-size:90%;">Params</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" colspan="5" id="S4.T3.1.1.1.4"><span class="ltx_text ltx_font_bold ltx_font_italic" id="S4.T3.1.1.1.4.1" style="font-size:90%;">class-wise<span class="ltx_text ltx_font_upright" id="S4.T3.1.1.1.4.1.1"> F1</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.5" rowspan="2"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.5.1" style="font-size:90%;">BACC</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.1.1.1.1" rowspan="2"><span class="ltx_text" id="S4.T3.1.1.1.1.1" style="font-size:90%;"><span class="ltx_text ltx_markedasmath ltx_font_bold" id="S4.T3.1.1.1.1.1.1">M</span><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.1.1.1.2">-F1</span></span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.2.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.2.1.1"><span class="ltx_text" id="S4.T3.1.1.2.1.1.1" style="font-size:90%;">CN</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.2.1.2"><span class="ltx_text" id="S4.T3.1.1.2.1.2.1" style="font-size:90%;">AD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.2.1.3"><span class="ltx_text" id="S4.T3.1.1.2.1.3.1" style="font-size:90%;">bvFTD</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S4.T3.1.1.2.1.4"><span class="ltx_text" id="S4.T3.1.1.2.1.4.1" style="font-size:90%;">nfvPPA</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T3.1.1.2.1.5"><span class="ltx_text" id="S4.T3.1.1.2.1.5.1" style="font-size:90%;">svPPA</span></th>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.3.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" colspan="9" id="S4.T3.1.1.3.2.1">
<span class="ltx_text" id="S4.T3.1.1.3.2.1.1" style="font-size:90%;">                        </span><span class="ltx_text ltx_font_italic" id="S4.T3.1.1.3.2.1.2" style="font-size:90%;">Classification only</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.1.1.4.1">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.1.1.1" style="font-size:90%;">3D-Vision Transformer</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.2"><span class="ltx_text" id="S4.T3.1.1.4.1.2.1" style="font-size:90%;">64M</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.1.3.1" style="font-size:90%;">88.44</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.4.1.4.1" style="font-size:90%;">72.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.5"><span class="ltx_text" id="S4.T3.1.1.4.1.5.1" style="font-size:90%;">62.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.6"><span class="ltx_text" id="S4.T3.1.1.4.1.6.1" style="font-size:90%;">12.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.4.1.7"><span class="ltx_text" id="S4.T3.1.1.4.1.7.1" style="font-size:90%;">78.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.8"><span class="ltx_text" id="S4.T3.1.1.4.1.8.1" style="font-size:90%;">63.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.4.1.9"><span class="ltx_text" id="S4.T3.1.1.4.1.9.1" style="font-size:90%;">62.84</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.5.2">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.5.2.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.2.1.1" style="font-size:90%;">SVM atrophies</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.5.2.2"><span class="ltx_text" id="S4.T3.1.1.5.2.2.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.3"><span class="ltx_text" id="S4.T3.1.1.5.2.3.1" style="font-size:90%;">86.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.4"><span class="ltx_text" id="S4.T3.1.1.5.2.4.1" style="font-size:90%;">69.66</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.5"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.2.5.1" style="font-size:90%;">73.91</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.6"><span class="ltx_text" id="S4.T3.1.1.5.2.6.1" style="font-size:90%;">0.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.5.2.7"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.5.2.7.1" style="font-size:90%;">84.21</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.8"><span class="ltx_text" id="S4.T3.1.1.5.2.8.1" style="font-size:90%;">62.39</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.5.2.9"><span class="ltx_text" id="S4.T3.1.1.5.2.9.1" style="font-size:90%;">62.85</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.6.3">
<td class="ltx_td ltx_align_center ltx_border_t" colspan="9" id="S4.T3.1.1.6.3.1">
<span class="ltx_text" id="S4.T3.1.1.6.3.1.1" style="font-size:90%;">                        </span><span class="ltx_text ltx_font_italic" id="S4.T3.1.1.6.3.1.2" style="font-size:90%;">LLMs providing diagnostic reasoning</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.7.4">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.7.4.1.1" style="font-size:90%;">gpt-4o</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.2"><span class="ltx_text" id="S4.T3.1.1.7.4.2.1" style="font-size:90%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.3"><span class="ltx_text" id="S4.T3.1.1.7.4.3.1" style="font-size:90%;">70.81</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.4"><span class="ltx_text" id="S4.T3.1.1.7.4.4.1" style="font-size:90%;">51.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.5"><span class="ltx_text" id="S4.T3.1.1.7.4.5.1" style="font-size:90%;">51.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.6"><span class="ltx_text" id="S4.T3.1.1.7.4.6.1" style="font-size:90%;">19.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.1.1.7.4.7"><span class="ltx_text" id="S4.T3.1.1.7.4.7.1" style="font-size:90%;">48.48</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.8"><span class="ltx_text" id="S4.T3.1.1.7.4.8.1" style="font-size:90%;">55.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.1.1.7.4.9"><span class="ltx_text" id="S4.T3.1.1.7.4.9.1" style="font-size:90%;">48.32</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.8.5">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.8.5.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.8.5.1.1" style="font-size:90%;">DeepSeek-R1-Distill-Llama GRPO †</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.8.5.2"><span class="ltx_text" id="S4.T3.1.1.8.5.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.3"><span class="ltx_text" id="S4.T3.1.1.8.5.3.1" style="font-size:90%;">84.16</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.4"><span class="ltx_text" id="S4.T3.1.1.8.5.4.1" style="font-size:90%;">51.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.5"><span class="ltx_text" id="S4.T3.1.1.8.5.5.1" style="font-size:90%;">70.59</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.6"><span class="ltx_text" id="S4.T3.1.1.8.5.6.1" style="font-size:90%;">11.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.8.5.7"><span class="ltx_text" id="S4.T3.1.1.8.5.7.1" style="font-size:90%;">80.00</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.8"><span class="ltx_text" id="S4.T3.1.1.8.5.8.1" style="font-size:90%;">62.48</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.8.5.9"><span class="ltx_text" id="S4.T3.1.1.8.5.9.1" style="font-size:90%;">59.55</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.9.6">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.1.1.9.6.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.1.1" style="font-size:90%;">LLaMA-3.1-Instruct GRPO †</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.9.6.2"><span class="ltx_text" id="S4.T3.1.1.9.6.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.6.3"><span class="ltx_text" id="S4.T3.1.1.9.6.3.1" style="font-size:90%;">85.86</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.6.4"><span class="ltx_text" id="S4.T3.1.1.9.6.4.1" style="font-size:90%;">53.33</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.6.5"><span class="ltx_text" id="S4.T3.1.1.9.6.5.1" style="font-size:90%;">73.17</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.6.6"><span class="ltx_text" id="S4.T3.1.1.9.6.6.1" style="font-size:90%;">41.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.1.1.9.6.7"><span class="ltx_text" id="S4.T3.1.1.9.6.7.1" style="font-size:90%;">71.43</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.6.8"><span class="ltx_text" id="S4.T3.1.1.9.6.8.1" style="font-size:90%;">67.33</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.1.1.9.6.9"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.9.6.9.1" style="font-size:90%;">65.09</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.1.1.10.7">
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T3.1.1.10.7.1"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.10.7.1.1" style="font-size:90%;">Qwen-3 GRPO †</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.1.10.7.2"><span class="ltx_text" id="S4.T3.1.1.10.7.2.1" style="font-size:90%;">8B</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.10.7.3"><span class="ltx_text" id="S4.T3.1.1.10.7.3.1" style="font-size:90%;">83.33</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.10.7.4"><span class="ltx_text" id="S4.T3.1.1.10.7.4.1" style="font-size:90%;">46.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.10.7.5"><span class="ltx_text" id="S4.T3.1.1.10.7.5.1" style="font-size:90%;">66.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.10.7.6"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.10.7.6.1" style="font-size:90%;">48.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.1.1.10.7.7"><span class="ltx_text" id="S4.T3.1.1.10.7.7.1" style="font-size:90%;">64.52</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.10.7.8"><span class="ltx_text ltx_font_bold" id="S4.T3.1.1.10.7.8.1" style="font-size:90%;">68.38</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.1.1.10.7.9"><span class="ltx_text" id="S4.T3.1.1.10.7.9.1" style="font-size:90%;">61.88</span></td>
</tr>
</tbody>
</table>
</span></div>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusions</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">We introduced a modular framework for the differential diagnosis of neurodegenerative dementias that combines high-resolution MRI analysis, synthetic radiology reporting, and LLM-based reasoning.
By shifting from post hoc explanations to inference-time diagnostic reasoning, our method provides anatomically grounded rationales, offering physicians a way to access the plausibility and trustworthiness of LLM-generated predictions.
Fine-tuning lightweight LLMs via reinforcement learning with Group Relative Policy Optimization (GRPO), we demonstrate that coherent diagnostic reasoning can be achieved without requiring supervised reasoning traces.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">This work demonstrates the promise of using reasoning models in clinical contexts—particularly for complex, multi-hypothesis tasks such as differential diagnosis. We take an important first step in this direction, opening avenues for future systems that combine data-driven prediction with structured and transparent reasoning.</p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgments</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This work benefited from the support of the project HoliBrain funded by the French National Research Agency (ANR-23-CE45-0020-01) and the prematuration project ChatvolBrain funded by the CNRS. Moreover, this project is supported by the Precision and global vascular brain health institute funded by the France 2030 investment plan as part of the IHU3 initiative (ANR-23-IAHU- 0001). Finally, this study received financial support from the French government in the framework of the University of Bordeaux’s France 2030 program / RRI "IMPACT and the PEPR StratifyAging. This work was also granted access to the HPC resources of IDRIS under the allocation 2022-AD011013848R1 made by GENCI.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lamptey et al. [2022]</span>
<span class="ltx_bibblock">
Richard N. L. Lamptey, Bivek Chaulagain, Riddhi Trivedi, Avinash Gothwal, Buddhadev Layek, and Jagdish Singh.

</span>
<span class="ltx_bibblock">A review of the common neurodegenerative disorders: Current therapeutic approaches and the potential role of nanotherapeutics.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">International Journal of Molecular Sciences</em>, 23(3):1851, 2022.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3390/ijms23031851</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8837071/" title="">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8837071/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Crous-Bou et al. [2017]</span>
<span class="ltx_bibblock">
M. Crous-Bou, C. Minguillón, N. Gramunt, et al.

</span>
<span class="ltx_bibblock">Alzheimer’s disease prevention: from risk factors to early intervention.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Alzheimer’s Research &amp; Therapy</em>, 9(1):71, 2017.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1186/s13195-017-0297-z</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1186/s13195-017-0297-z" title="">https://doi.org/10.1186/s13195-017-0297-z</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Domínguez-Fernández et al. [2023]</span>
<span class="ltx_bibblock">
Celtia Domínguez-Fernández, June Egiguren-Ortiz, Jone Razquin, Margarita Gómez-Galán, Laura De las Heras-García, Elena Paredes-Rodríguez, Egoitz Astigarraga, Cristina Miguélez, and Gabriel Barreda-Gómez.

</span>
<span class="ltx_bibblock">Review of technological challenges in personalised medicine and early diagnosis of neurodegenerative disorders.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">International Journal of Molecular Sciences</em>, 24(4), 2023.

</span>
<span class="ltx_bibblock">ISSN 1422-0067.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3390/ijms24043321</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.mdpi.com/1422-0067/24/4/3321" title="">https://www.mdpi.com/1422-0067/24/4/3321</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chouliaras and O’Brien [2023]</span>
<span class="ltx_bibblock">
Leonidas Chouliaras and John T. O’Brien.

</span>
<span class="ltx_bibblock">The use of neuroimaging techniques in the early and differential diagnosis of dementia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Molecular Psychiatry</em>, 28(10):4084–4097, October 2023.

</span>
<span class="ltx_bibblock">ISSN 1476-5578.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41380-023-02215-8</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Harper et al. [2014]</span>
<span class="ltx_bibblock">
Lorna Harper, Frederik Barkhof, Philip Scheltens, Jonathan M Schott, and Nick C Fox.

</span>
<span class="ltx_bibblock">An algorithmic approach to structural imaging in dementia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Journal of Neurology, Neurosurgery &amp; Psychiatry</em>, 85(6):692–698, 2014.

</span>
<span class="ltx_bibblock">ISSN 0022-3050.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1136/jnnp-2013-306285</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://jnnp.bmj.com/content/85/6/692" title="">https://jnnp.bmj.com/content/85/6/692</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2021a]</span>
<span class="ltx_bibblock">
Jingjing Hu, Qing Zhao, Renyuan Liu, Xin Zhang, Pin Lv, Maoxue Wang, Yang Wang, Kelei He, Yang Gao, and Bing Zhang.

</span>
<span class="ltx_bibblock">Deep learning-based classification and voxel-based visualization of frontotemporal dementia and alzheimer’s disease.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Frontiers in Neuroscience</em>, 14:626154, 2021a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3389/fnins.2020.626154</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2020a]</span>
<span class="ltx_bibblock">
Da Ma, Donghuan Lu, Karteek Popuri, Lei Wang, and Mirza Faisal Beg.

</span>
<span class="ltx_bibblock">Differential diagnosis of frontotemporal dementia, alzheimer’s disease, and normal aging using a multi-scale multi-type feature generative adversarial deep neural network on structural magnetic resonance images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Frontiers in Neuroscience</em>, 14:853, 2020a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3389/fnins.2020.00853</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2023a]</span>
<span class="ltx_bibblock">
Huy-Dung Nguyen, Michaël Clément, Vincent Planche, Boris Mansencal, and Pierrick Coupé.

</span>
<span class="ltx_bibblock">Deep grading for mri-based differential diagnosis of alzheimer’s disease and frontotemporal dementia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Artificial Intelligence in Medicine</em>, 140:102636, 2023a.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.artmed.2023.102636</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2024]</span>
<span class="ltx_bibblock">
Yitong Li, Morteza Ghahremani, Youssef Wally, and Christian Wachinger.

</span>
<span class="ltx_bibblock">DiaMond: Dementia diagnosis with multi-modal vision transformers using MRI and PET, October 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2023b]</span>
<span class="ltx_bibblock">
Huy-Dung Nguyen, Michaël Clément, Boris Mansencal, and Pierrick Coupé.

</span>
<span class="ltx_bibblock">3d transformer based on deformable patch location for differential diagnosis between alzheimer’s disease and frontotemporal dementia.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">14th International Workshop on Machine Learning in Medical Imaging (MLMI), Held in Conjunction with MICCAI 2023</em>, pages 53–63, Vancouver, Canada, October 2023b. Springer.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-031-45676-3_6</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://hal.science/hal-04201135" title="">https://hal.science/hal-04201135</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Selvaraju et al. [2016]</span>
<span class="ltx_bibblock">
Ramprasaath R. Selvaraju, Abhishek Das, Ramakrishna Vedantam, Michael Cogswell, Devi Parikh, and Dhruv Batra.

</span>
<span class="ltx_bibblock">Grad-cam: Why did you say that? visual explanations from deep networks via gradient-based localization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">CoRR</em>, abs/1610.02391, 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1610.02391" title="">http://arxiv.org/abs/1610.02391</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2022]</span>
<span class="ltx_bibblock">
Huy-Dung Nguyen, Michaël Clément, Boris Mansencal, and Pierrick Coupé.

</span>
<span class="ltx_bibblock">Interpretable differential diagnosis for alzheimer’s disease and frontotemporal dementia.

</span>
<span class="ltx_bibblock">In Linwei Wang, Qi Dou, P. Thomas Fletcher, Stefanie Speidel, and Shuo Li, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Medical Image Computing and Computer Assisted Intervention – MICCAI 2022</em>, pages 55–65, Cham, 2022. Springer Nature Switzerland.

</span>
<span class="ltx_bibblock">ISBN 978-3-031-16431-6.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nguyen et al. [2023c]</span>
<span class="ltx_bibblock">
Huy-Dung Nguyen, Michaël Clément, Vincent Planche, Boris Mansencal, and Pierrick Coupé.

</span>
<span class="ltx_bibblock">Deep grading for MRI-based differential diagnosis of Alzheimer’s disease and Frontotemporal dementia.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Artificial Intelligence in Medicine</em>, 144:102636, October 2023c.

</span>
<span class="ltx_bibblock">ISSN 1873-2860.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.artmed.2023.102636</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hartsock and Rasool [2024]</span>
<span class="ltx_bibblock">
Iryna Hartsock and Ghulam Rasool.

</span>
<span class="ltx_bibblock">Vision-language models for medical report generation and visual question answering: a review.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Frontiers in Artificial Intelligence</em>, Volume 7 - 2024, 2024.

</span>
<span class="ltx_bibblock">ISSN 2624-8212.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3389/frai.2024.1430984</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1430984" title="">https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1430984</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hamamci et al. [2024]</span>
<span class="ltx_bibblock">
Ibrahim Ethem Hamamci, Sezgin Er, and Bjoern Menze.

</span>
<span class="ltx_bibblock">Ct2rep: Automated radiology report generation for 3d medical imaging, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2403.06801" title="">https://arxiv.org/abs/2403.06801</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2025]</span>
<span class="ltx_bibblock">
Cheng-Yi Li, Kao-Jung Chang, Cheng-Fu Yang, Hsin-Yu Wu, Wenting Chen, Hritik Bansal, Ling Chen, Yi-Ping Yang, Yu-Chun Chen, Shih-Pin Chen, Shih-Jen Chen, Jiing-Feng Lirng, Kai-Wei Chang, and Shih-Hwa Chiou.

</span>
<span class="ltx_bibblock">Towards a holistic framework for multimodal llm in 3d brain ct radiology report generation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Nature Communications</em>, 16(1), March 2025.

</span>
<span class="ltx_bibblock">ISSN 2041-1723.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41467-025-57426-0</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1038/s41467-025-57426-0" title="">http://dx.doi.org/10.1038/s41467-025-57426-0</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hyland et al. [2024]</span>
<span class="ltx_bibblock">
Stephanie L. Hyland, Shruthi Bannur, Kenza Bouzid, Daniel C. Castro, Mercy Ranjit, Anton Schwaighofer, Fernando Pérez-García, Valentina Salvatelli, Shaury Srivastav, Anja Thieme, Noel Codella, Matthew P. Lungren, Maria Teodora Wetscherek, Ozan Oktay, and Javier Alvarez-Valle.

</span>
<span class="ltx_bibblock">MAIRA-1: A specialised large multimodal model for radiology report generation, April 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">DeepSeek-AI et al. [2025]</span>
<span class="ltx_bibblock">
DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong
Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu,
Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang.

</span>
<span class="ltx_bibblock">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2501.12948" title="">https://arxiv.org/abs/2501.12948</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shao et al. [2024]</span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.03300" title="">https://arxiv.org/abs/2402.03300</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Suara et al. [2024]</span>
<span class="ltx_bibblock">
Subhashis Suara, Aayush Jha, Pratik Sinha, and Arif Ahmed Sekh.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Is Grad-CAM Explainable in Medical Images?</em>, page 124–135.

</span>
<span class="ltx_bibblock">Springer Nature Switzerland, 2024.

</span>
<span class="ltx_bibblock">ISBN 9783031581816.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1007/978-3-031-58181-6_11</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://dx.doi.org/10.1007/978-3-031-58181-6_11" title="">http://dx.doi.org/10.1007/978-3-031-58181-6_11</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. [2023]</span>
<span class="ltx_bibblock">
Karan Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Abubakr Babiker, Nathanael Schärli, Aakanksha Chowdhery, Philip Mansfield, Dina Demner-Fushman, Blaise Agüera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan.

</span>
<span class="ltx_bibblock">Large language models encode clinical knowledge.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Nature</em>, 620(7972):172–180, August 2023.

</span>
<span class="ltx_bibblock">ISSN 1476-4687.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41586-023-06291-2</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singhal et al. [2025]</span>
<span class="ltx_bibblock">
Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Mohamed Amin, Le Hou, Kevin Clark, Stephen R. Pfohl, Heather Cole-Lewis, Darlene Neal, Qazi Mamunur Rashid, Mike Schaekermann, Amy Wang, Dev Dash, Jonathan H. Chen, Nigam H. Shah, Sami Lachgar, Philip Andrew Mansfield, Sushant Prakash, Bradley Green, Ewa Dominowska, Blaise Agüera y Arcas, Nenad Tomašev, Yun Liu, Renee Wong, Christopher Semturs, S. Sara Mahdavi, Joelle K. Barral, Dale R. Webster, Greg S. Corrado, Yossi Matias, Shekoofeh Azizi, Alan Karthikesalingam, and Vivek Natarajan.

</span>
<span class="ltx_bibblock">Toward expert-level medical question answering with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Nature Medicine</em>, 31(3):943–950, March 2025.

</span>
<span class="ltx_bibblock">ISSN 1546-170X.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41591-024-03423-7</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Patel and Lam [2023]</span>
<span class="ltx_bibblock">
Sajan B. Patel and Kyle Lam.

</span>
<span class="ltx_bibblock">ChatGPT: The future of discharge summaries?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">The Lancet. Digital Health</em>, 5(3):e107–e108, March 2023.

</span>
<span class="ltx_bibblock">ISSN 2589-7500.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/S2589-7500(23)00021-3</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Van Veen et al. [2024]</span>
<span class="ltx_bibblock">
Dave Van Veen, Cara Van Uden, Louis Blankemeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerová, Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason Hom, Sergios Gatidis, John Pauly, and Akshay S. Chaudhari.

</span>
<span class="ltx_bibblock">Adapted large language models can outperform medical experts in clinical text summarization.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Nature Medicine</em>, 30(4):1134–1142, April 2024.

</span>
<span class="ltx_bibblock">ISSN 1546-170X.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41591-024-02855-5</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023]</span>
<span class="ltx_bibblock">
Lavender Yao Jiang, Xujin Chris Liu, Nima Pour Nejatian, Mustafa Nasir-Moin, Duo Wang, Anas Abidin, Kevin Eaton, Howard Antony Riina, Ilya Laufer, Paawan Punjabi, Madeline Miceli, Nora C. Kim, Cordelia Orillac, Zane Schnurman, Christopher Livia, Hannah Weiss, David Kurland, Sean Neifert, Yosef Dastagirzada, Douglas Kondziolka, Alexander T. M. Cheung, Grace Yang, Ming Cao, Mona Flores, Anthony B. Costa, Yindalon Aphinyanaphongs, Kyunghyun Cho, and Eric Karl Oermann.

</span>
<span class="ltx_bibblock">Health system-scale language models are all-purpose prediction engines.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Nature</em>, 619(7969):357–362, July 2023.

</span>
<span class="ltx_bibblock">ISSN 1476-4687.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41586-023-06160-y</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">McDuff et al. [2023]</span>
<span class="ltx_bibblock">
Daniel McDuff, Mike Schaekermann, Tao Tu, Anil Palepu, Amy Wang, Jake Garrison, Karan Singhal, Yash Sharma, Shekoofeh Azizi, Kavita Kulkarni, Le Hou, Yong Cheng, Yun Liu, S. Sara Mahdavi, Sushant Prakash, Anupam Pathak, Christopher Semturs, Shwetak Patel, Dale R. Webster, Ewa Dominowska, Juraj Gottweis, Joelle Barral, Katherine Chou, Greg S. Corrado, Yossi Matias, Jake Sunshine, Alan Karthikesalingam, and Vivek Natarajan.

</span>
<span class="ltx_bibblock">Towards Accurate Differential Diagnosis with Large Language Models, November 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Han et al. [2025]</span>
<span class="ltx_bibblock">
Tianyu Han, Lisa C. Adams, Jens-Michalis Papaioannou, Paul Grundmann, Tom Oberhauser, Alexei Figueroa, Alexander Löser, Daniel Truhn, and Keno K. Bressem.

</span>
<span class="ltx_bibblock">MedAlpaca – An Open-Source Collection of Medical Conversational AI Models and Training Data, March 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lee et al. [2019]</span>
<span class="ltx_bibblock">
Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.

</span>
<span class="ltx_bibblock">BioBERT: A pre-trained biomedical language representation model for biomedical text mining, October 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Luo et al. [2022]</span>
<span class="ltx_bibblock">
Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu.

</span>
<span class="ltx_bibblock">BioGPT: Generative pre-trained transformer for biomedical text generation and mining.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Briefings in Bioinformatics</em>, 23(6):bbac409, November 2022.

</span>
<span class="ltx_bibblock">ISSN 1477-4054.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1093/bib/bbac409</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gu et al. [2021]</span>
<span class="ltx_bibblock">
Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon.

</span>
<span class="ltx_bibblock">Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing, September 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Moor et al. [2023]</span>
<span class="ltx_bibblock">
Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar.

</span>
<span class="ltx_bibblock">Med-Flamingo: A Multimodal Medical Few-shot Learner.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 3rd Machine Learning for Health Symposium</em>, pages 353–367. PMLR, December 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao.

</span>
<span class="ltx_bibblock">LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day, June 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Saab et al. [2024]</span>
<span class="ltx_bibblock">
Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, Juanma Zambrano Chaves, Szu-Yeu Hu, Mike Schaekermann, Aishwarya Kamath, Yong Cheng, David G. T. Barrett, Cathy Cheung, Basil Mustafa, Anil Palepu, Daniel McDuff, Le Hou, Tomer Golany, Luyang Liu, Jean-baptiste Alayrac, Neil Houlsby, Nenad Tomasev, Jan Freyberg, Charles Lau, Jonas Kemp, Jeremy Lai, Shekoofeh Azizi, Kimberly Kanada, SiWai Man, Kavita Kulkarni, Ruoxi Sun, Siamak Shakeri, Luheng He, Ben Caine, Albert Webson, Natasha Latysheva, Melvin Johnson, Philip Mansfield, Jian Lu, Ehud Rivlin, Jesper Anderson, Bradley Green, Renee Wong, Jonathan Krause, Jonathon Shlens, Ewa Dominowska, S. M. Ali Eslami, Katherine Chou, Claire Cui, Oriol Vinyals, Koray Kavukcuoglu, James Manyika, Jeff Dean, Demis Hassabis, Yossi Matias, Dale Webster, Joelle Barral, Greg Corrado, Christopher Semturs, S. Sara Mahdavi, Juraj Gottweis, Alan Karthikesalingam, and Vivek Natarajan.

</span>
<span class="ltx_bibblock">Capabilities of Gemini Models in Medicine, May 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2024]</span>
<span class="ltx_bibblock">
Lin Yang, Shawn Xu, Andrew Sellergren, Timo Kohlberger, Yuchen Zhou, Ira Ktena, Atilla Kiraly, Faruk Ahmed, Farhad Hormozdiari, Tiam Jaroensri, Eric Wang, Ellery Wulczyn, Fayaz Jamil, Theo Guidroz, Chuck Lau, Siyuan Qiao, Yun Liu, Akshay Goel, Kendall Park, Arnav Agharwal, Nick George, Yang Wang, Ryutaro Tanno, David G. T. Barrett, Wei-Hung Weng, S. Sara Mahdavi, Khaled Saab, Tao Tu, Sreenivasa Raju Kalidindi, Mozziyar Etemadi, Jorge Cuadros, Gregory Sorensen, Yossi Matias, Katherine Chou, Greg Corrado, Joelle Barral, Shravya Shetty, David Fleet, S. M. Ali Eslami, Daniel Tse, Shruthi Prabhakara, Cory McLean, Dave Steiner, Rory Pilgrim, Christopher Kelly, Shekoofeh Azizi, and Daniel Golden.

</span>
<span class="ltx_bibblock">Advancing Multimodal Medical Capabilities of Gemini, May 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Savage et al. [2024]</span>
<span class="ltx_bibblock">
T. Savage, A. Nayak, R. Gallo, et al.

</span>
<span class="ltx_bibblock">Diagnostic reasoning prompts reveal the potential for large language model interpretability in medicine.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">npj Digital Medicine</em>, 7:20, 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s41746-024-01010-1</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41746-024-01010-1" title="">https://doi.org/10.1038/s41746-024-01010-1</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhou et al. [2025]</span>
<span class="ltx_bibblock">
Shuang Zhou, Mingquan Lin, Sirui Ding, Jiashuo Wang, Canyu Chen, Genevieve B. Melton, James Zou, and Rui Zhang.

</span>
<span class="ltx_bibblock">Explainable differential diagnosis with dual-inference large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">npj Health Systems</em>, 2(1):12, 2025.

</span>
<span class="ltx_bibblock">ISSN 3005-1959.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1038/s44401-025-00015-6</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s44401-025-00015-6" title="">https://doi.org/10.1038/s44401-025-00015-6</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. [2024]</span>
<span class="ltx_bibblock">
Taeyoon Kwon, Kai Tzu-iunn Ong, Dongjin Kang, Seungjun Moon, Jeong Ryong Lee, Dosik Hwang, Beomseok Sohn, Yongsik Sim, Dongha Lee, and Jinyoung Yeo.

</span>
<span class="ltx_bibblock">Large language models are clinical reasoners: Reasoning-aware diagnosis framework with prompt-generated rationales.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, 38(16):18417–18425, Mar. 2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1609/aaai.v38i16.29802</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/29802" title="">https://ojs.aaai.org/index.php/AAAI/article/view/29802</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lai et al. [2025]</span>
<span class="ltx_bibblock">
Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang.

</span>
<span class="ltx_bibblock">Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2503.13939" title="">https://arxiv.org/abs/2503.13939</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coupé et al. [2020]</span>
<span class="ltx_bibblock">
Pierrick Coupé, Boris Mansencal, Michaël Clément, Rémi Giraud, Baudouin Denis de Senneville, Vinh-Thong Ta, Vincent Lepetit, and José V. Manjon.

</span>
<span class="ltx_bibblock">Assemblynet: A large ensemble of cnns for 3d whole brain mri segmentation.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">NeuroImage</em>, 219:117026, 2020.

</span>
<span class="ltx_bibblock">ISSN 1053-8119.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">https://doi.org/10.1016/j.neuroimage.2020.117026</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S1053811920305127" title="">https://www.sciencedirect.com/science/article/pii/S1053811920305127</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Coupé et al. [2017]</span>
<span class="ltx_bibblock">
Pierrick Coupé, Gwénaëlle Catheline, Enrique Lanuza, José V. Manjón, and Alzheimer’s Disease Neuroimaging Initiative.

</span>
<span class="ltx_bibblock">Towards a unified analysis of brain maturation and aging across the entire lifespan: A mri analysis.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Human Brain Mapping</em>, 38(11):5501–5518, November 2017.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1002/hbm.23743</span>.

</span>
<span class="ltx_bibblock">Epub 2017 Jul 24.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jack et al. [2008]</span>
<span class="ltx_bibblock">
Clifford R. Jack, Matt A. Bernstein, Nick C. Fox, Paul Thompson, Gene Alexander, Danielle Harvey, Bret Borowski, Paula J. Britson, Jennifer L. Whitwell, Chadwick Ward, Anders M. Dale, Joel P. Felmlee, Jeffrey L. Gunter, Derek L.G. Hill, Ron Killiany, Norbert Schuff, Sabrina Fox-Bosetti, Chen Lin, Colin Studholme, Charles S. DeCarli, Gunnar Krueger, Heidi A. Ward, Gregory J. Metzger, Katherine T. Scott, Richard Mallozzi, Daniel Blezek, Joshua Levy, Josef P. Debbins, Adam S. Fleisher, Marilyn Albert, Robert Green, George Bartzokis, Gary Glover, John Mugler, and Michael W. Weiner.

</span>
<span class="ltx_bibblock">The alzheimer’s disease neuroimaging initiative (adni): Mri methods.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Journal of Magnetic Resonance Imaging</em>, 27(4):685–691, April 2008.

</span>
<span class="ltx_bibblock">ISSN 1053-1807.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1002/jmri.21049</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al. [2024]</span>
<span class="ltx_bibblock">
OpenAI, Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, Red Avila, Igor Babuschkin, Suchir Balaji, Valerie Balcom, Paul Baltescu, Haiming Bao, Mohammad Bavarian, Jeff Belgum, Irwan Bello, Jake Berdine, Gabriel Bernadett-Shapiro, Christopher Berner, Lenny Bogdonoff, Oleg Boiko, Madelaine Boyd, Anna-Luisa Brakman, Greg Brockman, Tim Brooks, Miles Brundage, Kevin Button, Trevor Cai, Rosie Campbell, Andrew Cann, Brittany Carey, Chelsea Carlson, Rory Carmichael, Brooke Chan, Che Chang, Fotis Chantzis, Derek Chen, Sully Chen, Ruby Chen, Jason Chen, Mark Chen, Ben Chess, Chester Cho, Casey Chu, Hyung Won Chung, Dave Cummings, Jeremiah Currier, Yunxing Dai, Cory Decareaux, Thomas Degry, Noah Deutsch, Damien Deville, Arka Dhar, David Dohan, Steve Dowling, Sheila Dunning, Adrien Ecoffet, Atty Eleti, Tyna Eloundou, David Farhi, Liam Fedus, Niko Felix, Simón Posada Fishman, Juston Forte, Isabella Fulford,
Leo Gao, Elie Georges, Christian Gibson, Vik Goel, Tarun Gogineni, Gabriel Goh, Rapha Gontijo-Lopes, Jonathan Gordon, Morgan Grafstein, Scott Gray, Ryan Greene, Joshua Gross, Shixiang Shane Gu, Yufei Guo, Chris Hallacy, Jesse Han, Jeff Harris, Yuchen He, Mike Heaton, Johannes Heidecke, Chris Hesse, Alan Hickey, Wade Hickey, Peter Hoeschele, Brandon Houghton, Kenny Hsu, Shengli Hu, Xin Hu, Joost Huizinga, Shantanu Jain, Shawn Jain, Joanne Jang, Angela Jiang, Roger Jiang, Haozhun Jin, Denny Jin, Shino Jomoto, Billie Jonn, Heewoo Jun, Tomer Kaftan, Łukasz Kaiser, Ali Kamali, Ingmar Kanitscheider, Nitish Shirish Keskar, Tabarak Khan, Logan Kilpatrick, Jong Wook Kim, Christina Kim, Yongjik Kim, Jan Hendrik Kirchner, Jamie Kiros, Matt Knight, Daniel Kokotajlo, Łukasz Kondraciuk, Andrew Kondrich, Aris Konstantinidis, Kyle Kosic, Gretchen Krueger, Vishal Kuo, Michael Lampe, Ikai Lan, Teddy Lee, Jan Leike, Jade Leung, Daniel Levy, Chak Ming Li, Rachel Lim, Molly Lin, Stephanie Lin, Mateusz Litwin, Theresa
Lopez, Ryan Lowe, Patricia Lue, Anna Makanju, Kim Malfacini, Sam Manning, Todor Markov, Yaniv Markovski, Bianca Martin, Katie Mayer, Andrew Mayne, Bob McGrew, Scott Mayer McKinney, Christine McLeavey, Paul McMillan, Jake McNeil, David Medina, Aalok Mehta, Jacob Menick, Luke Metz, Andrey Mishchenko, Pamela Mishkin, Vinnie Monaco, Evan Morikawa, Daniel Mossing, Tong Mu, Mira Murati, Oleg Murk, David Mély, Ashvin Nair, Reiichiro Nakano, Rajeev Nayak, Arvind Neelakantan, Richard Ngo, Hyeonwoo Noh, Long Ouyang, Cullen O’Keefe, Jakub Pachocki, Alex Paino, Joe Palermo, Ashley Pantuliano, Giambattista Parascandolo, Joel Parish, Emy Parparita, Alex Passos, Mikhail Pavlov, Andrew Peng, Adam Perelman, Filipe de Avila Belbute Peres, Michael Petrov, Henrique Ponde de Oliveira Pinto, Michael, Pokorny, Michelle Pokrass, Vitchyr H. Pong, Tolly Powell, Alethea Power, Boris Power, Elizabeth Proehl, Raul Puri, Alec Radford, Jack Rae, Aditya Ramesh, Cameron Raymond, Francis Real, Kendra Rimbach, Carl Ross, Bob Rotsted,
Henri Roussez, Nick Ryder, Mario Saltarelli, Ted Sanders, Shibani Santurkar, Girish Sastry, Heather Schmidt, David Schnurr, John Schulman, Daniel Selsam, Kyla Sheppard, Toki Sherbakov, Jessica Shieh, Sarah Shoker, Pranav Shyam, Szymon Sidor, Eric Sigler, Maddie Simens, Jordan Sitkin, Katarina Slama, Ian Sohl, Benjamin Sokolowsky, Yang Song, Natalie Staudacher, Felipe Petroski Such, Natalie Summers, Ilya Sutskever, Jie Tang, Nikolas Tezak, Madeleine B. Thompson, Phil Tillet, Amin Tootoonchian, Elizabeth Tseng, Preston Tuggle, Nick Turley, Jerry Tworek, Juan Felipe Cerón Uribe, Andrea Vallone, Arun Vijayvergiya, Chelsea Voss, Carroll Wainwright, Justin Jay Wang, Alvin Wang, Ben Wang, Jonathan Ward, Jason Wei, C. J. Weinmann, Akila Welihinda, Peter Welinder, Jiayi Weng, Lilian Weng, Matt Wiethoff, Dave Willner, Clemens Winter, Samuel Wolrich, Hannah Wong, Lauren Workman, Sherwin Wu, Jeff Wu, Michael Wu, Kai Xiao, Tao Xu, Sarah Yoo, Kevin Yu, Qiming Yuan, Wojciech Zaremba, Rowan Zellers, Chong Zhang, Marvin
Zhang, Shengjia Zhao, Tianhao Zheng, Juntang Zhuang, William Zhuk, and Barret Zoph.

</span>
<span class="ltx_bibblock">GPT-4 Technical Report, March 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ankit Pal [2024]</span>
<span class="ltx_bibblock">
Malaikannan Sankarasubbu Ankit Pal.

</span>
<span class="ltx_bibblock">Openbiollms: Advancing open-source large language models for healthcare and life sciences.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B" title="">https://huggingface.co/aaditya/OpenBioLLM-Llama3-70B</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grattafiori et al. [2024]</span>
<span class="ltx_bibblock">
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang,
Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur,
Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher,
Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh
Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna
Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim
Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan,
Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao, Yundi
Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma.

</span>
<span class="ltx_bibblock">The llama 3 herd of models, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2407.21783" title="">https://arxiv.org/abs/2407.21783</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Qwen et al. [2025]</span>
<span class="ltx_bibblock">
Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu.

</span>
<span class="ltx_bibblock">Qwen2.5 technical report, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2412.15115" title="">https://arxiv.org/abs/2412.15115</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang et al. [2025]</span>
<span class="ltx_bibblock">
An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu.

</span>
<span class="ltx_bibblock">Qwen3 technical report, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2505.09388" title="">https://arxiv.org/abs/2505.09388</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kwon et al. [2023]</span>
<span class="ltx_bibblock">
Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica.

</span>
<span class="ltx_bibblock">Efficient memory management for large language model serving with pagedattention.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. [2021b]</span>
<span class="ltx_bibblock">
Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.

</span>
<span class="ltx_bibblock">Lora: Low-rank adaptation of large language models, 2021b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2106.09685" title="">https://arxiv.org/abs/2106.09685</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2025]</span>
<span class="ltx_bibblock">
Zichen Liu, Changyu Chen, Wenjun Li, Penghui Qi, Tianyu Pang, Chao Du, Wee Sun Lee, and Min Lin.

</span>
<span class="ltx_bibblock">Understanding r1-zero-like training: A critical perspective, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2503.20783" title="">https://arxiv.org/abs/2503.20783</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ma et al. [2020b]</span>
<span class="ltx_bibblock">
Da Ma, Donghuan Lu, Karteek Popuri, Lei Wang, Mirza Faisal Beg, and Alzheimer’s Disease Neuroimaging Initiative.

</span>
<span class="ltx_bibblock">Differential Diagnosis of Frontotemporal Dementia, Alzheimer’s Disease, and Normal Aging Using a Multi-Scale Multi-Type Feature Generative Adversarial Deep Neural Network on Structural Magnetic Resonance Images.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Frontiers in Neuroscience</em>, 14, October 2020b.

</span>
<span class="ltx_bibblock">ISSN 1662-453X.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3389/fnins.2020.00853</span>.

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Example of a Complete Synthetic Report and Human Comparison</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We provide an illustrative example of a complete synthetic radiology report generated from a 3D T1-weighted MRI scan using the pipeline detailed in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS1" title="3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">3.1</span></a>. The input scan was obtained from an open-access case on Radiopaedia.org <span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://radiopaedia.org/cases/frontotemporal-dementia-behavioural-variant-2" title="">https://radiopaedia.org/cases/frontotemporal-dementia-behavioural-variant-2</a></span></span></span>, enabling a direct comparison with expert-curated findings authored by neuroradiologist Dr. Frank Gaillard (founder of Radiopaedia).</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A1.F6" title="Figure 6 ‣ Appendix A Example of a Complete Synthetic Report and Human Comparison ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">6</span></a> displays the expert-written report (top) and the corresponding synthetic report generated by our model (bottom). To aid visual inspection, we highlight areas of overlap in atrophy pattern descriptions using color coding.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1">We observe that while both reports capture key neurodegenerative features consistent with the clinical picture—in this case, a diagnosis of behavioral variant frontotemporal dementia (bvFTD)—their reporting styles differ. The synthetic report is more exhaustive and systematically structured: it enumerates a wide range of anatomical regions, including both affected and unaffected areas. In contrast, the expert human report adopts a more concise, diagnosis-driven narrative, focusing selectively on findings most relevant to the suspected pathology. This reflects typical clinical reporting practice, where radiologists tailor descriptions to guide differential diagnosis rather than provide exhaustive anatomical reviews. Nonetheless, despite stylistic differences, the synthetic report successfully captures all key anatomical patterns relevant to the diagnosis, demonstrating its effectiveness in supporting the subsequent differential diagnosis task.</p>
</div>
<figure class="ltx_figure" id="A1.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="639" id="A1.F6.g1" src="extracted/6479989/human_syn_report.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A1.F6.2.1.1" style="font-size:90%;">Figure 6</span>: </span><span class="ltx_text" id="A1.F6.3.2" style="font-size:90%;">Comparison of expert-written (top) and synthetic (bottom) radiology reports for the same T1-weighted MRI scan. Colored highlights indicate overlapping descriptions of atrophic patterns.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Insight into GRPO Training Dynamics</h2>
<div class="ltx_para" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">This appendix provides additional insight into the training dynamics of the GRPO-fine-tuned 8B models. In Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A2.F7" title="Figure 7 ‣ Appendix B Insight into GRPO Training Dynamics ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">7</span></a>, we visualize key performance metrics across training iterations. The left panel illustrates the progression of the mean diagnostic accuracy reward on the training set, while the right panel shows the corresponding performance on the validation set.
All models exhibit a consistent increase in diagnostic accuracy as training progresses, with a similar upward trend on the validation set, highlighting the effectiveness and generalizability of the GRPO optimization process.
Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A2.F8" title="Figure 8 ‣ Appendix B Insight into GRPO Training Dynamics ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">8</span></a> presents the evolution of two additional metrics. The left panel depicts the mean length of generated completions (i.e., the number of generated tokens). Interestingly, the trend in response length varies across models. For instance, the Qwen3-8B and DeepSeek-Llama-8B models show a clear upward trend in maximum response length (more marked in the first), aligning with observations in  <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib46" title="">46</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib18" title="">18</a>]</cite>. This suggests a growing capacity for analysis, reasoning refinement, and even self-doubt, as the model develops more nuanced diagnostic justifications.
In contrast, models that are not natively trained for reasoning, such as Qwen-2.5-Instruct and LLaMA-3.1-Instruct, maintain a relatively stable response length throughout training. This stability could suggest early convergence in reasoning style, with responses remaining short and focused—reflecting a compressed but effective form of justification.
Finally, the right panel of Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#A2.F8" title="Figure 8 ‣ Appendix B Insight into GRPO Training Dynamics ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">8</span></a> shows the evolution of the KL divergence term relative to the original model. We observe that all models diverge from their initial response distributions over the course of training. This indicates that GRPO effectively reshapes the model’s output behavior while maintaining human-readability through this regularization KL penalty.</p>
</div>
<figure class="ltx_figure" id="A2.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F7.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="A2.F7.sf1.g1" src="extracted/6479989/GRPO_train_acc.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F7.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A2.F7.sf1.3.2" style="font-size:90%;">Training set: Mean diagnostic accuracy reward</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F7.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="A2.F7.sf2.g1" src="extracted/6479989/GRPO_eval_acc.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F7.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A2.F7.sf2.3.2" style="font-size:90%;">Validation set: Mean diagnostic accuracy reward</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F7.2.1.1" style="font-size:90%;">Figure 7</span>: </span><span class="ltx_text" id="A2.F7.3.2" style="font-size:90%;">Progression of diagnostic accuracy during the initial 60K steps of GRPO training across training and validation sets.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A2.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F8.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="A2.F8.sf1.g1" src="extracted/6479989/GRPO_mean_length.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F8.sf1.2.1.1" style="font-size:90%;">(a)</span> </span><span class="ltx_text" id="A2.F8.sf1.3.2" style="font-size:90%;">Mean generated response length (tokens)</span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="A2.F8.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="311" id="A2.F8.sf2.g1" src="extracted/6479989/GRPO_kl.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F8.sf2.2.1.1" style="font-size:90%;">(b)</span> </span><span class="ltx_text" id="A2.F8.sf2.3.2" style="font-size:90%;">KL divergence relative to the original model</span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A2.F8.2.1.1" style="font-size:90%;">Figure 8</span>: </span><span class="ltx_text" id="A2.F8.3.2" style="font-size:90%;">Evolution of mean response length and KL divergence over the first 60K steps of GRPO training.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Full Diagnostic Reasoning Examples</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We present complete diagnostic outputs generated by GRPO-trained models on a randomly sampled case of behavioral variant frontotemporal dementia (bvFTD). These examples highlight distinct reasoning styles that emerged during training. The LLaMA and Qwen models produce concise, focused justifications, emphasizing key diagnostic features with minimal elaboration. In contrast, the DeepSeek and Qwen-3 model exhibits a more expansive diagnostic narrative, engaging in lengthier analysis.</p>
</div>
<figure class="ltx_figure" id="A3.F9"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="776" id="A3.F9.g1" src="extracted/6479989/full_deepseek_BV.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F9.2.1.1" style="font-size:90%;">Figure 9</span>: </span><span class="ltx_text" id="A3.F9.3.2" style="font-size:90%;">Full diagnostic output from GRPO-trained DeepSeek-R1-Llama-8B on a bvFTD case.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A3.F10"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_portrait" height="801" id="A3.F10.g1" src="extracted/6479989/full_qwen3_BV.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F10.2.1.1" style="font-size:90%;">Figure 10</span>: </span><span class="ltx_text" id="A3.F10.3.2" style="font-size:90%;">Full diagnostic output from the GRPO-trained Qwen-3-8B model on a bvFTD case.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A3.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="389" id="A3.F11.g1" src="extracted/6479989/full_llama_BV.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F11.2.1.1" style="font-size:90%;">Figure 11</span>: </span><span class="ltx_text" id="A3.F11.3.2" style="font-size:90%;">Full diagnostic output from the GRPO-trained LlaMA-3.1-Instruct-8B model on a bvFTD case.</span></figcaption>
</figure>
<figure class="ltx_figure" id="A3.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="393" id="A3.F12.g1" src="extracted/6479989/full_qwen_BV.png" width="598"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="A3.F12.2.1.1" style="font-size:90%;">Figure 12</span>: </span><span class="ltx_text" id="A3.F12.3.2" style="font-size:90%;">Full diagnostic output from the GRPO-trained Qwen-2.5-Instruct-7B model on a bvFTD case.</span></figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Training Details for Classification-Only Baselines</h2>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Vision-transformer (ViT).</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px1.p1.1">We build on the 3D ViT architecture and training pipeline from Nguyen <span class="ltx_text ltx_font_italic" id="A4.SS0.SSS0.Px1.p1.1.1">et al.</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#bib.bib10" title="">10</a>]</cite>, originally designed for AD–FTD–CN classification. To support our extended diagnostic setting, we modify the final classification head to a 5-way MLP, enabling prediction across CN, AD, bvFTD, svPPA, and nfvPPA. All other architectural components, data preprocessing steps, and optimization settings (e.g., learning rate, batch size, data augmentation) are retained from the original implementation.</p>
</div>
</section>
<section class="ltx_paragraph" id="A4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Support Vector Machine (SVM).</h4>
<div class="ltx_para" id="A4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A4.SS0.SSS0.Px2.p1.1">We train SVM classifiers using the Structural Deviation Score features described in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2505.19954v1#S3.SS1" title="3.1 From Brain MRI to Text: A Modular Pipeline for Synthetic Report Generation ‣ 3 Approach ‣ An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning"><span class="ltx_text ltx_ref_tag">3.1</span></a>. Hyperparameter tuning is performed via grid search across two kernel types (<span class="ltx_text ltx_font_typewriter" id="A4.SS0.SSS0.Px2.p1.1.1">linear</span>, <span class="ltx_text ltx_font_typewriter" id="A4.SS0.SSS0.Px2.p1.1.2">rbf</span>) and 100 logarithmically spaced values of the regularization strength <math alttext="C\in[10^{-4},10^{1.5}]" class="ltx_Math" display="inline" id="A4.SS0.SSS0.Px2.p1.1.m1.2"><semantics id="A4.SS0.SSS0.Px2.p1.1.m1.2a"><mrow id="A4.SS0.SSS0.Px2.p1.1.m1.2.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.cmml"><mi id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.4" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.4.cmml">C</mi><mo id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.3" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.3.cmml">∈</mo><mrow id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.3.cmml"><mo id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.3" stretchy="false" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.3.cmml">[</mo><msup id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml"><mn id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.cmml">10</mn><mrow id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.cmml"><mo id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3a" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.cmml">−</mo><mn id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.2.cmml">4</mn></mrow></msup><mo id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.4" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.3.cmml">,</mo><msup id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.cmml"><mn id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.2" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.2.cmml">10</mn><mn id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.3" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.3.cmml">1.5</mn></msup><mo id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.5" stretchy="false" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A4.SS0.SSS0.Px2.p1.1.m1.2b"><apply id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2"><in id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.3.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.3"></in><ci id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.4.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.4">𝐶</ci><interval closure="closed" id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.3.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2"><apply id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.1.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1">superscript</csymbol><cn id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.cmml" type="integer" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.2">10</cn><apply id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3"><minus id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.1.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3"></minus><cn id="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.2.cmml" type="integer" xref="A4.SS0.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.2">4</cn></apply></apply><apply id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.1.cmml" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2">superscript</csymbol><cn id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.2.cmml" type="integer" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.2">10</cn><cn id="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.3.cmml" type="float" xref="A4.SS0.SSS0.Px2.p1.1.m1.2.2.2.2.2.3">1.5</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.SS0.SSS0.Px2.p1.1.m1.2c">C\in[10^{-4},10^{1.5}]</annotation><annotation encoding="application/x-llamapun" id="A4.SS0.SSS0.Px2.p1.1.m1.2d">italic_C ∈ [ 10 start_POSTSUPERSCRIPT - 4 end_POSTSUPERSCRIPT , 10 start_POSTSUPERSCRIPT 1.5 end_POSTSUPERSCRIPT ]</annotation></semantics></math>. The best model is selected based on balanced accuracy on the held-out validation folds.</p>
</div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 26 13:09:21 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement</title>
<!--Generated on Mon May 26 16:35:59 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Large Language Models,  Function Calling,  MultiScale Loss,  Chain of Thought Reasoning,  Supervised Fine-tuning" lang="en" name="keywords"/>
<base href="/html/2505.20192v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S1" title="In FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S2" title="In FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Methodology</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S2.SS1" title="In 2. Methodology ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>FunReason Data Refinement Pipeline</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S2.SS2" title="In 2. Methodology ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Self-Refinement Multiscale Loss (SRML)</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3" title="In FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiment</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.SS1" title="In 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Data preparation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.SS2" title="In 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Experiment Setting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.SS3" title="In 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Results on BFCL</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.SS4" title="In 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Results on HumanEval and MBPP</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.SS5" title="In 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Ablation Study</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S4" title="In FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S5" title="In FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Discussions, Limitations, and Societal Impacts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S6" title="In FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S7" title="In FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Acknowledgments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#A1" title="In FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Details of Data Refinement Pipeline</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line ltx_leqno">
<h1 class="ltx_title ltx_title_document">FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Bingguang Hao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_email"><a href="mailto:bingguanghao7@gmail.com">bingguanghao7@gmail.com</a>
</span>
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id1.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id2.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id3.3.id3">China</span>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Maolin Wang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id4.1.id1">City University of Hong Kong</span><span class="ltx_text ltx_affiliation_city" id="id5.2.id2">Hong Kong</span><span class="ltx_text ltx_affiliation_country" id="id6.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:morin.wang@my.cityu.edu.hk">morin.wang@my.cityu.edu.hk</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zengzhuang Xu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id7.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id8.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id9.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xuzengzhuang.xzz@antgroup.com">xuzengzhuang.xzz@antgroup.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Cunyin Peng
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id10.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id11.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id12.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:pengcunyin@gmail.com">pengcunyin@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yicheng Chen
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id13.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id14.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id15.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chenggejiayou@gmail.com">chenggejiayou@gmail.com</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiangyu Zhao
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id16.1.id1">City University of Hong Kong</span><span class="ltx_text ltx_affiliation_city" id="id17.2.id2">Hong Kong</span><span class="ltx_text ltx_affiliation_country" id="id18.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:xy.zhao@cityu.edu.hk">xy.zhao@cityu.edu.hk</a>
</span></span></span>
<span class="ltx_author_before">, </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Jinjie Gu
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id19.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id20.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id21.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:jinjie.gujj@antgroup.com">jinjie.gujj@antgroup.com</a>
</span></span></span>
<span class="ltx_author_before"> and </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chenyi Zhuang
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_affiliation_institution" id="id22.1.id1">Ant Group</span><span class="ltx_text ltx_affiliation_city" id="id23.2.id2">Hangzhou</span><span class="ltx_text ltx_affiliation_country" id="id24.3.id3">China</span>
</span>
<span class="ltx_contact ltx_role_email"><a href="mailto:chenyi.zcy@antgroup.com">chenyi.zcy@antgroup.com</a>
</span></span></span>
</div>
<div class="ltx_dates">(2025)</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract.</h6>
<p class="ltx_p" id="id25.id1">The integration of large language models (LLMs) with function calling has emerged as a crucial capability for enhancing their practical utility in real-world applications. However, effectively combining reasoning processes with accurate function execution remains a significant challenge. Traditional training approaches often struggle to balance the detailed reasoning steps with the precision of function calls, leading to suboptimal performance. To address these limitations, we introduce FunReason, a novel framework that enhances LLMs’ function calling capabilities through an automated data refinement strategy and a Self-Refinement Multiscale Loss (SRML) approach. FunReason leverages LLMs’ natural reasoning abilities to generate high-quality training examples, focusing on query parseability, reasoning coherence, and function call precision. The SRML approach dynamically balances the contribution of reasoning processes and function call accuracy during training, addressing the inherent trade-off between these two critical aspects. FunReason achieves performance comparable to GPT-4o while effectively mitigating catastrophic forgetting during fine-tuning. FunReason provides a comprehensive solution for enhancing LLMs’ function calling capabilities by introducing a balanced training methodology and a data refinement pipeline. For code and dataset, please refer to our repository at GitHub<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/BingguangHao/FunReason" title="">https://github.com/BingguangHao/FunReason</a></span></span></span>.</p>
</div>
<div class="ltx_keywords">Large Language Models, Function Calling, MultiScale Loss, Chain of Thought Reasoning, Supervised Fine-tuning
</div>
<span class="ltx_note ltx_note_frontmatter ltx_role_copyright" id="id1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">copyright: </span>acmlicensed</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_journalyear" id="id2"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">journalyear: </span>2025</span></span></span><span class="ltx_note ltx_note_frontmatter ltx_role_ccs" id="id3"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">ccs: </span>Computing methodologies Natural language processing</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1. </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The advent of Large Language Model (LLM) has ushered in a transformative era for Natural Language Processing (NLP) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib1" title="">ouyang2022training, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib2" title="">achiam2023gpt, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib3" title="">touvron2023llama, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib4" title="">bai2023qwen, </a>)</cite>, showcasing remarkable proficiency in understanding, generating, and reasoning with text <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib5" title="">zhong2024evaluation, </a>)</cite>. However, the potential of these models extends beyond mere textual manipulation. A critical frontier lies in their ability to interact dynamically with the real world through the invocation of external functions or tools, which is called <span class="ltx_text ltx_font_bold" id="S1.p1.1.1">Function Calling</span> <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib6" title="">wang2025function, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib7" title="">singh2024llm, </a>)</cite>. This feature serves as a pivotal bridge that allows LLM to seamlessly integrate with various applications <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib8" title="">topsakal2023creating, </a>)</cite> and provide contextually rich responses based on real-time information <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib9" title="">harvel2024can, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">While LLMs have demonstrated impressive reasoning capabilities through Chain-of-Thought (CoT) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib10" title="">lightman2023let, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib11" title="">guo2025deepseek, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib12" title="">team2025kimi, </a>)</cite>, effectively combining CoT with function calling remains challenging. Current approaches struggle to balance comprehensive reasoning with precise function execution, often resulting in either inaccurate calls or inconsistent reasoning steps <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib13" title="">chen2023two, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib14" title="">zhang2024xlam, </a>)</cite>. Moreover, existing function calling datasets lack the rich reasoning processes needed to train models that can both understand user intent and generate accurate function calls <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib15" title="">prabhakar2025apigen, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib16" title="">liu2024toolace, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To address these challenges, we introduce FunReason, a novel framework that enhances LLMs’ function calling capabilities through two key innovations. First, we develop an automated data refinement strategy that leverages LLMs’ natural reasoning abilities to generate high-quality training examples, focusing on query parseability, reasoning coherence, and function call precision. Second, we propose a Self-Refinement Multiscale Loss (SRML) approach that dynamically balances the contribution of reasoning processes and function call accuracy during training.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Our key contributions are summarized as follows:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We introduce <span class="ltx_text ltx_font_bold" id="S1.I1.i1.p1.1.1">FunReason</span>, an effective framework for enhancing LLMs’ function calling capabilities. At its core is a novel Self-Refinement Multiscale Loss (SRML) approach that dynamically balances the learning of reasoning processes and function call generation. Using StarCoder as our base model, we achieve performance comparable to GPT-4o while effectively mitigating catastrophic forgetting during fine-tuning <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib17" title="">kirkpatrick2017overcoming, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib18" title="">ren2024analyzing, </a>)</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We identify and address a critical challenge in function calling: traditional loss functions often overemphasize the lengthy reasoning process at the expense of function call accuracy. Our analysis reveals the inherent trade-off between reasoning quality and function call correctness, leading to the development of our balanced FunReason-SRML approach.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We develop an automated Function Call Data Refinement (FCDR) pipeline that generates high-quality training examples by evaluating query parseability, reasoning coherence, and function call precision. Using this pipeline, we create a comprehensive dataset of 60,000 samples that demonstrates broad applicability across multiple models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We discover that naturally generated Chain-of-Thought reasoning from capable models significantly outperforms manually constructed examples, highlighting the importance of leveraging LLMs’ inherent reasoning abilities for effective function calling.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our code, models, and refined dataset will be open-sourced to benefit the broader research community.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2. </span>Methodology</h2>
<figure class="ltx_figure" id="S2.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="364" id="S2.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1. </span>Overview of FunReason’s data refinement pipeline. The pipeline consists of five stages: Function Call Classification, Query <math alttext="\&amp;" class="ltx_Math" display="inline" id="S2.F1.3.m1.1"><semantics id="S2.F1.3.m1.1b"><mo id="S2.F1.3.m1.1.1" xref="S2.F1.3.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S2.F1.3.m1.1c"><and id="S2.F1.3.m1.1.1.cmml" xref="S2.F1.3.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.3.m1.1d">\&amp;</annotation><annotation encoding="application/x-llamapun" id="S2.F1.3.m1.1e">&amp;</annotation></semantics></math> Tool Identification, CoT Identification, Function <math alttext="\&amp;" class="ltx_Math" display="inline" id="S2.F1.4.m2.1"><semantics id="S2.F1.4.m2.1b"><mo id="S2.F1.4.m2.1.1" xref="S2.F1.4.m2.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S2.F1.4.m2.1c"><and id="S2.F1.4.m2.1.1.cmml" xref="S2.F1.4.m2.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S2.F1.4.m2.1d">\&amp;</annotation><annotation encoding="application/x-llamapun" id="S2.F1.4.m2.1e">&amp;</annotation></semantics></math> Parameter Identification, and Format Identification. Each stage ensures specific aspects of data quality, with failing examples either being discarded or regenerated.</figcaption>
</figure>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Effective function calling in LLMs requires both precise execution and comprehensive reasoning. However, current approaches often struggle with data quality issues and the challenge of balancing detailed Chain-of-Thought (CoT) reasoning with accurate function calls. To address these challenges, we introduce FunReason, a framework that combines an automated data refinement pipeline with a novel Self-Refinement Multiscale Loss (SRML) approach. Our method enhances function calling capabilities while maintaining the model’s original strengths through careful balancing of reasoning processes and function call generation during training.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1. </span>FunReason Data Refinement Pipeline</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The data refinement component of FunReason is designed to automatically improve the quality of function call data by leveraging LLMs. The pipeline systematically evaluates user queries and reasoning processes through a series of checks to ensure high-quality training examples.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.2">As shown in Fig  <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S2.F2" title="Figure 2 ‣ 2.1. FunReason Data Refinement Pipeline ‣ 2. Methodology ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">2</span></a>, our data refinement process involves multiple critical stages. First, we determine whether the Reference Answer constitutes a function call, with non-function calls being directly saved and excluded from further processing. For identified function calls, the Query <math alttext="\&amp;" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mo id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><and id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">\&amp;</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">&amp;</annotation></semantics></math> Tool Identification stage assesses the feasibility of resolving the query using the given tools, discarding queries where tool-based solutions are not viable. The CoT Identification stage then evaluates whether the reasoning effectively leads to the desired Reference Answer, excluding data that fails this evaluation. Following successful CoT identification, the Function <math alttext="\&amp;" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mo id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">&amp;</mo><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><and id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">\&amp;</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">&amp;</annotation></semantics></math> Parameter Identification stage verifies the accuracy of function names and parameters, including their values. If discrepancies are found, the Reference Answer undergoes regeneration. The final Format Identification stage verifies the correctness of the function call format, with formatting errors triggering Reference Answer regeneration and validated data being stored.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">As a result, we construct a new function call dataset with integrated CoT reasoning, named FunReason-SFT, using our data refinement pipeline. This synthetic dataset comprises 60,000 samples, and its effectiveness will be demonstrated across multiple mainstream models, with numerical results presented later in Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.SS1" title="3.1. Data preparation ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">3.1</span></a>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="428" id="S2.F2.g1" src="x2.png" width="831"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2. </span>FunReason’s multiscale loss architecture. The loss function separates and re-weights contributions from the reasoning process (<math alttext="L_{\text{think}}" class="ltx_Math" display="inline" id="S2.F2.3.m1.1"><semantics id="S2.F2.3.m1.1b"><msub id="S2.F2.3.m1.1.1" xref="S2.F2.3.m1.1.1.cmml"><mi id="S2.F2.3.m1.1.1.2" xref="S2.F2.3.m1.1.1.2.cmml">L</mi><mtext id="S2.F2.3.m1.1.1.3" xref="S2.F2.3.m1.1.1.3a.cmml">think</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.F2.3.m1.1c"><apply id="S2.F2.3.m1.1.1.cmml" xref="S2.F2.3.m1.1.1"><csymbol cd="ambiguous" id="S2.F2.3.m1.1.1.1.cmml" xref="S2.F2.3.m1.1.1">subscript</csymbol><ci id="S2.F2.3.m1.1.1.2.cmml" xref="S2.F2.3.m1.1.1.2">𝐿</ci><ci id="S2.F2.3.m1.1.1.3a.cmml" xref="S2.F2.3.m1.1.1.3"><mtext id="S2.F2.3.m1.1.1.3.cmml" mathsize="70%" xref="S2.F2.3.m1.1.1.3">think</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.3.m1.1d">L_{\text{think}}</annotation><annotation encoding="application/x-llamapun" id="S2.F2.3.m1.1e">italic_L start_POSTSUBSCRIPT think end_POSTSUBSCRIPT</annotation></semantics></math>) and function call generation (<math alttext="L_{\text{result}}" class="ltx_Math" display="inline" id="S2.F2.4.m2.1"><semantics id="S2.F2.4.m2.1b"><msub id="S2.F2.4.m2.1.1" xref="S2.F2.4.m2.1.1.cmml"><mi id="S2.F2.4.m2.1.1.2" xref="S2.F2.4.m2.1.1.2.cmml">L</mi><mtext id="S2.F2.4.m2.1.1.3" xref="S2.F2.4.m2.1.1.3a.cmml">result</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.F2.4.m2.1c"><apply id="S2.F2.4.m2.1.1.cmml" xref="S2.F2.4.m2.1.1"><csymbol cd="ambiguous" id="S2.F2.4.m2.1.1.1.cmml" xref="S2.F2.4.m2.1.1">subscript</csymbol><ci id="S2.F2.4.m2.1.1.2.cmml" xref="S2.F2.4.m2.1.1.2">𝐿</ci><ci id="S2.F2.4.m2.1.1.3a.cmml" xref="S2.F2.4.m2.1.1.3"><mtext id="S2.F2.4.m2.1.1.3.cmml" mathsize="70%" xref="S2.F2.4.m2.1.1.3">result</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.F2.4.m2.1d">L_{\text{result}}</annotation><annotation encoding="application/x-llamapun" id="S2.F2.4.m2.1e">italic_L start_POSTSUBSCRIPT result end_POSTSUBSCRIPT</annotation></semantics></math>), enabling balanced optimization of both components during training.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2. </span>Self-Refinement Multiscale Loss (SRML)</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.9"><span class="ltx_text ltx_font_bold" id="S2.SS2.p1.9.1">Rethinking the Objective of Fine-tuning.</span> The reasoning process of a large language model <math alttext="\mathcal{M}" class="ltx_Math" display="inline" id="S2.SS2.p1.1.m1.1"><semantics id="S2.SS2.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S2.SS2.p1.1.m1.1.1" xref="S2.SS2.p1.1.m1.1.1.cmml">ℳ</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.1.m1.1b"><ci id="S2.SS2.p1.1.m1.1.1.cmml" xref="S2.SS2.p1.1.m1.1.1">ℳ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.1.m1.1c">\mathcal{M}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.1.m1.1d">caligraphic_M</annotation></semantics></math> naturally decomposes into two components: chain-of-thought reasoning <math alttext="t" class="ltx_Math" display="inline" id="S2.SS2.p1.2.m2.1"><semantics id="S2.SS2.p1.2.m2.1a"><mi id="S2.SS2.p1.2.m2.1.1" xref="S2.SS2.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.2.m2.1b"><ci id="S2.SS2.p1.2.m2.1.1.cmml" xref="S2.SS2.p1.2.m2.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.2.m2.1d">italic_t</annotation></semantics></math> to analyze the query and the final function calling result <math alttext="f" class="ltx_Math" display="inline" id="S2.SS2.p1.3.m3.1"><semantics id="S2.SS2.p1.3.m3.1a"><mi id="S2.SS2.p1.3.m3.1.1" xref="S2.SS2.p1.3.m3.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.3.m3.1b"><ci id="S2.SS2.p1.3.m3.1.1.cmml" xref="S2.SS2.p1.3.m3.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.3.m3.1c">f</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.3.m3.1d">italic_f</annotation></semantics></math>. The token counts for these components are denoted as <math alttext="N_{t}" class="ltx_Math" display="inline" id="S2.SS2.p1.4.m4.1"><semantics id="S2.SS2.p1.4.m4.1a"><msub id="S2.SS2.p1.4.m4.1.1" xref="S2.SS2.p1.4.m4.1.1.cmml"><mi id="S2.SS2.p1.4.m4.1.1.2" xref="S2.SS2.p1.4.m4.1.1.2.cmml">N</mi><mi id="S2.SS2.p1.4.m4.1.1.3" xref="S2.SS2.p1.4.m4.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.4.m4.1b"><apply id="S2.SS2.p1.4.m4.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.4.m4.1.1.1.cmml" xref="S2.SS2.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p1.4.m4.1.1.2.cmml" xref="S2.SS2.p1.4.m4.1.1.2">𝑁</ci><ci id="S2.SS2.p1.4.m4.1.1.3.cmml" xref="S2.SS2.p1.4.m4.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.4.m4.1c">N_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.4.m4.1d">italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="N_{f}" class="ltx_Math" display="inline" id="S2.SS2.p1.5.m5.1"><semantics id="S2.SS2.p1.5.m5.1a"><msub id="S2.SS2.p1.5.m5.1.1" xref="S2.SS2.p1.5.m5.1.1.cmml"><mi id="S2.SS2.p1.5.m5.1.1.2" xref="S2.SS2.p1.5.m5.1.1.2.cmml">N</mi><mi id="S2.SS2.p1.5.m5.1.1.3" xref="S2.SS2.p1.5.m5.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.5.m5.1b"><apply id="S2.SS2.p1.5.m5.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.5.m5.1.1.1.cmml" xref="S2.SS2.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.p1.5.m5.1.1.2.cmml" xref="S2.SS2.p1.5.m5.1.1.2">𝑁</ci><ci id="S2.SS2.p1.5.m5.1.1.3.cmml" xref="S2.SS2.p1.5.m5.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.5.m5.1c">N_{f}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.5.m5.1d">italic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> respectively, with their sum <math alttext="N_{\text{all}}" class="ltx_Math" display="inline" id="S2.SS2.p1.6.m6.1"><semantics id="S2.SS2.p1.6.m6.1a"><msub id="S2.SS2.p1.6.m6.1.1" xref="S2.SS2.p1.6.m6.1.1.cmml"><mi id="S2.SS2.p1.6.m6.1.1.2" xref="S2.SS2.p1.6.m6.1.1.2.cmml">N</mi><mtext id="S2.SS2.p1.6.m6.1.1.3" xref="S2.SS2.p1.6.m6.1.1.3a.cmml">all</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.6.m6.1b"><apply id="S2.SS2.p1.6.m6.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.6.m6.1.1.1.cmml" xref="S2.SS2.p1.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p1.6.m6.1.1.2.cmml" xref="S2.SS2.p1.6.m6.1.1.2">𝑁</ci><ci id="S2.SS2.p1.6.m6.1.1.3a.cmml" xref="S2.SS2.p1.6.m6.1.1.3"><mtext id="S2.SS2.p1.6.m6.1.1.3.cmml" mathsize="70%" xref="S2.SS2.p1.6.m6.1.1.3">all</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.6.m6.1c">N_{\text{all}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.6.m6.1d">italic_N start_POSTSUBSCRIPT all end_POSTSUBSCRIPT</annotation></semantics></math>, denoting the total tokens. Based on this decomposition, we formulate an optimization objective that minimizes a weighted sum of losses across both reasoning components. Let <math alttext="L_{\text{think}}" class="ltx_Math" display="inline" id="S2.SS2.p1.7.m7.1"><semantics id="S2.SS2.p1.7.m7.1a"><msub id="S2.SS2.p1.7.m7.1.1" xref="S2.SS2.p1.7.m7.1.1.cmml"><mi id="S2.SS2.p1.7.m7.1.1.2" xref="S2.SS2.p1.7.m7.1.1.2.cmml">L</mi><mtext id="S2.SS2.p1.7.m7.1.1.3" xref="S2.SS2.p1.7.m7.1.1.3a.cmml">think</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.7.m7.1b"><apply id="S2.SS2.p1.7.m7.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.7.m7.1.1.1.cmml" xref="S2.SS2.p1.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.p1.7.m7.1.1.2.cmml" xref="S2.SS2.p1.7.m7.1.1.2">𝐿</ci><ci id="S2.SS2.p1.7.m7.1.1.3a.cmml" xref="S2.SS2.p1.7.m7.1.1.3"><mtext id="S2.SS2.p1.7.m7.1.1.3.cmml" mathsize="70%" xref="S2.SS2.p1.7.m7.1.1.3">think</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.7.m7.1c">L_{\text{think}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.7.m7.1d">italic_L start_POSTSUBSCRIPT think end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="L_{\text{result}}" class="ltx_Math" display="inline" id="S2.SS2.p1.8.m8.1"><semantics id="S2.SS2.p1.8.m8.1a"><msub id="S2.SS2.p1.8.m8.1.1" xref="S2.SS2.p1.8.m8.1.1.cmml"><mi id="S2.SS2.p1.8.m8.1.1.2" xref="S2.SS2.p1.8.m8.1.1.2.cmml">L</mi><mtext id="S2.SS2.p1.8.m8.1.1.3" xref="S2.SS2.p1.8.m8.1.1.3a.cmml">result</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.8.m8.1b"><apply id="S2.SS2.p1.8.m8.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.8.m8.1.1.1.cmml" xref="S2.SS2.p1.8.m8.1.1">subscript</csymbol><ci id="S2.SS2.p1.8.m8.1.1.2.cmml" xref="S2.SS2.p1.8.m8.1.1.2">𝐿</ci><ci id="S2.SS2.p1.8.m8.1.1.3a.cmml" xref="S2.SS2.p1.8.m8.1.1.3"><mtext id="S2.SS2.p1.8.m8.1.1.3.cmml" mathsize="70%" xref="S2.SS2.p1.8.m8.1.1.3">result</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.8.m8.1c">L_{\text{result}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.8.m8.1d">italic_L start_POSTSUBSCRIPT result end_POSTSUBSCRIPT</annotation></semantics></math> represent the average cross-entropy losses for the reasoning and result generation components, respectively. Then the training loss <math alttext="L_{\text{total}}" class="ltx_Math" display="inline" id="S2.SS2.p1.9.m9.1"><semantics id="S2.SS2.p1.9.m9.1a"><msub id="S2.SS2.p1.9.m9.1.1" xref="S2.SS2.p1.9.m9.1.1.cmml"><mi id="S2.SS2.p1.9.m9.1.1.2" xref="S2.SS2.p1.9.m9.1.1.2.cmml">L</mi><mtext id="S2.SS2.p1.9.m9.1.1.3" xref="S2.SS2.p1.9.m9.1.1.3a.cmml">total</mtext></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p1.9.m9.1b"><apply id="S2.SS2.p1.9.m9.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S2.SS2.p1.9.m9.1.1.1.cmml" xref="S2.SS2.p1.9.m9.1.1">subscript</csymbol><ci id="S2.SS2.p1.9.m9.1.1.2.cmml" xref="S2.SS2.p1.9.m9.1.1.2">𝐿</ci><ci id="S2.SS2.p1.9.m9.1.1.3a.cmml" xref="S2.SS2.p1.9.m9.1.1.3"><mtext id="S2.SS2.p1.9.m9.1.1.3.cmml" mathsize="70%" xref="S2.SS2.p1.9.m9.1.1.3">total</mtext></ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p1.9.m9.1c">L_{\text{total}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p1.9.m9.1d">italic_L start_POSTSUBSCRIPT total end_POSTSUBSCRIPT</annotation></semantics></math> can be written as follows:</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<table class="ltx_equationgroup ltx_eqn_table" id="S2.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E1X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="1"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(1)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\min_{\mathcal{M}}L_{\text{total}}" class="ltx_Math" display="inline" id="S2.E1X.2.1.1.m1.1"><semantics id="S2.E1X.2.1.1.m1.1a"><mrow id="S2.E1X.2.1.1.m1.1.1" xref="S2.E1X.2.1.1.m1.1.1.cmml"><munder id="S2.E1X.2.1.1.m1.1.1.1" xref="S2.E1X.2.1.1.m1.1.1.1.cmml"><mi id="S2.E1X.2.1.1.m1.1.1.1.2" xref="S2.E1X.2.1.1.m1.1.1.1.2.cmml">min</mi><mi class="ltx_font_mathcaligraphic" id="S2.E1X.2.1.1.m1.1.1.1.3" xref="S2.E1X.2.1.1.m1.1.1.1.3.cmml">ℳ</mi></munder><mo id="S2.E1X.2.1.1.m1.1.1a" lspace="0.167em" xref="S2.E1X.2.1.1.m1.1.1.cmml">⁡</mo><msub id="S2.E1X.2.1.1.m1.1.1.2" xref="S2.E1X.2.1.1.m1.1.1.2.cmml"><mi id="S2.E1X.2.1.1.m1.1.1.2.2" xref="S2.E1X.2.1.1.m1.1.1.2.2.cmml">L</mi><mtext id="S2.E1X.2.1.1.m1.1.1.2.3" xref="S2.E1X.2.1.1.m1.1.1.2.3a.cmml">total</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.E1X.2.1.1.m1.1b"><apply id="S2.E1X.2.1.1.m1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1"><apply id="S2.E1X.2.1.1.m1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.1.1.1.1.cmml" xref="S2.E1X.2.1.1.m1.1.1.1">subscript</csymbol><min id="S2.E1X.2.1.1.m1.1.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.2"></min><ci id="S2.E1X.2.1.1.m1.1.1.1.3.cmml" xref="S2.E1X.2.1.1.m1.1.1.1.3">ℳ</ci></apply><apply id="S2.E1X.2.1.1.m1.1.1.2.cmml" xref="S2.E1X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E1X.2.1.1.m1.1.1.2.1.cmml" xref="S2.E1X.2.1.1.m1.1.1.2">subscript</csymbol><ci id="S2.E1X.2.1.1.m1.1.1.2.2.cmml" xref="S2.E1X.2.1.1.m1.1.1.2.2">𝐿</ci><ci id="S2.E1X.2.1.1.m1.1.1.2.3a.cmml" xref="S2.E1X.2.1.1.m1.1.1.2.3"><mtext id="S2.E1X.2.1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S2.E1X.2.1.1.m1.1.1.2.3">total</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1X.2.1.1.m1.1c">\displaystyle\min_{\mathcal{M}}L_{\text{total}}</annotation><annotation encoding="application/x-llamapun" id="S2.E1X.2.1.1.m1.1d">roman_min start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT total end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\min_{\mathcal{M}}\left(\frac{\sum_{i=1}^{N_{all}}\sum_{j=1}^{V}%
-p_{ij}\log p_{ij}}{N_{all}}\right)" class="ltx_Math" display="inline" id="S2.E1X.3.2.2.m1.2"><semantics id="S2.E1X.3.2.2.m1.2a"><mrow id="S2.E1X.3.2.2.m1.2.2" xref="S2.E1X.3.2.2.m1.2.2.cmml"><mi id="S2.E1X.3.2.2.m1.2.2.3" xref="S2.E1X.3.2.2.m1.2.2.3.cmml"></mi><mo id="S2.E1X.3.2.2.m1.2.2.2" xref="S2.E1X.3.2.2.m1.2.2.2.cmml">=</mo><mrow id="S2.E1X.3.2.2.m1.2.2.1.1" xref="S2.E1X.3.2.2.m1.2.2.1.2.cmml"><munder id="S2.E1X.3.2.2.m1.2.2.1.1.1" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.cmml"><mi id="S2.E1X.3.2.2.m1.2.2.1.1.1.2" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.2.cmml">min</mi><mi class="ltx_font_mathcaligraphic" id="S2.E1X.3.2.2.m1.2.2.1.1.1.3" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.3.cmml">ℳ</mi></munder><mo id="S2.E1X.3.2.2.m1.2.2.1.1a" xref="S2.E1X.3.2.2.m1.2.2.1.2.cmml">⁡</mo><mrow id="S2.E1X.3.2.2.m1.2.2.1.1.2" xref="S2.E1X.3.2.2.m1.2.2.1.2.cmml"><mo id="S2.E1X.3.2.2.m1.2.2.1.1.2.1" xref="S2.E1X.3.2.2.m1.2.2.1.2.cmml">(</mo><mstyle displaystyle="true" id="S2.E1X.3.2.2.m1.1.1" xref="S2.E1X.3.2.2.m1.1.1.cmml"><mfrac id="S2.E1X.3.2.2.m1.1.1a" xref="S2.E1X.3.2.2.m1.1.1.cmml"><mrow id="S2.E1X.3.2.2.m1.1.1.2" xref="S2.E1X.3.2.2.m1.1.1.2.cmml"><mrow id="S2.E1X.3.2.2.m1.1.1.2.2" xref="S2.E1X.3.2.2.m1.1.1.2.2.cmml"><msubsup id="S2.E1X.3.2.2.m1.1.1.2.2.1" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.cmml"><mo id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.2" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.2.cmml">∑</mo><mrow id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.2" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.2.cmml">i</mi><mo id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.1" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.1.cmml">=</mo><mn id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.3" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.3.cmml">1</mn></mrow><msub id="S2.E1X.3.2.2.m1.1.1.2.2.1.3" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.2" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.2.cmml">N</mi><mrow id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.2" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.2.cmml">a</mi><mo id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.1" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.1.cmml">⁢</mo><mi id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.3" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.3.cmml">l</mi><mo id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.1a" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.1.cmml">⁢</mo><mi id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.4" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.4.cmml">l</mi></mrow></msub></msubsup><msubsup id="S2.E1X.3.2.2.m1.1.1.2.2.2" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.cmml"><mo id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.2" lspace="0.167em" rspace="0em" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.2.cmml">∑</mo><mrow id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.2" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.2.cmml">j</mi><mo id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.1" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.1.cmml">=</mo><mn id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.3" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E1X.3.2.2.m1.1.1.2.2.2.3" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.3.cmml">V</mi></msubsup></mrow><mo id="S2.E1X.3.2.2.m1.1.1.2.1" lspace="0em" xref="S2.E1X.3.2.2.m1.1.1.2.1.cmml">−</mo><mrow id="S2.E1X.3.2.2.m1.1.1.2.3" xref="S2.E1X.3.2.2.m1.1.1.2.3.cmml"><msub id="S2.E1X.3.2.2.m1.1.1.2.3.2" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.3.2.2" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.2.cmml">p</mi><mrow id="S2.E1X.3.2.2.m1.1.1.2.3.2.3" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.3.2.3.2" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.3.2.cmml">i</mi><mo id="S2.E1X.3.2.2.m1.1.1.2.3.2.3.1" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.3.1.cmml">⁢</mo><mi id="S2.E1X.3.2.2.m1.1.1.2.3.2.3.3" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.3.3.cmml">j</mi></mrow></msub><mo id="S2.E1X.3.2.2.m1.1.1.2.3.1" lspace="0.167em" xref="S2.E1X.3.2.2.m1.1.1.2.3.1.cmml">⁢</mo><mrow id="S2.E1X.3.2.2.m1.1.1.2.3.3" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.3.3.1" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.1.cmml">log</mi><mo id="S2.E1X.3.2.2.m1.1.1.2.3.3a" lspace="0.167em" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.cmml">⁡</mo><msub id="S2.E1X.3.2.2.m1.1.1.2.3.3.2" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.2" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.2.cmml">p</mi><mrow id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.2" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.2.cmml">i</mi><mo id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.1" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.1.cmml">⁢</mo><mi id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.3" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.3.cmml">j</mi></mrow></msub></mrow></mrow></mrow><msub id="S2.E1X.3.2.2.m1.1.1.3" xref="S2.E1X.3.2.2.m1.1.1.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.3.2" xref="S2.E1X.3.2.2.m1.1.1.3.2.cmml">N</mi><mrow id="S2.E1X.3.2.2.m1.1.1.3.3" xref="S2.E1X.3.2.2.m1.1.1.3.3.cmml"><mi id="S2.E1X.3.2.2.m1.1.1.3.3.2" xref="S2.E1X.3.2.2.m1.1.1.3.3.2.cmml">a</mi><mo id="S2.E1X.3.2.2.m1.1.1.3.3.1" xref="S2.E1X.3.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.E1X.3.2.2.m1.1.1.3.3.3" xref="S2.E1X.3.2.2.m1.1.1.3.3.3.cmml">l</mi><mo id="S2.E1X.3.2.2.m1.1.1.3.3.1a" xref="S2.E1X.3.2.2.m1.1.1.3.3.1.cmml">⁢</mo><mi id="S2.E1X.3.2.2.m1.1.1.3.3.4" xref="S2.E1X.3.2.2.m1.1.1.3.3.4.cmml">l</mi></mrow></msub></mfrac></mstyle><mo id="S2.E1X.3.2.2.m1.2.2.1.1.2.2" xref="S2.E1X.3.2.2.m1.2.2.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1X.3.2.2.m1.2b"><apply id="S2.E1X.3.2.2.m1.2.2.cmml" xref="S2.E1X.3.2.2.m1.2.2"><eq id="S2.E1X.3.2.2.m1.2.2.2.cmml" xref="S2.E1X.3.2.2.m1.2.2.2"></eq><csymbol cd="latexml" id="S2.E1X.3.2.2.m1.2.2.3.cmml" xref="S2.E1X.3.2.2.m1.2.2.3">absent</csymbol><apply id="S2.E1X.3.2.2.m1.2.2.1.2.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1"><apply id="S2.E1X.3.2.2.m1.2.2.1.1.1.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.2.2.1.1.1.1.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1.1">subscript</csymbol><min id="S2.E1X.3.2.2.m1.2.2.1.1.1.2.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.2"></min><ci id="S2.E1X.3.2.2.m1.2.2.1.1.1.3.cmml" xref="S2.E1X.3.2.2.m1.2.2.1.1.1.3">ℳ</ci></apply><apply id="S2.E1X.3.2.2.m1.1.1.cmml" xref="S2.E1X.3.2.2.m1.1.1"><divide id="S2.E1X.3.2.2.m1.1.1.1.cmml" xref="S2.E1X.3.2.2.m1.1.1"></divide><apply id="S2.E1X.3.2.2.m1.1.1.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2"><minus id="S2.E1X.3.2.2.m1.1.1.2.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.1"></minus><apply id="S2.E1X.3.2.2.m1.1.1.2.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2"><apply id="S2.E1X.3.2.2.m1.1.1.2.2.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.1.1.2.2.1.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1">superscript</csymbol><apply id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1">subscript</csymbol><sum id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.2"></sum><apply id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3"><eq id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.1"></eq><ci id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.2">𝑖</ci><cn id="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.3.cmml" type="integer" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.2.3.3">1</cn></apply></apply><apply id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3">subscript</csymbol><ci id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.2">𝑁</ci><apply id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3"><times id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.1"></times><ci id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.2">𝑎</ci><ci id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.3">𝑙</ci><ci id="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.4.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.1.3.3.4">𝑙</ci></apply></apply></apply><apply id="S2.E1X.3.2.2.m1.1.1.2.2.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.1.1.2.2.2.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2">superscript</csymbol><apply id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2">subscript</csymbol><sum id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.2"></sum><apply id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3"><eq id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.1"></eq><ci id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.2">𝑗</ci><cn id="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.3.cmml" type="integer" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.2.3.3">1</cn></apply></apply><ci id="S2.E1X.3.2.2.m1.1.1.2.2.2.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.2.2.3">𝑉</ci></apply></apply><apply id="S2.E1X.3.2.2.m1.1.1.2.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3"><times id="S2.E1X.3.2.2.m1.1.1.2.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.1"></times><apply id="S2.E1X.3.2.2.m1.1.1.2.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.1.1.2.3.2.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.2">subscript</csymbol><ci id="S2.E1X.3.2.2.m1.1.1.2.3.2.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.2">𝑝</ci><apply id="S2.E1X.3.2.2.m1.1.1.2.3.2.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.3"><times id="S2.E1X.3.2.2.m1.1.1.2.3.2.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.3.1"></times><ci id="S2.E1X.3.2.2.m1.1.1.2.3.2.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.3.2">𝑖</ci><ci id="S2.E1X.3.2.2.m1.1.1.2.3.2.3.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.2.3.3">𝑗</ci></apply></apply><apply id="S2.E1X.3.2.2.m1.1.1.2.3.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3"><log id="S2.E1X.3.2.2.m1.1.1.2.3.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.1"></log><apply id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2">subscript</csymbol><ci id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.2">𝑝</ci><apply id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3"><times id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.1"></times><ci id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.2">𝑖</ci><ci id="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.2.3.3.2.3.3">𝑗</ci></apply></apply></apply></apply></apply><apply id="S2.E1X.3.2.2.m1.1.1.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.3"><csymbol cd="ambiguous" id="S2.E1X.3.2.2.m1.1.1.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.3">subscript</csymbol><ci id="S2.E1X.3.2.2.m1.1.1.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.3.2">𝑁</ci><apply id="S2.E1X.3.2.2.m1.1.1.3.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.3.3"><times id="S2.E1X.3.2.2.m1.1.1.3.3.1.cmml" xref="S2.E1X.3.2.2.m1.1.1.3.3.1"></times><ci id="S2.E1X.3.2.2.m1.1.1.3.3.2.cmml" xref="S2.E1X.3.2.2.m1.1.1.3.3.2">𝑎</ci><ci id="S2.E1X.3.2.2.m1.1.1.3.3.3.cmml" xref="S2.E1X.3.2.2.m1.1.1.3.3.3">𝑙</ci><ci id="S2.E1X.3.2.2.m1.1.1.3.3.4.cmml" xref="S2.E1X.3.2.2.m1.1.1.3.3.4">𝑙</ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1X.3.2.2.m1.2c">\displaystyle=\min_{\mathcal{M}}\left(\frac{\sum_{i=1}^{N_{all}}\sum_{j=1}^{V}%
-p_{ij}\log p_{ij}}{N_{all}}\right)</annotation><annotation encoding="application/x-llamapun" id="S2.E1X.3.2.2.m1.2d">= roman_min start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT ( divide start_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_a italic_l italic_l end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT - italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_a italic_l italic_l end_POSTSUBSCRIPT end_ARG )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">This loss can be decomposed into two parts: reasoning loss and function call loss:</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<table class="ltx_equationgroup ltx_eqn_table" id="S2.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E2X">
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_left" rowspan="2"><span class="ltx_tag ltx_tag_equationgroup ltx_align_left">(2)</span></td>
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle\min_{\mathcal{M}}L_{\text{total}}" class="ltx_Math" display="inline" id="S2.E2X.2.1.1.m1.1"><semantics id="S2.E2X.2.1.1.m1.1a"><mrow id="S2.E2X.2.1.1.m1.1.1" xref="S2.E2X.2.1.1.m1.1.1.cmml"><munder id="S2.E2X.2.1.1.m1.1.1.1" xref="S2.E2X.2.1.1.m1.1.1.1.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.1.2" xref="S2.E2X.2.1.1.m1.1.1.1.2.cmml">min</mi><mi class="ltx_font_mathcaligraphic" id="S2.E2X.2.1.1.m1.1.1.1.3" xref="S2.E2X.2.1.1.m1.1.1.1.3.cmml">ℳ</mi></munder><mo id="S2.E2X.2.1.1.m1.1.1a" lspace="0.167em" xref="S2.E2X.2.1.1.m1.1.1.cmml">⁡</mo><msub id="S2.E2X.2.1.1.m1.1.1.2" xref="S2.E2X.2.1.1.m1.1.1.2.cmml"><mi id="S2.E2X.2.1.1.m1.1.1.2.2" xref="S2.E2X.2.1.1.m1.1.1.2.2.cmml">L</mi><mtext id="S2.E2X.2.1.1.m1.1.1.2.3" xref="S2.E2X.2.1.1.m1.1.1.2.3a.cmml">total</mtext></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.E2X.2.1.1.m1.1b"><apply id="S2.E2X.2.1.1.m1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1"><apply id="S2.E2X.2.1.1.m1.1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.1"><csymbol cd="ambiguous" id="S2.E2X.2.1.1.m1.1.1.1.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.1">subscript</csymbol><min id="S2.E2X.2.1.1.m1.1.1.1.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.2"></min><ci id="S2.E2X.2.1.1.m1.1.1.1.3.cmml" xref="S2.E2X.2.1.1.m1.1.1.1.3">ℳ</ci></apply><apply id="S2.E2X.2.1.1.m1.1.1.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.E2X.2.1.1.m1.1.1.2.1.cmml" xref="S2.E2X.2.1.1.m1.1.1.2">subscript</csymbol><ci id="S2.E2X.2.1.1.m1.1.1.2.2.cmml" xref="S2.E2X.2.1.1.m1.1.1.2.2">𝐿</ci><ci id="S2.E2X.2.1.1.m1.1.1.2.3a.cmml" xref="S2.E2X.2.1.1.m1.1.1.2.3"><mtext id="S2.E2X.2.1.1.m1.1.1.2.3.cmml" mathsize="70%" xref="S2.E2X.2.1.1.m1.1.1.2.3">total</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2X.2.1.1.m1.1c">\displaystyle\min_{\mathcal{M}}L_{\text{total}}</annotation><annotation encoding="application/x-llamapun" id="S2.E2X.2.1.1.m1.1d">roman_min start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT italic_L start_POSTSUBSCRIPT total end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\min_{\mathcal{M}}\left(\frac{N_{t}}{N_{\text{all}}}\cdot\frac{1%
}{N_{t}}\sum_{i=1}^{N_{t}}\sum_{j=1}^{V}-p_{ij}\log p_{ij}+\frac{N_{f}}{N_{%
\text{all}}}\cdot\frac{1}{N_{f}}\sum_{i=N_{t}+1}^{N_{t}+N_{f}}\sum_{j=1}^{V}-p%
_{ij}\log p_{ij}\right)" class="ltx_Math" display="inline" id="S2.E2X.3.2.2.m1.2"><semantics id="S2.E2X.3.2.2.m1.2a"><mrow id="S2.E2X.3.2.2.m1.2.2" xref="S2.E2X.3.2.2.m1.2.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.4" xref="S2.E2X.3.2.2.m1.2.2.4.cmml"></mi><mo id="S2.E2X.3.2.2.m1.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.3.cmml">=</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.3.cmml"><munder id="S2.E2X.3.2.2.m1.1.1.1.1.1" xref="S2.E2X.3.2.2.m1.1.1.1.1.1.cmml"><mi id="S2.E2X.3.2.2.m1.1.1.1.1.1.2" xref="S2.E2X.3.2.2.m1.1.1.1.1.1.2.cmml">min</mi><mi class="ltx_font_mathcaligraphic" id="S2.E2X.3.2.2.m1.1.1.1.1.1.3" xref="S2.E2X.3.2.2.m1.1.1.1.1.1.3.cmml">ℳ</mi></munder><mo id="S2.E2X.3.2.2.m1.2.2.2.2a" xref="S2.E2X.3.2.2.m1.2.2.2.3.cmml">⁡</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.3.cmml"><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.3.cmml">(</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.cmml"><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.cmml"><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.cmml"><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.cmml"><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.cmml"><mstyle displaystyle="true" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.cmml"><mfrac id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2a" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.cmml"><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.2.cmml">N</mi><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.3.cmml">t</mi></msub><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.2.cmml">N</mi><mtext id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.3a.cmml">all</mtext></msub></mfrac></mstyle><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.1.cmml">⋅</mo><mstyle displaystyle="true" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.cmml"><mfrac id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3a" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.cmml"><mn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.2.cmml">1</mn><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.2.cmml">N</mi><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.3.cmml">t</mi></msub></mfrac></mstyle></mrow><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.1.cmml">⁢</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.cmml"><mstyle displaystyle="true" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.cmml"><munderover id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1a" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.cmml"><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.2" movablelimits="false" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.2.cmml">∑</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.2.cmml">i</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.1.cmml">=</mo><mn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.3.cmml">1</mn></mrow><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.2.cmml">N</mi><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.3.cmml">t</mi></msub></munderover></mstyle><mstyle displaystyle="true" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.cmml"><munderover id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2a" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.cmml"><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.2" movablelimits="false" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.2.cmml">∑</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.2.cmml">j</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.1.cmml">=</mo><mn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.3.cmml">V</mi></munderover></mstyle></mrow></mrow><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.1.cmml">−</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.cmml"><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.2.cmml">p</mi><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.2.cmml">i</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.1.cmml">⁢</mo><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.3.cmml">j</mi></mrow></msub><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.1" lspace="0.167em" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.1.cmml">⁢</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.1.cmml">log</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3a" lspace="0.167em" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.cmml">⁡</mo><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.2.cmml">p</mi><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.2.cmml">i</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.1.cmml">⁢</mo><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.3.cmml">j</mi></mrow></msub></mrow></mrow></mrow><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.1.cmml">+</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.cmml"><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.cmml"><mstyle displaystyle="true" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.cmml"><mfrac id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2a" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.cmml"><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.2.cmml">N</mi><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.3.cmml">f</mi></msub><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.2.cmml">N</mi><mtext id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.3a.cmml">all</mtext></msub></mfrac></mstyle><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.1.cmml">⋅</mo><mstyle displaystyle="true" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.cmml"><mfrac id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3a" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.cmml"><mn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.2.cmml">1</mn><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.2.cmml">N</mi><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.3.cmml">f</mi></msub></mfrac></mstyle></mrow><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.1.cmml">⁢</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.cmml"><mstyle displaystyle="true" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.cmml"><munderover id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1a" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.cmml"><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.2" movablelimits="false" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.2.cmml">∑</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.2.cmml">i</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.1.cmml">=</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.cmml"><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.2.cmml">N</mi><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.3.cmml">t</mi></msub><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.1.cmml">+</mo><mn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.3.cmml">1</mn></mrow></mrow><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.cmml"><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.2.cmml">N</mi><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.3.cmml">t</mi></msub><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.1.cmml">+</mo><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.2.cmml">N</mi><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.3.cmml">f</mi></msub></mrow></munderover></mstyle><mstyle displaystyle="true" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.cmml"><munderover id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2a" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.cmml"><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.2" movablelimits="false" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.2.cmml">∑</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.2.cmml">j</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.1.cmml">=</mo><mn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.3.cmml">1</mn></mrow><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.3.cmml">V</mi></munderover></mstyle></mrow></mrow></mrow><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.1.cmml">−</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.cmml"><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.2.cmml">p</mi><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.2.cmml">i</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.1.cmml">⁢</mo><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.3.cmml">j</mi></mrow></msub><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.1" lspace="0.167em" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.1.cmml">⁢</mo><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.1.cmml">log</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3a" lspace="0.167em" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.cmml">⁡</mo><msub id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.2.cmml">p</mi><mrow id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.cmml"><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.2" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.2.cmml">i</mi><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.1" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.1.cmml">⁢</mo><mi id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.3" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.3.cmml">j</mi></mrow></msub></mrow></mrow></mrow><mo id="S2.E2X.3.2.2.m1.2.2.2.2.2.3" xref="S2.E2X.3.2.2.m1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2X.3.2.2.m1.2b"><apply id="S2.E2X.3.2.2.m1.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2"><eq id="S2.E2X.3.2.2.m1.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.3"></eq><csymbol cd="latexml" id="S2.E2X.3.2.2.m1.2.2.4.cmml" xref="S2.E2X.3.2.2.m1.2.2.4">absent</csymbol><apply id="S2.E2X.3.2.2.m1.2.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2"><apply id="S2.E2X.3.2.2.m1.1.1.1.1.1.cmml" xref="S2.E2X.3.2.2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.1.1.1.1.1.1.cmml" xref="S2.E2X.3.2.2.m1.1.1.1.1.1">subscript</csymbol><min id="S2.E2X.3.2.2.m1.1.1.1.1.1.2.cmml" xref="S2.E2X.3.2.2.m1.1.1.1.1.1.2"></min><ci id="S2.E2X.3.2.2.m1.1.1.1.1.1.3.cmml" xref="S2.E2X.3.2.2.m1.1.1.1.1.1.3">ℳ</ci></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1"><minus id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.1"></minus><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2"><plus id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.1"></plus><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2"><minus id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.1"></minus><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2"><times id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.1"></times><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2"><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.1">⋅</ci><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2"><divide id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2"></divide><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.2.3">𝑡</ci></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.3a.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.3"><mtext id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.3.cmml" mathsize="70%" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.2.3.3">all</mtext></ci></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3"><divide id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3"></divide><cn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.2.cmml" type="integer" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.2">1</cn><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.2.3.3.3">𝑡</ci></apply></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3"><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1">superscript</csymbol><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1">subscript</csymbol><sum id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.2"></sum><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3"><eq id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.1"></eq><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.2">𝑖</ci><cn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.3.cmml" type="integer" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.2.3.3">1</cn></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.1.3.3">𝑡</ci></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2">superscript</csymbol><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2">subscript</csymbol><sum id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.2"></sum><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3"><eq id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.1"></eq><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.2">𝑗</ci><cn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.3.cmml" type="integer" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.2.3.3">1</cn></apply></apply><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.2.3.2.3">𝑉</ci></apply></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3"><times id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.1"></times><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.2">𝑝</ci><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3"><times id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.1"></times><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.2">𝑖</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.2.3.3">𝑗</ci></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3"><log id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.1"></log><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.2">𝑝</ci><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3"><times id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.1"></times><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.2">𝑖</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.2.3.3.2.3.3">𝑗</ci></apply></apply></apply></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3"><times id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.1"></times><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2"><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.1">⋅</ci><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2"><divide id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2"></divide><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.2.3">𝑓</ci></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.3a.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.3"><mtext id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.3.cmml" mathsize="70%" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.2.3.3">all</mtext></ci></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3"><divide id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3"></divide><cn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.2.cmml" type="integer" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.2">1</cn><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.2.3.3.3">𝑓</ci></apply></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3"><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1">superscript</csymbol><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1">subscript</csymbol><sum id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.2"></sum><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3"><eq id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.1"></eq><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.2">𝑖</ci><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3"><plus id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.1"></plus><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.2.3">𝑡</ci></apply><cn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.3.cmml" type="integer" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.2.3.3.3">1</cn></apply></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3"><plus id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.1"></plus><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.2.3">𝑡</ci></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.2">𝑁</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.1.3.3.3">𝑓</ci></apply></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2">superscript</csymbol><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2">subscript</csymbol><sum id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.2"></sum><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3"><eq id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.1"></eq><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.2">𝑗</ci><cn id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.3.cmml" type="integer" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.2.3.3">1</cn></apply></apply><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.2.3.3.2.3">𝑉</ci></apply></apply></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3"><times id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.1"></times><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.2">𝑝</ci><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3"><times id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.1"></times><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.2">𝑖</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.2.3.3">𝑗</ci></apply></apply><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3"><log id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.1"></log><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2"><csymbol cd="ambiguous" id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2">subscript</csymbol><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.2">𝑝</ci><apply id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3"><times id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.1.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.1"></times><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.2.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.2">𝑖</ci><ci id="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.3.cmml" xref="S2.E2X.3.2.2.m1.2.2.2.2.2.1.3.3.2.3.3">𝑗</ci></apply></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2X.3.2.2.m1.2c">\displaystyle=\min_{\mathcal{M}}\left(\frac{N_{t}}{N_{\text{all}}}\cdot\frac{1%
}{N_{t}}\sum_{i=1}^{N_{t}}\sum_{j=1}^{V}-p_{ij}\log p_{ij}+\frac{N_{f}}{N_{%
\text{all}}}\cdot\frac{1}{N_{f}}\sum_{i=N_{t}+1}^{N_{t}+N_{f}}\sum_{j=1}^{V}-p%
_{ij}\log p_{ij}\right)</annotation><annotation encoding="application/x-llamapun" id="S2.E2X.3.2.2.m1.2d">= roman_min start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT ( divide start_ARG italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT all end_POSTSUBSCRIPT end_ARG ⋅ divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT - italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT + divide start_ARG italic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT all end_POSTSUBSCRIPT end_ARG ⋅ divide start_ARG 1 end_ARG start_ARG italic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG ∑ start_POSTSUBSCRIPT italic_i = italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_POSTSUPERSCRIPT ∑ start_POSTSUBSCRIPT italic_j = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT - italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT roman_log italic_p start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S2.E2Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=\min_{\mathcal{M}}\left(w_{t}\cdot L_{\text{think}}+w_{f}\cdot L%
_{\text{result}}\right)" class="ltx_Math" display="inline" id="S2.E2Xa.2.1.1.m1.2"><semantics id="S2.E2Xa.2.1.1.m1.2a"><mrow id="S2.E2Xa.2.1.1.m1.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.4" xref="S2.E2Xa.2.1.1.m1.2.2.4.cmml"></mi><mo id="S2.E2Xa.2.1.1.m1.2.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.3.cmml">=</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml"><munder id="S2.E2Xa.2.1.1.m1.1.1.1.1.1" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.2" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.2.cmml">min</mi><mi class="ltx_font_mathcaligraphic" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.3" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.3.cmml">ℳ</mi></munder><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2a" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">⁡</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml"><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">(</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.cmml"><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.cmml"><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.2.cmml">w</mi><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.3.cmml">t</mi></msub><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.1" lspace="0.222em" rspace="0.222em" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.1.cmml">⋅</mo><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.2.cmml">L</mi><mtext id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.3a.cmml">think</mtext></msub></mrow><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.1" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.1.cmml">+</mo><mrow id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.cmml"><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.2.cmml">w</mi><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.3.cmml">f</mi></msub><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.1" lspace="0.222em" rspace="0.222em" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.1.cmml">⋅</mo><msub id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.cmml"><mi id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.2" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.2.cmml">L</mi><mtext id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.3a.cmml">result</mtext></msub></mrow></mrow><mo id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.3" xref="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2Xa.2.1.1.m1.2b"><apply id="S2.E2Xa.2.1.1.m1.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2"><eq id="S2.E2Xa.2.1.1.m1.2.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.3"></eq><csymbol cd="latexml" id="S2.E2Xa.2.1.1.m1.2.2.4.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.4">absent</csymbol><apply id="S2.E2Xa.2.1.1.m1.2.2.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2"><apply id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1">subscript</csymbol><min id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.2"></min><ci id="S2.E2Xa.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S2.E2Xa.2.1.1.m1.1.1.1.1.1.3">ℳ</ci></apply><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1"><plus id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.1"></plus><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2"><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.1">⋅</ci><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.2">𝑤</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.2.3">𝑡</ci></apply><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.2">𝐿</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.3a.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.3"><mtext id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.3.cmml" mathsize="70%" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.2.3.3">think</mtext></ci></apply></apply><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3"><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.1">⋅</ci><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.2">𝑤</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.2.3">𝑓</ci></apply><apply id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3"><csymbol cd="ambiguous" id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.1.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3">subscript</csymbol><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.2.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.2">𝐿</ci><ci id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.3a.cmml" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.3"><mtext id="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.3.cmml" mathsize="70%" xref="S2.E2Xa.2.1.1.m1.2.2.2.2.2.1.3.3.3">result</mtext></ci></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2Xa.2.1.1.m1.2c">\displaystyle=\min_{\mathcal{M}}\left(w_{t}\cdot L_{\text{think}}+w_{f}\cdot L%
_{\text{result}}\right)</annotation><annotation encoding="application/x-llamapun" id="S2.E2Xa.2.1.1.m1.2d">= roman_min start_POSTSUBSCRIPT caligraphic_M end_POSTSUBSCRIPT ( italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ⋅ italic_L start_POSTSUBSCRIPT think end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT ⋅ italic_L start_POSTSUBSCRIPT result end_POSTSUBSCRIPT )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.7">where <math alttext="w_{t}=\frac{N_{t}}{N_{\text{all}}},w_{f}=\frac{N_{f}}{N_{\text{all}}}" class="ltx_Math" display="inline" id="S2.SS2.p5.1.m1.2"><semantics id="S2.SS2.p5.1.m1.2a"><mrow id="S2.SS2.p5.1.m1.2.2.2" xref="S2.SS2.p5.1.m1.2.2.3.cmml"><mrow id="S2.SS2.p5.1.m1.1.1.1.1" xref="S2.SS2.p5.1.m1.1.1.1.1.cmml"><msub id="S2.SS2.p5.1.m1.1.1.1.1.2" xref="S2.SS2.p5.1.m1.1.1.1.1.2.cmml"><mi id="S2.SS2.p5.1.m1.1.1.1.1.2.2" xref="S2.SS2.p5.1.m1.1.1.1.1.2.2.cmml">w</mi><mi id="S2.SS2.p5.1.m1.1.1.1.1.2.3" xref="S2.SS2.p5.1.m1.1.1.1.1.2.3.cmml">t</mi></msub><mo id="S2.SS2.p5.1.m1.1.1.1.1.1" xref="S2.SS2.p5.1.m1.1.1.1.1.1.cmml">=</mo><mfrac id="S2.SS2.p5.1.m1.1.1.1.1.3" xref="S2.SS2.p5.1.m1.1.1.1.1.3.cmml"><msub id="S2.SS2.p5.1.m1.1.1.1.1.3.2" xref="S2.SS2.p5.1.m1.1.1.1.1.3.2.cmml"><mi id="S2.SS2.p5.1.m1.1.1.1.1.3.2.2" xref="S2.SS2.p5.1.m1.1.1.1.1.3.2.2.cmml">N</mi><mi id="S2.SS2.p5.1.m1.1.1.1.1.3.2.3" xref="S2.SS2.p5.1.m1.1.1.1.1.3.2.3.cmml">t</mi></msub><msub id="S2.SS2.p5.1.m1.1.1.1.1.3.3" xref="S2.SS2.p5.1.m1.1.1.1.1.3.3.cmml"><mi id="S2.SS2.p5.1.m1.1.1.1.1.3.3.2" xref="S2.SS2.p5.1.m1.1.1.1.1.3.3.2.cmml">N</mi><mtext id="S2.SS2.p5.1.m1.1.1.1.1.3.3.3" xref="S2.SS2.p5.1.m1.1.1.1.1.3.3.3a.cmml">all</mtext></msub></mfrac></mrow><mo id="S2.SS2.p5.1.m1.2.2.2.3" xref="S2.SS2.p5.1.m1.2.2.3a.cmml">,</mo><mrow id="S2.SS2.p5.1.m1.2.2.2.2" xref="S2.SS2.p5.1.m1.2.2.2.2.cmml"><msub id="S2.SS2.p5.1.m1.2.2.2.2.2" xref="S2.SS2.p5.1.m1.2.2.2.2.2.cmml"><mi id="S2.SS2.p5.1.m1.2.2.2.2.2.2" xref="S2.SS2.p5.1.m1.2.2.2.2.2.2.cmml">w</mi><mi id="S2.SS2.p5.1.m1.2.2.2.2.2.3" xref="S2.SS2.p5.1.m1.2.2.2.2.2.3.cmml">f</mi></msub><mo id="S2.SS2.p5.1.m1.2.2.2.2.1" xref="S2.SS2.p5.1.m1.2.2.2.2.1.cmml">=</mo><mfrac id="S2.SS2.p5.1.m1.2.2.2.2.3" xref="S2.SS2.p5.1.m1.2.2.2.2.3.cmml"><msub id="S2.SS2.p5.1.m1.2.2.2.2.3.2" xref="S2.SS2.p5.1.m1.2.2.2.2.3.2.cmml"><mi id="S2.SS2.p5.1.m1.2.2.2.2.3.2.2" xref="S2.SS2.p5.1.m1.2.2.2.2.3.2.2.cmml">N</mi><mi id="S2.SS2.p5.1.m1.2.2.2.2.3.2.3" xref="S2.SS2.p5.1.m1.2.2.2.2.3.2.3.cmml">f</mi></msub><msub id="S2.SS2.p5.1.m1.2.2.2.2.3.3" xref="S2.SS2.p5.1.m1.2.2.2.2.3.3.cmml"><mi id="S2.SS2.p5.1.m1.2.2.2.2.3.3.2" xref="S2.SS2.p5.1.m1.2.2.2.2.3.3.2.cmml">N</mi><mtext id="S2.SS2.p5.1.m1.2.2.2.2.3.3.3" xref="S2.SS2.p5.1.m1.2.2.2.2.3.3.3a.cmml">all</mtext></msub></mfrac></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.1.m1.2b"><apply id="S2.SS2.p5.1.m1.2.2.3.cmml" xref="S2.SS2.p5.1.m1.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.2.2.3a.cmml" xref="S2.SS2.p5.1.m1.2.2.2.3">formulae-sequence</csymbol><apply id="S2.SS2.p5.1.m1.1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1"><eq id="S2.SS2.p5.1.m1.1.1.1.1.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.1"></eq><apply id="S2.SS2.p5.1.m1.1.1.1.1.2.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.1.1.1.1.2.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.2">subscript</csymbol><ci id="S2.SS2.p5.1.m1.1.1.1.1.2.2.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.2.2">𝑤</ci><ci id="S2.SS2.p5.1.m1.1.1.1.1.2.3.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.2.3">𝑡</ci></apply><apply id="S2.SS2.p5.1.m1.1.1.1.1.3.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3"><divide id="S2.SS2.p5.1.m1.1.1.1.1.3.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3"></divide><apply id="S2.SS2.p5.1.m1.1.1.1.1.3.2.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3.2"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.1.1.1.1.3.2.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3.2">subscript</csymbol><ci id="S2.SS2.p5.1.m1.1.1.1.1.3.2.2.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3.2.2">𝑁</ci><ci id="S2.SS2.p5.1.m1.1.1.1.1.3.2.3.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3.2.3">𝑡</ci></apply><apply id="S2.SS2.p5.1.m1.1.1.1.1.3.3.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3.3"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.1.1.1.1.3.3.1.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3.3">subscript</csymbol><ci id="S2.SS2.p5.1.m1.1.1.1.1.3.3.2.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3.3.2">𝑁</ci><ci id="S2.SS2.p5.1.m1.1.1.1.1.3.3.3a.cmml" xref="S2.SS2.p5.1.m1.1.1.1.1.3.3.3"><mtext id="S2.SS2.p5.1.m1.1.1.1.1.3.3.3.cmml" mathsize="50%" xref="S2.SS2.p5.1.m1.1.1.1.1.3.3.3">all</mtext></ci></apply></apply></apply><apply id="S2.SS2.p5.1.m1.2.2.2.2.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2"><eq id="S2.SS2.p5.1.m1.2.2.2.2.1.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.1"></eq><apply id="S2.SS2.p5.1.m1.2.2.2.2.2.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.2.2.2.2.2.1.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S2.SS2.p5.1.m1.2.2.2.2.2.2.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.2.2">𝑤</ci><ci id="S2.SS2.p5.1.m1.2.2.2.2.2.3.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.2.3">𝑓</ci></apply><apply id="S2.SS2.p5.1.m1.2.2.2.2.3.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3"><divide id="S2.SS2.p5.1.m1.2.2.2.2.3.1.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3"></divide><apply id="S2.SS2.p5.1.m1.2.2.2.2.3.2.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3.2"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.2.2.2.2.3.2.1.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3.2">subscript</csymbol><ci id="S2.SS2.p5.1.m1.2.2.2.2.3.2.2.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3.2.2">𝑁</ci><ci id="S2.SS2.p5.1.m1.2.2.2.2.3.2.3.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3.2.3">𝑓</ci></apply><apply id="S2.SS2.p5.1.m1.2.2.2.2.3.3.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3.3"><csymbol cd="ambiguous" id="S2.SS2.p5.1.m1.2.2.2.2.3.3.1.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3.3">subscript</csymbol><ci id="S2.SS2.p5.1.m1.2.2.2.2.3.3.2.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3.3.2">𝑁</ci><ci id="S2.SS2.p5.1.m1.2.2.2.2.3.3.3a.cmml" xref="S2.SS2.p5.1.m1.2.2.2.2.3.3.3"><mtext id="S2.SS2.p5.1.m1.2.2.2.2.3.3.3.cmml" mathsize="50%" xref="S2.SS2.p5.1.m1.2.2.2.2.3.3.3">all</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.1.m1.2c">w_{t}=\frac{N_{t}}{N_{\text{all}}},w_{f}=\frac{N_{f}}{N_{\text{all}}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.1.m1.2d">italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT = divide start_ARG italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT all end_POSTSUBSCRIPT end_ARG , italic_w start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = divide start_ARG italic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT end_ARG start_ARG italic_N start_POSTSUBSCRIPT all end_POSTSUBSCRIPT end_ARG</annotation></semantics></math>, thus <math alttext="w_{t}+w_{f}=1" class="ltx_Math" display="inline" id="S2.SS2.p5.2.m2.1"><semantics id="S2.SS2.p5.2.m2.1a"><mrow id="S2.SS2.p5.2.m2.1.1" xref="S2.SS2.p5.2.m2.1.1.cmml"><mrow id="S2.SS2.p5.2.m2.1.1.2" xref="S2.SS2.p5.2.m2.1.1.2.cmml"><msub id="S2.SS2.p5.2.m2.1.1.2.2" xref="S2.SS2.p5.2.m2.1.1.2.2.cmml"><mi id="S2.SS2.p5.2.m2.1.1.2.2.2" xref="S2.SS2.p5.2.m2.1.1.2.2.2.cmml">w</mi><mi id="S2.SS2.p5.2.m2.1.1.2.2.3" xref="S2.SS2.p5.2.m2.1.1.2.2.3.cmml">t</mi></msub><mo id="S2.SS2.p5.2.m2.1.1.2.1" xref="S2.SS2.p5.2.m2.1.1.2.1.cmml">+</mo><msub id="S2.SS2.p5.2.m2.1.1.2.3" xref="S2.SS2.p5.2.m2.1.1.2.3.cmml"><mi id="S2.SS2.p5.2.m2.1.1.2.3.2" xref="S2.SS2.p5.2.m2.1.1.2.3.2.cmml">w</mi><mi id="S2.SS2.p5.2.m2.1.1.2.3.3" xref="S2.SS2.p5.2.m2.1.1.2.3.3.cmml">f</mi></msub></mrow><mo id="S2.SS2.p5.2.m2.1.1.1" xref="S2.SS2.p5.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS2.p5.2.m2.1.1.3" xref="S2.SS2.p5.2.m2.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.2.m2.1b"><apply id="S2.SS2.p5.2.m2.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1"><eq id="S2.SS2.p5.2.m2.1.1.1.cmml" xref="S2.SS2.p5.2.m2.1.1.1"></eq><apply id="S2.SS2.p5.2.m2.1.1.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2"><plus id="S2.SS2.p5.2.m2.1.1.2.1.cmml" xref="S2.SS2.p5.2.m2.1.1.2.1"></plus><apply id="S2.SS2.p5.2.m2.1.1.2.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.p5.2.m2.1.1.2.2.1.cmml" xref="S2.SS2.p5.2.m2.1.1.2.2">subscript</csymbol><ci id="S2.SS2.p5.2.m2.1.1.2.2.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2.2.2">𝑤</ci><ci id="S2.SS2.p5.2.m2.1.1.2.2.3.cmml" xref="S2.SS2.p5.2.m2.1.1.2.2.3">𝑡</ci></apply><apply id="S2.SS2.p5.2.m2.1.1.2.3.cmml" xref="S2.SS2.p5.2.m2.1.1.2.3"><csymbol cd="ambiguous" id="S2.SS2.p5.2.m2.1.1.2.3.1.cmml" xref="S2.SS2.p5.2.m2.1.1.2.3">subscript</csymbol><ci id="S2.SS2.p5.2.m2.1.1.2.3.2.cmml" xref="S2.SS2.p5.2.m2.1.1.2.3.2">𝑤</ci><ci id="S2.SS2.p5.2.m2.1.1.2.3.3.cmml" xref="S2.SS2.p5.2.m2.1.1.2.3.3">𝑓</ci></apply></apply><cn id="S2.SS2.p5.2.m2.1.1.3.cmml" type="integer" xref="S2.SS2.p5.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.2.m2.1c">w_{t}+w_{f}=1</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.2.m2.1d">italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT + italic_w start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT = 1</annotation></semantics></math>. According to Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.T1" title="Table 1 ‣ 3.1. Data preparation ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">1</span></a>, statistical analysis of our FunReason-SFT dataset reveals that the number of tokens in the chain of thought (<math alttext="N_{t}" class="ltx_Math" display="inline" id="S2.SS2.p5.3.m3.1"><semantics id="S2.SS2.p5.3.m3.1a"><msub id="S2.SS2.p5.3.m3.1.1" xref="S2.SS2.p5.3.m3.1.1.cmml"><mi id="S2.SS2.p5.3.m3.1.1.2" xref="S2.SS2.p5.3.m3.1.1.2.cmml">N</mi><mi id="S2.SS2.p5.3.m3.1.1.3" xref="S2.SS2.p5.3.m3.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.3.m3.1b"><apply id="S2.SS2.p5.3.m3.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.3.m3.1.1.1.cmml" xref="S2.SS2.p5.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.p5.3.m3.1.1.2.cmml" xref="S2.SS2.p5.3.m3.1.1.2">𝑁</ci><ci id="S2.SS2.p5.3.m3.1.1.3.cmml" xref="S2.SS2.p5.3.m3.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.3.m3.1c">N_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.3.m3.1d">italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>) tends to be significantly larger than the number of tokens in the final result (<math alttext="N_{f}" class="ltx_Math" display="inline" id="S2.SS2.p5.4.m4.1"><semantics id="S2.SS2.p5.4.m4.1a"><msub id="S2.SS2.p5.4.m4.1.1" xref="S2.SS2.p5.4.m4.1.1.cmml"><mi id="S2.SS2.p5.4.m4.1.1.2" xref="S2.SS2.p5.4.m4.1.1.2.cmml">N</mi><mi id="S2.SS2.p5.4.m4.1.1.3" xref="S2.SS2.p5.4.m4.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.4.m4.1b"><apply id="S2.SS2.p5.4.m4.1.1.cmml" xref="S2.SS2.p5.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.4.m4.1.1.1.cmml" xref="S2.SS2.p5.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.p5.4.m4.1.1.2.cmml" xref="S2.SS2.p5.4.m4.1.1.2">𝑁</ci><ci id="S2.SS2.p5.4.m4.1.1.3.cmml" xref="S2.SS2.p5.4.m4.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.4.m4.1c">N_{f}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.4.m4.1d">italic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math>). This phenomenon of <math alttext="N_{t}\gg N_{f}" class="ltx_Math" display="inline" id="S2.SS2.p5.5.m5.1"><semantics id="S2.SS2.p5.5.m5.1a"><mrow id="S2.SS2.p5.5.m5.1.1" xref="S2.SS2.p5.5.m5.1.1.cmml"><msub id="S2.SS2.p5.5.m5.1.1.2" xref="S2.SS2.p5.5.m5.1.1.2.cmml"><mi id="S2.SS2.p5.5.m5.1.1.2.2" xref="S2.SS2.p5.5.m5.1.1.2.2.cmml">N</mi><mi id="S2.SS2.p5.5.m5.1.1.2.3" xref="S2.SS2.p5.5.m5.1.1.2.3.cmml">t</mi></msub><mo id="S2.SS2.p5.5.m5.1.1.1" xref="S2.SS2.p5.5.m5.1.1.1.cmml">≫</mo><msub id="S2.SS2.p5.5.m5.1.1.3" xref="S2.SS2.p5.5.m5.1.1.3.cmml"><mi id="S2.SS2.p5.5.m5.1.1.3.2" xref="S2.SS2.p5.5.m5.1.1.3.2.cmml">N</mi><mi id="S2.SS2.p5.5.m5.1.1.3.3" xref="S2.SS2.p5.5.m5.1.1.3.3.cmml">f</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.5.m5.1b"><apply id="S2.SS2.p5.5.m5.1.1.cmml" xref="S2.SS2.p5.5.m5.1.1"><csymbol cd="latexml" id="S2.SS2.p5.5.m5.1.1.1.cmml" xref="S2.SS2.p5.5.m5.1.1.1">much-greater-than</csymbol><apply id="S2.SS2.p5.5.m5.1.1.2.cmml" xref="S2.SS2.p5.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS2.p5.5.m5.1.1.2.1.cmml" xref="S2.SS2.p5.5.m5.1.1.2">subscript</csymbol><ci id="S2.SS2.p5.5.m5.1.1.2.2.cmml" xref="S2.SS2.p5.5.m5.1.1.2.2">𝑁</ci><ci id="S2.SS2.p5.5.m5.1.1.2.3.cmml" xref="S2.SS2.p5.5.m5.1.1.2.3">𝑡</ci></apply><apply id="S2.SS2.p5.5.m5.1.1.3.cmml" xref="S2.SS2.p5.5.m5.1.1.3"><csymbol cd="ambiguous" id="S2.SS2.p5.5.m5.1.1.3.1.cmml" xref="S2.SS2.p5.5.m5.1.1.3">subscript</csymbol><ci id="S2.SS2.p5.5.m5.1.1.3.2.cmml" xref="S2.SS2.p5.5.m5.1.1.3.2">𝑁</ci><ci id="S2.SS2.p5.5.m5.1.1.3.3.cmml" xref="S2.SS2.p5.5.m5.1.1.3.3">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.5.m5.1c">N_{t}\gg N_{f}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.5.m5.1d">italic_N start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT ≫ italic_N start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math> implies that during training, the weight assigned to the reasoning component (<math alttext="w_{t}" class="ltx_Math" display="inline" id="S2.SS2.p5.6.m6.1"><semantics id="S2.SS2.p5.6.m6.1a"><msub id="S2.SS2.p5.6.m6.1.1" xref="S2.SS2.p5.6.m6.1.1.cmml"><mi id="S2.SS2.p5.6.m6.1.1.2" xref="S2.SS2.p5.6.m6.1.1.2.cmml">w</mi><mi id="S2.SS2.p5.6.m6.1.1.3" xref="S2.SS2.p5.6.m6.1.1.3.cmml">t</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.6.m6.1b"><apply id="S2.SS2.p5.6.m6.1.1.cmml" xref="S2.SS2.p5.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.6.m6.1.1.1.cmml" xref="S2.SS2.p5.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.p5.6.m6.1.1.2.cmml" xref="S2.SS2.p5.6.m6.1.1.2">𝑤</ci><ci id="S2.SS2.p5.6.m6.1.1.3.cmml" xref="S2.SS2.p5.6.m6.1.1.3">𝑡</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.6.m6.1c">w_{t}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.6.m6.1d">italic_w start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT</annotation></semantics></math>) will generally exceed that of the function call component (<math alttext="w_{f}" class="ltx_Math" display="inline" id="S2.SS2.p5.7.m7.1"><semantics id="S2.SS2.p5.7.m7.1a"><msub id="S2.SS2.p5.7.m7.1.1" xref="S2.SS2.p5.7.m7.1.1.cmml"><mi id="S2.SS2.p5.7.m7.1.1.2" xref="S2.SS2.p5.7.m7.1.1.2.cmml">w</mi><mi id="S2.SS2.p5.7.m7.1.1.3" xref="S2.SS2.p5.7.m7.1.1.3.cmml">f</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.p5.7.m7.1b"><apply id="S2.SS2.p5.7.m7.1.1.cmml" xref="S2.SS2.p5.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.p5.7.m7.1.1.1.cmml" xref="S2.SS2.p5.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.p5.7.m7.1.1.2.cmml" xref="S2.SS2.p5.7.m7.1.1.2">𝑤</ci><ci id="S2.SS2.p5.7.m7.1.1.3.cmml" xref="S2.SS2.p5.7.m7.1.1.3">𝑓</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p5.7.m7.1c">w_{f}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p5.7.m7.1d">italic_w start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT</annotation></semantics></math>), even when weights are determined solely based on token proportions. This observation highlights the need for a more balanced approach to optimize both reasoning quality and function call accuracy.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="332" id="S2.F3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3. </span>FunReason’s self-refinement training loop. The model iteratively generates, refines, and learns from its own outputs through the FCDR pipeline, enabling continuous improvement of both reasoning and function calling capabilities.</figcaption>
</figure>
<div class="ltx_para" id="S2.SS2.p6">
<p class="ltx_p" id="S2.SS2.p6.1">To reflect the observations from the dataset and the preceding analysis, we introduce adjustable weights in the new loss function to more flexibly balance the optimization of the model in both the reasoning and result generation stages. Therefore, we propose the following Multiscale loss function:</p>
</div>
<div class="ltx_para" id="S2.SS2.p7">
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="L_{\text{MSL}}=\alpha\cdot L_{\text{think}}+\beta\cdot L_{\text{result}}" class="ltx_Math" display="block" id="S2.Ex1.m1.1"><semantics id="S2.Ex1.m1.1a"><mrow id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml"><msub id="S2.Ex1.m1.1.1.2" xref="S2.Ex1.m1.1.1.2.cmml"><mi id="S2.Ex1.m1.1.1.2.2" xref="S2.Ex1.m1.1.1.2.2.cmml">L</mi><mtext id="S2.Ex1.m1.1.1.2.3" xref="S2.Ex1.m1.1.1.2.3a.cmml">MSL</mtext></msub><mo id="S2.Ex1.m1.1.1.1" xref="S2.Ex1.m1.1.1.1.cmml">=</mo><mrow id="S2.Ex1.m1.1.1.3" xref="S2.Ex1.m1.1.1.3.cmml"><mrow id="S2.Ex1.m1.1.1.3.2" xref="S2.Ex1.m1.1.1.3.2.cmml"><mi id="S2.Ex1.m1.1.1.3.2.2" xref="S2.Ex1.m1.1.1.3.2.2.cmml">α</mi><mo id="S2.Ex1.m1.1.1.3.2.1" lspace="0.222em" rspace="0.222em" xref="S2.Ex1.m1.1.1.3.2.1.cmml">⋅</mo><msub id="S2.Ex1.m1.1.1.3.2.3" xref="S2.Ex1.m1.1.1.3.2.3.cmml"><mi id="S2.Ex1.m1.1.1.3.2.3.2" xref="S2.Ex1.m1.1.1.3.2.3.2.cmml">L</mi><mtext id="S2.Ex1.m1.1.1.3.2.3.3" xref="S2.Ex1.m1.1.1.3.2.3.3a.cmml">think</mtext></msub></mrow><mo id="S2.Ex1.m1.1.1.3.1" xref="S2.Ex1.m1.1.1.3.1.cmml">+</mo><mrow id="S2.Ex1.m1.1.1.3.3" xref="S2.Ex1.m1.1.1.3.3.cmml"><mi id="S2.Ex1.m1.1.1.3.3.2" xref="S2.Ex1.m1.1.1.3.3.2.cmml">β</mi><mo id="S2.Ex1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.Ex1.m1.1.1.3.3.1.cmml">⋅</mo><msub id="S2.Ex1.m1.1.1.3.3.3" xref="S2.Ex1.m1.1.1.3.3.3.cmml"><mi id="S2.Ex1.m1.1.1.3.3.3.2" xref="S2.Ex1.m1.1.1.3.3.3.2.cmml">L</mi><mtext id="S2.Ex1.m1.1.1.3.3.3.3" xref="S2.Ex1.m1.1.1.3.3.3.3a.cmml">result</mtext></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.1b"><apply id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1"><eq id="S2.Ex1.m1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1"></eq><apply id="S2.Ex1.m1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.2">subscript</csymbol><ci id="S2.Ex1.m1.1.1.2.2.cmml" xref="S2.Ex1.m1.1.1.2.2">𝐿</ci><ci id="S2.Ex1.m1.1.1.2.3a.cmml" xref="S2.Ex1.m1.1.1.2.3"><mtext id="S2.Ex1.m1.1.1.2.3.cmml" mathsize="70%" xref="S2.Ex1.m1.1.1.2.3">MSL</mtext></ci></apply><apply id="S2.Ex1.m1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.3"><plus id="S2.Ex1.m1.1.1.3.1.cmml" xref="S2.Ex1.m1.1.1.3.1"></plus><apply id="S2.Ex1.m1.1.1.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2"><ci id="S2.Ex1.m1.1.1.3.2.1.cmml" xref="S2.Ex1.m1.1.1.3.2.1">⋅</ci><ci id="S2.Ex1.m1.1.1.3.2.2.cmml" xref="S2.Ex1.m1.1.1.3.2.2">𝛼</ci><apply id="S2.Ex1.m1.1.1.3.2.3.cmml" xref="S2.Ex1.m1.1.1.3.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.2.3.1.cmml" xref="S2.Ex1.m1.1.1.3.2.3">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.2.3.2.cmml" xref="S2.Ex1.m1.1.1.3.2.3.2">𝐿</ci><ci id="S2.Ex1.m1.1.1.3.2.3.3a.cmml" xref="S2.Ex1.m1.1.1.3.2.3.3"><mtext id="S2.Ex1.m1.1.1.3.2.3.3.cmml" mathsize="70%" xref="S2.Ex1.m1.1.1.3.2.3.3">think</mtext></ci></apply></apply><apply id="S2.Ex1.m1.1.1.3.3.cmml" xref="S2.Ex1.m1.1.1.3.3"><ci id="S2.Ex1.m1.1.1.3.3.1.cmml" xref="S2.Ex1.m1.1.1.3.3.1">⋅</ci><ci id="S2.Ex1.m1.1.1.3.3.2.cmml" xref="S2.Ex1.m1.1.1.3.3.2">𝛽</ci><apply id="S2.Ex1.m1.1.1.3.3.3.cmml" xref="S2.Ex1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.3.3.3.1.cmml" xref="S2.Ex1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.Ex1.m1.1.1.3.3.3.2.cmml" xref="S2.Ex1.m1.1.1.3.3.3.2">𝐿</ci><ci id="S2.Ex1.m1.1.1.3.3.3.3a.cmml" xref="S2.Ex1.m1.1.1.3.3.3.3"><mtext id="S2.Ex1.m1.1.1.3.3.3.3.cmml" mathsize="70%" xref="S2.Ex1.m1.1.1.3.3.3.3">result</mtext></ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.1c">L_{\text{MSL}}=\alpha\cdot L_{\text{think}}+\beta\cdot L_{\text{result}}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.1d">italic_L start_POSTSUBSCRIPT MSL end_POSTSUBSCRIPT = italic_α ⋅ italic_L start_POSTSUBSCRIPT think end_POSTSUBSCRIPT + italic_β ⋅ italic_L start_POSTSUBSCRIPT result end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS2.p8">
<p class="ltx_p" id="S2.SS2.p8.8">where <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS2.p8.1.m1.1"><semantics id="S2.SS2.p8.1.m1.1a"><mi id="S2.SS2.p8.1.m1.1.1" xref="S2.SS2.p8.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.1.m1.1b"><ci id="S2.SS2.p8.1.m1.1.1.cmml" xref="S2.SS2.p8.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.1.m1.1d">italic_α</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS2.p8.2.m2.1"><semantics id="S2.SS2.p8.2.m2.1a"><mi id="S2.SS2.p8.2.m2.1.1" xref="S2.SS2.p8.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.2.m2.1b"><ci id="S2.SS2.p8.2.m2.1.1.cmml" xref="S2.SS2.p8.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.2.m2.1d">italic_β</annotation></semantics></math> are adjustable weight parameters that satisfy the constraint <math alttext="\alpha+\beta=1" class="ltx_Math" display="inline" id="S2.SS2.p8.3.m3.1"><semantics id="S2.SS2.p8.3.m3.1a"><mrow id="S2.SS2.p8.3.m3.1.1" xref="S2.SS2.p8.3.m3.1.1.cmml"><mrow id="S2.SS2.p8.3.m3.1.1.2" xref="S2.SS2.p8.3.m3.1.1.2.cmml"><mi id="S2.SS2.p8.3.m3.1.1.2.2" xref="S2.SS2.p8.3.m3.1.1.2.2.cmml">α</mi><mo id="S2.SS2.p8.3.m3.1.1.2.1" xref="S2.SS2.p8.3.m3.1.1.2.1.cmml">+</mo><mi id="S2.SS2.p8.3.m3.1.1.2.3" xref="S2.SS2.p8.3.m3.1.1.2.3.cmml">β</mi></mrow><mo id="S2.SS2.p8.3.m3.1.1.1" xref="S2.SS2.p8.3.m3.1.1.1.cmml">=</mo><mn id="S2.SS2.p8.3.m3.1.1.3" xref="S2.SS2.p8.3.m3.1.1.3.cmml">1</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.3.m3.1b"><apply id="S2.SS2.p8.3.m3.1.1.cmml" xref="S2.SS2.p8.3.m3.1.1"><eq id="S2.SS2.p8.3.m3.1.1.1.cmml" xref="S2.SS2.p8.3.m3.1.1.1"></eq><apply id="S2.SS2.p8.3.m3.1.1.2.cmml" xref="S2.SS2.p8.3.m3.1.1.2"><plus id="S2.SS2.p8.3.m3.1.1.2.1.cmml" xref="S2.SS2.p8.3.m3.1.1.2.1"></plus><ci id="S2.SS2.p8.3.m3.1.1.2.2.cmml" xref="S2.SS2.p8.3.m3.1.1.2.2">𝛼</ci><ci id="S2.SS2.p8.3.m3.1.1.2.3.cmml" xref="S2.SS2.p8.3.m3.1.1.2.3">𝛽</ci></apply><cn id="S2.SS2.p8.3.m3.1.1.3.cmml" type="integer" xref="S2.SS2.p8.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.3.m3.1c">\alpha+\beta=1</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.3.m3.1d">italic_α + italic_β = 1</annotation></semantics></math> and <math alttext="\alpha,\beta\in[0,1]" class="ltx_Math" display="inline" id="S2.SS2.p8.4.m4.4"><semantics id="S2.SS2.p8.4.m4.4a"><mrow id="S2.SS2.p8.4.m4.4.5" xref="S2.SS2.p8.4.m4.4.5.cmml"><mrow id="S2.SS2.p8.4.m4.4.5.2.2" xref="S2.SS2.p8.4.m4.4.5.2.1.cmml"><mi id="S2.SS2.p8.4.m4.3.3" xref="S2.SS2.p8.4.m4.3.3.cmml">α</mi><mo id="S2.SS2.p8.4.m4.4.5.2.2.1" xref="S2.SS2.p8.4.m4.4.5.2.1.cmml">,</mo><mi id="S2.SS2.p8.4.m4.4.4" xref="S2.SS2.p8.4.m4.4.4.cmml">β</mi></mrow><mo id="S2.SS2.p8.4.m4.4.5.1" xref="S2.SS2.p8.4.m4.4.5.1.cmml">∈</mo><mrow id="S2.SS2.p8.4.m4.4.5.3.2" xref="S2.SS2.p8.4.m4.4.5.3.1.cmml"><mo id="S2.SS2.p8.4.m4.4.5.3.2.1" stretchy="false" xref="S2.SS2.p8.4.m4.4.5.3.1.cmml">[</mo><mn id="S2.SS2.p8.4.m4.1.1" xref="S2.SS2.p8.4.m4.1.1.cmml">0</mn><mo id="S2.SS2.p8.4.m4.4.5.3.2.2" xref="S2.SS2.p8.4.m4.4.5.3.1.cmml">,</mo><mn id="S2.SS2.p8.4.m4.2.2" xref="S2.SS2.p8.4.m4.2.2.cmml">1</mn><mo id="S2.SS2.p8.4.m4.4.5.3.2.3" stretchy="false" xref="S2.SS2.p8.4.m4.4.5.3.1.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.4.m4.4b"><apply id="S2.SS2.p8.4.m4.4.5.cmml" xref="S2.SS2.p8.4.m4.4.5"><in id="S2.SS2.p8.4.m4.4.5.1.cmml" xref="S2.SS2.p8.4.m4.4.5.1"></in><list id="S2.SS2.p8.4.m4.4.5.2.1.cmml" xref="S2.SS2.p8.4.m4.4.5.2.2"><ci id="S2.SS2.p8.4.m4.3.3.cmml" xref="S2.SS2.p8.4.m4.3.3">𝛼</ci><ci id="S2.SS2.p8.4.m4.4.4.cmml" xref="S2.SS2.p8.4.m4.4.4">𝛽</ci></list><interval closure="closed" id="S2.SS2.p8.4.m4.4.5.3.1.cmml" xref="S2.SS2.p8.4.m4.4.5.3.2"><cn id="S2.SS2.p8.4.m4.1.1.cmml" type="integer" xref="S2.SS2.p8.4.m4.1.1">0</cn><cn id="S2.SS2.p8.4.m4.2.2.cmml" type="integer" xref="S2.SS2.p8.4.m4.2.2">1</cn></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.4.m4.4c">\alpha,\beta\in[0,1]</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.4.m4.4d">italic_α , italic_β ∈ [ 0 , 1 ]</annotation></semantics></math>. The significance of introducing <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS2.p8.5.m5.1"><semantics id="S2.SS2.p8.5.m5.1a"><mi id="S2.SS2.p8.5.m5.1.1" xref="S2.SS2.p8.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.5.m5.1b"><ci id="S2.SS2.p8.5.m5.1.1.cmml" xref="S2.SS2.p8.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.5.m5.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.5.m5.1d">italic_α</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS2.p8.6.m6.1"><semantics id="S2.SS2.p8.6.m6.1a"><mi id="S2.SS2.p8.6.m6.1.1" xref="S2.SS2.p8.6.m6.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.6.m6.1b"><ci id="S2.SS2.p8.6.m6.1.1.cmml" xref="S2.SS2.p8.6.m6.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.6.m6.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.6.m6.1d">italic_β</annotation></semantics></math> lies in the fact that we can flexibly adjust the model’s emphasis on the reasoning process and the final result based on the actual performance and requirements of the model. By adjusting the specific values of <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS2.p8.7.m7.1"><semantics id="S2.SS2.p8.7.m7.1a"><mi id="S2.SS2.p8.7.m7.1.1" xref="S2.SS2.p8.7.m7.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.7.m7.1b"><ci id="S2.SS2.p8.7.m7.1.1.cmml" xref="S2.SS2.p8.7.m7.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.7.m7.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.7.m7.1d">italic_α</annotation></semantics></math> and <math alttext="\beta" class="ltx_Math" display="inline" id="S2.SS2.p8.8.m8.1"><semantics id="S2.SS2.p8.8.m8.1a"><mi id="S2.SS2.p8.8.m8.1.1" xref="S2.SS2.p8.8.m8.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.p8.8.m8.1b"><ci id="S2.SS2.p8.8.m8.1.1.cmml" xref="S2.SS2.p8.8.m8.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.p8.8.m8.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.p8.8.m8.1d">italic_β</annotation></semantics></math>, we can exert finer control over the model’s behavior, allowing it to achieve a better balance between reasoning depth and result accuracy, thereby more effectively improving the overall performance of the RLLM.</p>
</div>
<div class="ltx_para" id="S2.SS2.p9">
<p class="ltx_p" id="S2.SS2.p9.1"><span class="ltx_text ltx_font_bold" id="S2.SS2.p9.1.1">Self Refinement Strategy.</span> Following the MSL training, we employ a Self-Refinement strategy to further enhance the model capabilities, as depicted in Figure  <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S2.F3" title="Figure 3 ‣ 2.2. Self-Refinement Multiscale Loss (SRML) ‣ 2. Methodology ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">3</span></a>. In this phase, the MSL-trained model samples from the original xLAM dataset, generating its own Chain-of-Thought reasoning and corresponding function calls. Subsequently, this self-generated data is fed into the FCDR pipeline for automated inspection and improvement. Only the refined data, having passed the rigorous quality checks of the FCDR, is then used to further update the model’s parameters. This iterative process allows the model to learn from its own improved outputs, leading to continuous self-enhancement of its function calling and reasoning abilities.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3. </span>Experiment</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The experiments in this paper are divided into four parts. The first part elaborates on the CoT data generation based on xLAM  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib14" title="">zhang2024xlam, </a>)</cite> and the refinement of the dataset. Experiment setting is introduced in the second part. In the third part, we compare the FunReason models with current powerful general models, SFT and RL based specialized models for function call on the BFCL benchmark  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib19" title="">patil2024gorilla, </a>)</cite>, and showcase the performance of our code based model on two code benchmarks. In the fourth part, we conduct a detailed analysis of the hyperparameters.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1. </span>Data preparation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">To enhance function call capabilities, we utilize the open-source xlam-function-calling-60k dataset  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib14" title="">zhang2024xlam, </a>)</cite>, which comprises 60,000 queries from 3,673 executable APIs across 21 categories. All queries and function tools are in JSON format, with reference answers involving function selection and parameter value assignment.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">To better understand the characteristics of our training data, we conducted detailed analyses. First, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.T1" title="Table 1 ‣ 3.1. Data preparation ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">1</span></a>, our FunReason-FCDR dataset exhibits a significant token imbalance: Chain-of-Thought reasoning segments contain approximately 10 times more tokens (mean=350.74) than function call results (mean=31.07), highlighting the need for our multiscale loss approach. Additionally, we generated two initial CoT datasets: xLAM-strategy using GPT-4o with prescribed reasoning strategies, and xLAM-cot using QwQ-32B’s <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib20" title="">qwq, </a>)</cite> natural reasoning process (deployed via vLLM with temperature 0.1 and max length 20480). After training Qwen2.5-Coder-7B-Inst on both datasets, we found that naturally generated CoT data consistently outperformed strategy-based data (Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.T2" title="Table 2 ‣ 3.1. Data preparation ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">2</span></a>). Based on these empirical findings, we selected xLAM-cot for our subsequent data refinement process.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.1">
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T1.1.1.1" rowspan="2"><span class="ltx_text" id="S3.T1.1.1.1.1">Dataset</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T1.1.1.2">CoT Token Len.</td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S3.T1.1.1.3">Result Token Len.</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.1">Mean</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.2">Median</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.3">Mean</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.1.2.4">Median</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.3">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="S3.T1.1.3.1">FunReason-FCDR</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.3.2">350.74</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.3.3">248.00</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.3.4">31.07</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S3.T1.1.3.5">27.00</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1. </span>Token Statistics for the Refined Dataset: Note the approximately 10x higher token count for the Chain-of-Thought (CoT) compared to the final results.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.1">
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.2.1">Non-Live AST Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.3.1">Live Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.4.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T2.1.2.1">Qwen2.5-Coder-7B-Inst</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.2">84.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.3">69.78</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.1.2.4">76.93</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.3">
<td class="ltx_td ltx_align_left" id="S3.T2.1.3.1">Qwen2.5-Coder-7B-Inst(xLAM-strategy)</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.2">87.00</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.3">66.81</td>
<td class="ltx_td ltx_align_center" id="S3.T2.1.3.4">76.91</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T2.1.4.1">Qwen2.5-Coder-7B-Inst(xLAM-cot)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.4.2">85.67</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.4.3">71.73</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.1.4.4">78.70</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2. </span>Performance of prescribed mode of thought and natural reasoning mode.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">Data refinement was performed on the xLAM-cot dataset, wherein QwQ-32B itself was employed to execute the process, allowing for the full exploitation of its self-correction capabilities. The dataset resulting from this refinement pipeline was subsequently designated as FunReason-FCDR. The efficacy of our data refinement procedure is demonstrated in the results presented in Table  <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.T3" title="Table 3 ‣ 3.1. Data preparation ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.1">
<tr class="ltx_tr" id="S3.T3.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.2.1">Non-Live AST Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.3.1">Live Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.4.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.1.2.1">Llama-3.1-8B-Inst</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.2">84.21</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.3">61.08</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.2.4">72.65</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.3">
<td class="ltx_td ltx_align_left" id="S3.T3.1.3.1">Llama-3.2-3B-Inst</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.2">80.56</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.3">55.80</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.3.4">68.18</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.4">
<td class="ltx_td ltx_align_left" id="S3.T3.1.4.1">Qwen2.5-Coder-7B-Inst</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.2">84.08</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.3">69.78</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.4.4">76.93</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.1.5.1">Llama-3.1-8B-Inst(xlam-cot)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.5.2">84.02</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.5.3">74.98</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.5.4">79.50</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.6">
<td class="ltx_td ltx_align_left" id="S3.T3.1.6.1">Llama-3.2-3B-Inst(xlam-cot)</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.2">71.19</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.3">63.56</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.6.4">67.38</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.7">
<td class="ltx_td ltx_align_left" id="S3.T3.1.7.1">Qwen2.5-Coder-7B-Inst(xlam-cot)</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.2">85.67</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.3">71.73</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.7.4">78.70</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.8">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T3.1.8.1">Llama-3.1-8B-Inst(FunReason-FCDR)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.8.2">83.58</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.8.3">79.33</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.1.8.4">81.46</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.9">
<td class="ltx_td ltx_align_left" id="S3.T3.1.9.1">Llama-3.2-3B-Inst(FunReason-FCDR)</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.9.2">75.94</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.9.3">70.00</td>
<td class="ltx_td ltx_align_center" id="S3.T3.1.9.4">72.97</td>
</tr>
<tr class="ltx_tr" id="S3.T3.1.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T3.1.10.1">Qwen2.5-Coder-7B-Inst(FunReason-FCDR)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.10.2">86.27</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.10.3">77.60</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.1.10.4">81.94</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3. </span>The Efficiency of the Data Refinement in FunReason.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2. </span>Experiment Setting</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p1.1.1">Datasets.</span> The experiment is conducted on our FunReason-FCDR dataset, which contains 60,000 high-quality CoT data made by the natural inference of QwQ-32B on xLAM and further processed by our FunReason-FCDR. It ultimately organizes in the share-gpt format for the convenience of training.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">Implementation Details.</span> All SFT and FunReason-SRML experiments are conducted using LLaMA Factory  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib21" title="">zheng2024llamafactory, </a>)</cite>. To ensure consistency and comparability across different experimental setups, we maintain fixed hyperparameters for both the SFT and FunReason-SRML training phases. Specifically, the batch size is consistently set to 512, the learning rate is fixed at <math alttext="4\times 10^{-5}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><mrow id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mn id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">4</mn><mo id="S3.SS2.p2.1.m1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S3.SS2.p2.1.m1.1.1.1.cmml">×</mo><msup id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mn id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">10</mn><mrow id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml"><mo id="S3.SS2.p2.1.m1.1.1.3.3a" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">−</mo><mn id="S3.SS2.p2.1.m1.1.1.3.3.2" xref="S3.SS2.p2.1.m1.1.1.3.3.2.cmml">5</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><times id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1.1"></times><cn id="S3.SS2.p2.1.m1.1.1.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.2">4</cn><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3">superscript</csymbol><cn id="S3.SS2.p2.1.m1.1.1.3.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.3.2">10</cn><apply id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"><minus id="S3.SS2.p2.1.m1.1.1.3.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3"></minus><cn id="S3.SS2.p2.1.m1.1.1.3.3.2.cmml" type="integer" xref="S3.SS2.p2.1.m1.1.1.3.3.2">5</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">4\times 10^{-5}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">4 × 10 start_POSTSUPERSCRIPT - 5 end_POSTSUPERSCRIPT</annotation></semantics></math> throughout the training procedures. All trainings run in a single node equipped with 8 NVIDIA A100 80GB GPUs.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Backbones.</span> To demonstrate the effectiveness and wide applicability of our FunReason framework, we selected Llama-3.2-3B-Instruct from the Llama family and Qwen-2.5-Coder-Instruct  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib22" title="">hui2024qwen2, </a>)</cite> from the Qwen family as base models for fine-tuning.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">Baselines.</span> To conduct a broad comparison, we select similarly sized SFT-based models Toolace-8B  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib16" title="">liu2024toolace, </a>)</cite> and xLAM-2-8B  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib15" title="">prabhakar2025apigen, </a>)</cite>, RL-based models Qwen2.5-7B-Inst (ToolRL)  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib23" title="">qian2025toolrl, </a>)</cite> and Tool-N1-7B (xLAM)  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib24" title="">zhang2025nemotron, </a>)</cite> within the function call specialized domain, and large-scale models represented by GPT-4o in the general domain as baselines.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p5.1.1">Evaluation Metrics.</span> The evaluation focuses on the performance of single-turn tool-calling. To this end, we assess our methodology on a highly representative benchmark, the Berkeley Function Calling Leaderboard (BFCL)  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib19" title="">patil2024gorilla, </a>)</cite>. For BFCL, we conduct evaluations on the Non-live and Live subsets, corresponding to synthetic and real-world data, respectively. Performance across all subsets of BFCL is reported in terms of accuracy. We further evaluate our model on two code benchmarks, HumanEval  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib25" title="">chen2021evaluating, </a>)</cite> and MBPP  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib26" title="">austin2021program, </a>)</cite> (include HumanEval+  <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib27" title="">liu2023your, </a>)</cite> and MBPP+). The two well-recognized benchmark datasets are widely adopted to measure a model’s programming capabilities, providing crucial evaluation standards for the research and development of code LLMs. The code evaluation results will be presented using the pass@1 metric.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3. </span>Results on BFCL</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">The performance of FunReason on BFCL is detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.T4" title="Table 4 ‣ 3.3. Results on BFCL ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">4</span></a>, considering the evaluation of single-turn tool-calling capabilities. We report accuracy across both Non-live (synthetic data) and Live (real-world data) subsets, as per the official BFCL evaluation script. Our analysis reveals:</p>
</div>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T4.1">
<tr class="ltx_tr" id="S3.T4.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.2.1">Non-Live AST Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.3.1">Live Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T4.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.4.1">Overall</span></td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.1.2.1">GPT-4o-2024-11-20</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.2">86.81</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.3">78.85</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.2.4">82.83</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.3">
<td class="ltx_td ltx_align_left" id="S3.T4.1.3.1">GPT-4.5-Preview-2025-02-27</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.3.2">86.12</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.3.3">79.34</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.3.4">82.73</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.4">
<td class="ltx_td ltx_align_left" id="S3.T4.1.4.1">GPT-4o-mini-2024-07-18</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.2">85.21</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.3">74.41</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.4.4">79.81</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.5">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.1.5.1">Llama-4-Maverick-17B-128E-Instruct-FP8</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.5.2">86.65</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.5.3">58.55</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.5.4">72.60</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.6">
<td class="ltx_td ltx_align_left" id="S3.T4.1.6.1">Llama-3.3-70B-Instruct</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.6.2">85.08</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.6.3">62.67</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.6.4">73.88</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.7">
<td class="ltx_td ltx_align_left" id="S3.T4.1.7.1">QwQ-32B</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.7.2">86.48</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.7.3">75.48</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.7.4">80.98</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.8">
<td class="ltx_td ltx_align_left" id="S3.T4.1.8.1">Qwen2.5-7B-Instruct</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.8.2">86.46</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.8.3">67.44</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.8.4">76.95</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.9">
<td class="ltx_td ltx_align_left" id="S3.T4.1.9.1">xLAM-2-8b-fc-r</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.9.2">84.40</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.9.3">66.90</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.9.4">75.65</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.10">
<td class="ltx_td ltx_align_left" id="S3.T4.1.10.1">ToolACE-8B</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.10.2">87.54</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.10.3">78.59</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.10.4">82.57</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.11">
<td class="ltx_td ltx_align_left" id="S3.T4.1.11.1">Llama-3.2-3B-Inst</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.11.2">80.56</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.11.3">55.80</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.11.4">68.18</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.12">
<td class="ltx_td ltx_align_left" id="S3.T4.1.12.1">Qwen2.5-Coder-7B-Inst</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.12.2">84.08</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.12.3">69.78</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.12.4">76.93</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.13">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.1.13.1">Llama-3.2-3B-Inst(ToolRL) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib24" title="">zhang2025nemotron, </a>)</cite>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.2">74.38</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.3">56.86</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.13.4">65.62</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.14">
<td class="ltx_td ltx_align_left" id="S3.T4.1.14.1">Qwen2.5-7B-Inst(ToolRL) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib24" title="">zhang2025nemotron, </a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.14.2">86.17</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.14.3">74.90</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.14.4">80.54</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.15">
<td class="ltx_td ltx_align_left" id="S3.T4.1.15.1">TooL-N1-7B(xLAM) <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib23" title="">qian2025toolrl, </a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.15.2"><span class="ltx_text ltx_font_bold" id="S3.T4.1.15.2.1">87.77</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.15.3">76.24</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.15.4">82.01</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.16">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T4.1.16.1">Llama-3.2-3B-Inst(FCDR-SFT)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.16.2">75.94</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.16.3">70.00</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.1.16.4">72.97</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.17">
<td class="ltx_td ltx_align_left" id="S3.T4.1.17.1">Llama-3.2-3B-Inst(FunReason)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.17.2">77.75</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.17.3">71.51</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.17.4">74.63</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.18">
<td class="ltx_td ltx_align_left" id="S3.T4.1.18.1">Qwen2.5-Coder-7B-Inst(FCDR-SFT)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.18.2">86.27</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.18.3">77.60</td>
<td class="ltx_td ltx_align_center" id="S3.T4.1.18.4">81.94</td>
</tr>
<tr class="ltx_tr" id="S3.T4.1.19">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T4.1.19.1">Qwen2.5-Coder-7B-Inst(FunReason)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.19.2">87.00</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.19.3"><span class="ltx_text ltx_font_bold" id="S3.T4.1.19.3.1">80.31</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.1.19.4"><span class="ltx_text ltx_font_bold" id="S3.T4.1.19.4.1">83.66</span></td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4. </span>Performance on BFCL (last updated April 25, 2025), with all metrics calculated using the official script. The best result within each category is highlighted in <span class="ltx_text ltx_font_bold" id="S3.T4.3.1">bold</span>.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">FunReason-7B, built upon Qwen2.5-Coder-7B-Inst and fine-tuned using our FunReason-SRML approach, achieves a leading overall accuracy of 83.66%. This result demonstrates strong performance across both synthetic and real-world scenarios, with particularly impressive results on the challenging Live subset (80.31%).
Our model outperforms strong closed-source baselines, including GPT-4o (82.83%) and GPT-4.5-Preview (82.73%). This is particularly noteworthy given that these models typically set the standard for language model performance. The gap is even more pronounced when compared to GPT-4o-mini (79.81%).</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1">The comparison between FunReason and FCDR-SFT variants reveals the effectiveness of our approach. For the Llama-3.2-3B-Inst architecture, FunReason improves overall accuracy from 72.97% to 74.63%. The enhancement is even more significant for the Qwen2.5-Coder-7B-Inst model, where FunReason achieves 83.66% compared to FCDR-SFT’s 81.94%.
Our approach demonstrates better performance than specialized RL-based methods for tool-calling. While TooL-N1-7B (xLAM) achieves slightly higher Non-live accuracy (87.77%), our model’s superior Live performance (80.31% vs 76.24%) leads to better overall results (83.66% vs 82.01%). Similarly, we outperform Qwen2.5-7B-Inst(ToolRL) (80.54%) across both subsets.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1">Despite using a 7B parameter model, FunReason outperforms larger models like Llama-4-Maverick-17B (72.60%), Llama-3.3-70B (73.88%), and QwQ-32B (80.98%). This suggests that our method’s effectiveness is not merely a function of model scale but rather stems from the sophisticated fine-tuning approach.
A particularly notable aspect of our results is the strong performance on the Live subset (80.31%), which represents real-world scenarios. This suggests that FunReason has developed robust generalization capabilities, avoiding overfitting to synthetic data patterns. The gap between Non-live and Live performance (6.69 percentage points) is also smaller than most baselines, indicating better consistency across different evaluation settings.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1">These results validate our hypothesis that carefully designed loss-structured fine-tuning methods can outperform both traditional SFT and preference-based reinforcement learning approaches for tool-calling tasks. The success of FunReason-SRML suggests that explicit consideration of the loss distribution during fine-tuning is crucial for optimizing model performance on function calling tasks that incorporate Chain-of-Thought reasoning.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4. </span>Results on HumanEval and MBPP</h3>
<figure class="ltx_table" id="S3.T5">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T5.1">
<tr class="ltx_tr" id="S3.T5.1.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="S3.T5.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.1">Models</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.1.1.2"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.2.1">Humaneval</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.1.1.3"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.3.1">Humaneval+</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.1.1.4"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.4.1">MBPP</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T5.1.1.5"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.5.1">MBPP+</span></td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.2">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T5.1.2.1">Qwen2.5-Coder-7B-Inst</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.2.2">0.866</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.2.3">0.823</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.2.4">0.812</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.2.5">0.693</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.3">
<td class="ltx_td ltx_align_left ltx_border_t" id="S3.T5.1.3.1">Qwen2.5-Coder-7B-Inst(SFT)</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.3.2">0.470</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.3.3">0.445</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.3.4">0.690</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.1.3.5">0.593</td>
</tr>
<tr class="ltx_tr" id="S3.T5.1.4">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S3.T5.1.4.1">Qwen2.5-Coder-7B-Inst(FunReason)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.4.2">0.841</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.4.3">0.787</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.4.4">0.794</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.1.4.5">0.653</td>
</tr>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5. </span>Performance of SFT and FunReason models on HumanEval and MBPP (including their HumanEval+ and MBPP+) compared with Qwen2.5-Coder-7B-Inst.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">To further assess FunReason’s ability to mitigate catastrophic forgetting, we evaluate our models on widely recognized code generation benchmarks, HumanEval and MBPP (including their plus variants). The results, presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.T5" title="Table 5 ‣ 3.4. Results on HumanEval and MBPP ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">5</span></a>, demonstrate a crucial advantage of our method. Specifically, FunReason-7B maintains strong performance across all benchmarks, with pass@1 scores only marginally lower (within 4%) compared to the base Qwen2.5-Coder model. For instance, on HumanEval, FunReason achieves 0.841 compared to the base model’s 0.866, and on MBPP, it reaches 0.794 versus 0.812. This minimal performance degradation stands in stark contrast to models fine-tuned using standard SFT, where we observe substantial declines across all metrics. The SFT variant shows dramatic drops in pass@1 scores - notably falling from 0.866 to 0.470 on HumanEval and from 0.823 to 0.445 on HumanEval+. These findings strongly indicate that our FunReason-SRML approach effectively alleviates the issue of catastrophic forgetting, a common problem associated with full SFT, thereby preserving the base model’s coding proficiency while enhancing its specialized function calling abilities.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5. </span>Ablation Study</h3>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="277" id="S3.F4.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4. </span>(a) Token Statistics for the Refined Dataset. (b) Performance across different values of <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.F4.2.m1.1"><semantics id="S3.F4.2.m1.1b"><mi id="S3.F4.2.m1.1.1" xref="S3.F4.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.F4.2.m1.1c"><ci id="S3.F4.2.m1.1.1.cmml" xref="S3.F4.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.F4.2.m1.1d">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.F4.2.m1.1e">italic_α</annotation></semantics></math>.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.3"><span class="ltx_text ltx_font_bold" id="S3.SS5.p1.3.1">Ablations on Hyperparameter Variation.</span> To investigate the impact of the reasoning process’s relative importance, represented by the hyperparameters <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_α</annotation></semantics></math>, on model performance, we conduct a series of ablation studies. Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.F4" title="Figure 4 ‣ 3.5. Ablation Study ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">4</span></a> (a) illustrates the Live Accuracy of two distinct models: Qwen2.5-Coder-7B-Instruct(DRML) and Llama-3.2-3B-Instruct(DRML). Our experiments reveal notable trends across both architectures, for both Qwen2.5-Coder-7B-Instruct(DRML) and Llama-3.2-3B-Instruct(DRML), we observe a consistent increase in Live Accuracy as the hyperparameter value increases from 0.1, and beyond a certain point, the increase of the hyperparameter <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><mi id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><ci id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_α</annotation></semantics></math> leads to a drop in accuracy. According to statistic result in Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#S3.F4" title="Figure 4 ‣ 3.5. Ablation Study ‣ 3. Experiment ‣ FunReason: Enhancing Large Language Models’ Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement"><span class="ltx_text ltx_ref_tag">4</span></a> (b), the default value of <math alttext="\alpha" class="ltx_Math" display="inline" id="S3.SS5.p1.3.m3.1"><semantics id="S3.SS5.p1.3.m3.1a"><mi id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><ci id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.3.m3.1d">italic_α</annotation></semantics></math> is above 0.9 which yields the suboptimal result. The finding reveals that the model suffers under the original SFT loss setting, and our method balances the need for thorough reasoning and precise function calls by properly weighting the loss at different parts of the generation process, including intermediate reasoning and the final call.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4. </span>Related Work</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Function Calling in Large Language Models.</span> The ability of LLM to interact with external tools and APIs through function calling has emerged as a crucial aspect of their practical utility <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib28" title="">wang2024executable, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib7" title="">singh2024llm, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib19" title="">patil2024gorilla, </a>)</cite>. This capability allows LLMs to transcend the limitations of solely processing and generating text, enabling them to ground their responses in real-world data and automate complex tasks <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib29" title="">wang2024gta, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib30" title="">kavathekar2025small, </a>)</cite>. Early explorations in this area focused on enabling LLMs to understand descriptions of functions and generate the necessary calls with appropriate parameters <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib31" title="">zeng2024adaptable, </a>)</cite>. Researchers have investigated various techniques to improve the accuracy and reliability of function calling, including sophisticated prompt engineering strategies that guide the model towards the desired output format and content <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib32" title="">marvin2023prompt, </a>)</cite>. More recently, fine-tuning approaches have gained prominence in enhancing function calling capabilities <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib14" title="">zhang2024xlam, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib16" title="">liu2024toolace, </a>)</cite>. By training LLMs on datasets specifically curated for function calling scenarios, these methods aim to instill a deeper understanding of function semantics and parameter requirements. The quality and diversity of these fine-tuning datasets play a critical role in the resulting performance of the LLM in function calling tasks <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib15" title="">prabhakar2025apigen, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib33" title="">lin2025robust, </a>)</cite>. RL has also been explored as a means to optimize LLMs for function calling <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib23" title="">qian2025toolrl, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib24" title="">zhang2025nemotron, </a>)</cite>. RL-based approaches often involve defining reward functions that incentivize the generation of correct and executable function calls. While RL offers the potential to directly optimize for task success, it often requires carefully designed reward functions, particularly in data-limited scenarios <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib34" title="">schulman2017proximal, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib35" title="">shao2024deepseekmath, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib36" title="">yu2025dapo, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Loss Function Design for RLLM.</span> The advent of CoT has marked a significant paradigm shift in leveraging the reasoning abilities of LLM <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib11" title="">guo2025deepseek, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib12" title="">team2025kimi, </a>)</cite>. By explicitly prompting the model to articulate its step-by-step thought process before arriving at a final answer, CoT has demonstrated remarkable improvements in performance across a spectrum of complex reasoning tasks <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib37" title="">kojima2022large, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib38" title="">wei2022chain, </a>)</cite>. These tasks include arithmetic reasoning <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib39" title="">ahn2024large, </a>)</cite>, symbolic reasoning <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib40" title="">wu2023symbol, </a>)</cite>, and code generation <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib41" title="">jiang2024survey, </a>)</cite>, where the ability to break down a problem into smaller, manageable steps is crucial <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib42" title="">tam2024llm, </a>)</cite>. The integration of CoT with function calling mechanisms holds substantial promise for enhancing the accuracy and reliability of tool-use <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib23" title="">qian2025toolrl, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib24" title="">zhang2025nemotron, </a>)</cite>. By reasoning through the user’s request and identifying the most appropriate course of action, including the selection of relevant tools and their parameters, the model can make more informed decisions about function invocation. The explicit reasoning process provided by CoT can also improve the transparency and interpretability of the model’s actions <cite class="ltx_cite ltx_citemacro_citep">(<a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib43" title="">wang2025otc, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib23" title="">qian2025toolrl, </a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20192v1#bib.bib24" title="">zhang2025nemotron, </a>)</cite>.</p>
</div>
<div class="ltx_para" id="S4.p3">
<p class="ltx_p" id="S4.p3.1">However, the integration of CoT into the training process for function calling introduces a unique challenge related to the design of the loss function. Traditional training methodologies, primarily SFT, typically treat the entire generated sequence (including the CoT reasoning and the final function call) uniformly when calculating the loss <cite class="ltx_cite ltx_citemacro_citep">(<span class="ltx_ref ltx_missing_citation ltx_ref_self">li2024getting</span>)</cite>. This can lead to an imbalance, where the potentially lengthy sequences of reasoning steps dominate the loss calculation, potentially overshadowing the importance of the final, often shorter, function call. Consequently, the model might be incentivized to generate elaborate and seemingly plausible reasoning chains, even if they do not ultimately lead to a correct or executable function call. This inherent tension between the verbose reasoning process and the need for a succinct and precise function call necessitates a more nuanced approach to loss function design that can effectively balance these two critical aspects of the task. Our work addresses this challenge by introducing a DRML approach that explicitly considers the different roles and importance of the reasoning process and the final function call during training.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5. </span>Discussions, Limitations, and Societal Impacts</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Our work builds upon these foundational efforts in function calling and CoT reasoning by introducing a novel training methodology and a comprehensive data refinement strategy tailored specifically for enhancing LLMs’ ability to interact with external tools. Unlike prior work that may have focused primarily on either improving the reasoning capabilities or the output formatting of function calls, our approach aims to achieve a synergistic balance between these two critical aspects. By introducing a multiscale loss that differentially weights the reasoning process and the final function call, we aim to optimize the model not only for generating coherent and logical reasoning but also for producing accurate and executable function invocations. Furthermore, our data refinement pipeline addresses the crucial issue of data quality, which is often a bottleneck in training effective function calling models, particularly when incorporating the complexities of CoT. By automatically evaluating and improving the quality of function calling data across multiple dimensions, we strive to create a more robust and reliable training foundation for LLMs to learn this essential capability.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">While FunReason achieves strong performance, limitations exist in its current form. The multiscale loss approach may not fully capture the complex relationship between reasoning depth and execution precision, particularly for highly specialized domain functions. There are also inherent trade-offs between model size and inference speed that affect real-world deployment. Regarding societal impact, enhanced function calling capabilities could streamline automation and improve human-AI collaboration across various sectors. However, this could also enable malicious actors to more effectively exploit APIs or automate harmful tasks, necessitating careful consideration of access controls and monitoring mechanisms in deployment.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6. </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this paper, we introduce FunReason, a novel framework with SRML approach specifically designed to enhance the function calling capabilities of LLMs. Through extensive experiments on the BFCL benchmark, we demonstrate that our FunReason achieves performance comparable to GPT-4o, surpassing existing RL-based methods. Furthermore, our approach effectively mitigates the critical issue of catastrophic forgetting during fine-tuning, as evidenced by the results on the HumanEval and MBPP code benchmarks. Complementing our loss function, we developed a comprehensive data refinement strategy that leverages LLMs to automatically evaluate and improve the quality of function calling data. Notably, our findings indicate that naturally generated CoT data from reasoning models outperforms artificially constructed CoT based on predefined strategies.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7. </span>Acknowledgments</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This work was supported by Ant Group Research Intern Program.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al.

</span>
<span class="ltx_bibblock">Training language models to follow instructions with human feedback.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib1.1.1">Advances in neural information processing systems</span>, 35:27730–27744, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al.

</span>
<span class="ltx_bibblock">Gpt-4 technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib2.1.1">arXiv preprint arXiv:2303.08774</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2302.13971</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei Huang, et al.

</span>
<span class="ltx_bibblock">Qwen technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2309.16609</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, et al.

</span>
<span class="ltx_bibblock">Evaluation of openai o1: Opportunities and challenges of agi.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib5.1.1">arXiv preprint arXiv:2409.18486</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
MAOLIN WANG, YINGYI ZHANG, CUNYIN PENG, YICHENG CHEN, WEI ZHOU, JINJIE GU, CHENYI ZHUANG, RUOCHENG GUO, BOWEN YU, WANYU WANG, et al.

</span>
<span class="ltx_bibblock">Function calling in large language models: Industrial practices, challenges, and future directions.

</span>
<span class="ltx_bibblock">2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Simranjit Singh, Andreas Karatzas, Michael Fore, Iraklis Anagnostopoulos, and Dimitrios Stamoulis.

</span>
<span class="ltx_bibblock">An llm-tool compiler for fused parallel function calling.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib7.1.1">arXiv preprint arXiv:2405.17438</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
Oguzhan Topsakal and Tahir Cetin Akinci.

</span>
<span class="ltx_bibblock">Creating large language model applications utilizing langchain: A primer on developing llm apps fast.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib8.1.1">International Conference on Applied Engineering and Natural Sciences</span>, volume 1, pages 1050–1056, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
Nicholas Harvel, Felipe Bivort Haiek, Anupriya Ankolekar, and David James Brunner.

</span>
<span class="ltx_bibblock">Can llms answer investment banking questions? using domain-tuned functions to improve llm performance on knowledge-intensive analytical tasks.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib9.1.1">Proceedings of the AAAI Symposium Series</span>, volume 3, pages 125–133, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe.

</span>
<span class="ltx_bibblock">Let’s verify step by step.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib10.1.1">The Twelfth International Conference on Learning Representations</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al.

</span>
<span class="ltx_bibblock">Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib11.1.1">arXiv preprint arXiv:2501.12948</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al.

</span>
<span class="ltx_bibblock">Kimi k1. 5: Scaling reinforcement learning with llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2501.12599</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
Angelica Chen, Jason Phang, Alicia Parrish, Vishakh Padmakumar, Chen Zhao, Samuel R Bowman, and Kyunghyun Cho.

</span>
<span class="ltx_bibblock">Two failures of self-consistency in the multi-step reasoning of llms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2305.14279</span>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, et al.

</span>
<span class="ltx_bibblock">xlam: A family of large action models to empower ai agent systems.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib14.1.1">arXiv preprint arXiv:2409.03215</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
Akshara Prabhakar, Zuxin Liu, Weiran Yao, Jianguo Zhang, Ming Zhu, Shiyu Wang, Zhiwei Liu, Tulika Awalgaonkar, Haolin Chen, Thai Hoang, et al.

</span>
<span class="ltx_bibblock">Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib15.1.1">arXiv preprint arXiv:2504.03601</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al.

</span>
<span class="ltx_bibblock">Toolace: Winning the points of llm function calling.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2409.00920</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al.

</span>
<span class="ltx_bibblock">Overcoming catastrophic forgetting in neural networks.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib17.1.1">Proceedings of the national academy of sciences</span>, 114(13):3521–3526, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
Weijieying Ren, Xinlong Li, Lei Wang, Tianxiang Zhao, and Wei Qin.

</span>
<span class="ltx_bibblock">Analyzing and reducing catastrophic forgetting in parameter efficient tuning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2402.18865</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
Shishir G Patil, Tianjun Zhang, Xin Wang, and Joseph E Gonzalez.

</span>
<span class="ltx_bibblock">Gorilla: Large language model connected with massive apis.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib19.1.1">Advances in Neural Information Processing Systems</span>, 37:126544–126565, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Qwen Team.

</span>
<span class="ltx_bibblock">Qwq: Reflect deeply on the boundaries of the unknown.

</span>
<span class="ltx_bibblock">2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma.

</span>
<span class="ltx_bibblock">Llamafactory: Unified efficient fine-tuning of 100+ language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib21.1.1">arXiv preprint arXiv:2403.13372</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al.

</span>
<span class="ltx_bibblock">Qwen2. 5-coder technical report.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2409.12186</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji.

</span>
<span class="ltx_bibblock">Toolrl: Reward is all tool learning needs.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib23.1.1">arXiv preprint arXiv:2504.13958</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu.

</span>
<span class="ltx_bibblock">Nemotron-research-tool-n1: Tool-using language models with reinforced reasoning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2505.00024</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al.

</span>
<span class="ltx_bibblock">Evaluating large language models trained on code.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib25.1.1">arXiv preprint arXiv:2107.03374</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al.

</span>
<span class="ltx_bibblock">Program synthesis with large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2108.07732</span>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang.

</span>
<span class="ltx_bibblock">Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib27.1.1">Advances in Neural Information Processing Systems</span>, 36:21558–21572, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
Xingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji.

</span>
<span class="ltx_bibblock">Executable code actions elicit better llm agents.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib28.1.1">Forty-first International Conference on Machine Learning</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Jize Wang, Ma Zerun, Yining Li, Songyang Zhang, Cailian Chen, Kai Chen, and Xinyi Le.

</span>
<span class="ltx_bibblock">Gta: a benchmark for general tool agents.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib29.1.1">The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Ishan Kavathekar, Raghav Donakanti, Ponnurangam Kumaraguru, and Karthik Vaidhyanathan.

</span>
<span class="ltx_bibblock">Small models, big tasks: An exploratory empirical study on small language models for function calling.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2504.19277</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
Guancheng Zeng, Wentao Ding, Beining Xu, Chi Zhang, Wenqiang Han, Gang Li, Jingjing Mo, Pengxu Qiu, Xinran Tao, Wang Tao, et al.

</span>
<span class="ltx_bibblock">Adaptable and precise: Enterprise-scenario llm function-calling capability training pipeline.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2412.15660</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Ggaliwango Marvin, Nakayiza Hellen, Daudi Jjingo, and Joyce Nakatumba-Nabende.

</span>
<span class="ltx_bibblock">Prompt engineering in large language models.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib32.1.1">International conference on data intelligence and cognitive informatics</span>, pages 387–402. Springer, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Qiqiang Lin, Muning Wen, Qiuying Peng, Guanyu Nie, Junwei Liao, Xiaoyun Mo, Jiamu Zhou, Cheng Cheng, Yin Zhao, Jun Wang, et al.

</span>
<span class="ltx_bibblock">Robust function-calling for on-device language model via function masking.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib33.1.1">The Thirteenth International Conference on Learning Representations</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov.

</span>
<span class="ltx_bibblock">Proximal policy optimization algorithms.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib34.1.1">arXiv preprint arXiv:1707.06347</span>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Y Wu, et al.

</span>
<span class="ltx_bibblock">Deepseekmath: Pushing the limits of mathematical reasoning in open language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2402.03300</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al.

</span>
<span class="ltx_bibblock">Dapo: An open-source llm reinforcement learning system at scale.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib36.1.1">arXiv preprint arXiv:2503.14476</span>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa.

</span>
<span class="ltx_bibblock">Large language models are zero-shot reasoners.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.1.1">Advances in neural information processing systems</span>, 35:22199–22213, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al.

</span>
<span class="ltx_bibblock">Chain-of-thought prompting elicits reasoning in large language models.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib38.1.1">Advances in neural information processing systems</span>, 35:24824–24837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin.

</span>
<span class="ltx_bibblock">Large language models for mathematical reasoning: Progresses and challenges.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib39.1.1">arXiv preprint arXiv:2402.00157</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
Xiaoqian Wu, Yong-Lu Li, Jianhua Sun, and Cewu Lu.

</span>
<span class="ltx_bibblock">Symbol-llm: leverage language models for symbolic system in visual human activity reasoning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib40.1.1">Advances in Neural Information Processing Systems</span>, 36:29680–29691, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
Juyong Jiang, Fan Wang, Jiasi Shen, Sungju Kim, and Sunghun Kim.

</span>
<span class="ltx_bibblock">A survey on large language models for code generation.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2406.00515</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
Derek Tam, Margaret Li, Prateek Yadav, Rickard Brüel Gabrielsson, Jiacheng Zhu, Kristjan Greenewald, Mikhail Yurochkin, Mohit Bansal, Colin Raffel, and Leshem Choshen.

</span>
<span class="ltx_bibblock">Llm merging: Building llms efficiently through merging.

</span>
<span class="ltx_bibblock">In <span class="ltx_text ltx_font_italic" id="bib.bib42.1.1">NeurIPS 2024 Competition Track</span>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
Hongru Wang, Cheng Qian, Wanjun Zhong, Xiusi Chen, Jiahao Qiu, Shijue Huang, Bowen Jin, Mengdi Wang, Kam-Fai Wong, and Heng Ji.

</span>
<span class="ltx_bibblock">Otc: Optimal tool calls via reinforcement learning.

</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib43.1.1">arXiv preprint arXiv:2504.14870</span>, 2025.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Details of Data Refinement Pipeline </h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">In the pipeline of data refinement, we utilize QwQ-32B to assist in different levels of identification and correction, the prompts are shown as follows.</p>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A1.1.p1">
<svg class="ltx_picture" height="226.26" id="A1.1.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,226.26) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 220.36 C 0 223.62 2.64 226.26 5.91 226.26 L 594.09 226.26 C 597.36 226.26 600 223.62 600 220.36 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 202.15 L 598.03 202.15 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 208.06)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.1.p1.pic1.3.3.3.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.1.p1.pic1.3.3.3.1.1.1">Response Identification</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="176.56" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="width:402.3pt;">
<span class="ltx_p" id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2"><span class="ltx_text" id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2" style="font-size:90%;">Workflow
<br class="ltx_break"/>1) Determine whether the Reference Answer is a function_tool call or a response statement. The function call may not be in the standard function_tool call format. If it is a function_tool call, output &lt;judge&gt;True&lt;/judge&gt;; if it is a response, output &lt;judge&gt;False&lt;/judge&gt;.
<br class="ltx_break"/>Hint: Common function_tool call formats include the following, where function_name is generally from candidate_function_tools:
<br class="ltx_break"/><math alttext="[" class="ltx_Math" display="inline" id="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1" stretchy="false" xref="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">[</mo><annotation-xml encoding="MathML-Content" id="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1.1">[</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1c">[</annotation><annotation encoding="application/x-llamapun" id="A1.1.p1.pic1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.m1.1d">[</annotation></semantics></math>func_name1(params_name1=params_value1, params_name2=params_value2…), func_name2(
params_name3=params_value3, params_name4=params_value4…)<math alttext="]" class="ltx_Math" display="inline" id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1"><semantics id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1a"><mo id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1" stretchy="false" xref="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml">]</mo><annotation-xml encoding="MathML-Content" id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1b"><ci id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1.cmml" xref="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1.1">]</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1c">]</annotation><annotation encoding="application/x-llamapun" id="A1.1.p1.pic1.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.2.m2.1d">]</annotation></semantics></math>
<br class="ltx_break"/>Output format: &lt;think&gt;thought process&lt;/think&gt;&lt;judge&gt;True/False&lt;/judge&gt;
<br class="ltx_break"/>Input
<br class="ltx_break"/>Reference Answer: &lt;refANS&gt;
<br class="ltx_break"/></span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A1.2.p1">
<svg class="ltx_picture" height="193.05" id="A1.2.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,193.05) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 187.15 C 0 190.41 2.64 193.05 5.91 193.05 L 594.09 193.05 C 597.36 193.05 600 190.41 600 187.15 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 168.94 L 598.03 168.94 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 174.85)"><foreignobject color="#FFFFFF" height="12.3" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.2.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.2.p1.pic1.1.1.1.1.1.1">Query and Tool Identification</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="143.35" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.2.p1.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.2.p1.pic1.2.2.2.1.1.1"><span class="ltx_text" id="A1.2.p1.pic1.2.2.2.1.1.1.1" style="font-size:90%;">Workflow
<br class="ltx_break"/>Judge whether the User Query and Candidate Function Tools meets the following requirements:
<br class="ltx_break"/>1) Determine whether the parameter values of the function call can be analyzed from the User Query and the function name can be analyzed from the Candidate Function Tools. If parameter values and function names can be analyzed, output &lt;judge&gt;True&lt;/judge&gt;; otherwise, output &lt;judge&gt;False&lt;/judge&gt;.
<br class="ltx_break"/>Output format: &lt;think&gt;thought process&lt;/think&gt;&lt;judge&gt;True/False&lt;/judge&gt;
<br class="ltx_break"/>Input
<br class="ltx_break"/>User Query: &lt;query&gt;
<br class="ltx_break"/>Candidate Function Tools: &lt;tools&gt;</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A1.3.p1">
<svg class="ltx_picture" height="223.57" id="A1.3.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,223.57) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 217.67 C 0 220.93 2.64 223.57 5.91 223.57 L 594.09 223.57 C 597.36 223.57 600 220.93 600 217.67 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 202.15 L 598.03 202.15 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 208.06)"><foreignobject color="#FFFFFF" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.3.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.3.p1.pic1.1.1.1.1.1.1">CoT Identification</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="176.56" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.3.p1.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.3.p1.pic1.2.2.2.1.1.1"><span class="ltx_text" id="A1.3.p1.pic1.2.2.2.1.1.1.1" style="font-size:90%;">Workflow
<br class="ltx_break"/>Judge whether the Chain-of-Thought and lead the Reference Function Call by evaluating the following requirements:
<br class="ltx_break"/>1) Determine whether the Chain-of-Thought starts form a proper position.
2) Determine whether every step tightly follows the above step and
make correct inference.
3) Determine whether the last step of reasoning points to the correct answer.
If the Chain-of-Thought meets all the requirements, output &lt;judge&gt;True&lt;/judge&gt;; otherwise, output &lt;judge&gt;False&lt;/judge&gt;.
<br class="ltx_break"/>Output format: &lt;think&gt;thought process&lt;/think&gt;&lt;judge&gt;True/False&lt;/judge&gt;
<br class="ltx_break"/>Input
<br class="ltx_break"/>Chain-of-Thought: &lt;CoT process&gt;
<br class="ltx_break"/>Reference Function Call: &lt;refFC&gt;</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A1.4.p1">
<svg class="ltx_picture" height="356.41" id="A1.4.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,356.41) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 350.5 C 0 353.76 2.64 356.41 5.91 356.41 L 594.09 356.41 C 597.36 356.41 600 353.76 600 350.5 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 334.99 L 598.03 334.99 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 340.89)"><foreignobject color="#FFFFFF" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.4.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.4.p1.pic1.1.1.1.1.1.1">Function and Parameter Identification</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="309.4" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.4.p1.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.4.p1.pic1.2.2.2.1.1.1"><span class="ltx_text" id="A1.4.p1.pic1.2.2.2.1.1.1.1" style="font-size:90%;">Workflow
<br class="ltx_break"/>Judge whether the function names and parameters in the Reference Function Call meets the following requirements:
<br class="ltx_break"/>1) Determine whether the function names and parameter values of the function call are correct. If the function names and parameters are valid, output &lt;judge&gt;True&lt;/judge&gt;; otherwise, output &lt;judge&gt;False&lt;/judge&gt;.
<br class="ltx_break"/>2) When the function names and parameters requirements are met, output the new function call result according to the Reference Function Call. When the function names and parameters requirements are not met, modify the Reference Function Call format to obtain a new function call result that meets the name and parameters requirements. 
<br class="ltx_break"/>Output format:
<br class="ltx_break"/>&lt;think&gt;thought process&lt;/think&gt;&lt;judge&gt;True/False&lt;/judge&gt;
<br class="ltx_break"/>&lt;NewFC&gt;
<br class="ltx_break"/><span class="ltx_text" id="A1.4.p1.pic1.2.2.2.1.1.1.1.1">[func_name1(params_name1=params_value1, params_name2=params_value2…),</span>
<br class="ltx_break"/>func_name2(params_name3=params_value3,
params_name4=params_value4…)]
<br class="ltx_break"/>&lt;/NewFC&gt;
<br class="ltx_break"/>Input
<br class="ltx_break"/>User query: &lt;query&gt;
<br class="ltx_break"/>Candidate function tools: &lt;tools&gt;
<br class="ltx_break"/>Reference Function Call: &lt;refFC&gt;</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_para ltx_noindent ltx_align_center" id="A1.5.p1">
<svg class="ltx_picture" height="454.65" id="A1.5.p1.pic1" overflow="visible" version="1.1" width="600"><g fill="#000000" stroke="#000000" stroke-width="0.4pt" transform="translate(0,454.65) matrix(1 0 0 -1 0 0)"><g fill="#404040" fill-opacity="1.0"><path d="M 0 5.91 L 0 448.75 C 0 452.01 2.64 454.65 5.91 454.65 L 594.09 454.65 C 597.36 454.65 600 452.01 600 448.75 L 600 5.91 C 600 2.64 597.36 0 594.09 0 L 5.91 0 C 2.64 0 0 2.64 0 5.91 Z" style="stroke:none"></path></g><g fill="#F2F2F2" fill-opacity="1.0"><path d="M 1.97 5.91 L 1.97 433.23 L 598.03 433.23 L 598.03 5.91 C 598.03 3.73 596.27 1.97 594.09 1.97 L 5.91 1.97 C 3.73 1.97 1.97 3.73 1.97 5.91 Z" style="stroke:none"></path></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 439.14)"><foreignobject color="#FFFFFF" height="9.61" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.5.p1.pic1.1.1.1.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.5.p1.pic1.1.1.1.1.1.1">Format Identification</span>
</span></foreignobject></g><g fill-opacity="1.0" transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject color="#000000" height="407.64" overflow="visible" transform="matrix(1 0 0 -1 0 16.6)" width="556.69">
<span class="ltx_inline-block ltx_minipage ltx_align_bottom" id="A1.5.p1.pic1.2.2.2.1.1" style="width:402.3pt;">
<span class="ltx_p" id="A1.5.p1.pic1.2.2.2.1.1.1"><span class="ltx_text" id="A1.5.p1.pic1.2.2.2.1.1.1.1" style="font-size:90%;">Workflow
<br class="ltx_break"/>Judge whether the answer meets the following requirements:
<br class="ltx_break"/>1) The output function_tool format must satisfy the format: <span class="ltx_text" id="A1.5.p1.pic1.2.2.2.1.1.1.1.1">[func_name1(params_name1=params_value1, params_name2=params_value2…),</span>
<br class="ltx_break"/>func_name2(params_name3=params_value3,
params_name4=params_value4…)]
<br class="ltx_break"/>Note that the parameter name param_name should not be enclosed in " or ’, for example:
<br class="ltx_break"/>1. [getPrivacyViolationRisk(data="paramvalue1", purpose="paramvalue2")]
<br class="ltx_break"/>2. [getPrivacyViolationRisk(data=’paramvalue1’, purpose=’paramvalue2’)]
<br class="ltx_break"/>Where the paramValue parameter value, based on its value type, needs to be enclosed in quotes if it is a string.
<br class="ltx_break"/>2) When the output meets the format requirements, output &lt;judge&gt;True&lt;/judge&gt;; otherwise, output &lt;judge&gt;False&lt;/judge&gt;.
<br class="ltx_break"/>3) When the format requirements are met, output the new function call result according to the Reference Function Call. When the format requirements are not met, modify the Reference Function Call format to obtain a new function call result that meets the format requirements. Do not add or modify parameters and parameter values; only modify the output format.
<br class="ltx_break"/>Output format:
&lt;think&gt;thought process&lt;/think&gt;&lt;judge&gt;True/False&lt;/judge&gt;
<br class="ltx_break"/>&lt;NewFC&gt;
<br class="ltx_break"/><span class="ltx_text" id="A1.5.p1.pic1.2.2.2.1.1.1.1.2">[func_name1(params_name1=params_value1, params_name2=params_value2…),</span>
<br class="ltx_break"/>func_name2(params_name3=params_value3,
params_name4=params_value4…)]
<br class="ltx_break"/>&lt;/NewFC&gt;
<br class="ltx_break"/>Input
<br class="ltx_break"/>User query: &lt;query&gt;
<br class="ltx_break"/>Candidate function tools: &lt;tools&gt;
<br class="ltx_break"/>Reference Function Call: &lt;refFC&gt;</span></span>
</span></foreignobject></g></g></svg>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 26 16:35:59 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

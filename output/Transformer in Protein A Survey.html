<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Transformer in Protein: A Survey</title>
<!--Generated on Mon May 26 14:57:45 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Transformers,  proteomics,  protein interactions,  bioinformatics.
" lang="en" name="keywords"/>
<base href="/html/2505.20098v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S1" title="In Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2" title="In Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Foundations</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS1" title="In II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Transformer Architecture and Self-Attention Mechanism</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS1.SSS1" title="In II-A Transformer Architecture and Self-Attention Mechanism ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span>1 </span>Single-head vs. Multi-head Self-Attention in Protein Transformers</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS2" title="In II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">(Self-)supervised pre-training</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS3" title="In II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Transformer Derivatives in Protein Informatics</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS3.SSS1" title="In II-C Transformer Derivatives in Protein Informatics ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>1 </span>Key Derivatives of the Transformer Architecture</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS3.SSS2" title="In II-C Transformer Derivatives in Protein Informatics ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span>2 </span>Future Directions for Transformer Derivatives</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS4" title="In II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span> </span><span class="ltx_text ltx_font_italic">Fundamentals of Protein Structure and Function</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS4.SSS1" title="In II-D Fundamentals of Protein Structure and Function ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>1 </span>Hierarchical Organization of Protein Structure</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS4.SSS2" title="In II-D Fundamentals of Protein Structure and Function ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>2 </span>Protein Folding and Structural Stability</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS4.SSS3" title="In II-D Fundamentals of Protein Structure and Function ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-D</span>3 </span>The Relationship Between Protein Structure and Function</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3" title="In Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Transformers in Protein</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.SS1" title="In III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Transformers for Protein Structure Prediction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.SS2" title="In III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Transformers for Protein Function Prediction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.SS3" title="In III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Transformers for Protein-Protein Interactions (PPIs)</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.SS4" title="In III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-D</span> </span><span class="ltx_text ltx_font_italic">Transformers for Drug Discovery and Target Identification</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S4" title="In Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Advantages &amp; Challenges of Transformer Models</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S4.SS1" title="In IV Advantages &amp; Challenges of Transformer Models ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Advantages of Transformers in Protein Informatics</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S4.SS2" title="In IV Advantages &amp; Challenges of Transformer Models ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Challenges of Applying Transformers in Protein Research</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S5" title="In Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">FUTURE DIRECTIONS</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S6" title="In Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">CONCLUSION</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S7" title="In Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Resources and Reproducibility</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S7.SS1" title="In VII Resources and Reproducibility ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-A</span> </span><span class="ltx_text ltx_font_italic">Open Source Implementation</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S7.SS2" title="In VII Resources and Reproducibility ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VII-B</span> </span><span class="ltx_text ltx_font_italic">Datasets Used</span></span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Transformer in Protein: A Survey
<br class="ltx_break"/>
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Xiaowen Ling
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id1.1.id1">Department of Electronics and Computer Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id2.2.id2">Shenzhen MSU-BIT University
<br class="ltx_break"/></span>Shenzhen, China 
<br class="ltx_break"/>1120220517@smbu.edu.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhiqiang Li
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id3.1.id1">Department of Electronics and Computer Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id4.2.id2">Shenzhen MSU-BIT University
<br class="ltx_break"/></span>Shenzhen, China 
<br class="ltx_break"/>18046988607@163.com
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yanbin Wang<sup class="ltx_sup" id="id5.1.id1">*</sup>
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id6.2.id1">Department of Electronics and Computer Engineering</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id7.3.id2">Shenzhen MSU-BIT University
<br class="ltx_break"/></span>Shenzhen, China 
<br class="ltx_break"/>wyb@smbu.edu.cn
</span></span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Zhuhong You
</span><span class="ltx_author_notes">
<span class="ltx_contact ltx_role_affiliation"><span class="ltx_text ltx_font_italic" id="id8.1.id1">School of Computer Science and Technology</span>
<br class="ltx_break"/><span class="ltx_text ltx_font_italic" id="id9.2.id2">Northwestern Polytechnical University
<br class="ltx_break"/></span>Xi’an, China 
<br class="ltx_break"/>zhuhongyou@nwpu.edu.cn
</span></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id10.id1">As protein informatics advances rapidly, the demand for enhanced predictive accuracy, structural analysis, and functional understanding has intensified. Transformer models, as powerful deep learning architectures, have demonstrated unprecedented potential in addressing diverse challenges across protein research. However, a comprehensive review of Transformer applications in this field remains lacking. This paper bridges this gap by surveying over 100 studies, offering an in-depth analysis of practical implementations and research progress of Transformers in protein-related tasks. Our review systematically covers critical domains, including protein structure prediction, function prediction, protein-protein interaction analysis, functional annotation, and drug discovery/target identification.
To contextualize these advancements across various protein domains, we adopt a domain-oriented classification system. We first introduce foundational concepts: the Transformer architecture and attention mechanisms, categorize Transformer variants tailored for protein science, and summarize essential protein knowledge. For each research domain, we outline its objectives and background, critically evaluate prior methods and their limitations, and highlight transformative contributions enabled by Transformer models. We also curate and summarize pivotal datasets and open-source code resources to facilitate reproducibility and benchmarking.
Finally, we discuss persistent challenges in applying Transformers to protein informatics and propose future research directions. This review aims to provide a consolidated foundation for the synergistic integration of Transformer and protein informatics, fostering further innovation and expanded applications in the field.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Transformers, proteomics, protein interactions, bioinformatics.

</div>
<span class="ltx_note ltx_role_footnotetext" id="footnotex1"><sup class="ltx_note_mark">†</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">†</sup><span class="ltx_note_type">footnotetext: </span>
<sup class="ltx_sup" id="footnotex1.1">*</sup>Corresponding author: Yanbin Wang (email: wyb@smbu.edu.cn)
</span></span></span>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Protein informatics, a cornerstone of bioinformatics, is dedicated to deciphering protein structures, functions, and interactions. The exponential growth of biological data—particularly in proteomics—has intensified the demand for computational methods capable of efficiently and accurately interpreting complex protein datasets. While traditional approaches have laid a foundational understanding, their limitations in handling the scale and intricacy of modern protein data have spurred the adoption of more sophisticated models.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this context, transformer models have emerged as a transformative solution across disciplines <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib6" title="">6</a>]</cite>, owing to their ability to process variable-length sequences and model long-range dependencies via self-attention mechanisms. These attributes are especially valuable in protein informatics, where sequence-structure-function relationships often hinge on distal interactions and hierarchical patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib8" title="">8</a>]</cite>. Notably, transformer-based architectures (e.g., BERT, GPT) have achieved breakthroughs in protein structure prediction <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib9" title="">9</a>]</cite>, interaction analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib10" title="">10</a>]</cite>, and functional annotation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">The rising prominence of transformers in protein research is evident from publication trends. As depicted in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>(a), the number of studies leveraging transformer models has surged in recent years, reflecting their growing adoption for protein-related tasks. To assess the impact of this trend in high-impact venues, we analyzed publications in Nature, Science, and Cell (including sub-journals) (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>(b)). Furthermore, we classified these works into key subfields of protein research (Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S1.F1" title="Figure 1 ‣ I Introduction ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">1</span></a>(b)), revealing the breadth of transformer applications in the discipline.</p>
</div>
<figure class="ltx_figure" id="S1.F1">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S1.F1.sf1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf1.2.1.1" style="font-size:90%;">(a)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S1.F1.sf2.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf2.2.1.1" style="font-size:90%;">(b)</span> </span></figcaption>
</figure>
</div>
<div class="ltx_flex_cell ltx_flex_size_3">
<figure class="ltx_figure ltx_figure_panel ltx_align_center" id="S1.F1.sf3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="145" id="S1.F1.sf3.g1" src="x3.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.sf3.2.1.1" style="font-size:90%;">(c)</span> </span></figcaption>
</figure>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Analysis of Transformer models in protein research using data from the Web of Science Core Collection. (a) Counts the number of publications and citations over the past four years, highlighting the growing influence of this research area. (b) Shows the distribution of publications across prestigious journals such as Nature, Science, and Cell. (c) Provides an overview of publications categorized by subfields from 2022 to 2025 YTD, illustrating the diverse applications of Transformer models in protein.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Transformer models have demonstrated remarkable potential in protein research, yet the field lacks a comprehensive synthesis of their applications, leaving critical gaps in understanding their full impact on protein informatics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib13" title="">13</a>]</cite>. This absence extends to systematic collections of essential resources, including benchmark datasets and algorithmic implementations that could accelerate progress. In response, our review undertakes a methodical examination of over 100 studies, analyzing both theoretical advances and practical applications across four pivotal domains: protein structure prediction, functional annotation, interaction analysis, and pharmaceutical discovery.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">The review begins by establishing fundamental concepts, including transformer architectures, their protein-specific adaptations, and relevant biological principles. Building on this foundation, we employ a domain-oriented framework to evaluate each application area, first contextualizing its scientific objectives before critically assessing how transformer-based approaches compare to conventional methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib16" title="">16</a>]</cite>. Our analysis not only highlights transformative contributions but also curates vital computational resources, compiling authoritative datasets and state-of-the-art codebases to facilitate reproducibility and future innovation.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">While documenting these advances, we identify persistent challenges in deploying transformers for protein studies, particularly regarding computational efficiency, data requirements, and model interpretability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib18" title="">18</a>]</cite>. These limitations inform our proposed research directions, which emphasize architectural specialization for biological tasks and hybrid approaches combining machine learning with biophysical principles. By synthesizing current knowledge and outlining pathways for improvement, this work seeks to strengthen the synergy between artificial intelligence and protein science, ultimately enabling novel methodologies that could reshape both fields <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib19" title="">19</a>]</cite>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Foundations</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Originally developed for NLP, Transformer models have proven highly effective in protein informatics due to their ability to process sequential data and capture long-range dependencies in protein sequences. Their success stems from two key innovations: (1) the self-attention mechanism, which overcomes traditional recurrent architectures’ limitations in modeling distant sequence relationships, and (2) the pre-training paradigm using large-scale (un)labeled data with (self-)supervised learning, followed by task-specific fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib21" title="">21</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">The paper is organized as follows: Sections <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS1" title="II-A Transformer Architecture and Self-Attention Mechanism ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-A</span></span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS2" title="II-B (Self-)supervised pre-training ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-B</span></span></a> introduce Transformer fundamentals, Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS3" title="II-C Transformer Derivatives in Protein Informatics ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-C</span></span></a> discusses protein-specific variants, and Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.SS4" title="II-D Fundamentals of Protein Structure and Function ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">II-D</span></span></a> reviews essential protein biology. This foundation supports our analysis of Transformer applications in protein informatics (Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3" title="III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">III</span></a>).</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Transformer Architecture and Self-Attention Mechanism</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The Transformer model, a novel network architecture introduced by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib1" title="">1</a>]</cite>, entirely discards recurrence and convolution, relying solely on attention mechanisms. This design allows the model to dynamically assess the importance of different sequence elements. Their paper highlights that experiments on two machine translation tasks demonstrate the model’s superior quality, greater feasibility, and significantly reduced training time. This self-attention mechanism computes the relationships between all pairs of input elements simultaneously, enabling efficient parallel processing, which is crucial for handling large datasets in protein informatics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib3" title="">3</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib22" title="">22</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.3">Self-attention dynamically models pairwise relevance between elements in a sequence (e.g., quantifying co-occurrence probabilities between words in a sentence) to explicitly capture intrasequence dependencies. As a core computational primitive in Transformer architectures, this mechanism enables differentiable relational reasoning through full-sequence interaction modeling for structured prediction tasks. Computationally, each self-attention layer iteratively refines the feature representation at every sequence position via global contextual aggregation.
This is achieved by defining three learnable weight matrices:</p>
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.1">Query matrix: <math alttext="W^{Q}\in\mathbb{R}^{d\times d_{q}}" class="ltx_Math" display="inline" id="S2.I1.i1.p1.1.m1.1"><semantics id="S2.I1.i1.p1.1.m1.1a"><mrow id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml"><msup id="S2.I1.i1.p1.1.m1.1.1.2" xref="S2.I1.i1.p1.1.m1.1.1.2.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.2.2" xref="S2.I1.i1.p1.1.m1.1.1.2.2.cmml">W</mi><mi id="S2.I1.i1.p1.1.m1.1.1.2.3" xref="S2.I1.i1.p1.1.m1.1.1.2.3.cmml">Q</mi></msup><mo id="S2.I1.i1.p1.1.m1.1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S2.I1.i1.p1.1.m1.1.1.3" xref="S2.I1.i1.p1.1.m1.1.1.3.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.3.2" xref="S2.I1.i1.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.I1.i1.p1.1.m1.1.1.3.3" xref="S2.I1.i1.p1.1.m1.1.1.3.3.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.3.3.2" xref="S2.I1.i1.p1.1.m1.1.1.3.3.2.cmml">d</mi><mo id="S2.I1.i1.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i1.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S2.I1.i1.p1.1.m1.1.1.3.3.3" xref="S2.I1.i1.p1.1.m1.1.1.3.3.3.cmml"><mi id="S2.I1.i1.p1.1.m1.1.1.3.3.3.2" xref="S2.I1.i1.p1.1.m1.1.1.3.3.3.2.cmml">d</mi><mi id="S2.I1.i1.p1.1.m1.1.1.3.3.3.3" xref="S2.I1.i1.p1.1.m1.1.1.3.3.3.3.cmml">q</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><apply id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1"><in id="S2.I1.i1.p1.1.m1.1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.1"></in><apply id="S2.I1.i1.p1.1.m1.1.1.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2">superscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2.2">𝑊</ci><ci id="S2.I1.i1.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.2.3">𝑄</ci></apply><apply id="S2.I1.i1.p1.1.m1.1.1.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S2.I1.i1.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3"><times id="S2.I1.i1.p1.1.m1.1.1.3.3.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3.1"></times><ci id="S2.I1.i1.p1.1.m1.1.1.3.3.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3.2">𝑑</ci><apply id="S2.I1.i1.p1.1.m1.1.1.3.3.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.I1.i1.p1.1.m1.1.1.3.3.3.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.I1.i1.p1.1.m1.1.1.3.3.3.2.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3.3.2">𝑑</ci><ci id="S2.I1.i1.p1.1.m1.1.1.3.3.3.3.cmml" xref="S2.I1.i1.p1.1.m1.1.1.3.3.3.3">𝑞</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">W^{Q}\in\mathbb{R}^{d\times d_{q}}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.1.m1.1d">italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.1">Key matrix: <math alttext="W^{K}\in\mathbb{R}^{d\times d_{k}}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><mrow id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><msup id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2.2" xref="S2.I1.i2.p1.1.m1.1.1.2.2.cmml">W</mi><mi id="S2.I1.i2.p1.1.m1.1.1.2.3" xref="S2.I1.i2.p1.1.m1.1.1.2.3.cmml">K</mi></msup><mo id="S2.I1.i2.p1.1.m1.1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.3.2" xref="S2.I1.i2.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.I1.i2.p1.1.m1.1.1.3.3" xref="S2.I1.i2.p1.1.m1.1.1.3.3.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.3.3.2" xref="S2.I1.i2.p1.1.m1.1.1.3.3.2.cmml">d</mi><mo id="S2.I1.i2.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i2.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S2.I1.i2.p1.1.m1.1.1.3.3.3" xref="S2.I1.i2.p1.1.m1.1.1.3.3.3.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.3.3.3.2" xref="S2.I1.i2.p1.1.m1.1.1.3.3.3.2.cmml">d</mi><mi id="S2.I1.i2.p1.1.m1.1.1.3.3.3.3" xref="S2.I1.i2.p1.1.m1.1.1.3.3.3.3.cmml">k</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><in id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.1"></in><apply id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">superscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.2">𝑊</ci><ci id="S2.I1.i2.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2.3">𝐾</ci></apply><apply id="S2.I1.i2.p1.1.m1.1.1.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S2.I1.i2.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.3"><times id="S2.I1.i2.p1.1.m1.1.1.3.3.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.3.1"></times><ci id="S2.I1.i2.p1.1.m1.1.1.3.3.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.3.2">𝑑</ci><apply id="S2.I1.i2.p1.1.m1.1.1.3.3.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.3.3.3.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.3.3.3.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.3.3.2">𝑑</ci><ci id="S2.I1.i2.p1.1.m1.1.1.3.3.3.3.cmml" xref="S2.I1.i2.p1.1.m1.1.1.3.3.3.3">𝑘</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">W^{K}\in\mathbb{R}^{d\times d_{k}}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.1">Value matrix: <math alttext="W^{V}\in\mathbb{R}^{d\times d_{v}}" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><mrow id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><msup id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2.2" xref="S2.I1.i3.p1.1.m1.1.1.2.2.cmml">W</mi><mi id="S2.I1.i3.p1.1.m1.1.1.2.3" xref="S2.I1.i3.p1.1.m1.1.1.2.3.cmml">V</mi></msup><mo id="S2.I1.i3.p1.1.m1.1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.3.2" xref="S2.I1.i3.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.I1.i3.p1.1.m1.1.1.3.3" xref="S2.I1.i3.p1.1.m1.1.1.3.3.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.3.3.2" xref="S2.I1.i3.p1.1.m1.1.1.3.3.2.cmml">d</mi><mo id="S2.I1.i3.p1.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.I1.i3.p1.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S2.I1.i3.p1.1.m1.1.1.3.3.3" xref="S2.I1.i3.p1.1.m1.1.1.3.3.3.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.3.3.3.2" xref="S2.I1.i3.p1.1.m1.1.1.3.3.3.2.cmml">d</mi><mi id="S2.I1.i3.p1.1.m1.1.1.3.3.3.3" xref="S2.I1.i3.p1.1.m1.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><in id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.1"></in><apply id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.2.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">superscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.2.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.2">𝑊</ci><ci id="S2.I1.i3.p1.1.m1.1.1.2.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2.3">𝑉</ci></apply><apply id="S2.I1.i3.p1.1.m1.1.1.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.3.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3">superscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.3.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.2">ℝ</ci><apply id="S2.I1.i3.p1.1.m1.1.1.3.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3"><times id="S2.I1.i3.p1.1.m1.1.1.3.3.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3.1"></times><ci id="S2.I1.i3.p1.1.m1.1.1.3.3.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3.2">𝑑</ci><apply id="S2.I1.i3.p1.1.m1.1.1.3.3.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.3.3.3.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.3.3.3.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3.3.2">𝑑</ci><ci id="S2.I1.i3.p1.1.m1.1.1.3.3.3.3.cmml" xref="S2.I1.i3.p1.1.m1.1.1.3.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">W^{V}\in\mathbb{R}^{d\times d_{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d × italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math></p>
</div>
</li>
</ul>
<p class="ltx_p" id="S2.SS1.p2.2">where <math alttext="d_{q}=d_{k}" class="ltx_Math" display="inline" id="S2.SS1.p2.1.m1.1"><semantics id="S2.SS1.p2.1.m1.1a"><mrow id="S2.SS1.p2.1.m1.1.1" xref="S2.SS1.p2.1.m1.1.1.cmml"><msub id="S2.SS1.p2.1.m1.1.1.2" xref="S2.SS1.p2.1.m1.1.1.2.cmml"><mi id="S2.SS1.p2.1.m1.1.1.2.2" xref="S2.SS1.p2.1.m1.1.1.2.2.cmml">d</mi><mi id="S2.SS1.p2.1.m1.1.1.2.3" xref="S2.SS1.p2.1.m1.1.1.2.3.cmml">q</mi></msub><mo id="S2.SS1.p2.1.m1.1.1.1" xref="S2.SS1.p2.1.m1.1.1.1.cmml">=</mo><msub id="S2.SS1.p2.1.m1.1.1.3" xref="S2.SS1.p2.1.m1.1.1.3.cmml"><mi id="S2.SS1.p2.1.m1.1.1.3.2" xref="S2.SS1.p2.1.m1.1.1.3.2.cmml">d</mi><mi id="S2.SS1.p2.1.m1.1.1.3.3" xref="S2.SS1.p2.1.m1.1.1.3.3.cmml">k</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.1.m1.1b"><apply id="S2.SS1.p2.1.m1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1"><eq id="S2.SS1.p2.1.m1.1.1.1.cmml" xref="S2.SS1.p2.1.m1.1.1.1"></eq><apply id="S2.SS1.p2.1.m1.1.1.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.2.1.cmml" xref="S2.SS1.p2.1.m1.1.1.2">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.2.2.cmml" xref="S2.SS1.p2.1.m1.1.1.2.2">𝑑</ci><ci id="S2.SS1.p2.1.m1.1.1.2.3.cmml" xref="S2.SS1.p2.1.m1.1.1.2.3">𝑞</ci></apply><apply id="S2.SS1.p2.1.m1.1.1.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p2.1.m1.1.1.3.1.cmml" xref="S2.SS1.p2.1.m1.1.1.3">subscript</csymbol><ci id="S2.SS1.p2.1.m1.1.1.3.2.cmml" xref="S2.SS1.p2.1.m1.1.1.3.2">𝑑</ci><ci id="S2.SS1.p2.1.m1.1.1.3.3.cmml" xref="S2.SS1.p2.1.m1.1.1.3.3">𝑘</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.1.m1.1c">d_{q}=d_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT = italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math>. The input sequence <math alttext="X" class="ltx_Math" display="inline" id="S2.SS1.p2.2.m2.1"><semantics id="S2.SS1.p2.2.m2.1a"><mi id="S2.SS1.p2.2.m2.1.1" xref="S2.SS1.p2.2.m2.1.1.cmml">X</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p2.2.m2.1b"><ci id="S2.SS1.p2.2.m2.1.1.cmml" xref="S2.SS1.p2.2.m2.1.1">𝑋</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p2.2.m2.1c">X</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p2.2.m2.1d">italic_X</annotation></semantics></math> is projected as:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S7.EGx1">
<tbody id="S2.Ex1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle Q" class="ltx_Math" display="inline" id="S2.Ex1.m1.1"><semantics id="S2.Ex1.m1.1a"><mi id="S2.Ex1.m1.1.1" xref="S2.Ex1.m1.1.1.cmml">Q</mi><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.1b"><ci id="S2.Ex1.m1.1.1.cmml" xref="S2.Ex1.m1.1.1">𝑄</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.1c">\displaystyle Q</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.1d">italic_Q</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=XW^{Q}" class="ltx_Math" display="inline" id="S2.Ex1.m2.1"><semantics id="S2.Ex1.m2.1a"><mrow id="S2.Ex1.m2.1.1" xref="S2.Ex1.m2.1.1.cmml"><mi id="S2.Ex1.m2.1.1.2" xref="S2.Ex1.m2.1.1.2.cmml"></mi><mo id="S2.Ex1.m2.1.1.1" xref="S2.Ex1.m2.1.1.1.cmml">=</mo><mrow id="S2.Ex1.m2.1.1.3" xref="S2.Ex1.m2.1.1.3.cmml"><mi id="S2.Ex1.m2.1.1.3.2" xref="S2.Ex1.m2.1.1.3.2.cmml">X</mi><mo id="S2.Ex1.m2.1.1.3.1" xref="S2.Ex1.m2.1.1.3.1.cmml">⁢</mo><msup id="S2.Ex1.m2.1.1.3.3" xref="S2.Ex1.m2.1.1.3.3.cmml"><mi id="S2.Ex1.m2.1.1.3.3.2" xref="S2.Ex1.m2.1.1.3.3.2.cmml">W</mi><mi id="S2.Ex1.m2.1.1.3.3.3" xref="S2.Ex1.m2.1.1.3.3.3.cmml">Q</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m2.1b"><apply id="S2.Ex1.m2.1.1.cmml" xref="S2.Ex1.m2.1.1"><eq id="S2.Ex1.m2.1.1.1.cmml" xref="S2.Ex1.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex1.m2.1.1.2.cmml" xref="S2.Ex1.m2.1.1.2">absent</csymbol><apply id="S2.Ex1.m2.1.1.3.cmml" xref="S2.Ex1.m2.1.1.3"><times id="S2.Ex1.m2.1.1.3.1.cmml" xref="S2.Ex1.m2.1.1.3.1"></times><ci id="S2.Ex1.m2.1.1.3.2.cmml" xref="S2.Ex1.m2.1.1.3.2">𝑋</ci><apply id="S2.Ex1.m2.1.1.3.3.cmml" xref="S2.Ex1.m2.1.1.3.3"><csymbol cd="ambiguous" id="S2.Ex1.m2.1.1.3.3.1.cmml" xref="S2.Ex1.m2.1.1.3.3">superscript</csymbol><ci id="S2.Ex1.m2.1.1.3.3.2.cmml" xref="S2.Ex1.m2.1.1.3.3.2">𝑊</ci><ci id="S2.Ex1.m2.1.1.3.3.3.cmml" xref="S2.Ex1.m2.1.1.3.3.3">𝑄</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m2.1c">\displaystyle=XW^{Q}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m2.1d">= italic_X italic_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle K" class="ltx_Math" display="inline" id="S2.Ex2.m1.1"><semantics id="S2.Ex2.m1.1a"><mi id="S2.Ex2.m1.1.1" xref="S2.Ex2.m1.1.1.cmml">K</mi><annotation-xml encoding="MathML-Content" id="S2.Ex2.m1.1b"><ci id="S2.Ex2.m1.1.1.cmml" xref="S2.Ex2.m1.1.1">𝐾</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m1.1c">\displaystyle K</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m1.1d">italic_K</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=XW^{K}" class="ltx_Math" display="inline" id="S2.Ex2.m2.1"><semantics id="S2.Ex2.m2.1a"><mrow id="S2.Ex2.m2.1.1" xref="S2.Ex2.m2.1.1.cmml"><mi id="S2.Ex2.m2.1.1.2" xref="S2.Ex2.m2.1.1.2.cmml"></mi><mo id="S2.Ex2.m2.1.1.1" xref="S2.Ex2.m2.1.1.1.cmml">=</mo><mrow id="S2.Ex2.m2.1.1.3" xref="S2.Ex2.m2.1.1.3.cmml"><mi id="S2.Ex2.m2.1.1.3.2" xref="S2.Ex2.m2.1.1.3.2.cmml">X</mi><mo id="S2.Ex2.m2.1.1.3.1" xref="S2.Ex2.m2.1.1.3.1.cmml">⁢</mo><msup id="S2.Ex2.m2.1.1.3.3" xref="S2.Ex2.m2.1.1.3.3.cmml"><mi id="S2.Ex2.m2.1.1.3.3.2" xref="S2.Ex2.m2.1.1.3.3.2.cmml">W</mi><mi id="S2.Ex2.m2.1.1.3.3.3" xref="S2.Ex2.m2.1.1.3.3.3.cmml">K</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex2.m2.1b"><apply id="S2.Ex2.m2.1.1.cmml" xref="S2.Ex2.m2.1.1"><eq id="S2.Ex2.m2.1.1.1.cmml" xref="S2.Ex2.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex2.m2.1.1.2.cmml" xref="S2.Ex2.m2.1.1.2">absent</csymbol><apply id="S2.Ex2.m2.1.1.3.cmml" xref="S2.Ex2.m2.1.1.3"><times id="S2.Ex2.m2.1.1.3.1.cmml" xref="S2.Ex2.m2.1.1.3.1"></times><ci id="S2.Ex2.m2.1.1.3.2.cmml" xref="S2.Ex2.m2.1.1.3.2">𝑋</ci><apply id="S2.Ex2.m2.1.1.3.3.cmml" xref="S2.Ex2.m2.1.1.3.3"><csymbol cd="ambiguous" id="S2.Ex2.m2.1.1.3.3.1.cmml" xref="S2.Ex2.m2.1.1.3.3">superscript</csymbol><ci id="S2.Ex2.m2.1.1.3.3.2.cmml" xref="S2.Ex2.m2.1.1.3.3.2">𝑊</ci><ci id="S2.Ex2.m2.1.1.3.3.3.cmml" xref="S2.Ex2.m2.1.1.3.3.3">𝐾</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex2.m2.1c">\displaystyle=XW^{K}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex2.m2.1d">= italic_X italic_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
<tbody id="S2.Ex3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle V" class="ltx_Math" display="inline" id="S2.Ex3.m1.1"><semantics id="S2.Ex3.m1.1a"><mi id="S2.Ex3.m1.1.1" xref="S2.Ex3.m1.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S2.Ex3.m1.1b"><ci id="S2.Ex3.m1.1.1.cmml" xref="S2.Ex3.m1.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m1.1c">\displaystyle V</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m1.1d">italic_V</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=XW^{V}" class="ltx_Math" display="inline" id="S2.Ex3.m2.1"><semantics id="S2.Ex3.m2.1a"><mrow id="S2.Ex3.m2.1.1" xref="S2.Ex3.m2.1.1.cmml"><mi id="S2.Ex3.m2.1.1.2" xref="S2.Ex3.m2.1.1.2.cmml"></mi><mo id="S2.Ex3.m2.1.1.1" xref="S2.Ex3.m2.1.1.1.cmml">=</mo><mrow id="S2.Ex3.m2.1.1.3" xref="S2.Ex3.m2.1.1.3.cmml"><mi id="S2.Ex3.m2.1.1.3.2" xref="S2.Ex3.m2.1.1.3.2.cmml">X</mi><mo id="S2.Ex3.m2.1.1.3.1" xref="S2.Ex3.m2.1.1.3.1.cmml">⁢</mo><msup id="S2.Ex3.m2.1.1.3.3" xref="S2.Ex3.m2.1.1.3.3.cmml"><mi id="S2.Ex3.m2.1.1.3.3.2" xref="S2.Ex3.m2.1.1.3.3.2.cmml">W</mi><mi id="S2.Ex3.m2.1.1.3.3.3" xref="S2.Ex3.m2.1.1.3.3.3.cmml">V</mi></msup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex3.m2.1b"><apply id="S2.Ex3.m2.1.1.cmml" xref="S2.Ex3.m2.1.1"><eq id="S2.Ex3.m2.1.1.1.cmml" xref="S2.Ex3.m2.1.1.1"></eq><csymbol cd="latexml" id="S2.Ex3.m2.1.1.2.cmml" xref="S2.Ex3.m2.1.1.2">absent</csymbol><apply id="S2.Ex3.m2.1.1.3.cmml" xref="S2.Ex3.m2.1.1.3"><times id="S2.Ex3.m2.1.1.3.1.cmml" xref="S2.Ex3.m2.1.1.3.1"></times><ci id="S2.Ex3.m2.1.1.3.2.cmml" xref="S2.Ex3.m2.1.1.3.2">𝑋</ci><apply id="S2.Ex3.m2.1.1.3.3.cmml" xref="S2.Ex3.m2.1.1.3.3"><csymbol cd="ambiguous" id="S2.Ex3.m2.1.1.3.3.1.cmml" xref="S2.Ex3.m2.1.1.3.3">superscript</csymbol><ci id="S2.Ex3.m2.1.1.3.3.2.cmml" xref="S2.Ex3.m2.1.1.3.3.2">𝑊</ci><ci id="S2.Ex3.m2.1.1.3.3.3.cmml" xref="S2.Ex3.m2.1.1.3.3.3">𝑉</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex3.m2.1c">\displaystyle=XW^{V}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex3.m2.1d">= italic_X italic_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">The output <math alttext="Z\in\mathbb{R}^{n\times d_{v}}" class="ltx_Math" display="inline" id="S2.SS1.p3.1.m1.1"><semantics id="S2.SS1.p3.1.m1.1a"><mrow id="S2.SS1.p3.1.m1.1.1" xref="S2.SS1.p3.1.m1.1.1.cmml"><mi id="S2.SS1.p3.1.m1.1.1.2" xref="S2.SS1.p3.1.m1.1.1.2.cmml">Z</mi><mo id="S2.SS1.p3.1.m1.1.1.1" xref="S2.SS1.p3.1.m1.1.1.1.cmml">∈</mo><msup id="S2.SS1.p3.1.m1.1.1.3" xref="S2.SS1.p3.1.m1.1.1.3.cmml"><mi id="S2.SS1.p3.1.m1.1.1.3.2" xref="S2.SS1.p3.1.m1.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p3.1.m1.1.1.3.3" xref="S2.SS1.p3.1.m1.1.1.3.3.cmml"><mi id="S2.SS1.p3.1.m1.1.1.3.3.2" xref="S2.SS1.p3.1.m1.1.1.3.3.2.cmml">n</mi><mo id="S2.SS1.p3.1.m1.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p3.1.m1.1.1.3.3.1.cmml">×</mo><msub id="S2.SS1.p3.1.m1.1.1.3.3.3" xref="S2.SS1.p3.1.m1.1.1.3.3.3.cmml"><mi id="S2.SS1.p3.1.m1.1.1.3.3.3.2" xref="S2.SS1.p3.1.m1.1.1.3.3.3.2.cmml">d</mi><mi id="S2.SS1.p3.1.m1.1.1.3.3.3.3" xref="S2.SS1.p3.1.m1.1.1.3.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p3.1.m1.1b"><apply id="S2.SS1.p3.1.m1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1"><in id="S2.SS1.p3.1.m1.1.1.1.cmml" xref="S2.SS1.p3.1.m1.1.1.1"></in><ci id="S2.SS1.p3.1.m1.1.1.2.cmml" xref="S2.SS1.p3.1.m1.1.1.2">𝑍</ci><apply id="S2.SS1.p3.1.m1.1.1.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.1.3.1.cmml" xref="S2.SS1.p3.1.m1.1.1.3">superscript</csymbol><ci id="S2.SS1.p3.1.m1.1.1.3.2.cmml" xref="S2.SS1.p3.1.m1.1.1.3.2">ℝ</ci><apply id="S2.SS1.p3.1.m1.1.1.3.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3.3"><times id="S2.SS1.p3.1.m1.1.1.3.3.1.cmml" xref="S2.SS1.p3.1.m1.1.1.3.3.1"></times><ci id="S2.SS1.p3.1.m1.1.1.3.3.2.cmml" xref="S2.SS1.p3.1.m1.1.1.3.3.2">𝑛</ci><apply id="S2.SS1.p3.1.m1.1.1.3.3.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p3.1.m1.1.1.3.3.3.1.cmml" xref="S2.SS1.p3.1.m1.1.1.3.3.3">subscript</csymbol><ci id="S2.SS1.p3.1.m1.1.1.3.3.3.2.cmml" xref="S2.SS1.p3.1.m1.1.1.3.3.3.2">𝑑</ci><ci id="S2.SS1.p3.1.m1.1.1.3.3.3.3.cmml" xref="S2.SS1.p3.1.m1.1.1.3.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p3.1.m1.1c">Z\in\mathbb{R}^{n\times d_{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p3.1.m1.1d">italic_Z ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> of the self-attention layer is computed by:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="Z=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{q}}}\right)V" class="ltx_Math" display="block" id="S2.E1.m1.1"><semantics id="S2.E1.m1.1a"><mrow id="S2.E1.m1.1.2" xref="S2.E1.m1.1.2.cmml"><mi id="S2.E1.m1.1.2.2" xref="S2.E1.m1.1.2.2.cmml">Z</mi><mo id="S2.E1.m1.1.2.1" xref="S2.E1.m1.1.2.1.cmml">=</mo><mrow id="S2.E1.m1.1.2.3" xref="S2.E1.m1.1.2.3.cmml"><mtext id="S2.E1.m1.1.2.3.2" xref="S2.E1.m1.1.2.3.2a.cmml">softmax</mtext><mo id="S2.E1.m1.1.2.3.1" xref="S2.E1.m1.1.2.3.1.cmml">⁢</mo><mrow id="S2.E1.m1.1.2.3.3.2" xref="S2.E1.m1.1.1.cmml"><mo id="S2.E1.m1.1.2.3.3.2.1" xref="S2.E1.m1.1.1.cmml">(</mo><mfrac id="S2.E1.m1.1.1" xref="S2.E1.m1.1.1.cmml"><mrow id="S2.E1.m1.1.1.2" xref="S2.E1.m1.1.1.2.cmml"><mi id="S2.E1.m1.1.1.2.2" xref="S2.E1.m1.1.1.2.2.cmml">Q</mi><mo id="S2.E1.m1.1.1.2.1" xref="S2.E1.m1.1.1.2.1.cmml">⁢</mo><msup id="S2.E1.m1.1.1.2.3" xref="S2.E1.m1.1.1.2.3.cmml"><mi id="S2.E1.m1.1.1.2.3.2" xref="S2.E1.m1.1.1.2.3.2.cmml">K</mi><mi id="S2.E1.m1.1.1.2.3.3" xref="S2.E1.m1.1.1.2.3.3.cmml">T</mi></msup></mrow><msqrt id="S2.E1.m1.1.1.3" xref="S2.E1.m1.1.1.3.cmml"><msub id="S2.E1.m1.1.1.3.2" xref="S2.E1.m1.1.1.3.2.cmml"><mi id="S2.E1.m1.1.1.3.2.2" xref="S2.E1.m1.1.1.3.2.2.cmml">d</mi><mi id="S2.E1.m1.1.1.3.2.3" xref="S2.E1.m1.1.1.3.2.3.cmml">q</mi></msub></msqrt></mfrac><mo id="S2.E1.m1.1.2.3.3.2.2" xref="S2.E1.m1.1.1.cmml">)</mo></mrow><mo id="S2.E1.m1.1.2.3.1a" xref="S2.E1.m1.1.2.3.1.cmml">⁢</mo><mi id="S2.E1.m1.1.2.3.4" xref="S2.E1.m1.1.2.3.4.cmml">V</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.1b"><apply id="S2.E1.m1.1.2.cmml" xref="S2.E1.m1.1.2"><eq id="S2.E1.m1.1.2.1.cmml" xref="S2.E1.m1.1.2.1"></eq><ci id="S2.E1.m1.1.2.2.cmml" xref="S2.E1.m1.1.2.2">𝑍</ci><apply id="S2.E1.m1.1.2.3.cmml" xref="S2.E1.m1.1.2.3"><times id="S2.E1.m1.1.2.3.1.cmml" xref="S2.E1.m1.1.2.3.1"></times><ci id="S2.E1.m1.1.2.3.2a.cmml" xref="S2.E1.m1.1.2.3.2"><mtext id="S2.E1.m1.1.2.3.2.cmml" xref="S2.E1.m1.1.2.3.2">softmax</mtext></ci><apply id="S2.E1.m1.1.1.cmml" xref="S2.E1.m1.1.2.3.3.2"><divide id="S2.E1.m1.1.1.1.cmml" xref="S2.E1.m1.1.2.3.3.2"></divide><apply id="S2.E1.m1.1.1.2.cmml" xref="S2.E1.m1.1.1.2"><times id="S2.E1.m1.1.1.2.1.cmml" xref="S2.E1.m1.1.1.2.1"></times><ci id="S2.E1.m1.1.1.2.2.cmml" xref="S2.E1.m1.1.1.2.2">𝑄</ci><apply id="S2.E1.m1.1.1.2.3.cmml" xref="S2.E1.m1.1.1.2.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.2.3.1.cmml" xref="S2.E1.m1.1.1.2.3">superscript</csymbol><ci id="S2.E1.m1.1.1.2.3.2.cmml" xref="S2.E1.m1.1.1.2.3.2">𝐾</ci><ci id="S2.E1.m1.1.1.2.3.3.cmml" xref="S2.E1.m1.1.1.2.3.3">𝑇</ci></apply></apply><apply id="S2.E1.m1.1.1.3.cmml" xref="S2.E1.m1.1.1.3"><root id="S2.E1.m1.1.1.3a.cmml" xref="S2.E1.m1.1.1.3"></root><apply id="S2.E1.m1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.3.2"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.3.2.1.cmml" xref="S2.E1.m1.1.1.3.2">subscript</csymbol><ci id="S2.E1.m1.1.1.3.2.2.cmml" xref="S2.E1.m1.1.1.3.2.2">𝑑</ci><ci id="S2.E1.m1.1.1.3.2.3.cmml" xref="S2.E1.m1.1.1.3.2.3">𝑞</ci></apply></apply></apply><ci id="S2.E1.m1.1.2.3.4.cmml" xref="S2.E1.m1.1.2.3.4">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.1c">Z=\text{softmax}\left(\frac{QK^{T}}{\sqrt{d_{q}}}\right)V</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.1d">italic_Z = softmax ( divide start_ARG italic_Q italic_K start_POSTSUPERSCRIPT italic_T end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_q end_POSTSUBSCRIPT end_ARG end_ARG ) italic_V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="432" id="S2.F2.g1" src="x4.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S2.F2.3.2" style="font-size:90%;">Architecture of the Transformer Model<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib1" title="">1</a>]</cite>Originally proposed for machine translation tasks, the Transformer model transforms a source-language input sequence into a target-language output sequence through a dual-path architecture: (1) The encoder pathway processes input tokens via embedding projection and N identical blocks containing multi-head attention and feed-forward layers to generate continuous representations, while (2) the decoder pathway autoregressively produces output tokens by jointly attending to both the encoded source sequence and its own right-shifted inputs - where target sequences are shifted rightward with prepended ⟨SOS⟩ tokens during training to prevent trivial copying, while the loss is computed against the original sequence appended with ⟨EOS⟩ tokens. Both encoder and decoder stacks employ N modularized layers integrating multi-head attention mechanisms, position-wise feed-forward networks, and residual connections with layer normalization, enabling effective modeling of cross-lingual structural dependencies.</span></figcaption>
</figure>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">For any given entity within the sequence, the self-attention mechanism fundamentally operates by:</p>
</div>
<div class="ltx_para" id="S2.SS1.p5">
<p class="ltx_p" id="S2.SS1.p5.1">1.Computing the dot products between its query vector and all key vectors in the sequence,</p>
</div>
<div class="ltx_para" id="S2.SS1.p6">
<p class="ltx_p" id="S2.SS1.p6.1">2.Applying softmax normalization to these dot products to obtain attention weights.</p>
</div>
<div class="ltx_para" id="S2.SS1.p7">
<p class="ltx_p" id="S2.SS1.p7.1">The updated representation of each entity is subsequently computed as a convex combination (weighted sum) of all sequence entities, where the combination coefficients correspond to the derived attention weights (Fig.<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.F2" title="Figure 2 ‣ II-A Transformer Architecture and Self-Attention Mechanism ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>, the third row-left block).</p>
</div>
<div class="ltx_para" id="S2.SS1.p8">
<p class="ltx_p" id="S2.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S2.SS1.p8.1.1">Masked Self-Attention Mechanism:</span> In standard self-attention layers, each position can attend to all other positions in the sequence. However, for autoregressive sequence generation tasks, the decoder employs masked self-attention to prevent information leakage from future positions. This is implemented through an element-wise masking operation:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\text{Attention}_{\text{masked}}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{%
softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_{k}}}\odot\mathbf{M}%
\right)\mathbf{V}" class="ltx_Math" display="block" id="S2.E2.m1.4"><semantics id="S2.E2.m1.4a"><mrow id="S2.E2.m1.4.4" xref="S2.E2.m1.4.4.cmml"><mrow id="S2.E2.m1.4.4.3" xref="S2.E2.m1.4.4.3.cmml"><msub id="S2.E2.m1.4.4.3.2" xref="S2.E2.m1.4.4.3.2.cmml"><mtext id="S2.E2.m1.4.4.3.2.2" xref="S2.E2.m1.4.4.3.2.2a.cmml">Attention</mtext><mtext id="S2.E2.m1.4.4.3.2.3" xref="S2.E2.m1.4.4.3.2.3a.cmml">masked</mtext></msub><mo id="S2.E2.m1.4.4.3.1" xref="S2.E2.m1.4.4.3.1.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.3.3.2" xref="S2.E2.m1.4.4.3.3.1.cmml"><mo id="S2.E2.m1.4.4.3.3.2.1" stretchy="false" xref="S2.E2.m1.4.4.3.3.1.cmml">(</mo><mi id="S2.E2.m1.1.1" xref="S2.E2.m1.1.1.cmml">𝐐</mi><mo id="S2.E2.m1.4.4.3.3.2.2" xref="S2.E2.m1.4.4.3.3.1.cmml">,</mo><mi id="S2.E2.m1.2.2" xref="S2.E2.m1.2.2.cmml">𝐊</mi><mo id="S2.E2.m1.4.4.3.3.2.3" xref="S2.E2.m1.4.4.3.3.1.cmml">,</mo><mi id="S2.E2.m1.3.3" xref="S2.E2.m1.3.3.cmml">𝐕</mi><mo id="S2.E2.m1.4.4.3.3.2.4" stretchy="false" xref="S2.E2.m1.4.4.3.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E2.m1.4.4.2" xref="S2.E2.m1.4.4.2.cmml">=</mo><mrow id="S2.E2.m1.4.4.1" xref="S2.E2.m1.4.4.1.cmml"><mtext id="S2.E2.m1.4.4.1.3" xref="S2.E2.m1.4.4.1.3a.cmml">softmax</mtext><mo id="S2.E2.m1.4.4.1.2" xref="S2.E2.m1.4.4.1.2.cmml">⁢</mo><mrow id="S2.E2.m1.4.4.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.cmml"><mo id="S2.E2.m1.4.4.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.cmml">(</mo><mrow id="S2.E2.m1.4.4.1.1.1.1" xref="S2.E2.m1.4.4.1.1.1.1.cmml"><mfrac id="S2.E2.m1.4.4.1.1.1.1.2" xref="S2.E2.m1.4.4.1.1.1.1.2.cmml"><msup id="S2.E2.m1.4.4.1.1.1.1.2.2" xref="S2.E2.m1.4.4.1.1.1.1.2.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.2.2.2" xref="S2.E2.m1.4.4.1.1.1.1.2.2.2.cmml">𝐐𝐊</mi><mo id="S2.E2.m1.4.4.1.1.1.1.2.2.3" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3.cmml">⊤</mo></msup><msqrt id="S2.E2.m1.4.4.1.1.1.1.2.3" xref="S2.E2.m1.4.4.1.1.1.1.2.3.cmml"><msub id="S2.E2.m1.4.4.1.1.1.1.2.3.2" xref="S2.E2.m1.4.4.1.1.1.1.2.3.2.cmml"><mi id="S2.E2.m1.4.4.1.1.1.1.2.3.2.2" xref="S2.E2.m1.4.4.1.1.1.1.2.3.2.2.cmml">d</mi><mi id="S2.E2.m1.4.4.1.1.1.1.2.3.2.3" xref="S2.E2.m1.4.4.1.1.1.1.2.3.2.3.cmml">k</mi></msub></msqrt></mfrac><mo id="S2.E2.m1.4.4.1.1.1.1.1" lspace="0.222em" rspace="0.222em" xref="S2.E2.m1.4.4.1.1.1.1.1.cmml">⊙</mo><mi id="S2.E2.m1.4.4.1.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.3.cmml">𝐌</mi></mrow><mo id="S2.E2.m1.4.4.1.1.1.3" xref="S2.E2.m1.4.4.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E2.m1.4.4.1.2a" xref="S2.E2.m1.4.4.1.2.cmml">⁢</mo><mi id="S2.E2.m1.4.4.1.4" xref="S2.E2.m1.4.4.1.4.cmml">𝐕</mi></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E2.m1.4b"><apply id="S2.E2.m1.4.4.cmml" xref="S2.E2.m1.4.4"><eq id="S2.E2.m1.4.4.2.cmml" xref="S2.E2.m1.4.4.2"></eq><apply id="S2.E2.m1.4.4.3.cmml" xref="S2.E2.m1.4.4.3"><times id="S2.E2.m1.4.4.3.1.cmml" xref="S2.E2.m1.4.4.3.1"></times><apply id="S2.E2.m1.4.4.3.2.cmml" xref="S2.E2.m1.4.4.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.3.2.1.cmml" xref="S2.E2.m1.4.4.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.3.2.2a.cmml" xref="S2.E2.m1.4.4.3.2.2"><mtext id="S2.E2.m1.4.4.3.2.2.cmml" xref="S2.E2.m1.4.4.3.2.2">Attention</mtext></ci><ci id="S2.E2.m1.4.4.3.2.3a.cmml" xref="S2.E2.m1.4.4.3.2.3"><mtext id="S2.E2.m1.4.4.3.2.3.cmml" mathsize="70%" xref="S2.E2.m1.4.4.3.2.3">masked</mtext></ci></apply><vector id="S2.E2.m1.4.4.3.3.1.cmml" xref="S2.E2.m1.4.4.3.3.2"><ci id="S2.E2.m1.1.1.cmml" xref="S2.E2.m1.1.1">𝐐</ci><ci id="S2.E2.m1.2.2.cmml" xref="S2.E2.m1.2.2">𝐊</ci><ci id="S2.E2.m1.3.3.cmml" xref="S2.E2.m1.3.3">𝐕</ci></vector></apply><apply id="S2.E2.m1.4.4.1.cmml" xref="S2.E2.m1.4.4.1"><times id="S2.E2.m1.4.4.1.2.cmml" xref="S2.E2.m1.4.4.1.2"></times><ci id="S2.E2.m1.4.4.1.3a.cmml" xref="S2.E2.m1.4.4.1.3"><mtext id="S2.E2.m1.4.4.1.3.cmml" xref="S2.E2.m1.4.4.1.3">softmax</mtext></ci><apply id="S2.E2.m1.4.4.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1"><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.1.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.1">direct-product</csymbol><apply id="S2.E2.m1.4.4.1.1.1.1.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2"><divide id="S2.E2.m1.4.4.1.1.1.1.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2"></divide><apply id="S2.E2.m1.4.4.1.1.1.1.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.2.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.2">superscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.2.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.2.2">𝐐𝐊</ci><csymbol cd="latexml" id="S2.E2.m1.4.4.1.1.1.1.2.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.2.3">top</csymbol></apply><apply id="S2.E2.m1.4.4.1.1.1.1.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.3"><root id="S2.E2.m1.4.4.1.1.1.1.2.3a.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.3"></root><apply id="S2.E2.m1.4.4.1.1.1.1.2.3.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.3.2"><csymbol cd="ambiguous" id="S2.E2.m1.4.4.1.1.1.1.2.3.2.1.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.3.2">subscript</csymbol><ci id="S2.E2.m1.4.4.1.1.1.1.2.3.2.2.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.3.2.2">𝑑</ci><ci id="S2.E2.m1.4.4.1.1.1.1.2.3.2.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.2.3.2.3">𝑘</ci></apply></apply></apply><ci id="S2.E2.m1.4.4.1.1.1.1.3.cmml" xref="S2.E2.m1.4.4.1.1.1.1.3">𝐌</ci></apply><ci id="S2.E2.m1.4.4.1.4.cmml" xref="S2.E2.m1.4.4.1.4">𝐕</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E2.m1.4c">\text{Attention}_{\text{masked}}(\mathbf{Q},\mathbf{K},\mathbf{V})=\text{%
softmax}\left(\frac{\mathbf{Q}\mathbf{K}^{\top}}{\sqrt{d_{k}}}\odot\mathbf{M}%
\right)\mathbf{V}</annotation><annotation encoding="application/x-llamapun" id="S2.E2.m1.4d">Attention start_POSTSUBSCRIPT masked end_POSTSUBSCRIPT ( bold_Q , bold_K , bold_V ) = softmax ( divide start_ARG bold_QK start_POSTSUPERSCRIPT ⊤ end_POSTSUPERSCRIPT end_ARG start_ARG square-root start_ARG italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT end_ARG end_ARG ⊙ bold_M ) bold_V</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS1.p9">
<p class="ltx_p" id="S2.SS1.p9.1">where:</p>
<ul class="ltx_itemize" id="S2.I2">
<li class="ltx_item" id="S2.I2.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i1.p1">
<p class="ltx_p" id="S2.I2.i1.p1.1"><math alttext="\mathbf{M}\in\{0,-\infty\}^{n\times n}" class="ltx_Math" display="inline" id="S2.I2.i1.p1.1.m1.2"><semantics id="S2.I2.i1.p1.1.m1.2a"><mrow id="S2.I2.i1.p1.1.m1.2.2" xref="S2.I2.i1.p1.1.m1.2.2.cmml"><mi id="S2.I2.i1.p1.1.m1.2.2.3" xref="S2.I2.i1.p1.1.m1.2.2.3.cmml">𝐌</mi><mo id="S2.I2.i1.p1.1.m1.2.2.2" xref="S2.I2.i1.p1.1.m1.2.2.2.cmml">∈</mo><msup id="S2.I2.i1.p1.1.m1.2.2.1" xref="S2.I2.i1.p1.1.m1.2.2.1.cmml"><mrow id="S2.I2.i1.p1.1.m1.2.2.1.1.1" xref="S2.I2.i1.p1.1.m1.2.2.1.1.2.cmml"><mo id="S2.I2.i1.p1.1.m1.2.2.1.1.1.2" stretchy="false" xref="S2.I2.i1.p1.1.m1.2.2.1.1.2.cmml">{</mo><mn id="S2.I2.i1.p1.1.m1.1.1" xref="S2.I2.i1.p1.1.m1.1.1.cmml">0</mn><mo id="S2.I2.i1.p1.1.m1.2.2.1.1.1.3" xref="S2.I2.i1.p1.1.m1.2.2.1.1.2.cmml">,</mo><mrow id="S2.I2.i1.p1.1.m1.2.2.1.1.1.1" xref="S2.I2.i1.p1.1.m1.2.2.1.1.1.1.cmml"><mo id="S2.I2.i1.p1.1.m1.2.2.1.1.1.1a" xref="S2.I2.i1.p1.1.m1.2.2.1.1.1.1.cmml">−</mo><mi id="S2.I2.i1.p1.1.m1.2.2.1.1.1.1.2" mathvariant="normal" xref="S2.I2.i1.p1.1.m1.2.2.1.1.1.1.2.cmml">∞</mi></mrow><mo id="S2.I2.i1.p1.1.m1.2.2.1.1.1.4" stretchy="false" xref="S2.I2.i1.p1.1.m1.2.2.1.1.2.cmml">}</mo></mrow><mrow id="S2.I2.i1.p1.1.m1.2.2.1.3" xref="S2.I2.i1.p1.1.m1.2.2.1.3.cmml"><mi id="S2.I2.i1.p1.1.m1.2.2.1.3.2" xref="S2.I2.i1.p1.1.m1.2.2.1.3.2.cmml">n</mi><mo id="S2.I2.i1.p1.1.m1.2.2.1.3.1" lspace="0.222em" rspace="0.222em" xref="S2.I2.i1.p1.1.m1.2.2.1.3.1.cmml">×</mo><mi id="S2.I2.i1.p1.1.m1.2.2.1.3.3" xref="S2.I2.i1.p1.1.m1.2.2.1.3.3.cmml">n</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.I2.i1.p1.1.m1.2b"><apply id="S2.I2.i1.p1.1.m1.2.2.cmml" xref="S2.I2.i1.p1.1.m1.2.2"><in id="S2.I2.i1.p1.1.m1.2.2.2.cmml" xref="S2.I2.i1.p1.1.m1.2.2.2"></in><ci id="S2.I2.i1.p1.1.m1.2.2.3.cmml" xref="S2.I2.i1.p1.1.m1.2.2.3">𝐌</ci><apply id="S2.I2.i1.p1.1.m1.2.2.1.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1"><csymbol cd="ambiguous" id="S2.I2.i1.p1.1.m1.2.2.1.2.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1">superscript</csymbol><set id="S2.I2.i1.p1.1.m1.2.2.1.1.2.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1.1.1"><cn id="S2.I2.i1.p1.1.m1.1.1.cmml" type="integer" xref="S2.I2.i1.p1.1.m1.1.1">0</cn><apply id="S2.I2.i1.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1.1.1.1"><minus id="S2.I2.i1.p1.1.m1.2.2.1.1.1.1.1.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1.1.1.1"></minus><infinity id="S2.I2.i1.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1.1.1.1.2"></infinity></apply></set><apply id="S2.I2.i1.p1.1.m1.2.2.1.3.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1.3"><times id="S2.I2.i1.p1.1.m1.2.2.1.3.1.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1.3.1"></times><ci id="S2.I2.i1.p1.1.m1.2.2.1.3.2.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1.3.2">𝑛</ci><ci id="S2.I2.i1.p1.1.m1.2.2.1.3.3.cmml" xref="S2.I2.i1.p1.1.m1.2.2.1.3.3">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i1.p1.1.m1.2c">\mathbf{M}\in\{0,-\infty\}^{n\times n}</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i1.p1.1.m1.2d">bold_M ∈ { 0 , - ∞ } start_POSTSUPERSCRIPT italic_n × italic_n end_POSTSUPERSCRIPT</annotation></semantics></math> is an upper triangular mask matrix:</p>
<table class="ltx_equation ltx_eqn_table" id="S2.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="M_{ij}=\begin{cases}0&amp;\text{if }i\geq j\\
-\infty&amp;\text{if }i&lt;j\end{cases}" class="ltx_Math" display="block" id="S2.E3.m1.4"><semantics id="S2.E3.m1.4a"><mrow id="S2.E3.m1.4.5" xref="S2.E3.m1.4.5.cmml"><msub id="S2.E3.m1.4.5.2" xref="S2.E3.m1.4.5.2.cmml"><mi id="S2.E3.m1.4.5.2.2" xref="S2.E3.m1.4.5.2.2.cmml">M</mi><mrow id="S2.E3.m1.4.5.2.3" xref="S2.E3.m1.4.5.2.3.cmml"><mi id="S2.E3.m1.4.5.2.3.2" xref="S2.E3.m1.4.5.2.3.2.cmml">i</mi><mo id="S2.E3.m1.4.5.2.3.1" xref="S2.E3.m1.4.5.2.3.1.cmml">⁢</mo><mi id="S2.E3.m1.4.5.2.3.3" xref="S2.E3.m1.4.5.2.3.3.cmml">j</mi></mrow></msub><mo id="S2.E3.m1.4.5.1" xref="S2.E3.m1.4.5.1.cmml">=</mo><mrow id="S2.E3.m1.4.4" xref="S2.E3.m1.4.5.3.1.cmml"><mo id="S2.E3.m1.4.4.5" xref="S2.E3.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.E3.m1.4.4.4" rowspacing="0pt" xref="S2.E3.m1.4.5.3.1.cmml"><mtr id="S2.E3.m1.4.4.4a" xref="S2.E3.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.4.4.4b" xref="S2.E3.m1.4.5.3.1.cmml"><mn id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml">0</mn></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.4.4.4c" xref="S2.E3.m1.4.5.3.1.cmml"><mrow id="S2.E3.m1.2.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.1.cmml"><mrow id="S2.E3.m1.2.2.2.2.2.1.2" xref="S2.E3.m1.2.2.2.2.2.1.2.cmml"><mtext id="S2.E3.m1.2.2.2.2.2.1.2.2" xref="S2.E3.m1.2.2.2.2.2.1.2.2a.cmml">if </mtext><mo id="S2.E3.m1.2.2.2.2.2.1.2.1" xref="S2.E3.m1.2.2.2.2.2.1.2.1.cmml">⁢</mo><mi id="S2.E3.m1.2.2.2.2.2.1.2.3" xref="S2.E3.m1.2.2.2.2.2.1.2.3.cmml">i</mi></mrow><mo id="S2.E3.m1.2.2.2.2.2.1.1" xref="S2.E3.m1.2.2.2.2.2.1.1.cmml">≥</mo><mi id="S2.E3.m1.2.2.2.2.2.1.3" xref="S2.E3.m1.2.2.2.2.2.1.3.cmml">j</mi></mrow></mtd></mtr><mtr id="S2.E3.m1.4.4.4d" xref="S2.E3.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.4.4.4e" xref="S2.E3.m1.4.5.3.1.cmml"><mrow id="S2.E3.m1.3.3.3.3.1.1" xref="S2.E3.m1.3.3.3.3.1.1.cmml"><mo id="S2.E3.m1.3.3.3.3.1.1a" xref="S2.E3.m1.3.3.3.3.1.1.cmml">−</mo><mi id="S2.E3.m1.3.3.3.3.1.1.2" mathvariant="normal" xref="S2.E3.m1.3.3.3.3.1.1.2.cmml">∞</mi></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.4.4.4f" xref="S2.E3.m1.4.5.3.1.cmml"><mrow id="S2.E3.m1.4.4.4.4.2.1" xref="S2.E3.m1.4.4.4.4.2.1.cmml"><mrow id="S2.E3.m1.4.4.4.4.2.1.2" xref="S2.E3.m1.4.4.4.4.2.1.2.cmml"><mtext id="S2.E3.m1.4.4.4.4.2.1.2.2" xref="S2.E3.m1.4.4.4.4.2.1.2.2a.cmml">if </mtext><mo id="S2.E3.m1.4.4.4.4.2.1.2.1" xref="S2.E3.m1.4.4.4.4.2.1.2.1.cmml">⁢</mo><mi id="S2.E3.m1.4.4.4.4.2.1.2.3" xref="S2.E3.m1.4.4.4.4.2.1.2.3.cmml">i</mi></mrow><mo id="S2.E3.m1.4.4.4.4.2.1.1" xref="S2.E3.m1.4.4.4.4.2.1.1.cmml">&lt;</mo><mi id="S2.E3.m1.4.4.4.4.2.1.3" xref="S2.E3.m1.4.4.4.4.2.1.3.cmml">j</mi></mrow></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E3.m1.4b"><apply id="S2.E3.m1.4.5.cmml" xref="S2.E3.m1.4.5"><eq id="S2.E3.m1.4.5.1.cmml" xref="S2.E3.m1.4.5.1"></eq><apply id="S2.E3.m1.4.5.2.cmml" xref="S2.E3.m1.4.5.2"><csymbol cd="ambiguous" id="S2.E3.m1.4.5.2.1.cmml" xref="S2.E3.m1.4.5.2">subscript</csymbol><ci id="S2.E3.m1.4.5.2.2.cmml" xref="S2.E3.m1.4.5.2.2">𝑀</ci><apply id="S2.E3.m1.4.5.2.3.cmml" xref="S2.E3.m1.4.5.2.3"><times id="S2.E3.m1.4.5.2.3.1.cmml" xref="S2.E3.m1.4.5.2.3.1"></times><ci id="S2.E3.m1.4.5.2.3.2.cmml" xref="S2.E3.m1.4.5.2.3.2">𝑖</ci><ci id="S2.E3.m1.4.5.2.3.3.cmml" xref="S2.E3.m1.4.5.2.3.3">𝑗</ci></apply></apply><apply id="S2.E3.m1.4.5.3.1.cmml" xref="S2.E3.m1.4.4"><csymbol cd="latexml" id="S2.E3.m1.4.5.3.1.1.cmml" xref="S2.E3.m1.4.4.5">cases</csymbol><cn id="S2.E3.m1.1.1.1.1.1.1.cmml" type="integer" xref="S2.E3.m1.1.1.1.1.1.1">0</cn><apply id="S2.E3.m1.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.1"><geq id="S2.E3.m1.2.2.2.2.2.1.1.cmml" xref="S2.E3.m1.2.2.2.2.2.1.1"></geq><apply id="S2.E3.m1.2.2.2.2.2.1.2.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2"><times id="S2.E3.m1.2.2.2.2.2.1.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2.1"></times><ci id="S2.E3.m1.2.2.2.2.2.1.2.2a.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2.2"><mtext id="S2.E3.m1.2.2.2.2.2.1.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2.2">if </mtext></ci><ci id="S2.E3.m1.2.2.2.2.2.1.2.3.cmml" xref="S2.E3.m1.2.2.2.2.2.1.2.3">𝑖</ci></apply><ci id="S2.E3.m1.2.2.2.2.2.1.3.cmml" xref="S2.E3.m1.2.2.2.2.2.1.3">𝑗</ci></apply><apply id="S2.E3.m1.3.3.3.3.1.1.cmml" xref="S2.E3.m1.3.3.3.3.1.1"><minus id="S2.E3.m1.3.3.3.3.1.1.1.cmml" xref="S2.E3.m1.3.3.3.3.1.1"></minus><infinity id="S2.E3.m1.3.3.3.3.1.1.2.cmml" xref="S2.E3.m1.3.3.3.3.1.1.2"></infinity></apply><apply id="S2.E3.m1.4.4.4.4.2.1.cmml" xref="S2.E3.m1.4.4.4.4.2.1"><lt id="S2.E3.m1.4.4.4.4.2.1.1.cmml" xref="S2.E3.m1.4.4.4.4.2.1.1"></lt><apply id="S2.E3.m1.4.4.4.4.2.1.2.cmml" xref="S2.E3.m1.4.4.4.4.2.1.2"><times id="S2.E3.m1.4.4.4.4.2.1.2.1.cmml" xref="S2.E3.m1.4.4.4.4.2.1.2.1"></times><ci id="S2.E3.m1.4.4.4.4.2.1.2.2a.cmml" xref="S2.E3.m1.4.4.4.4.2.1.2.2"><mtext id="S2.E3.m1.4.4.4.4.2.1.2.2.cmml" xref="S2.E3.m1.4.4.4.4.2.1.2.2">if </mtext></ci><ci id="S2.E3.m1.4.4.4.4.2.1.2.3.cmml" xref="S2.E3.m1.4.4.4.4.2.1.2.3">𝑖</ci></apply><ci id="S2.E3.m1.4.4.4.4.2.1.3.cmml" xref="S2.E3.m1.4.4.4.4.2.1.3">𝑗</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E3.m1.4c">M_{ij}=\begin{cases}0&amp;\text{if }i\geq j\\
-\infty&amp;\text{if }i&lt;j\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S2.E3.m1.4d">italic_M start_POSTSUBSCRIPT italic_i italic_j end_POSTSUBSCRIPT = { start_ROW start_CELL 0 end_CELL start_CELL if italic_i ≥ italic_j end_CELL end_ROW start_ROW start_CELL - ∞ end_CELL start_CELL if italic_i &lt; italic_j end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</li>
<li class="ltx_item" id="S2.I2.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i2.p1">
<p class="ltx_p" id="S2.I2.i2.p1.1"><math alttext="\odot" class="ltx_Math" display="inline" id="S2.I2.i2.p1.1.m1.1"><semantics id="S2.I2.i2.p1.1.m1.1a"><mo id="S2.I2.i2.p1.1.m1.1.1" xref="S2.I2.i2.p1.1.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S2.I2.i2.p1.1.m1.1b"><csymbol cd="latexml" id="S2.I2.i2.p1.1.m1.1.1.cmml" xref="S2.I2.i2.p1.1.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i2.p1.1.m1.1c">\odot</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i2.p1.1.m1.1d">⊙</annotation></semantics></math> denotes the Hadamard (element-wise) product</p>
</div>
</li>
<li class="ltx_item" id="S2.I2.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I2.i3.p1">
<p class="ltx_p" id="S2.I2.i3.p1.1"><math alttext="d_{k}" class="ltx_Math" display="inline" id="S2.I2.i3.p1.1.m1.1"><semantics id="S2.I2.i3.p1.1.m1.1a"><msub id="S2.I2.i3.p1.1.m1.1.1" xref="S2.I2.i3.p1.1.m1.1.1.cmml"><mi id="S2.I2.i3.p1.1.m1.1.1.2" xref="S2.I2.i3.p1.1.m1.1.1.2.cmml">d</mi><mi id="S2.I2.i3.p1.1.m1.1.1.3" xref="S2.I2.i3.p1.1.m1.1.1.3.cmml">k</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I2.i3.p1.1.m1.1b"><apply id="S2.I2.i3.p1.1.m1.1.1.cmml" xref="S2.I2.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I2.i3.p1.1.m1.1.1.1.cmml" xref="S2.I2.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I2.i3.p1.1.m1.1.1.2.cmml" xref="S2.I2.i3.p1.1.m1.1.1.2">𝑑</ci><ci id="S2.I2.i3.p1.1.m1.1.1.3.cmml" xref="S2.I2.i3.p1.1.m1.1.1.3">𝑘</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I2.i3.p1.1.m1.1c">d_{k}</annotation><annotation encoding="application/x-llamapun" id="S2.I2.i3.p1.1.m1.1d">italic_d start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT</annotation></semantics></math> is the dimension of key vectors</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS1.p10">
<p class="ltx_p" id="S2.SS1.p10.1">The masking operation ensures that when predicting the <math alttext="i" class="ltx_Math" display="inline" id="S2.SS1.p10.1.m1.1"><semantics id="S2.SS1.p10.1.m1.1a"><mi id="S2.SS1.p10.1.m1.1.1" xref="S2.SS1.p10.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p10.1.m1.1b"><ci id="S2.SS1.p10.1.m1.1.1.cmml" xref="S2.SS1.p10.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p10.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p10.1.m1.1d">italic_i</annotation></semantics></math>-th position:</p>
<ul class="ltx_itemize" id="S2.I3">
<li class="ltx_item" id="S2.I3.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i1.p1">
<p class="ltx_p" id="S2.I3.i1.p1.1">Only positions <math alttext="j\leq i" class="ltx_Math" display="inline" id="S2.I3.i1.p1.1.m1.1"><semantics id="S2.I3.i1.p1.1.m1.1a"><mrow id="S2.I3.i1.p1.1.m1.1.1" xref="S2.I3.i1.p1.1.m1.1.1.cmml"><mi id="S2.I3.i1.p1.1.m1.1.1.2" xref="S2.I3.i1.p1.1.m1.1.1.2.cmml">j</mi><mo id="S2.I3.i1.p1.1.m1.1.1.1" xref="S2.I3.i1.p1.1.m1.1.1.1.cmml">≤</mo><mi id="S2.I3.i1.p1.1.m1.1.1.3" xref="S2.I3.i1.p1.1.m1.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I3.i1.p1.1.m1.1b"><apply id="S2.I3.i1.p1.1.m1.1.1.cmml" xref="S2.I3.i1.p1.1.m1.1.1"><leq id="S2.I3.i1.p1.1.m1.1.1.1.cmml" xref="S2.I3.i1.p1.1.m1.1.1.1"></leq><ci id="S2.I3.i1.p1.1.m1.1.1.2.cmml" xref="S2.I3.i1.p1.1.m1.1.1.2">𝑗</ci><ci id="S2.I3.i1.p1.1.m1.1.1.3.cmml" xref="S2.I3.i1.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.i1.p1.1.m1.1c">j\leq i</annotation><annotation encoding="application/x-llamapun" id="S2.I3.i1.p1.1.m1.1d">italic_j ≤ italic_i</annotation></semantics></math> contribute to the attention weights</p>
</div>
</li>
<li class="ltx_item" id="S2.I3.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I3.i2.p1">
<p class="ltx_p" id="S2.I3.i2.p1.2">Future positions (<math alttext="j&gt;i" class="ltx_Math" display="inline" id="S2.I3.i2.p1.1.m1.1"><semantics id="S2.I3.i2.p1.1.m1.1a"><mrow id="S2.I3.i2.p1.1.m1.1.1" xref="S2.I3.i2.p1.1.m1.1.1.cmml"><mi id="S2.I3.i2.p1.1.m1.1.1.2" xref="S2.I3.i2.p1.1.m1.1.1.2.cmml">j</mi><mo id="S2.I3.i2.p1.1.m1.1.1.1" xref="S2.I3.i2.p1.1.m1.1.1.1.cmml">&gt;</mo><mi id="S2.I3.i2.p1.1.m1.1.1.3" xref="S2.I3.i2.p1.1.m1.1.1.3.cmml">i</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.I3.i2.p1.1.m1.1b"><apply id="S2.I3.i2.p1.1.m1.1.1.cmml" xref="S2.I3.i2.p1.1.m1.1.1"><gt id="S2.I3.i2.p1.1.m1.1.1.1.cmml" xref="S2.I3.i2.p1.1.m1.1.1.1"></gt><ci id="S2.I3.i2.p1.1.m1.1.1.2.cmml" xref="S2.I3.i2.p1.1.m1.1.1.2">𝑗</ci><ci id="S2.I3.i2.p1.1.m1.1.1.3.cmml" xref="S2.I3.i2.p1.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I3.i2.p1.1.m1.1c">j&gt;i</annotation><annotation encoding="application/x-llamapun" id="S2.I3.i2.p1.1.m1.1d">italic_j &gt; italic_i</annotation></semantics></math>) receive attention scores of <math alttext="0" class="ltx_Math" display="inline" id="S2.I3.i2.p1.2.m2.1"><semantics id="S2.I3.i2.p1.2.m2.1a"><mn id="S2.I3.i2.p1.2.m2.1.1" xref="S2.I3.i2.p1.2.m2.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.I3.i2.p1.2.m2.1b"><cn id="S2.I3.i2.p1.2.m2.1.1.cmml" type="integer" xref="S2.I3.i2.p1.2.m2.1.1">0</cn></annotation-xml></semantics></math> (after softmax)</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS1.p11">
<p class="ltx_p" id="S2.SS1.p11.1">This implementation maintains the parallel computation advantages of self-attention while enforcing the autoregressive property required for sequence generation.</p>
</div>
<div class="ltx_para" id="S2.SS1.p12">
<p class="ltx_p" id="S2.SS1.p12.7"><span class="ltx_text ltx_font_bold" id="S2.SS1.p12.7.1">Multi-Head Attention Mechanism:</span> The mechanism employs <math alttext="h" class="ltx_Math" display="inline" id="S2.SS1.p12.1.m1.1"><semantics id="S2.SS1.p12.1.m1.1a"><mi id="S2.SS1.p12.1.m1.1.1" xref="S2.SS1.p12.1.m1.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p12.1.m1.1b"><ci id="S2.SS1.p12.1.m1.1.1.cmml" xref="S2.SS1.p12.1.m1.1.1">ℎ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p12.1.m1.1c">h</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p12.1.m1.1d">italic_h</annotation></semantics></math> parallel self-attention heads (<math alttext="h\!=\!8" class="ltx_Math" display="inline" id="S2.SS1.p12.2.m2.1"><semantics id="S2.SS1.p12.2.m2.1a"><mrow id="S2.SS1.p12.2.m2.1.1" xref="S2.SS1.p12.2.m2.1.1.cmml"><mi id="S2.SS1.p12.2.m2.1.1.2" xref="S2.SS1.p12.2.m2.1.1.2.cmml">h</mi><mo id="S2.SS1.p12.2.m2.1.1.1" lspace="0.108em" rspace="0.108em" xref="S2.SS1.p12.2.m2.1.1.1.cmml">=</mo><mn id="S2.SS1.p12.2.m2.1.1.3" xref="S2.SS1.p12.2.m2.1.1.3.cmml">8</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p12.2.m2.1b"><apply id="S2.SS1.p12.2.m2.1.1.cmml" xref="S2.SS1.p12.2.m2.1.1"><eq id="S2.SS1.p12.2.m2.1.1.1.cmml" xref="S2.SS1.p12.2.m2.1.1.1"></eq><ci id="S2.SS1.p12.2.m2.1.1.2.cmml" xref="S2.SS1.p12.2.m2.1.1.2">ℎ</ci><cn id="S2.SS1.p12.2.m2.1.1.3.cmml" type="integer" xref="S2.SS1.p12.2.m2.1.1.3">8</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p12.2.m2.1c">h\!=\!8</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p12.2.m2.1d">italic_h = 8</annotation></semantics></math> in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib1" title="">1</a>]</cite>), each with independent weight matrices <math alttext="\{\mathbf{W}^{Q}_{i},\mathbf{W}^{K}_{i},\mathbf{W}^{V}_{i}\}" class="ltx_Math" display="inline" id="S2.SS1.p12.3.m3.3"><semantics id="S2.SS1.p12.3.m3.3a"><mrow id="S2.SS1.p12.3.m3.3.3.3" xref="S2.SS1.p12.3.m3.3.3.4.cmml"><mo id="S2.SS1.p12.3.m3.3.3.3.4" stretchy="false" xref="S2.SS1.p12.3.m3.3.3.4.cmml">{</mo><msubsup id="S2.SS1.p12.3.m3.1.1.1.1" xref="S2.SS1.p12.3.m3.1.1.1.1.cmml"><mi id="S2.SS1.p12.3.m3.1.1.1.1.2.2" xref="S2.SS1.p12.3.m3.1.1.1.1.2.2.cmml">𝐖</mi><mi id="S2.SS1.p12.3.m3.1.1.1.1.3" xref="S2.SS1.p12.3.m3.1.1.1.1.3.cmml">i</mi><mi id="S2.SS1.p12.3.m3.1.1.1.1.2.3" xref="S2.SS1.p12.3.m3.1.1.1.1.2.3.cmml">Q</mi></msubsup><mo id="S2.SS1.p12.3.m3.3.3.3.5" xref="S2.SS1.p12.3.m3.3.3.4.cmml">,</mo><msubsup id="S2.SS1.p12.3.m3.2.2.2.2" xref="S2.SS1.p12.3.m3.2.2.2.2.cmml"><mi id="S2.SS1.p12.3.m3.2.2.2.2.2.2" xref="S2.SS1.p12.3.m3.2.2.2.2.2.2.cmml">𝐖</mi><mi id="S2.SS1.p12.3.m3.2.2.2.2.3" xref="S2.SS1.p12.3.m3.2.2.2.2.3.cmml">i</mi><mi id="S2.SS1.p12.3.m3.2.2.2.2.2.3" xref="S2.SS1.p12.3.m3.2.2.2.2.2.3.cmml">K</mi></msubsup><mo id="S2.SS1.p12.3.m3.3.3.3.6" xref="S2.SS1.p12.3.m3.3.3.4.cmml">,</mo><msubsup id="S2.SS1.p12.3.m3.3.3.3.3" xref="S2.SS1.p12.3.m3.3.3.3.3.cmml"><mi id="S2.SS1.p12.3.m3.3.3.3.3.2.2" xref="S2.SS1.p12.3.m3.3.3.3.3.2.2.cmml">𝐖</mi><mi id="S2.SS1.p12.3.m3.3.3.3.3.3" xref="S2.SS1.p12.3.m3.3.3.3.3.3.cmml">i</mi><mi id="S2.SS1.p12.3.m3.3.3.3.3.2.3" xref="S2.SS1.p12.3.m3.3.3.3.3.2.3.cmml">V</mi></msubsup><mo id="S2.SS1.p12.3.m3.3.3.3.7" stretchy="false" xref="S2.SS1.p12.3.m3.3.3.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p12.3.m3.3b"><set id="S2.SS1.p12.3.m3.3.3.4.cmml" xref="S2.SS1.p12.3.m3.3.3.3"><apply id="S2.SS1.p12.3.m3.1.1.1.1.cmml" xref="S2.SS1.p12.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p12.3.m3.1.1.1.1.1.cmml" xref="S2.SS1.p12.3.m3.1.1.1.1">subscript</csymbol><apply id="S2.SS1.p12.3.m3.1.1.1.1.2.cmml" xref="S2.SS1.p12.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p12.3.m3.1.1.1.1.2.1.cmml" xref="S2.SS1.p12.3.m3.1.1.1.1">superscript</csymbol><ci id="S2.SS1.p12.3.m3.1.1.1.1.2.2.cmml" xref="S2.SS1.p12.3.m3.1.1.1.1.2.2">𝐖</ci><ci id="S2.SS1.p12.3.m3.1.1.1.1.2.3.cmml" xref="S2.SS1.p12.3.m3.1.1.1.1.2.3">𝑄</ci></apply><ci id="S2.SS1.p12.3.m3.1.1.1.1.3.cmml" xref="S2.SS1.p12.3.m3.1.1.1.1.3">𝑖</ci></apply><apply id="S2.SS1.p12.3.m3.2.2.2.2.cmml" xref="S2.SS1.p12.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p12.3.m3.2.2.2.2.1.cmml" xref="S2.SS1.p12.3.m3.2.2.2.2">subscript</csymbol><apply id="S2.SS1.p12.3.m3.2.2.2.2.2.cmml" xref="S2.SS1.p12.3.m3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p12.3.m3.2.2.2.2.2.1.cmml" xref="S2.SS1.p12.3.m3.2.2.2.2">superscript</csymbol><ci id="S2.SS1.p12.3.m3.2.2.2.2.2.2.cmml" xref="S2.SS1.p12.3.m3.2.2.2.2.2.2">𝐖</ci><ci id="S2.SS1.p12.3.m3.2.2.2.2.2.3.cmml" xref="S2.SS1.p12.3.m3.2.2.2.2.2.3">𝐾</ci></apply><ci id="S2.SS1.p12.3.m3.2.2.2.2.3.cmml" xref="S2.SS1.p12.3.m3.2.2.2.2.3">𝑖</ci></apply><apply id="S2.SS1.p12.3.m3.3.3.3.3.cmml" xref="S2.SS1.p12.3.m3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p12.3.m3.3.3.3.3.1.cmml" xref="S2.SS1.p12.3.m3.3.3.3.3">subscript</csymbol><apply id="S2.SS1.p12.3.m3.3.3.3.3.2.cmml" xref="S2.SS1.p12.3.m3.3.3.3.3"><csymbol cd="ambiguous" id="S2.SS1.p12.3.m3.3.3.3.3.2.1.cmml" xref="S2.SS1.p12.3.m3.3.3.3.3">superscript</csymbol><ci id="S2.SS1.p12.3.m3.3.3.3.3.2.2.cmml" xref="S2.SS1.p12.3.m3.3.3.3.3.2.2">𝐖</ci><ci id="S2.SS1.p12.3.m3.3.3.3.3.2.3.cmml" xref="S2.SS1.p12.3.m3.3.3.3.3.2.3">𝑉</ci></apply><ci id="S2.SS1.p12.3.m3.3.3.3.3.3.cmml" xref="S2.SS1.p12.3.m3.3.3.3.3.3">𝑖</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p12.3.m3.3c">\{\mathbf{W}^{Q}_{i},\mathbf{W}^{K}_{i},\mathbf{W}^{V}_{i}\}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p12.3.m3.3d">{ bold_W start_POSTSUPERSCRIPT italic_Q end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_W start_POSTSUPERSCRIPT italic_K end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT , bold_W start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT }</annotation></semantics></math> for <math alttext="i\in[0,h\!-\!1]" class="ltx_Math" display="inline" id="S2.SS1.p12.4.m4.2"><semantics id="S2.SS1.p12.4.m4.2a"><mrow id="S2.SS1.p12.4.m4.2.2" xref="S2.SS1.p12.4.m4.2.2.cmml"><mi id="S2.SS1.p12.4.m4.2.2.3" xref="S2.SS1.p12.4.m4.2.2.3.cmml">i</mi><mo id="S2.SS1.p12.4.m4.2.2.2" xref="S2.SS1.p12.4.m4.2.2.2.cmml">∈</mo><mrow id="S2.SS1.p12.4.m4.2.2.1.1" xref="S2.SS1.p12.4.m4.2.2.1.2.cmml"><mo id="S2.SS1.p12.4.m4.2.2.1.1.2" stretchy="false" xref="S2.SS1.p12.4.m4.2.2.1.2.cmml">[</mo><mn id="S2.SS1.p12.4.m4.1.1" xref="S2.SS1.p12.4.m4.1.1.cmml">0</mn><mo id="S2.SS1.p12.4.m4.2.2.1.1.3" xref="S2.SS1.p12.4.m4.2.2.1.2.cmml">,</mo><mrow id="S2.SS1.p12.4.m4.2.2.1.1.1" xref="S2.SS1.p12.4.m4.2.2.1.1.1.cmml"><mi id="S2.SS1.p12.4.m4.2.2.1.1.1.2" xref="S2.SS1.p12.4.m4.2.2.1.1.1.2.cmml">h</mi><mo id="S2.SS1.p12.4.m4.2.2.1.1.1.1" lspace="0.052em" rspace="0.052em" xref="S2.SS1.p12.4.m4.2.2.1.1.1.1.cmml">−</mo><mn id="S2.SS1.p12.4.m4.2.2.1.1.1.3" xref="S2.SS1.p12.4.m4.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S2.SS1.p12.4.m4.2.2.1.1.4" stretchy="false" xref="S2.SS1.p12.4.m4.2.2.1.2.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p12.4.m4.2b"><apply id="S2.SS1.p12.4.m4.2.2.cmml" xref="S2.SS1.p12.4.m4.2.2"><in id="S2.SS1.p12.4.m4.2.2.2.cmml" xref="S2.SS1.p12.4.m4.2.2.2"></in><ci id="S2.SS1.p12.4.m4.2.2.3.cmml" xref="S2.SS1.p12.4.m4.2.2.3">𝑖</ci><interval closure="closed" id="S2.SS1.p12.4.m4.2.2.1.2.cmml" xref="S2.SS1.p12.4.m4.2.2.1.1"><cn id="S2.SS1.p12.4.m4.1.1.cmml" type="integer" xref="S2.SS1.p12.4.m4.1.1">0</cn><apply id="S2.SS1.p12.4.m4.2.2.1.1.1.cmml" xref="S2.SS1.p12.4.m4.2.2.1.1.1"><minus id="S2.SS1.p12.4.m4.2.2.1.1.1.1.cmml" xref="S2.SS1.p12.4.m4.2.2.1.1.1.1"></minus><ci id="S2.SS1.p12.4.m4.2.2.1.1.1.2.cmml" xref="S2.SS1.p12.4.m4.2.2.1.1.1.2">ℎ</ci><cn id="S2.SS1.p12.4.m4.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS1.p12.4.m4.2.2.1.1.1.3">1</cn></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p12.4.m4.2c">i\in[0,h\!-\!1]</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p12.4.m4.2d">italic_i ∈ [ 0 , italic_h - 1 ]</annotation></semantics></math>. Given input <math alttext="\mathbf{X}" class="ltx_Math" display="inline" id="S2.SS1.p12.5.m5.1"><semantics id="S2.SS1.p12.5.m5.1a"><mi id="S2.SS1.p12.5.m5.1.1" xref="S2.SS1.p12.5.m5.1.1.cmml">𝐗</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.p12.5.m5.1b"><ci id="S2.SS1.p12.5.m5.1.1.cmml" xref="S2.SS1.p12.5.m5.1.1">𝐗</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p12.5.m5.1c">\mathbf{X}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p12.5.m5.1d">bold_X</annotation></semantics></math>, it concatenates all heads’ outputs <math alttext="[\mathbf{Z}_{0},...,\mathbf{Z}_{h-1}]\in\mathbb{R}^{n\times hd_{v}}" class="ltx_Math" display="inline" id="S2.SS1.p12.6.m6.3"><semantics id="S2.SS1.p12.6.m6.3a"><mrow id="S2.SS1.p12.6.m6.3.3" xref="S2.SS1.p12.6.m6.3.3.cmml"><mrow id="S2.SS1.p12.6.m6.3.3.2.2" xref="S2.SS1.p12.6.m6.3.3.2.3.cmml"><mo id="S2.SS1.p12.6.m6.3.3.2.2.3" stretchy="false" xref="S2.SS1.p12.6.m6.3.3.2.3.cmml">[</mo><msub id="S2.SS1.p12.6.m6.2.2.1.1.1" xref="S2.SS1.p12.6.m6.2.2.1.1.1.cmml"><mi id="S2.SS1.p12.6.m6.2.2.1.1.1.2" xref="S2.SS1.p12.6.m6.2.2.1.1.1.2.cmml">𝐙</mi><mn id="S2.SS1.p12.6.m6.2.2.1.1.1.3" xref="S2.SS1.p12.6.m6.2.2.1.1.1.3.cmml">0</mn></msub><mo id="S2.SS1.p12.6.m6.3.3.2.2.4" xref="S2.SS1.p12.6.m6.3.3.2.3.cmml">,</mo><mi id="S2.SS1.p12.6.m6.1.1" mathvariant="normal" xref="S2.SS1.p12.6.m6.1.1.cmml">…</mi><mo id="S2.SS1.p12.6.m6.3.3.2.2.5" xref="S2.SS1.p12.6.m6.3.3.2.3.cmml">,</mo><msub id="S2.SS1.p12.6.m6.3.3.2.2.2" xref="S2.SS1.p12.6.m6.3.3.2.2.2.cmml"><mi id="S2.SS1.p12.6.m6.3.3.2.2.2.2" xref="S2.SS1.p12.6.m6.3.3.2.2.2.2.cmml">𝐙</mi><mrow id="S2.SS1.p12.6.m6.3.3.2.2.2.3" xref="S2.SS1.p12.6.m6.3.3.2.2.2.3.cmml"><mi id="S2.SS1.p12.6.m6.3.3.2.2.2.3.2" xref="S2.SS1.p12.6.m6.3.3.2.2.2.3.2.cmml">h</mi><mo id="S2.SS1.p12.6.m6.3.3.2.2.2.3.1" xref="S2.SS1.p12.6.m6.3.3.2.2.2.3.1.cmml">−</mo><mn id="S2.SS1.p12.6.m6.3.3.2.2.2.3.3" xref="S2.SS1.p12.6.m6.3.3.2.2.2.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS1.p12.6.m6.3.3.2.2.6" stretchy="false" xref="S2.SS1.p12.6.m6.3.3.2.3.cmml">]</mo></mrow><mo id="S2.SS1.p12.6.m6.3.3.3" xref="S2.SS1.p12.6.m6.3.3.3.cmml">∈</mo><msup id="S2.SS1.p12.6.m6.3.3.4" xref="S2.SS1.p12.6.m6.3.3.4.cmml"><mi id="S2.SS1.p12.6.m6.3.3.4.2" xref="S2.SS1.p12.6.m6.3.3.4.2.cmml">ℝ</mi><mrow id="S2.SS1.p12.6.m6.3.3.4.3" xref="S2.SS1.p12.6.m6.3.3.4.3.cmml"><mrow id="S2.SS1.p12.6.m6.3.3.4.3.2" xref="S2.SS1.p12.6.m6.3.3.4.3.2.cmml"><mi id="S2.SS1.p12.6.m6.3.3.4.3.2.2" xref="S2.SS1.p12.6.m6.3.3.4.3.2.2.cmml">n</mi><mo id="S2.SS1.p12.6.m6.3.3.4.3.2.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p12.6.m6.3.3.4.3.2.1.cmml">×</mo><mi id="S2.SS1.p12.6.m6.3.3.4.3.2.3" xref="S2.SS1.p12.6.m6.3.3.4.3.2.3.cmml">h</mi></mrow><mo id="S2.SS1.p12.6.m6.3.3.4.3.1" xref="S2.SS1.p12.6.m6.3.3.4.3.1.cmml">⁢</mo><msub id="S2.SS1.p12.6.m6.3.3.4.3.3" xref="S2.SS1.p12.6.m6.3.3.4.3.3.cmml"><mi id="S2.SS1.p12.6.m6.3.3.4.3.3.2" xref="S2.SS1.p12.6.m6.3.3.4.3.3.2.cmml">d</mi><mi id="S2.SS1.p12.6.m6.3.3.4.3.3.3" xref="S2.SS1.p12.6.m6.3.3.4.3.3.3.cmml">v</mi></msub></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p12.6.m6.3b"><apply id="S2.SS1.p12.6.m6.3.3.cmml" xref="S2.SS1.p12.6.m6.3.3"><in id="S2.SS1.p12.6.m6.3.3.3.cmml" xref="S2.SS1.p12.6.m6.3.3.3"></in><list id="S2.SS1.p12.6.m6.3.3.2.3.cmml" xref="S2.SS1.p12.6.m6.3.3.2.2"><apply id="S2.SS1.p12.6.m6.2.2.1.1.1.cmml" xref="S2.SS1.p12.6.m6.2.2.1.1.1"><csymbol cd="ambiguous" id="S2.SS1.p12.6.m6.2.2.1.1.1.1.cmml" xref="S2.SS1.p12.6.m6.2.2.1.1.1">subscript</csymbol><ci id="S2.SS1.p12.6.m6.2.2.1.1.1.2.cmml" xref="S2.SS1.p12.6.m6.2.2.1.1.1.2">𝐙</ci><cn id="S2.SS1.p12.6.m6.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS1.p12.6.m6.2.2.1.1.1.3">0</cn></apply><ci id="S2.SS1.p12.6.m6.1.1.cmml" xref="S2.SS1.p12.6.m6.1.1">…</ci><apply id="S2.SS1.p12.6.m6.3.3.2.2.2.cmml" xref="S2.SS1.p12.6.m6.3.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.p12.6.m6.3.3.2.2.2.1.cmml" xref="S2.SS1.p12.6.m6.3.3.2.2.2">subscript</csymbol><ci id="S2.SS1.p12.6.m6.3.3.2.2.2.2.cmml" xref="S2.SS1.p12.6.m6.3.3.2.2.2.2">𝐙</ci><apply id="S2.SS1.p12.6.m6.3.3.2.2.2.3.cmml" xref="S2.SS1.p12.6.m6.3.3.2.2.2.3"><minus id="S2.SS1.p12.6.m6.3.3.2.2.2.3.1.cmml" xref="S2.SS1.p12.6.m6.3.3.2.2.2.3.1"></minus><ci id="S2.SS1.p12.6.m6.3.3.2.2.2.3.2.cmml" xref="S2.SS1.p12.6.m6.3.3.2.2.2.3.2">ℎ</ci><cn id="S2.SS1.p12.6.m6.3.3.2.2.2.3.3.cmml" type="integer" xref="S2.SS1.p12.6.m6.3.3.2.2.2.3.3">1</cn></apply></apply></list><apply id="S2.SS1.p12.6.m6.3.3.4.cmml" xref="S2.SS1.p12.6.m6.3.3.4"><csymbol cd="ambiguous" id="S2.SS1.p12.6.m6.3.3.4.1.cmml" xref="S2.SS1.p12.6.m6.3.3.4">superscript</csymbol><ci id="S2.SS1.p12.6.m6.3.3.4.2.cmml" xref="S2.SS1.p12.6.m6.3.3.4.2">ℝ</ci><apply id="S2.SS1.p12.6.m6.3.3.4.3.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3"><times id="S2.SS1.p12.6.m6.3.3.4.3.1.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.1"></times><apply id="S2.SS1.p12.6.m6.3.3.4.3.2.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.2"><times id="S2.SS1.p12.6.m6.3.3.4.3.2.1.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.2.1"></times><ci id="S2.SS1.p12.6.m6.3.3.4.3.2.2.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.2.2">𝑛</ci><ci id="S2.SS1.p12.6.m6.3.3.4.3.2.3.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.2.3">ℎ</ci></apply><apply id="S2.SS1.p12.6.m6.3.3.4.3.3.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.3"><csymbol cd="ambiguous" id="S2.SS1.p12.6.m6.3.3.4.3.3.1.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.3">subscript</csymbol><ci id="S2.SS1.p12.6.m6.3.3.4.3.3.2.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.3.2">𝑑</ci><ci id="S2.SS1.p12.6.m6.3.3.4.3.3.3.cmml" xref="S2.SS1.p12.6.m6.3.3.4.3.3.3">𝑣</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p12.6.m6.3c">[\mathbf{Z}_{0},...,\mathbf{Z}_{h-1}]\in\mathbb{R}^{n\times hd_{v}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p12.6.m6.3d">[ bold_Z start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , … , bold_Z start_POSTSUBSCRIPT italic_h - 1 end_POSTSUBSCRIPT ] ∈ blackboard_R start_POSTSUPERSCRIPT italic_n × italic_h italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> and projects them through <math alttext="\mathbf{W}\in\mathbb{R}^{hd_{v}\times d}" class="ltx_Math" display="inline" id="S2.SS1.p12.7.m7.1"><semantics id="S2.SS1.p12.7.m7.1a"><mrow id="S2.SS1.p12.7.m7.1.1" xref="S2.SS1.p12.7.m7.1.1.cmml"><mi id="S2.SS1.p12.7.m7.1.1.2" xref="S2.SS1.p12.7.m7.1.1.2.cmml">𝐖</mi><mo id="S2.SS1.p12.7.m7.1.1.1" xref="S2.SS1.p12.7.m7.1.1.1.cmml">∈</mo><msup id="S2.SS1.p12.7.m7.1.1.3" xref="S2.SS1.p12.7.m7.1.1.3.cmml"><mi id="S2.SS1.p12.7.m7.1.1.3.2" xref="S2.SS1.p12.7.m7.1.1.3.2.cmml">ℝ</mi><mrow id="S2.SS1.p12.7.m7.1.1.3.3" xref="S2.SS1.p12.7.m7.1.1.3.3.cmml"><mrow id="S2.SS1.p12.7.m7.1.1.3.3.2" xref="S2.SS1.p12.7.m7.1.1.3.3.2.cmml"><mi id="S2.SS1.p12.7.m7.1.1.3.3.2.2" xref="S2.SS1.p12.7.m7.1.1.3.3.2.2.cmml">h</mi><mo id="S2.SS1.p12.7.m7.1.1.3.3.2.1" xref="S2.SS1.p12.7.m7.1.1.3.3.2.1.cmml">⁢</mo><msub id="S2.SS1.p12.7.m7.1.1.3.3.2.3" xref="S2.SS1.p12.7.m7.1.1.3.3.2.3.cmml"><mi id="S2.SS1.p12.7.m7.1.1.3.3.2.3.2" xref="S2.SS1.p12.7.m7.1.1.3.3.2.3.2.cmml">d</mi><mi id="S2.SS1.p12.7.m7.1.1.3.3.2.3.3" xref="S2.SS1.p12.7.m7.1.1.3.3.2.3.3.cmml">v</mi></msub></mrow><mo id="S2.SS1.p12.7.m7.1.1.3.3.1" lspace="0.222em" rspace="0.222em" xref="S2.SS1.p12.7.m7.1.1.3.3.1.cmml">×</mo><mi id="S2.SS1.p12.7.m7.1.1.3.3.3" xref="S2.SS1.p12.7.m7.1.1.3.3.3.cmml">d</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.p12.7.m7.1b"><apply id="S2.SS1.p12.7.m7.1.1.cmml" xref="S2.SS1.p12.7.m7.1.1"><in id="S2.SS1.p12.7.m7.1.1.1.cmml" xref="S2.SS1.p12.7.m7.1.1.1"></in><ci id="S2.SS1.p12.7.m7.1.1.2.cmml" xref="S2.SS1.p12.7.m7.1.1.2">𝐖</ci><apply id="S2.SS1.p12.7.m7.1.1.3.cmml" xref="S2.SS1.p12.7.m7.1.1.3"><csymbol cd="ambiguous" id="S2.SS1.p12.7.m7.1.1.3.1.cmml" xref="S2.SS1.p12.7.m7.1.1.3">superscript</csymbol><ci id="S2.SS1.p12.7.m7.1.1.3.2.cmml" xref="S2.SS1.p12.7.m7.1.1.3.2">ℝ</ci><apply id="S2.SS1.p12.7.m7.1.1.3.3.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3"><times id="S2.SS1.p12.7.m7.1.1.3.3.1.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.1"></times><apply id="S2.SS1.p12.7.m7.1.1.3.3.2.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.2"><times id="S2.SS1.p12.7.m7.1.1.3.3.2.1.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.2.1"></times><ci id="S2.SS1.p12.7.m7.1.1.3.3.2.2.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.2.2">ℎ</ci><apply id="S2.SS1.p12.7.m7.1.1.3.3.2.3.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.2.3"><csymbol cd="ambiguous" id="S2.SS1.p12.7.m7.1.1.3.3.2.3.1.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.2.3">subscript</csymbol><ci id="S2.SS1.p12.7.m7.1.1.3.3.2.3.2.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.2.3.2">𝑑</ci><ci id="S2.SS1.p12.7.m7.1.1.3.3.2.3.3.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.2.3.3">𝑣</ci></apply></apply><ci id="S2.SS1.p12.7.m7.1.1.3.3.3.cmml" xref="S2.SS1.p12.7.m7.1.1.3.3.3">𝑑</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.p12.7.m7.1c">\mathbf{W}\in\mathbb{R}^{hd_{v}\times d}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.p12.7.m7.1d">bold_W ∈ blackboard_R start_POSTSUPERSCRIPT italic_h italic_d start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT × italic_d end_POSTSUPERSCRIPT</annotation></semantics></math> (Fig.<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S2.F2" title="Figure 2 ‣ II-A Transformer Architecture and Self-Attention Mechanism ‣ II Foundations ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">2</span></a>, the third row).</p>
</div>
<div class="ltx_para" id="S2.SS1.p13">
<p class="ltx_p" id="S2.SS1.p13.1">The fundamental distinction between self-attention and convolutional operations lies in their filter generation mechanism. Unlike convolution’s static filters that remain fixed regardless of input, self-attention dynamically computes input-dependent filters. This approach exhibits two key advantages: (1) permutation invariance, making it robust to input ordering and variable numbers of input points, and (2) the ability to process irregular, non-grid structured data natively.</p>
</div>
<div class="ltx_para" id="S2.SS1.p14">
<p class="ltx_p" id="S2.SS1.p14.1">Research has demonstrated that when augmented with positional encodings, self-attention can theoretically subsume convolutional operations as a special case for local feature extraction<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib23" title="">23</a>]</cite>. Subsequent empirical studies by Cordonnier et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib24" title="">24</a>]</cite>confirmed that properly configured multi-head self-attention can replicate and extend convolutional behaviors. Specifically, self-attention offers superior expressiveness by simultaneously learning both global and local features while adaptively determining optimal kernel weights and receptive fields - capabilities that parallel advanced techniques like deformable convolutions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS1.SSS1.5.1.1">II-A</span>1 </span>Single-head vs. Multi-head Self-Attention in Protein Transformers</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">In the context of protein modeling, self-attention mechanisms serve as the core component for capturing relationships between amino acid residues. While single-head self-attention applies a single attention distribution across the sequence, its representational capacity is limited. In contrast, modern protein-related Transformer models universally adopt multi-head self-attention, which enables the model to attend to multiple subspaces simultaneously. This is particularly beneficial in protein structure prediction and sequence modeling tasks, where both local motifs and long-range interactions are crucial. For example, models like AlphaFold, ESM-Fold, and OmegaFold rely on multi-head attention to effectively capture diverse structural and evolutionary dependencies. As such, multi-head self-attention has become a standard design choice in protein Transformer architectures.</p>
</div>
<figure class="ltx_figure" id="S2.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="419" id="S2.F3.g1" src="x5.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S2.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S2.F3.3.2" style="font-size:90%;">Hierarchical Classification of Self-Attention Mechanisms in Protein Models. This diagram illustrates the categorization of self-attention mechanisms adapted from vision transformer literature. The top-level distinction is made between Single-Head Self-Attention and Multi-Head Self-Attention. Under Multi-Head Self-Attention, several extensions are shown, including standard Transformer models, Pre-trained Transformers such as ESM and ProtBERT, Adapter-based Transformers, and Multi-modal Attention mechanisms that integrate sequence and structure or sequence and function. Additional branches include Spatial Self-Attention, which encompasses Graph-based attention and SE(3)-Equivariant Attention, and Hybrid Attention, which combines CNN with Transformer architectures.</span></figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">(Self-)supervised pre-training</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Transformer models applied to protein-related tasks typically adopt a two-stage training paradigm, inspired by advances in natural language processing and adapted to the biological sequence domain. In the first stage, models are pre-trained on large-scale protein datasets, either in a supervised or self-supervised manner, to learn generalizable representations of protein sequences or structures. In the second stage, the pre-trained models are fine-tuned on downstream tasks such as protein function prediction, structure modeling, subcellular localization, and protein–protein interaction prediction.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Due to the scarcity and high cost of experimental annotations in the biological sciences, self-supervised learning (SSL) has emerged as a particularly effective approach for pre-training. SSL enables models to learn meaningful representations from unlabeled protein data by solving carefully designed proxy tasks. One widely adopted strategy is masked language modeling, inspired by BERT, where a proportion of amino acid residues in protein sequences are masked and the model is trained to recover them. This formulation has been used in models such as ProtBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib26" title="">26</a>]</cite>, ESM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib27" title="">27</a>]</cite>, and PeptideBERT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib28" title="">28</a>]</cite>, effectively capturing both local and global dependencies within sequences.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">Other SSL objectives in protein modeling include inter-residue distance prediction<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib29" title="">29</a>]</cite>, contact map reconstruction, and learning from multiple sequence alignments (MSAs) to incorporate evolutionary relationships<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib30" title="">30</a>]</cite>. Recently, cross-modal SSL strategies have also been explored, where models jointly process sequence data and structural inputs (e.g., 3D coordinates or contact matrices), enabling them to learn biologically grounded representations<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib31" title="">31</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p4">
<p class="ltx_p" id="S2.SS2.p4.1">Broadly, current SSL approaches in protein modeling can be categorized into three main types:</p>
<ol class="ltx_enumerate" id="S2.I4">
<li class="ltx_item" id="S2.I4.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S2.I4.i1.p1">
<p class="ltx_p" id="S2.I4.i1.p1.1">Generative methods, which aim to reconstruct protein sequences or structures from partial or noisy inputs. Examples include masked token prediction, sequence inpainting, and structure reconstruction from sequence embeddings.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S2.I4.i2.p1">
<p class="ltx_p" id="S2.I4.i2.p1.1">Context-based methods, which leverage spatial, sequential, or evolutionary relationships among amino acid residues. Tasks such as next-token prediction, fragment reordering, and predicting masked MSAs fall under this category.</p>
</div>
</li>
<li class="ltx_item" id="S2.I4.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S2.I4.i3.p1">
<p class="ltx_p" id="S2.I4.i3.p1.1">Cross-modal methods, which integrate information from multiple biological modalities—for instance, aligning sequence data with 3D structural information, evolutionary profiles, or even textual annotations—to learn joint representations that reflect multimodal biological knowledge.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S2.SS2.p5">
<p class="ltx_p" id="S2.SS2.p5.1">Overall, (self-)supervised pre-training has become a cornerstone in protein Transformer research, enabling improved generalization across a range of downstream tasks and opening new avenues for data-efficient modeling. As protein databases continue to grow in size and diversity, the role of SSL in scaling and advancing protein understanding will likely remain essential.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Transformer Derivatives in Protein Informatics</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Transformer models, originally developed for sequence-based tasks, have been adapted and extended to meet the specific needs of protein informatics. These derivatives maintain the core principles of the Transformer architecture, such as self-attention mechanisms and parallel processing, while incorporating modifications to enhance performance on tasks such as structure prediction, sequence-to-structure mapping, and functional annotation. This section provides an overview of the major Transformer derivatives developed for protein informatics, highlighting their key features, key references, and applications.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS1.5.1.1">II-C</span>1 </span>Key Derivatives of the Transformer Architecture</h4>
<div class="ltx_para" id="S2.SS3.SSS1.p1">
<p class="ltx_p" id="S2.SS3.SSS1.p1.1">In order to provide an exhaustive overview, a comprehensive tabulation of the salient Transformer derivatives is presented in Appendix A (Table 1 and Table 2). This compilation delineates the pivotal attributes, pertinent application domains, and exemplary models of each Transformer variant, thereby offering a succinct synopsis conducive to an enhanced comprehension of their distinct roles and contributions within the realm of protein informatics.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS3.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS3.SSS2.5.1.1">II-C</span>2 </span>Future Directions for Transformer Derivatives</h4>
<div class="ltx_para" id="S2.SS3.SSS2.p1">
<p class="ltx_p" id="S2.SS3.SSS2.p1.1">The development of Transformer derivatives in protein informatics is an active area of research. Future work in this area is likely to focus on further optimizing these models for specific tasks, exploring new architectures that combine the strengths of different approaches, and addressing the challenges associated with computational efficiency and interpretability. Additionally, there is a growing interest in developing models that can handle multi-modal data, such as integrating protein sequences with structural information and other biological data types. These advancements are expected to lead to even more powerful and versatile models for protein informatics.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS4.5.1.1">II-D</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS4.6.2">Fundamentals of Protein Structure and Function</span>
</h3>
<div class="ltx_para" id="S2.SS4.p1">
<p class="ltx_p" id="S2.SS4.p1.1">Proteins are fundamental macromolecules that play indispensable roles in virtually all biological processes, including catalysis, molecular transport, structural integrity, and signal transduction. The remarkable functional diversity of proteins arises from their distinct three-dimensional conformations, which are organized hierarchically into primary, secondary, tertiary, and quaternary structural levels. This section presents a systematic overview of these structural hierarchies, emphasizing the critical importance of protein folding and stability, the intricate relationship between structure and biological function, and the persistent challenges associated with accurately predicting protein structures.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS1.5.1.1">II-D</span>1 </span>Hierarchical Organization of Protein Structure</h4>
<div class="ltx_para" id="S2.SS4.SSS1.p1">
<ul class="ltx_itemize" id="S2.I5">
<li class="ltx_item" id="S2.I5.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I5.i1.p1">
<p class="ltx_p" id="S2.I5.i1.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i1.p1.1.1">Primary Structure:</span>
The primary structure refers to the linear sequence of amino acids linked by peptide bonds, which determines the folding trajectory and the ultimate three-dimensional conformation critical to protein function. Mutations at this level can disrupt folding and lead to dysfunction or disease <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib32" title="">32</a>]</cite>. Traditional methods such as sequence alignment and homology modeling have long been used to infer functional and evolutionary relationships <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib33" title="">33</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib34" title="">34</a>]</cite>. Transformer-based models have since enhanced primary structure analysis by capturing long-range dependencies, offering a deeper understanding of sequence-function relationships<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib35" title="">35</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib36" title="">36</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I5.i2.p1">
<p class="ltx_p" id="S2.I5.i2.p1.2"><span class="ltx_text ltx_font_bold" id="S2.I5.i2.p1.2.1">Secondary Structure:</span>
Secondary structures, including <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.I5.i2.p1.1.m1.1"><semantics id="S2.I5.i2.p1.1.m1.1a"><mi id="S2.I5.i2.p1.1.m1.1.1" xref="S2.I5.i2.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i2.p1.1.m1.1b"><ci id="S2.I5.i2.p1.1.m1.1.1.cmml" xref="S2.I5.i2.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i2.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i2.p1.1.m1.1d">italic_α</annotation></semantics></math>-helices and <math alttext="\beta" class="ltx_Math" display="inline" id="S2.I5.i2.p1.2.m2.1"><semantics id="S2.I5.i2.p1.2.m2.1a"><mi id="S2.I5.i2.p1.2.m2.1.1" xref="S2.I5.i2.p1.2.m2.1.1.cmml">β</mi><annotation-xml encoding="MathML-Content" id="S2.I5.i2.p1.2.m2.1b"><ci id="S2.I5.i2.p1.2.m2.1.1.cmml" xref="S2.I5.i2.p1.2.m2.1.1">𝛽</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I5.i2.p1.2.m2.1c">\beta</annotation><annotation encoding="application/x-llamapun" id="S2.I5.i2.p1.2.m2.1d">italic_β</annotation></semantics></math>-sheets, arise from local hydrogen bonding patterns within the backbone. These elements provide mechanical stability and facilitate early folding events<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib37" title="">37</a>]</cite>. Experimental techniques such as X-ray crystallography and NMR have elucidated these motifs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib38" title="">38</a>]</cite>, while classical predictors like Chou-Fasman and GOR rely on residue propensities<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib39" title="">39</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib40" title="">40</a>]</cite>. Modern Transformer architectures significantly improve prediction accuracy by modeling context-aware residue interactions<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib41" title="">41</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib42" title="">42</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I5.i3.p1">
<p class="ltx_p" id="S2.I5.i3.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i3.p1.1.1">Tertiary Structure:</span>
Tertiary structure reflects the full 3D conformation of a polypeptide, integrating secondary elements and side-chain orientations to form functional domains, such as active and binding sites<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib43" title="">43</a>]</cite>. Prediction remains challenging due to the complexity of intramolecular forces. However, Transformer-based models—particularly AlphaFold—have revolutionized this task by learning spatial relationships from large-scale sequence–structure pairs<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib44" title="">44</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib45" title="">45</a>]</cite>.</p>
</div>
</li>
<li class="ltx_item" id="S2.I5.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="S2.I5.i4.p1">
<p class="ltx_p" id="S2.I5.i4.p1.1"><span class="ltx_text ltx_font_bold" id="S2.I5.i4.p1.1.1">Quaternary Structure:</span>
Quaternary structure entails the assembly of multiple polypeptide chains into functional protein complexes. This level is essential for the activity of many proteins, including hemoglobin and ribosomes. Accurate modeling of inter-subunit interactions is vital for drug discovery and structural annotation. Recent Transformer frameworks exhibit promising capabilities in predicting protein-protein interactions and complex formation<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib46" title="">46</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib47" title="">47</a>]</cite>.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS2.5.1.1">II-D</span>2 </span>Protein Folding and Structural Stability</h4>
<div class="ltx_para" id="S2.SS4.SSS2.p1">
<p class="ltx_p" id="S2.SS4.SSS2.p1.1">Protein folding is an intrinsic and thermodynamically driven process through which a linear polypeptide chain acquires its specific three-dimensional native conformation, guided by intramolecular interactions among amino acid residues. This process adheres to the thermodynamic principle of free energy minimization, whereby the polypeptide traverses a conformational landscape—often described by the ”folding funnel” model—to reach a low-energy functionally active state <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib48" title="">48</a>]</cite>. However, aberrations in the folding pathway can result in misfolding and subsequent aggregation, leading to pathological conditions such as Alzheimer’s disease, in which misfolded proteins accumulate into non-functional aggregates that compromise cellular homeostasis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib49" title="">49</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS2.p2">
<p class="ltx_p" id="S2.SS4.SSS2.p2.1">Protein stability denotes the capacity of a protein to preserve its native structure under varying physicochemical conditions, including fluctuations in pH, temperature, and ionic strength. Stability is modulated by intrinsic factors such as amino acid composition and structural interactions, as well as extrinsic environmental variables. While traditional approaches such as molecular dynamics simulations have been widely utilized to investigate protein stability, recent advancements have highlighted the growing efficacy of transformer-based models in predicting stability. These models leverage deep learning to extract and interpret sequence-structure relationships that underlie conformational robustness under stress conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib50" title="">50</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib51" title="">51</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS4.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection"><span class="ltx_text" id="S2.SS4.SSS3.5.1.1">II-D</span>3 </span>The Relationship Between Protein Structure and Function</h4>
<div class="ltx_para" id="S2.SS4.SSS3.p1">
<p class="ltx_p" id="S2.SS4.SSS3.p1.1">The structure of a protein is intricately tied to its biological function, as the specific spatial arrangement of amino acid residues determines the molecule’s ability to carry out precise biochemical tasks. Functional domains within proteins, such as catalytic or ligand-binding regions, are often evolutionarily conserved and serve as indicators of specific molecular activities. For instance, enzymatic proteins feature active sites with highly specialized geometries that facilitate substrate conversion, while immunoglobulins contain hypervariable regions designed to recognize and bind target antigens with high specificity.</p>
</div>
<div class="ltx_para" id="S2.SS4.SSS3.p2">
<p class="ltx_p" id="S2.SS4.SSS3.p2.1">Accurate prediction of protein function from sequence and structural data remains a central challenge in bioinformatics. Conventional methods typically employ homology-based inference, extrapolating function from proteins with known annotations and similar structures. In contrast, Transformer-based architectures—such as ProtTrans have demonstrated enhanced generalization across phylogenetically diverse sequences by leveraging large-scale pretraining on protein corpora. These models have shown promising capabilities in predicting functional sites, identifying subcellular localization patterns, and modeling biomolecular interactions with improved resolution <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib52" title="">52</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib53" title="">53</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Overview of Selected Major Transformer Derivatives in Protein Informatics</figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S2.T1.4.1.1">
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S2.T1.4.1.1.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.1.1.1.1">
<span class="ltx_p" id="S2.T1.4.1.1.1.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.4.1.1.1.1.1.1" style="font-size:90%;">Model Name</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S2.T1.4.1.1.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.1.1.2.1">
<span class="ltx_p" id="S2.T1.4.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.4.1.1.2.1.1.1" style="font-size:90%;">Key Features</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S2.T1.4.1.1.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.1.1.3.1">
<span class="ltx_p" id="S2.T1.4.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.4.1.1.3.1.1.1" style="font-size:90%;">Key Applications</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S2.T1.4.1.1.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.1.1.4.1">
<span class="ltx_p" id="S2.T1.4.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S2.T1.4.1.1.4.1.1.1" style="font-size:90%;">Key References</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.4.2.1">
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.2.1.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.2.1.1.1">
<span class="ltx_p" id="S2.T1.4.2.1.1.1.1"><span class="ltx_text" id="S2.T1.4.2.1.1.1.1.1" style="font-size:90%;">AlphaFold</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.2.1.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.2.1.2.1">
<span class="ltx_p" id="S2.T1.4.2.1.2.1.1"><span class="ltx_text" id="S2.T1.4.2.1.2.1.1.1" style="font-size:90%;">Hybrid architecture with Evoformer and Structure Module, and advanced attention mechanisms, and Multi-task training with structural supervision.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.2.1.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.2.1.3.1">
<span class="ltx_p" id="S2.T1.4.2.1.3.1.1"><span class="ltx_text" id="S2.T1.4.2.1.3.1.1.1" style="font-size:90%;">3D structure prediction, and protein folding and sequence-to-structure mapping.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S2.T1.4.2.1.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.2.1.4.1">
<span class="ltx_p" id="S2.T1.4.2.1.4.1.1"><span class="ltx_text" id="S2.T1.4.2.1.4.1.1.1" style="font-size:90%;">Jumper et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.2.1.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib54" title="">54</a><span class="ltx_text" id="S2.T1.4.2.1.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.3.2">
<td class="ltx_td ltx_align_justify" id="S2.T1.4.3.2.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.3.2.1.1">
<span class="ltx_p" id="S2.T1.4.3.2.1.1.1"><span class="ltx_text" id="S2.T1.4.3.2.1.1.1.1" style="font-size:90%;">ProtTrans</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.3.2.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.3.2.2.1">
<span class="ltx_p" id="S2.T1.4.3.2.2.1.1"><span class="ltx_text" id="S2.T1.4.3.2.2.1.1.1" style="font-size:90%;">Pre-trained on large-scale protein sequences, and integration of sequence and evolutionary context.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.3.2.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.3.2.3.1">
<span class="ltx_p" id="S2.T1.4.3.2.3.1.1"><span class="ltx_text" id="S2.T1.4.3.2.3.1.1.1" style="font-size:90%;">Function prediction, and large-scale protein annotation and transfer learning for downstream tasks.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.3.2.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.3.2.4.1">
<span class="ltx_p" id="S2.T1.4.3.2.4.1.1"><span class="ltx_text" id="S2.T1.4.3.2.4.1.1.1" style="font-size:90%;">Elnaggar et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.3.2.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib55" title="">55</a><span class="ltx_text" id="S2.T1.4.3.2.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.4.3">
<td class="ltx_td ltx_align_justify" id="S2.T1.4.4.3.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.3.1.1">
<span class="ltx_p" id="S2.T1.4.4.3.1.1.1"><span class="ltx_text" id="S2.T1.4.4.3.1.1.1.1" style="font-size:90%;">RoseTTAFold</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.4.3.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.3.2.1">
<span class="ltx_p" id="S2.T1.4.4.3.2.1.1"><span class="ltx_text" id="S2.T1.4.4.3.2.1.1.1" style="font-size:90%;">Computationally efficient, and end-to-end structure prediction.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.4.3.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.3.3.1">
<span class="ltx_p" id="S2.T1.4.4.3.3.1.1"><span class="ltx_text" id="S2.T1.4.4.3.3.1.1.1" style="font-size:90%;">Membrane protein structure prediction, and accelerated structural biology.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.4.3.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.4.3.4.1">
<span class="ltx_p" id="S2.T1.4.4.3.4.1.1"><span class="ltx_text" id="S2.T1.4.4.3.4.1.1.1" style="font-size:90%;">Baek et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.4.3.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib56" title="">56</a><span class="ltx_text" id="S2.T1.4.4.3.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.5.4">
<td class="ltx_td ltx_align_justify" id="S2.T1.4.5.4.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.5.4.1.1">
<span class="ltx_p" id="S2.T1.4.5.4.1.1.1"><span class="ltx_text" id="S2.T1.4.5.4.1.1.1.1" style="font-size:90%;">ESM-Fold</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.5.4.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.5.4.2.1">
<span class="ltx_p" id="S2.T1.4.5.4.2.1.1"><span class="ltx_text" id="S2.T1.4.5.4.2.1.1.1" style="font-size:90%;">MSA-free architecture, and protein language model-based and High efficiency and scalability.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.5.4.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.5.4.3.1">
<span class="ltx_p" id="S2.T1.4.5.4.3.1.1"><span class="ltx_text" id="S2.T1.4.5.4.3.1.1.1" style="font-size:90%;">Genome-wide structural annotation, and orphan protein and rare species analysis.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.5.4.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.5.4.4.1">
<span class="ltx_p" id="S2.T1.4.5.4.4.1.1"><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.1" style="font-size:90%;">Lin et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib57" title="">57</a><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.4" style="font-size:90%;">, Arana et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib58" title="">58</a><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.7" style="font-size:90%;">, Jumper et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib59" title="">59</a><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.10" style="font-size:90%;">, Senior et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib60" title="">60</a><span class="ltx_text" id="S2.T1.4.5.4.4.1.1.12.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.6.5">
<td class="ltx_td ltx_align_justify" id="S2.T1.4.6.5.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.6.5.1.1">
<span class="ltx_p" id="S2.T1.4.6.5.1.1.1"><span class="ltx_text" id="S2.T1.4.6.5.1.1.1.1" style="font-size:90%;">TAPE</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.6.5.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.6.5.2.1">
<span class="ltx_p" id="S2.T1.4.6.5.2.1.1"><span class="ltx_text" id="S2.T1.4.6.5.2.1.1.1" style="font-size:90%;">Transferable protein embeddings, and transformer-based multi-task learning.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.6.5.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.6.5.3.1">
<span class="ltx_p" id="S2.T1.4.6.5.3.1.1"><span class="ltx_text" id="S2.T1.4.6.5.3.1.1.1" style="font-size:90%;">Protein-protein interaction (PPI) prediction, and low-resource fine-tuning scenarios.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.6.5.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.6.5.4.1">
<span class="ltx_p" id="S2.T1.4.6.5.4.1.1"><span class="ltx_text" id="S2.T1.4.6.5.4.1.1.1" style="font-size:90%;">Rao et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.6.5.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib61" title="">61</a><span class="ltx_text" id="S2.T1.4.6.5.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.7.6">
<td class="ltx_td ltx_align_justify" id="S2.T1.4.7.6.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.7.6.1.1">
<span class="ltx_p" id="S2.T1.4.7.6.1.1.1"><span class="ltx_text" id="S2.T1.4.7.6.1.1.1.1" style="font-size:90%;">DstruCCN</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.7.6.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.7.6.2.1">
<span class="ltx_p" id="S2.T1.4.7.6.2.1.1"><span class="ltx_text" id="S2.T1.4.7.6.2.1.1.1" style="font-size:90%;">Hybrid architecture: CNN + Transformer, and binding site matrix-guided 3D reconstruction and superior performance on low-homology proteins.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.7.6.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.7.6.3.1">
<span class="ltx_p" id="S2.T1.4.7.6.3.1.1"><span class="ltx_text" id="S2.T1.4.7.6.3.1.1.1" style="font-size:90%;">Single-sequence protein structure prediction, and Low-homology or orphan protein modeling.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.7.6.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.7.6.4.1">
<span class="ltx_p" id="S2.T1.4.7.6.4.1.1"><span class="ltx_text" id="S2.T1.4.7.6.4.1.1.1" style="font-size:90%;">Zhou et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.7.6.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib62" title="">62</a><span class="ltx_text" id="S2.T1.4.7.6.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.8.7">
<td class="ltx_td ltx_align_justify" id="S2.T1.4.8.7.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.8.7.1.1">
<span class="ltx_p" id="S2.T1.4.8.7.1.1.1"><span class="ltx_text" id="S2.T1.4.8.7.1.1.1.1" style="font-size:90%;">ProteinBERT</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.8.7.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.8.7.2.1">
<span class="ltx_p" id="S2.T1.4.8.7.2.1.1"><span class="ltx_text" id="S2.T1.4.8.7.2.1.1.1" style="font-size:90%;">BERT-based architecture adapted for proteins, and pretrained on large-scale protein datasets.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.8.7.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.8.7.3.1">
<span class="ltx_p" id="S2.T1.4.8.7.3.1.1"><span class="ltx_text" id="S2.T1.4.8.7.3.1.1.1" style="font-size:90%;">Enzyme classification and function annotation, and Ligand binding and subcellular localization prediction.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.8.7.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.8.7.4.1">
<span class="ltx_p" id="S2.T1.4.8.7.4.1.1"><span class="ltx_text" id="S2.T1.4.8.7.4.1.1.1" style="font-size:90%;">Brandes et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.8.7.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib63" title="">63</a><span class="ltx_text" id="S2.T1.4.8.7.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.9.8">
<td class="ltx_td ltx_align_justify" id="S2.T1.4.9.8.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.9.8.1.1">
<span class="ltx_p" id="S2.T1.4.9.8.1.1.1"><span class="ltx_text" id="S2.T1.4.9.8.1.1.1.1" style="font-size:90%;">Trans-MoRFs</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.9.8.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.9.8.2.1">
<span class="ltx_p" id="S2.T1.4.9.8.2.1.1"><span class="ltx_text" id="S2.T1.4.9.8.2.1.1.1" style="font-size:90%;">High performance on IDR prediction, and self-attention for long-range dependency modeling.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.9.8.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.9.8.3.1">
<span class="ltx_p" id="S2.T1.4.9.8.3.1.1"><span class="ltx_text" id="S2.T1.4.9.8.3.1.1.1" style="font-size:90%;">MoRF identification in intrinsically disordered regions, functional annotation of dynamic protein regions.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.9.8.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.9.8.4.1">
<span class="ltx_p" id="S2.T1.4.9.8.4.1.1"><span class="ltx_text" id="S2.T1.4.9.8.4.1.1.1" style="font-size:90%;">Meng et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.9.8.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib64" title="">64</a><span class="ltx_text" id="S2.T1.4.9.8.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.10.9">
<td class="ltx_td ltx_align_justify" id="S2.T1.4.10.9.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.10.9.1.1">
<span class="ltx_p" id="S2.T1.4.10.9.1.1.1"><span class="ltx_text" id="S2.T1.4.10.9.1.1.1.1" style="font-size:90%;">ChemBERTa</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.10.9.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.10.9.2.1">
<span class="ltx_p" id="S2.T1.4.10.9.2.1.1"><span class="ltx_text" id="S2.T1.4.10.9.2.1.1.1" style="font-size:90%;">BERT-based architecture adapted for chemical data, and fine-tuned on protein-ligand interaction datasets.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.10.9.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.10.9.3.1">
<span class="ltx_p" id="S2.T1.4.10.9.3.1.1"><span class="ltx_text" id="S2.T1.4.10.9.3.1.1.1" style="font-size:90%;">Early-stage virtual screening, and chemical property and interaction modeling.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify" id="S2.T1.4.10.9.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.10.9.4.1">
<span class="ltx_p" id="S2.T1.4.10.9.4.1.1"><span class="ltx_text" id="S2.T1.4.10.9.4.1.1.1" style="font-size:90%;">Chithrananda et al.</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.10.9.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib65" title="">65</a><span class="ltx_text" id="S2.T1.4.10.9.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S2.T1.4.11.10">
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.4.11.10.1" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.11.10.1.1">
<span class="ltx_p" id="S2.T1.4.11.10.1.1.1"><span class="ltx_text" id="S2.T1.4.11.10.1.1.1.1" style="font-size:90%;">DeepChem</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.4.11.10.2" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.11.10.2.1">
<span class="ltx_p" id="S2.T1.4.11.10.2.1.1"><span class="ltx_text" id="S2.T1.4.11.10.2.1.1.1" style="font-size:90%;">Integrates diverse deep learning models, including Transformers, and comprehensive drug discovery platform</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.4.11.10.3" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.11.10.3.1">
<span class="ltx_p" id="S2.T1.4.11.10.3.1.1"><span class="ltx_text" id="S2.T1.4.11.10.3.1.1.1" style="font-size:90%;">Molecular property prediction for drug candidates, and flexible integration for customized workflows.</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S2.T1.4.11.10.4" style="padding-top:2.25pt;padding-bottom:2.25pt;">
<span class="ltx_inline-block ltx_align_top" id="S2.T1.4.11.10.4.1">
<span class="ltx_p" id="S2.T1.4.11.10.4.1.1"><span class="ltx_text" id="S2.T1.4.11.10.4.1.1.1" style="font-size:90%;">Ramsundar</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.T1.4.11.10.4.1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib66" title="">66</a><span class="ltx_text" id="S2.T1.4.11.10.4.1.1.3.2" style="font-size:90%;">]</span></cite></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Transformers in Protein</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">In recent years, Transformer models—originally developed for natural language processing—have been increasingly applied to protein informatics, leading to significant advancements across the field. By leveraging self-attention mechanisms, these models effectively capture complex dependencies within biological sequences, enabling breakthroughs in tasks such as prediction of protein structure and function prediction, protein–protein interaction modeling, and drug discovery. This section reviews the current progress and key applications of Transformer-based approaches in protein informatics, focusing on four major domains: protein structure prediction(<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.SS1" title="III-A Transformers for Protein Structure Prediction ‣ III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-A</span></span></a>), protein function prediction(<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.SS2" title="III-B Transformers for Protein Function Prediction ‣ III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-B</span></span></a>), protein-protein interactions (PPIs)(<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.SS3" title="III-C Transformers for Protein-Protein Interactions (PPIs) ‣ III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-C</span></span></a>), and drug discovery(<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.SS4" title="III-D Transformers for Drug Discovery and Target Identification ‣ III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag"><span class="ltx_text">III-D</span></span></a>).</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Transformers for Protein Structure Prediction</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">Protein structure prediction remains one of the most critical challenges in computational biology, as it underpins our understanding of protein function, interactions, and potential for therapeutic intervention. For decades, researchers have relied on experimental methods such as X-ray crystallography and NMR spectroscopy, but these techniques are time consuming, costly, and require highly purified protein samples. As a result, computational methods for structure prediction have garnered significant attention, with deep learning methods, particularly Transformer models, playing an increasingly important role.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The application of Transformer models in protein structure prediction has significantly advanced the field by providing more accurate and efficient methods for predicting protein structures from sequences. These models have demonstrated remarkable performance in handling complex folds and long-range interactions, which are crucial for understanding protein function and interactions.</p>
</div>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.1">1.<span class="ltx_text ltx_font_bold" id="S3.SS1.p3.1.1">AlphaFold</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib54" title="">54</a>]</cite>, developed by DeepMind, has revolutionized protein structure prediction by achieving near-experimental accuracy through an innovative deep learning framework. The model leverages a hybrid architecture combining an Evoformer (a Transformer-like module with axial attention) to analyze multi-sequence alignments and evolutionary couplings, and a Structure Module that refines atomic coordinates using geometric constraints. Key to its success are attention mechanisms, including triangular self-attention, which explicitly models 3D spatial relationships between residues while capturing long-range dependencies critical for folding. Trained on the Protein Data Bank (PDB) with a multi-task loss function incorporating distograms and physical constraints, AlphaFold attained unprecedented performance in CASP14 (2020), achieving a median Global Distance Test (GDT) score of 92.4. Beyond static structures, subsequent releases like AlphaFold-Multimer extended predictions to protein complexes, and the AlphaFold DB provided open access to over 200 million structures, transforming fields from drug discovery to synthetic biology. Despite limitations in modeling dynamic conformations or disordered regions, AlphaFold’s open-source release has democratized structural biology, inspiring tools like RoseTTAFold and ESMFold, and underscoring the potential of AI to accelerate scientific discovery.</p>
</div>
<div class="ltx_para" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1">2.<span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">RoseTTAFold</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib56" title="">56</a>]</cite>, another notable model developed by the University of Washington’s Rosetta group, uses a similar approach to AlphaFold but is designed to be more computationally efficient . RoseTTAFold builds upon the Transformer architecture by combining it with a graph-based approach, which enables it to predict protein structures faster without sacrificing accuracy. This model has been particularly valuable for solving structures of membrane proteins, which have historically been difficult to predict with traditional methods. Both AlphaFold and RoseTTAFold highlight the significant potential of Transformer models to accelerate the process of structural biology, facilitating drug discovery and therapeutic development.</p>
</div>
<div class="ltx_para" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1">3.<span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">ESM-Fold</span>, developed by Meta AI, represents a significant innovation in protein structure prediction by employing language models trained on extensive protein sequence datasets<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib57" title="">57</a>]</cite>. Unlike traditional methods that rely heavily on multiple sequence alignments (MSA) to infer evolutionary relationships and structural features, ESM-Fold bypasses the need for MSA entirely, extracting meaningful information directly from single sequences. This approach significantly reduces computational requirements and avoids the bottlenecks associated with generating alignments<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib57" title="">57</a>]</cite>. Trained on billions of protein sequences, ESM-Fold captures the intricate contextual relationships between amino acids, enabling it to predict the three-dimensional structure of proteins with remarkable accuracy, even for sequences with minimal or no homology to known structures<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib57" title="">57</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib58" title="">58</a>]</cite>. Its efficiency makes it highly suitable for high-throughput studies, the analysis of newly sequenced genomes, and applications in synthetic biology<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib59" title="">59</a>]</cite>. ESM-Fold also excels at predicting structures for orphan proteins or those from understudied organisms, where homologous sequence data may be scarce.By leveraging the power of language models, ESM-Fold reduces computational demands and broadens the scope of proteins that can be effectively analyzed, accelerating advancements in protein science and its applications in drug discovery and bioengineering<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib57" title="">57</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib60" title="">60</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1">4.<span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">OmegaFold</span>, developed to meet the growing demand for efficient and accurate protein structure prediction, represents a breakthrough in utilizing Transformer-based architectures for real-time analysis<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib67" title="">67</a>]</cite>. Unlike traditional models that depend on multiple sequence alignments (MSA) to derive evolutionary relationships, OmegaFold operates directly on single protein sequences. This streamlined approach significantly reduces computational requirements and enhances applicability, particularly in high-throughput studies and scenarios where homologous sequence data is limited. OmegaFold incorporates advanced Transformer-based mechanisms to capture both local and long-range dependencies within protein sequences, enabling it to predict three-dimensional structures with high accuracy. By bypassing the MSA step, OmegaFold avoids the computational bottlenecks associated with alignment generation, making it ideal for rapid analyses of newly sequenced proteins or proteins from diverse and uncharacterized organisms. OmegaFold’s exceptional performance on orphan proteins and novel sequences, and its ability to generalize across various protein families, make it a versatile tool for high-throughput studies, including drug discovery and synthetic biology applications. In validation studies, OmegaFold has shown competitive accuracy compared to leading models, including AlphaFold and RoseTTAFold, particularly for single-sequence inputs. Overall, OmegaFold offers a transformative approach to protein structure prediction by leveraging the power of Transformer-based models while eliminating the reliance on MSA. Its combination of speed, accuracy, and adaptability makes it an essential resource for researchers tackling the challenges of modern structural biology<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib67" title="">67</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">5.<span class="ltx_text ltx_font_bold" id="S3.SS1.p7.1.1">ProtGPT2</span>, inspired by the success of generative pretraining in natural language processing (e.g., GPT models), applies similar methodologies to protein informatics<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib68" title="">68</a>]</cite>. Designed to generate novel protein sequences that are both structurally viable and functionally diverse, ProtGPT2 leverages a Transformer-based architecture optimized for sequence generation. Trained on extensive protein sequence databases, the model captures the complex statistical relationships between amino acids, enabling it to propose de novo sequences with realistic biophysical properties, as confirmed through computational validations such as secondary structure prediction and solvent accessibility analysis. A key advantage of ProtGPT2 is its focus on structural and functional diversity, generating a wide array of sequences that enhance the exploration of sequence-function landscapes. This diversity is particularly valuable in applications like directed evolution, where a diverse library can increase the chances of identifying sequences with desired characteristics. ProtGPT2 also integrates structural constraints into its generative process, ensuring that the proposed sequences are compatible with known folding patterns, minimizing the risk of aggregation or misfolding. Its ability to design sequences for entirely novel protein folds makes it a powerful tool for tackling challenges that cannot be addressed by template-based methods. Applications of ProtGPT2 extend to biocatalyst development and therapeutic protein design, where it accelerates the engineering cycle by reducing the need for extensive experimental screening<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib69" title="">69</a>]</cite>. ProtGPT2 exemplifies the transformative potential of generative pretraining in protein sequence design, positioning it as a cornerstone technology for future innovations in protein engineering and synthetic biology.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<p class="ltx_p" id="S3.SS1.p8.1">6.<span class="ltx_text ltx_font_bold" id="S3.SS1.p8.1.1">RFDiffusion</span>, developed by the Baker Lab at the University of Washington, applies diffusion modeling—originally used in image generation—to de novo protein design by generating novel backbones through a forward-backward noise process<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib70" title="">70</a>]</cite>. Unlike traditional sequence- or template-based methods, it reconstructs viable structures from noise, enabling exploration of conformational space under physical and biochemical constraints. The model excels in producing experimentally viable proteins, supported by geometric, interaction, and evolutionary scoring. Applications span therapeutic design, enzyme engineering, and scaffold generation, though challenges remain in computational cost and constraint accuracy. As a pioneering integration of generative modeling and structural biology, RFDiffusion marks a significant advance in computational protein engineering.</p>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1">7.<span class="ltx_text ltx_font_bold" id="S3.SS1.p9.1.1">GraphTrans</span>, a model integrating graph neural networks (GNNs) with Transformer layers, enhances protein complex prediction and structure refinement by capturing both spatial residue relationships and long-range sequence dependencies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib71" title="">71</a>]</cite>. GNNs model topological interactions within protein complexes, while Transformers enrich global context understanding, enabling GraphTrans to predict and iteratively refine complex structures with high accuracy. This synergy allows the model to correct spatial inaccuracies and better represent multi-subunit interactions, highlighting its value in advancing protein structure analysis and functional annotation.</p>
</div>
<div class="ltx_para" id="S3.SS1.p10">
<p class="ltx_p" id="S3.SS1.p10.1">8. In recent years, the integration of diverse deep learning architectures has emerged as a promising strategy to enhance the accuracy and robustness of protein structure prediction. Zhou et al. (2024) proposed a novel model named <span class="ltx_text ltx_font_bold" id="S3.SS1.p10.1.1">DstruCCN</span>, which combines Convolutional Neural Networks (CNNs) and a supervised Transformer-based protein language model to perform single-sequence protein structure prediction<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib62" title="">62</a>]</cite>. The CNN component focuses on capturing local spatial dependencies among residues, while the Transformer module models long-range sequence dependencies by leveraging its self-attention mechanism. The features extracted by both networks are fused to predict the binding site matrix of proteins, which is then used to reconstruct the three-dimensional (3D) structure through energy minimization-based refinement.</p>
</div>
<div class="ltx_para" id="S3.SS1.p11">
<p class="ltx_p" id="S3.SS1.p11.1">The proposed approach outperforms traditional CNN-only or Transformer-only architectures, particularly in modeling complex protein folds with sparse evolutionary information. This is especially relevant for orphan proteins or sequences with limited homologs, where traditional MSA-based methods may underperform. Moreover, the model’s modularity allows for flexible integration into broader protein structure prediction pipelines. The success of DstruCCN highlights the potential of hybrid deep neural networks in advancing single-sequence protein modeling, which is an increasingly important direction in post-AlphaFold research.</p>
</div>
<div class="ltx_para" id="S3.SS1.p12">
<p class="ltx_p" id="S3.SS1.p12.1">9. In the evolving landscape of protein secondary structure prediction (PSSP), the integration of diverse deep learning architectures has shown significant promise. Chen et al. (2024) introduced <span class="ltx_text ltx_font_bold" id="S3.SS1.p12.1.1">MFTrans</span>, a novel deep learning-based multi-feature fusion network designed to enhance the precision and efficiency of PSSP<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib72" title="">72</a>]</cite>. This model employs a Multiple Sequence Alignment (MSA) Transformer in combination with a multi-view deep learning architecture to effectively capture both global and local features of protein sequences. MFTrans integrates diverse features generated by protein sequences, including MSA, sequence information, evolutionary information, and hidden state information, using a multi-feature fusion strategy. The MSA Transformer is utilized to interleave row and column attention across the input MSA, while a Transformer encoder and decoder are introduced to enhance the extracted high-level features. A hybrid network architecture, combining a Convolutional Neural Network (CNN) with a bidirectional Gated Recurrent Unit (BiGRU) network, is used to further extract high-level features after feature fusion.</p>
</div>
<div class="ltx_para" id="S3.SS1.p13">
<p class="ltx_p" id="S3.SS1.p13.1">In independent tests, experimental results demonstrate that MFTrans has superior generalization ability, outperforming other state-of-the-art PSSP models by 3% on average on public benchmarks including CASP12, CASP13, CASP14, TEST2016, TEST2018, and CB513 . Case studies further highlight its advanced performance in predicting mutation sites. The success of MFTrans underscores the potential of integrating multiple feature representations and hybrid neural network architectures in advancing the field of protein secondary structure prediction.</p>
</div>
<div class="ltx_para" id="S3.SS1.p14">
<p class="ltx_p" id="S3.SS1.p14.1">10. Recent advancements in deep learning have spurred the development of hybrid architectures that effectively integrate complementary modeling strategies for improved protein structure prediction. One such approach is <span class="ltx_text ltx_font_bold" id="S3.SS1.p14.1.1">TransConv</span>(2025), a model that fuses transformer-based attention mechanisms with convolutional neural networks to enhance the accuracy of secondary structure prediction<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib73" title="">73</a>]</cite>. While transformers excel at capturing long-range dependencies across protein sequences, convolutional layers are adept at extracting local features critical to identifying structural motifs. By combining these two paradigms, TransConv achieves a balanced representation of both global and local sequence-context relationships. This integrated framework enables the model to better capture structural patterns derived from backbone hydrogen bonding interactions, a key determinant of secondary structure. Experimental evaluations on standard benchmark datasets have demonstrated that TransConv consistently outperforms several state-of-the-art methods, highlighting its potential as a robust tool for efficient and accurate secondary structure prediction.</p>
</div>
<div class="ltx_para" id="S3.SS1.p15">
<p class="ltx_p" id="S3.SS1.p15.1">11. Beyond conventional sequence-based modeling, recent research has demonstrated the efficacy of transformers in reconstructing atomic protein structures from cryo-electron microscopy (cryo-EM) density maps. One representative work introduces <span class="ltx_text ltx_font_bold" id="S3.SS1.p15.1.1">Cryo2Struct</span> (2024), a fully automated framework that employs a 3D transformer network to identify atomic coordinates and amino acid types directly from volumetric cryo-EM data<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib74" title="">74</a>]</cite>. To convert these predictions into coherent backbone conformations, the model incorporates a Hidden Markov Model (HMM) that sequentially connects the detected atoms, thereby forming complete protein backbones. This end-to-end pipeline enables template-free de novo structure prediction, a crucial advancement for modeling proteins lacking homologous or predicted templates. Compared to traditional ab initio methods such as Phenix, Cryo2Struct demonstrates superior accuracy and completeness, even across varying density resolutions and protein sizes. This approach highlights the versatility of transformer models in bridging experimental data with computational inference, pushing forward the frontiers of structural bioinformatics.</p>
</div>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="395" id="S3.F4.g1" src="x6.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S3.F4.3.2" style="font-size:90%;">Illustrative diagram of three fundamental supervised learning tasks.
Supervised learning in machine learning (ML) is typically categorized into classification and regression tasks. While two-dimensional representations are used here for conceptual clarity, real-world datasets often reside in high-dimensional feature spaces.
(a) In binary classification, each sample belongs to one of two possible categories. For instance, a model may classify protein variants as either stable or unstable<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib75" title="">75</a>]</cite>, or determine whether a protein is a G-protein-coupled receptor or not, based on sequence-derived features and machine learning models<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib76" title="">76</a>]</cite>.
(b) Multi-class classification involves assigning samples to one of several discrete classes. For example, recent studies have developed machine learning models to predict the subcellular localization of human proteins—such as nucleus, cytoplasm, mitochondria, and extracellular regions—based on features extracted from immunohistochemistry images<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib77" title="">77</a>]</cite>.
(c) In regression tasks, the goal is to predict continuous numerical properties of proteins. For instance, recent models have been developed to estimate protein solubility levels directly from sequence-derived or structural features, enabling fine-grained prediction beyond binary soluble/insoluble classification<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib78" title="">78</a>]</cite>.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Transformers for Protein Function Prediction</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Prediction of protein function is crucial to understand the role of proteins in cellular processes and their potential as drug targets. Traditional methods for prediction of protein functions are based heavily on sequence homology and annotations from protein databases. However, these methods are unable to predict functions for proteins that lack close homologs or belong to uncharacterized families.</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p2.1.1">1.ProteinBERT.</span> Recent advancements in Transformer-based models have significantly enhanced protein function prediction. One notable example is ProtBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib63" title="">63</a>]</cite>, a model adapted from the BERT architecture.
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S3.F5" title="Figure 5 ‣ III-B Transformers for Protein Function Prediction ‣ III Transformers in Protein ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">5</span></a> illustrates the architecture of the original BERT model, which ProteinBERT inherits and adapts for protein sequences.
ProteinBERT is pre-trained on large-scale protein sequence datasets and learns to capture contextual dependencies between amino acids. When fine-tuned for specific tasks such as enzyme classification, ligand binding prediction, or subcellular localization, ProteinBERT achieves state-of-the-art performance.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="685" id="S3.F5.g1" src="x7.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F5.2.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S3.F5.3.2" style="font-size:90%;">
The structure of the BERT model.
This figure shows the Bidirectional Encoder Representations from Transformers (BERT) architecture,
highlighting both pre-training and fine-tuning phases.
</span></figcaption>
</figure>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">A key strength of ProteinBERT lies in its ability to encode rich semantic representations of protein sequences. By learning position-aware, context-sensitive embeddings, it can identify conserved motifs and functionally relevant regions even in remote homologs. This enables it to predict functions for novel or poorly annotated proteins that traditional alignment-based tools often miss.</p>
</div>
<div class="ltx_para" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">2.ProtTrans.</span> Building on this foundation, ProtTrans <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib55" title="">55</a>]</cite> pushes the boundary further by leveraging larger model architectures and broader training corpora, including billions of protein sequences. ProtTrans is capable of predicting diverse protein properties such as stability, binding affinity, and molecular function. By integrating sequence-based information with evolutionary context, it provides robust predictions even for proteins with limited annotation. This makes ProtTrans a powerful tool for large-scale protein annotation in genomic studies.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">Unlike earlier models, ProtTrans comprises a collection of Transformer architectures, including BERT, XLNet, Albert, and T5—each trained on massive datasets such as UniRef100 and BFD, encompassing billions of protein sequences. These models are trained using self-supervised learning objectives, particularly masked language modeling and sequence-to-sequence prediction. This enables them to learn contextual and evolutionary patterns in protein sequences without requiring labeled data.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<p class="ltx_p" id="S3.SS2.p6.1">The key innovation in ProtTrans lies in its ability to generalize across diverse protein families by capturing both local motifs and global structural signals embedded in the sequence. The use of multiple model types further enhances its flexibility across tasks, as each architecture offers unique advantages in handling long-range dependencies, directional context, or compression efficiency. Additionally, ProtTrans exhibits strong transfer learning capabilities, performing well on downstream tasks even with limited training examples, which is particularly valuable in domains with sparse functional annotations.</p>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.1">3. In recent years, Transformer-based models have also shown considerable potential in the domain of protein function analysis, particularly in identifying biologically important regions such as binding sites.One notable example is <span class="ltx_text ltx_font_bold" id="S3.SS2.p7.1.1">Deep-ProBind</span>(2025), a hybrid model designed to predict protein-binding peptides by integrating both sequence-derived and structural features<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib79" title="">79</a>]</cite>.This approach utilizes Bidirectional Encoder Representations from Transformers (BERT) alongside a Pseudo Position-Specific Scoring Matrix transformed via Discrete Wavelet Transform (PsePSSM-DWT), enabling the extraction of complex sequence patterns and evolutionary characteristics.A key contribution of Deep-ProBind is the incorporation of the SHapley Additive exPlanations (SHAP) algorithm, which selects informative hybrid features prior to classification using a deep neural network.In extensive benchmark testing, the model achieved an accuracy of 92.67% using tenfold cross-validation, and 93.62% on independent datasets, outperforming traditional machine learning methods and other existing models by a notable margin.Although Deep-ProBind is not a structural prediction tool per se, its strategic use of structural descriptors underscores the broader utility of transformer-based models in understanding protein behavior and interactions, which is critical for pharmacological applications and peptide drug discovery.</p>
</div>
<div class="ltx_para" id="S3.SS2.p8">
<p class="ltx_p" id="S3.SS2.p8.1">4. In recent years, growing attention has been directed toward understanding the functional roles of intrinsically disordered regions (IDRs) in proteins, especially their short interaction-prone segments known as molecular recognition features (MoRFs). Accurately identifying MoRFs remains a computational challenge due to their disorder-to-order transitions and the limited availability of experimentally validated annotations. To overcome these limitations, a recent study proposed <span class="ltx_text ltx_font_bold" id="S3.SS2.p8.1.1">Trans-MoRFs</span> (2025), a novel predictor based on the transformer architecture<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib64" title="">64</a>]</cite>. Trans-MoRFs leverage self-attention mechanisms to effectively capture long-range dependencies within protein sequences, making them particularly robust across proteins of varying lengths. The model demonstrated strong predictive performance, achieving a mean area under the curve (AUC) of 0.94 on multiple benchmarks, and significantly outperformed existing MoRF predictors across several evaluation metrics. This work exemplifies the expanding role of transformer models in structural and functional protein annotation, extending their applicability beyond structured domains to disordered and dynamic regions, thereby enhancing our understanding of protein function and supporting drug target discovery.</p>
</div>
<div class="ltx_para" id="S3.SS2.p9">
<p class="ltx_p" id="S3.SS2.p9.1">5. Recent advances in protein function prediction have begun to emphasize not only predictive performance but also interpretability, particularly when using complex models such as transformers. A notable example is the study by Wenzel et al.<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib80" title="">80</a>]</cite>(2024), which investigates the internal mechanisms of transformer models fine-tuned for protein function tasks, including Gene Ontology (GO) term and Enzyme Commission (EC) number prediction. The authors extended the widely used explainable AI (XAI) method, Integrated Gradients, to analyze not only input attributions but also latent representations within transformer architectures. Their approach allowed for the identification of amino acid residues that the model attends to most strongly, revealing biologically meaningful patterns. Specifically, they demonstrated that certain transformer heads consistently correspond to known functional regions, such as transmembrane domains and enzymatic active sites. This work highlights the growing potential of interpretable transformer models in uncovering biologically relevant insights from sequence data, thereby enhancing both the trustworthiness and utility of deep learning in protein function annotation.</p>
</div>
<div class="ltx_para" id="S3.SS2.p10">
<p class="ltx_p" id="S3.SS2.p10.1">6. Traditional graph-based methods may struggle to represent these multi-hop relationships, limiting the extraction of deeper functional signals. To address this, recent advances have explored the integration of graph serialization with Transformer-based architectures.</p>
</div>
<div class="ltx_para" id="S3.SS2.p11">
<p class="ltx_p" id="S3.SS2.p11.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p11.1.1">SEGT-GO</span>(2025)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib81" title="">81</a>]</cite> introduces a novel framework that transforms multi-hop neighborhoods within PPI networks into serialized representations, allowing a Graph Transformer to learn latent functional dependencies across the interaction space. By converting local and global network topologies into sequences of embeddings, the model capitalizes on the attention mechanism’s ability to model non-local relationships. In addition, the framework incorporates SHAP-based explainable AI techniques, leveraging game-theoretic principles to refine input relevance and reduce noise, thereby enhancing interpretability and robustness.</p>
</div>
<div class="ltx_para" id="S3.SS2.p12">
<p class="ltx_p" id="S3.SS2.p12.1">Experimental evaluations across both large-scale and limited-size datasets demonstrate SEGT-GO’s superiority over previous state-of-the-art models such as DeepGraphGO. Notably, the method shows strong generalization capabilities in cross-species scenarios and when predicting functions of previously unseen proteins, highlighting the potential of combining Transformer-based graph modeling with explainable AI in scalable protein function analysis.</p>
</div>
<div class="ltx_para" id="S3.SS2.p13">
<p class="ltx_p" id="S3.SS2.p13.1">7. In recent years, Transformer-based approaches have increasingly demonstrated their utility in automating complex tasks within protein function prediction, a domain traditionally hindered by the high cost and low scalability of experimental methods. One recent contribution is <span class="ltx_text ltx_font_bold" id="S3.SS2.p13.1.1">MAGO</span> (2025)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib82" title="">82</a>]</cite>, a model that combines the representational power of Transformer architectures with the optimization capabilities of Automated Machine Learning (AutoML) to improve large-scale protein annotation. Unlike earlier machine learning pipelines, MAGO autonomously selects and tunes model configurations while leveraging contextual embeddings extracted from raw protein sequences. Furthermore, the enhanced variant MAGO+ incorporates evolutionary signals via integration with BLASTp, effectively unifying deep learning-based feature learning and alignment-based similarity. Benchmark evaluations reveal that both MAGO and MAGO+ surpass several state-of-the-art approaches in Fmax and other performance metrics, with improvements found to be statistically significant. These results not only reaffirm the value of attention mechanisms for capturing functional patterns in biological sequences but also illustrate how AutoML can systematically refine deep learning frameworks for robust cross-protein generalization, paving the way for more adaptive and interpretable solutions in computational proteomics.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Transformers for Protein-Protein Interactions (PPIs)</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Protein-protein interactions (PPIs) are fundamental to nearly all cellular activities, including signal transduction, gene regulation, and immune response. Accurately predicting PPIs is essential for drug discovery, understanding disease mechanisms, and identifying therapeutic targets. While experimental methods such as yeast two-hybrid screening and co-immunoprecipitation are widely used, they are resource-intensive, time-consuming, and limited in scalability.</p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1">To overcome these limitations, deep learning, especially Transformer-based models, has been increasingly applied to computational PPI prediction. These models leverage sequence and structural information to identify interaction patterns and functional relationships between proteins.</p>
</div>
<div class="ltx_para" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">1.DeepPPI: Sequence-based attention for interaction prediction</span>
DeepPPI<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib83" title="">83</a>]</cite> is a deep neural network model that predicts protein interactions based solely on amino acid sequences. It integrates an attention mechanism that enables the model to focus on biologically relevant sequence motifs and potential interaction regions. This targeted attention improves prediction accuracy, particularly in cases where structural information is unavailable. One of the key advantages of DeepPPI is its ability to generalize across proteins with low sequence similarity, making it robust for large-scale proteomic analyses. However, its reliance on primary sequence data alone may limit performance when structural features play a dominant role in mediating interactions.</p>
</div>
<div class="ltx_para" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">2.TAPE: Multi-task Transformer framework</span>
The TAPE (Task-Aware Protein Embedding) framework<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib61" title="">61</a>]</cite> builds on a Transformer-based architecture with a multi-task learning setup. It is pretrained on over 30 million protein sequences, learning generalized embeddings that can be fine-tuned for specific tasks, including PPI prediction. TAPE’s strength lies in its ability to transfer knowledge across tasks, significantly reducing the need for large labeled datasets in downstream applications. The model captures complex sequence patterns and functional relationships, which enhances performance in diverse biological prediction tasks. Nonetheless, without explicit incorporation of structural data, TAPE may struggle to model spatial constraints that are critical for certain PPIs.</p>
</div>
<div class="ltx_para" id="S3.SS3.p5">
<p class="ltx_p" id="S3.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p5.1.1">3.Graph Attention Networks (GATs): Structure-aware interaction modeling</span>
Graph-based approaches like Graph Attention Networks (GATs)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib84" title="">84</a>]</cite> treat proteins as graphs, where amino acids are nodes and edges represent spatial or functional relationships. The attention mechanism in GATs assigns different importance weights to neighboring nodes, enabling the model to emphasize critical regions such as binding pockets or conserved structural motifs. This structure-aware modeling makes GATs particularly effective in capturing 3D context and non-linear interaction pathways, which are essential for accurately predicting interactions in complex molecular environments. However, GATs often require high-quality structural input, and the computational cost associated with large protein graphs can be substantial.</p>
</div>
<div class="ltx_para" id="S3.SS3.p6">
<p class="ltx_p" id="S3.SS3.p6.1">4. Recent efforts in protein–protein interaction site (PPIS) prediction have explored hybrid models that combine structural and sequential features for enhanced accuracy. A notable contribution in this direction is the <span class="ltx_text ltx_font_bold" id="S3.SS3.p6.1.1">GACT-PPIS</span>(2024) model, which integrates an enhanced graph attention mechanism with deep transformer networks to predict interaction sites on protein surfaces<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib85" title="">85</a>]</cite>. Specifically, the model utilizes an Enhanced Graph Attention Network (EGAT) enriched with residual and identity mappings, in conjunction with a transformer encoder to capture long-range dependencies in amino acid sequences. Graph Convolutional Networks (GCNs) are also employed to aggregate local neighborhood information from the protein’s structural graph. This multi-level fusion of spatial and sequential data enables GACT-PPIS to outperform several state-of-the-art models across multiple benchmark datasets, including Test-60 and UBTest-31-6, showing superior performance in terms of Recall, F1-score, AUROC, AUPRC, and overall generalization. The success of GACT-PPIS highlights the value of combining graph-based and transformer-based representations for precise residue-level interaction prediction.</p>
</div>
<div class="ltx_para" id="S3.SS3.p7">
<p class="ltx_p" id="S3.SS3.p7.1">5. Recent advancements have also explored the application of Transformer-based models in predicting protein–protein interaction (PPI) binding sites, a critical aspect of understanding cellular mechanisms and therapeutic target identification. A representative study introduced <span class="ltx_text ltx_font_bold" id="S3.SS3.p7.1.1">TranP-B-site</span> (2025), a model that utilizes Transformer-generated embeddings to encode amino acid sequence information, combined with convolutional neural networks (CNNs) to detect PPI binding sites<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib86" title="">86</a>]</cite>. The model extracts two types of features for each residue—one-hot encodings for local patterns and Transformer-based embeddings for global context—and processes them using a hybrid architecture. Specifically, a windowing strategy is employed to capture localized information, which is then separately processed by CNN and fully connected layers before final classification. This dual-pathway design enables the model to integrate both fine-grained and holistic representations effectively. TranP-B-site demonstrated notable improvements over previous state-of-the-art sequence-based PPI models, achieving significant gains in accuracy and Matthews Correlation Coefficient (MCC) on independent test datasets. Furthermore, the model exhibited robust performance on a newly curated dataset derived from the PDB, indicating its generalization capability across diverse protein sequences. This work underscores the utility of Transformer embeddings in enhancing predictive power for interaction site identification and highlights a promising direction for future PPI modeling.</p>
</div>
<div class="ltx_para" id="S3.SS3.p8">
<p class="ltx_p" id="S3.SS3.p8.1">6. Uncertainty estimation has recently become a critical factor in enhancing the reliability of protein–protein interaction (PPI) predictions using Transformer-based models. One recent contribution in this area is the <span class="ltx_text ltx_font_bold" id="S3.SS3.p8.1.1">TUnA</span> framework(2024), which integrates uncertainty modeling into sequence-based PPI prediction<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib87" title="">87</a>]</cite>. TUnA leverages Transformer encoders in combination with ESM-2 protein embeddings to extract informative representations of amino acid sequences. Notably, it incorporates a Spectral-normalized Neural Gaussian Process to quantify prediction uncertainty, enabling the model not only to make accurate predictions but also to estimate the confidence level of its outputs. This is particularly valuable for applications involving previously unseen proteins, where conventional models often struggle to generalize. TUnA has demonstrated state-of-the-art performance on benchmark datasets and showed that its uncertainty-aware predictions could effectively filter out unreliable outputs, thereby reducing false positives and narrowing the gap between in silico predictions and experimental validation. This approach illustrates a promising direction in building interpretable and trustworthy Transformer-based frameworks for PPI prediction.</p>
</div>
<div class="ltx_para" id="S3.SS3.p9">
<p class="ltx_p" id="S3.SS3.p9.1">7. Recent research has also explored the use of Transformer architectures for predicting protein-protein binding affinity (BA), a key aspect of understanding molecular interactions. One study conducted a comparative analysis of convolutional neural networks (CNNs) and Transformer-based models to evaluate their effectiveness in predicting binding affinity using only protein sequence data(2024)<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib88" title="">88</a>]</cite>. The models were designed to output Gibbs free energy changes, which serve as a quantitative measure of interaction strength between protein pairs.</p>
</div>
<div class="ltx_para" id="S3.SS3.p10">
<p class="ltx_p" id="S3.SS3.p10.1">In this work, multiple model variants were developed using both TensorFlow and PyTorch, including a Transformer-based model built upon ProteinBERT. The CNN architecture was effective at capturing local sequence features, while the Transformer model leveraged self-attention mechanisms to learn long-range dependencies within protein sequences. Different sequence encoding techniques were used, including one-hot encoding, sequence-statistics-content, and position-specific scoring matrices.</p>
</div>
<div class="ltx_para" id="S3.SS3.p11">
<p class="ltx_p" id="S3.SS3.p11.1">The results demonstrated that both architectures could achieve comparable predictive accuracy, although they presented distinct trade-offs. The CNN model could process full-length sequences but required significantly more preprocessing time. In contrast, the Transformer model maintained competitive accuracy with less computational overhead, though it excluded very long sequences due to input length limitations. This study underscores the growing potential of Transformer-based models in capturing complex sequence-level features for protein-protein interaction prediction and binding affinity estimation.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS4.5.1.1">III-D</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS4.6.2">Transformers for Drug Discovery and Target Identification</span>
</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1">Transformer-based models have recently revolutionized drug discovery by enabling more accurate virtual screening, predicting protein-ligand binding affinities, and supporting de novo drug design. Their ability to learn from large-scale chemical and biological datasets allows for efficient prediction of interactions between small molecules and protein targets, significantly reducing the cost and time associated with traditional experimental methods.</p>
</div>
<div class="ltx_para" id="S3.SS4.p2">
<p class="ltx_p" id="S3.SS4.p2.1">In recent years, Transformer-based models have rapidly gained prominence in the domain of drug discovery, particularly for their ability to handle complex and heterogeneous data. A comprehensive review by Jian Jiang et al. (2024) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib89" title="">89</a>]</cite>emphasizes the transformative role of Transformers in various aspects of the drug discovery pipeline, including drug target identification, molecular design, and property prediction. The authors highlight that the hierarchical attention mechanisms inherent to Transformer architectures enable them to effectively model sequential dependencies in biological and chemical data. These models have shown strong adaptability when pre-trained on large-scale datasets, allowing transferability across tasks such as virtual screening, lead optimization, and even protein engineering. Notably, their interdisciplinary application across biology, chemistry, and pharmacology facilitates an integrative approach to discovery, bridging the gaps between domains. The review also outlines emerging directions, including the use of Transformers for single-cell data analysis, chemical language modeling, and biological image interpretation, underscoring their growing influence in drug discovery and biomedical research more broadly.</p>
</div>
<div class="ltx_para" id="S3.SS4.p3">
<p class="ltx_p" id="S3.SS4.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p3.1.1">1.ChemBERTa</span> is a prominent model in this area, fine-tuned on protein-ligand interaction datasets. Leveraging a BERT-style architecture, it captures complex patterns in chemical structures and their binding behaviors. ChemBERTa has demonstrated superior performance compared to traditional methods like molecular docking and molecular dynamics simulations in predicting binding affinities. This enables faster and more cost-effective early-stage drug screening by reducing reliance on time-consuming lab experiments <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib65" title="">65</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p4">
<p class="ltx_p" id="S3.SS4.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p4.1.1">2.MolBERT</span> utilizes Transformer models to learn molecular representations from SMILES (Simplified Molecular Input Line Entry System) strings. Pretrained on large-scale molecular datasets, it can predict not only binding affinity with protein targets but also toxicity and efficacy profiles of candidate compounds. This enables MolBERT to serve as a versatile tool throughout the drug development pipeline, supporting both safety assessment and lead optimization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib90" title="">90</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p5">
<p class="ltx_p" id="S3.SS4.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p5.1.1">3.De novo Drug Design with Graph Transformers:</span> In addition to these models, Transformer-based approaches have been used for de novo drug design, an innovative method for generating entirely new molecules with specific binding properties. Graph-based models combined with Transformer architectures enable the generation of novel molecular structures by predicting optimal atom-to-atom connections for desired binding properties. This approach not only speeds up the drug design process but also allows for a more efficient exploration of chemical space compared to traditional drug design methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib91" title="">91</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p6">
<p class="ltx_p" id="S3.SS4.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p6.1.1">4.MolFormer</span> represents a next-generation Transformer model aimed at understanding the three-dimensional (3D) conformations of drug-like molecules. It predicts how small molecules adopt specific geometries within protein binding sites, aiding in the rational design of compounds with favorable spatial orientation and interaction properties. MolFormer is particularly valuable in refining molecular structures during the early stages of lead optimization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib92" title="">92</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p7">
<p class="ltx_p" id="S3.SS4.p7.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p7.1.1">5.DeepChem</span> is a machine learning platform that integrates various deep models, including Transformer-based architectures, for comprehensive drug discovery tasks. It supports the prediction of multiple molecular properties such as solubility, toxicity, and binding affinity. By combining rich chemical representations with advanced learning techniques, DeepChem enables rapid prioritization of promising drug candidates<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib66" title="">66</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS4.p8">
<p class="ltx_p" id="S3.SS4.p8.1"><span class="ltx_text ltx_font_bold" id="S3.SS4.p8.1.1">6.RL-based Transformer Models for Molecule Generation:</span> Reinforcement Learning-based models combined with Transformers have been applied to drug design, where a model learns the optimal drug molecule design through iterative feedback. These models leverage reinforcement learning strategies to iteratively generate new molecules that not only bind effectively to the target protein but also optimize other properties such as solubility and toxicity. By using a reward-based system, these models can effectively explore the vast chemical space and suggest novel molecules for drug development.</p>
</div>
<div class="ltx_para" id="S3.SS4.p9">
<p class="ltx_p" id="S3.SS4.p9.1">7. Recent advancements in drug discovery have utilized transformer-based models for enhanced molecular property prediction. A notable approach is the BERT-based model for HIV replication inhibition, which fine-tunes the transformer architecture on the MoleculeNet-HIV dataset represented by SMILES notation. By leveraging BERT’s ability to capture intricate molecular patterns, the model achieves high prediction accuracy and strong generalization capabilities<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib93" title="">93</a>]</cite>(2024). This method demonstrates significant potential for accelerating drug discovery, reducing both time and cost, and offering a promising solution for more efficient target identification and molecular design.</p>
</div>
<div class="ltx_para" id="S3.SS4.p10">
<p class="ltx_p" id="S3.SS4.p10.1">8. Recent advances in Transformer-based generative models have opened new avenues for de novo drug design, especially when integrated with optimization strategies targeting multiple pharmaceutical objectives. One notable approach proposes a comprehensive framework that combines latent Transformer architectures with a many-objective optimization pipeline<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib94" title="">94</a>]</cite>(2024). This system jointly considers key pharmacokinetic and pharmacodynamic criteria—such as absorption, distribution, metabolism, excretion, toxicity (ADMET), and molecular docking—within a multi-objective design strategy.</p>
</div>
<div class="ltx_para" id="S3.SS4.p11">
<p class="ltx_p" id="S3.SS4.p11.1">In particular, two latent Transformer-based models, ReLSO and FragNet, were evaluated for their ability to encode and reconstruct molecular structures. ReLSO demonstrated superior performance, offering more coherent latent space organization and reconstruction accuracy. Building upon this, the framework incorporated six different many-objective metaheuristic algorithms—including those based on evolutionary principles and particle swarm optimization—to explore candidate compounds targeting human lysophosphatidic acid receptor 1 (LPA1), a protein implicated in cancer.</p>
</div>
<div class="ltx_para" id="S3.SS4.p12">
<p class="ltx_p" id="S3.SS4.p12.1">Among the tested strategies, a decomposition-based evolutionary algorithm yielded the most favorable balance across multiple drug design objectives, achieving high binding affinity, low toxicity, and strong drug-likeness. This work exemplifies the synergy between Transformer-driven molecular generation and computational intelligence methods for multi-criteria optimization, highlighting a scalable path toward more robust and realistic drug candidate discovery.</p>
</div>
<div class="ltx_para" id="S3.SS4.p13">
<p class="ltx_p" id="S3.SS4.p13.1">9. The emergence of the Transformer architecture has profoundly influenced computational chemistry, enabling a paradigm shift in how molecular data is interpreted, modeled, and utilized in drug discovery. Inspired by parallels between chemical notation and natural language, researchers have successfully applied language modeling techniques to challenges such as retrosynthetic analysis, molecular generation, and exploration of vast chemical spaces. Initial approaches focused on task-specific models trained on linear molecular representations (e.g., SMILES), but recent developments have expanded these capabilities to incorporate diverse modalities—including spectroscopic data, synthetic pathways, and even human-readable language inputs.</p>
</div>
<div class="ltx_para" id="S3.SS4.p14">
<p class="ltx_p" id="S3.SS4.p14.1">This evolution has culminated in the application of large language models (LLMs), which offer a unified framework for solving a broad range of chemistry-related problems. By leveraging the flexibility and expressiveness of natural language, LLMs are increasingly being used to integrate multi-source data and generalize across chemical tasks. These advancements point toward a future where machine learning models serve as adaptive and intelligent agents in the drug discovery pipeline, capable of synthesizing knowledge across domains and accelerating decision-making at every stage <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib95" title="">95</a>]</cite>(2024).</p>
</div>
<div class="ltx_para" id="S3.SS4.p15">
<p class="ltx_p" id="S3.SS4.p15.1">These advancements demonstrate the power of Transformer-based models in revolutionizing drug discovery by improving the prediction of protein-ligand interactions, enabling de novo design of novel drug candidates, and refining the properties of existing molecules. With their ability to process large-scale datasets and predict complex molecular behaviors, these models are accelerating the development of new therapeutics, bringing us closer to more effective treatments in less time.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Advantages &amp; Challenges of Transformer Models</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Advantages of Transformers in Protein Informatics</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">Transformer models have profoundly reshaped the landscape of protein informatics, offering a set of compelling advantages over traditional computational approaches.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p2.1.1">1.Capturing Long-Range Dependencies:</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">The self-attention mechanism of Transformers enables them to model long-range interactions in protein sequences, which are crucial for tasks such as tertiary and quaternary structure prediction. Unlike recurrent neural networks (RNNs), Transformers analyze all sequence elements simultaneously, efficiently capturing context from distant residues<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib96" title="">96</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p4.1.1">2.Scalability and Pretraining Benefits:</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Transformer models, especially in their pre-trained forms (e.g., ProtTrans, ESM), are scalable to massive datasets, providing a foundation for transfer learning across various protein tasks. This is advantageous when computational resources are limited or when training data for specific protein tasks are scarce <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib97" title="">97</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib98" title="">98</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p6.1.1">3.Parallel Computation and Efficiency:</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">Transformers allow for parallelized computation during training and inference due to their non-recurrent architecture, significantly reducing training time. This scalability enables their use in high-throughput tasks like structural screening and synthetic protein design.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p8.1.1">4.Multimodal Integration:</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.1">Advanced Transformer derivatives like AlphaFold2 and ESM-3 can integrate multiple types of biological information, including evolutionary profiles, structural constraints, and biochemical features, enabling more holistic and accurate predictions.</p>
</div>
<div class="ltx_para" id="S4.SS1.p10">
<p class="ltx_p" id="S4.SS1.p10.1"><span class="ltx_text ltx_font_bold" id="S4.SS1.p10.1.1">5.Generalizability Across Diverse Protein Families:</span></p>
</div>
<div class="ltx_para" id="S4.SS1.p11">
<p class="ltx_p" id="S4.SS1.p11.1">Transformers trained on large, diverse protein datasets are capable of generalizing to unseen sequences, including those from under-characterized or evolutionarily distant organisms. This makes them especially valuable for functional annotation in metagenomics and novel protein discovery<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib99" title="">99</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Challenges of Applying Transformers in Protein Research</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Despite their advantages, Transformer models face several persistent challenges that limit their broader adoption and utility.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p2.1.1">1.Computational Resource Intensity:</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">Scalability Issues: Transformer models are inherently resource intensive due to their quadratic complexity with respect to the sequence length in the self-attention mechanism. For protein sequences, which can be thousands of amino acids long, this computational demand becomes a significant bottleneck <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib92" title="">92</a>]</cite>. Training models like AlphaFold2 require high-performance computing clusters with specialized hardware such as GPUs or TPUs, making it inaccessible to many researchers and institutions with limited resources.</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">Memory Consumption: The large number of parameters in these models leads to substantial memory consumption. For example, models such as ProtT5-XL-UniRef50 have more than 3 billion parameters, which requires advanced memory management techniques and hardware with large memory capacities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib100" title="">100</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Inference Time: The time required for inference can also be prohibitive, especially when processing large datasets or performing real-time analysis. This limitation affects the practical deployment of Transformer models in applications such as high-throughput drug screening, where speed is crucial <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib101" title="">101</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">Energy Consumption and Environmental Impact: The extensive computational resources required not only increase costs but also have environmental implications due to high energy consumption, raising concerns about the sustainability of using such models at scale <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib102" title="">102</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p7">
<p class="ltx_p" id="S4.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p7.1.1">2.Data Availability and Quality:</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p8">
<p class="ltx_p" id="S4.SS2.p8.1">Incomplete and Biased Datasets: Protein databases such as UniProt and PDB, although extensive, are biased toward certain organisms (e.g., model organisms such as humans and E. coli) and well-studied proteins. This bias leads to models that perform well on familiar protein families but poorly on underrepresented ones <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib103" title="">103</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p9">
<p class="ltx_p" id="S4.SS2.p9.1">Lack of annotations: Many proteins lack functional annotations or experimental validation, which limits supervised modeling of models for tasks such as function prediction or PPI analysis. The scarcity of high-quality labeled data hampers the ability of models to learn accurate representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib104" title="">104</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p10">
<p class="ltx_p" id="S4.SS2.p10.1">Noisy data: Experimental errors, inconsistent labeling, and outdated annotations introduce noise into the datasets, which can mislead the training process and degrade model performance. Cleaning and curating such large datasets is a non-trivial task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib92" title="">92</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p11">
<p class="ltx_p" id="S4.SS2.p11.1">Limited Structural Data: Although AlphaFold has predicted structures for many proteins,experimentally determined structures remain limited. Structural data is crucial for training models on tasks that require 3D information, such as predicting protein-ligand interactions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib105" title="">105</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p12">
<p class="ltx_p" id="S4.SS2.p12.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p12.1.1">3.Model Interpretability:</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p13">
<p class="ltx_p" id="S4.SS2.p13.1">Black-Box Nature: Transformer models are often criticized for being black boxes, providing little insight into how input features contribute to the output predictions. This opacity is problematic in biological contexts where understanding the reasoning behind a prediction is as important as the prediction itself <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib106" title="">106</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p14">
<p class="ltx_p" id="S4.SS2.p14.1">Regulatory Concerns: In clinical applications, regulatory bodies require explanations for decisions made by computational models, especially when they impact patient care. The lack of interpretability in Transformer models poses a barrier to their acceptance in such settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib107" title="">107</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p15">
<p class="ltx_p" id="S4.SS2.p15.1">Biological Insight: Without interpretability, it is challenging to derive new biological knowledge from models. Understanding which parts of the protein sequence or structure are most influential in a prediction could lead to discoveries of new functional motifs or interaction sites <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib108" title="">108</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p16">
<p class="ltx_p" id="S4.SS2.p16.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p16.1.1">4.Generalization Across Diverse Protein Families:</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p17">
<p class="ltx_p" id="S4.SS2.p17.1">Overfitting to Training Data: Transformer models may be overfitting to the protein families represented in the training data, failing to generalize to novel or rare proteins. This limitation reduces their utility in exploring uncharacterized proteins or those from non-model organisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib104" title="">104</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p18">
<p class="ltx_p" id="S4.SS2.p18.1">Difficulty with Novel Folds: Proteins with novel folds not seen during training pose a significant challenge. The models may not accurately predict the structures or functions of these proteins due to a lack of prior examples, limiting their applicability in discovering new protein classes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib109" title="">109</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p19">
<p class="ltx_p" id="S4.SS2.p19.1">Sequence-length variation: Proteins exhibit a wide range of sequence lengths. Transformer models may struggle with extremely long sequences due to computational constraints, or with very short sequences where context is minimal, affecting their performance across the proteome <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib92" title="">92</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p20">
<p class="ltx_p" id="S4.SS2.p20.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p20.1.1">5.Multi-modal Integration</span></p>
</div>
<div class="ltx_para" id="S4.SS2.p21">
<p class="ltx_p" id="S4.SS2.p21.1">Complexity of biological data: Incorporating diverse data types, such as genomic, transcriptomic, proteomic, and metabolomic data, is inherently complex. Each data type has different characteristics and noise profiles, which complicates the development of models that can effectively integrate them <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib110" title="">110</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p22">
<p class="ltx_p" id="S4.SS2.p22.1">Lack of Unified Frameworks:There is a shortage of modeling frameworks that can seamlessly combine sequence data with structural and functional information. Existing models are often specialized for a single data modality, limiting their ability to capture the full spectrum of protein biology <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib111" title="">111</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p23">
<p class="ltx_p" id="S4.SS2.p23.1">Data Alignment and Synchronization:Aligning data from different sources and ensuring they correspond accurately to the same proteins or biological contexts is challenging. Misalignment can lead to erroneous conclusions and degrade model performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib112" title="">112</a>]</cite>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">FUTURE DIRECTIONS</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">The application of Transformer models in protein informatics has yielded remarkable advancements; however, several promising research avenues remain to be explored. These directions are essential for overcoming current limitations and unlocking the full potential of Transformers in biological and biomedical applications.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">1.Multi-modal Data Integration</span></p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">One of the most promising directions is the integration of multimodal data into transformer models to enhance the accuracy of quaternary structure prediction. By combining data from various biological sources, including sequence information, structural data, evolutionary patterns, and biochemical properties, models could capture a more comprehensive picture of protein-protein interactions and complex assembly. Specifically:</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">1). Structural and Functional Annotations: Incorporating data from cryo-electron microscopy (cryo-EM), X-ray crystallography, and NMR spectroscopy could help Transformer models learn spatial relationships within complexes with high precision. Recent advancements in structural databases, such as AlphaFold’s structural proteome predictions, offer vast amounts of annotated 3D structure data that could be leveraged to train Transformers with 3D spatial awareness, improving quaternary structure predictions significantly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib113" title="">113</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p5">
<p class="ltx_p" id="S5.p5.1">2). Evolutionary and Co-evolutionary Data: Protein complexes often involve residues that have evolved to co-interact. By integrating evolutionary data from multiple sequence alignments (MSAs) with co-evolutionary metrics, Transformers can learn to detect conserved residues critical to protein interfaces. Models like ESM and AlphaFold have already begun using evolutionary signals in structure prediction; expanding this to complex-level predictions could help detect intricate patterns that dictate multimeric assembly <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib114" title="">114</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib115" title="">115</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p6">
<p class="ltx_p" id="S5.p6.1">3). Biophysical Properties: Including data on biophysical properties, such as binding affinity, hydrophobicity, and electrostatic interactions, could allow models to differentiate between transient and stable complexes. Such detailed information would enable Transformers to predict not only whether two proteins interact but also the strength and nature of these interactions, which is crucial for understanding complex stability and functionality.</p>
</div>
<div class="ltx_para" id="S5.p7">
<p class="ltx_p" id="S5.p7.1"><span class="ltx_text ltx_font_bold" id="S5.p7.1.1">2.Hybrid Modeling Approaches</span></p>
</div>
<div class="ltx_para" id="S5.p8">
<p class="ltx_p" id="S5.p8.1">Hybrid modeling approaches, combining deep learning with classical structural biology techniques, offer another compelling direction for quaternary structure prediction. Hybrid methods can provide more accurate and computationally efficient predictions by leveraging the strengths of different modeling techniques:</p>
</div>
<div class="ltx_para" id="S5.p9">
<p class="ltx_p" id="S5.p9.1">1). Combining Transformers with Molecular Dynamics (MD) Simulations: While Transformer models are proficient at predicting static quaternary structures, MD simulations capture the dynamic behavior of protein interactions. A hybrid approach where Transformers predict initial configurations, followed by refinement through MD, could yield both accurate and biologically relevant representations of protein complexes in motion <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib116" title="">116</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib117" title="">117</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p10">
<p class="ltx_p" id="S5.p10.1">2). Integrating Deep Learning with Physics-Based Models: Physics-based methods, such as molecular mechanics, are highly accurate for modeling atomic interactions within protein complexes. Combining these methods with Transformer models would allow initial coarse-grained predictions that can be refined with physics-based details, thus balancing computational efficiency with high-resolution accuracy. This approach could be particularly useful for large complexes, such as viral capsids, where fine-grained modeling is computationally prohibitive.</p>
</div>
<div class="ltx_para" id="S5.p11">
<p class="ltx_p" id="S5.p11.1">3). Leveraging Experimental Data for Model Validation: Incorporating experimentally derived data, such as distance constraints from FRET or cross-linking mass spectrometry, can validate Transformer predictions and correct model outputs based on real-world data. By grounding Transformer predictions in experimental results, hybrid approaches could achieve high confidence and reliability in complex structure predictions.</p>
</div>
<figure class="ltx_table" id="S5.T2">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T2.2.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S5.T2.3.2" style="font-size:90%;">Published Algorithms</span></figcaption>
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T2.4.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.2.1">Time</span></th>
<th class="ltx_td ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T2.4.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.1.1.3.1">
<span class="ltx_p" id="S5.T2.4.1.1.3.1.1"><span class="ltx_text ltx_font_bold" id="S5.T2.4.1.1.3.1.1.1">Repository URL</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T2.4.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.2.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">AlphaFold</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T2.4.2.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2021</td>
<td class="ltx_td ltx_align_justify ltx_border_t" id="S5.T2.4.2.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.2.1.3.1">
<span class="ltx_p" id="S5.T2.4.2.1.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/deepmind/alphafold" title="">https://github.com/deepmind/alphafold</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.3.2">
<td class="ltx_td ltx_align_left" id="S5.T2.4.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">RoseTTAFold</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2021</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.3.2.3.1">
<span class="ltx_p" id="S5.T2.4.3.2.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/RosettaCommons/RoseTTAFold" title="">https://github.com/RosettaCommons/RoseTTAFold</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.4.3">
<td class="ltx_td ltx_align_left" id="S5.T2.4.4.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ESM-Fold</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.4.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2022</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.4.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.4.3.3.1">
<span class="ltx_p" id="S5.T2.4.4.3.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/facebookresearch/esm" title="">https://github.com/facebookresearch/esm</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.5.4">
<td class="ltx_td ltx_align_left" id="S5.T2.4.5.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">OmegaFold</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.5.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2022</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.5.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.5.4.3.1">
<span class="ltx_p" id="S5.T2.4.5.4.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/HeliXonProtein/OmegaFold" title="">https://github.com/HeliXonProtein/OmegaFold</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.6.5">
<td class="ltx_td ltx_align_left" id="S5.T2.4.6.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ProtGPT2</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.6.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2022</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.6.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.6.5.3.1">
<span class="ltx_p" id="S5.T2.4.6.5.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/docs/transformers/main_classes/trainer" title="">https://huggingface.co/docs/transformers/main_classes/trainer</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.7.6">
<td class="ltx_td ltx_align_left" id="S5.T2.4.7.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">RFDiffusion</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.7.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2023</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.7.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.7.6.3.1">
<span class="ltx_p" id="S5.T2.4.7.6.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/RosettaCommons/RFdiffusion" title="">https://github.com/RosettaCommons/RFdiffusion</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.8.7">
<td class="ltx_td ltx_align_left" id="S5.T2.4.8.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ProteinBERT</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.8.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2022</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.8.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.8.7.3.1">
<span class="ltx_p" id="S5.T2.4.8.7.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/nadavbra/protein_bert" title="">https://github.com/nadavbra/protein_bert</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.9.8">
<td class="ltx_td ltx_align_left" id="S5.T2.4.9.8.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ProtTrans</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.9.8.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2021</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.9.8.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.9.8.3.1">
<span class="ltx_p" id="S5.T2.4.9.8.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/agemagician/ProtTrans" title="">https://github.com/agemagician/ProtTrans</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.10.9">
<td class="ltx_td ltx_align_left" id="S5.T2.4.10.9.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">DeepPPI</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.10.9.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2017</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.10.9.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.10.9.3.1">
<span class="ltx_p" id="S5.T2.4.10.9.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://ailab.ahu.edu.cn:8087/DeepPPI/index.html" title="">http://ailab.ahu.edu.cn:8087/DeepPPI/index.html</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.11.10">
<td class="ltx_td ltx_align_left" id="S5.T2.4.11.10.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">TAPE</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.11.10.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2019</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.11.10.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.11.10.3.1">
<span class="ltx_p" id="S5.T2.4.11.10.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/songlab-cal/tape" title="">https://github.com/songlab-cal/tape</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.12.11">
<td class="ltx_td ltx_align_left" id="S5.T2.4.12.11.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ChemBERTa</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.12.11.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2020</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.12.11.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.12.11.3.1">
<span class="ltx_p" id="S5.T2.4.12.11.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/seyonechithrananda/bert-loves-chemistry" title="">https://github.com/seyonechithrananda/bert-loves-chemistry</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.13.12">
<td class="ltx_td ltx_align_left" id="S5.T2.4.13.12.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">MolBERT</td>
<td class="ltx_td ltx_align_left" id="S5.T2.4.13.12.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2021</td>
<td class="ltx_td ltx_align_justify" id="S5.T2.4.13.12.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.13.12.3.1">
<span class="ltx_p" id="S5.T2.4.13.12.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/cxfjiang/MolBERT" title="">https://github.com/cxfjiang/MolBERT</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T2.4.14.13">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.4.14.13.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">TransConv</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T2.4.14.13.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2025</td>
<td class="ltx_td ltx_align_justify ltx_border_bb" id="S5.T2.4.14.13.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T2.4.14.13.3.1">
<span class="ltx_p" id="S5.T2.4.14.13.3.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sayantanDs/transconv" title="">https://github.com/sayantanDs/transconv</a></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S5.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S5.T3.2.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S5.T3.3.2" style="font-size:90%;">Summary of Datasets Available for Transformer-Based Protein Research Applications</span></figcaption>
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.4" style="width:433.6pt;height:335.1pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(-1.7pt,1.3pt) scale(0.992371431755214,0.992371431755214) ;">
<table class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.4.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S5.T3.4.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.1.1.1">Model</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.1.2.1">Dataset</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.1.3.1">Size</span></th>
<th class="ltx_td ltx_nopad_r ltx_align_justify ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.1.1.4.1">
<span class="ltx_p" id="S5.T3.4.1.1.1.4.1.1"><span class="ltx_text ltx_font_bold" id="S5.T3.4.1.1.1.4.1.1.1">Access Method</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S5.T3.4.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.1.2.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ESM-Fold</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.1.2.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">UR50 (sample UR90)</td>
<td class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.1.2.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">30,051</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_t" id="S5.T3.4.1.2.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.2.1.4.1">
<span class="ltx_p" id="S5.T3.4.1.2.1.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/nferruz/dataset_fastas" title="">https://huggingface.co/datasets/nferruz/dataset_fastas</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.3.2">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ProtGPT2</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">UR50_2021_04</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">9,935,212</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify" id="S5.T3.4.1.3.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.3.2.4.1">
<span class="ltx_p" id="S5.T3.4.1.3.2.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/datasets/nferruz/UR50_2021_04." title="">https://huggingface.co/datasets/nferruz/UR50_2021_04.</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.4.3">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.4.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">RFDiffusion</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.4.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">RoseTTAFold Diffusion</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.4.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">11,714</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify" id="S5.T3.4.1.4.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.4.3.4.1">
<span class="ltx_p" id="S5.T3.4.1.4.3.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://figshare.com/s/439fdd59488215753bc3" title="">https://figshare.com/s/439fdd59488215753bc3</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.5.4">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.5.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ProteinBERT</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.5.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">UniRef90</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.5.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">43.85 GB (compressed)</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify" id="S5.T3.4.1.5.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.5.4.4.1">
<span class="ltx_p" id="S5.T3.4.1.5.4.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref90/uniref90.fasta.gz" title="">ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref90/uniref90.fasta.gz</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.6.5">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.6.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ProtTrans</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.6.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">UniRef100, UniRef50, BFD</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.6.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">2.3B sequences</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify" id="S5.T3.4.1.6.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.6.5.4.1">
<span class="ltx_p" id="S5.T3.4.1.6.5.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="http://doi.ieeecomputersociety.org/10.1109/TPAMI.2021.3095381" title="">http://doi.ieeecomputersociety.org/10.1109/TPAMI.2021.3095381</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.7.6">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.7.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">TAPE</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.7.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">Pfam</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.7.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">7 GB (compressed)</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify" id="S5.T3.4.1.7.6.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.7.6.4.1">
<span class="ltx_p" id="S5.T3.4.1.7.6.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/songlab-cal/tape" title="">https://github.com/songlab-cal/tape</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.8.7">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.8.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">ChemBERTa</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.8.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">PubChem-10M</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.8.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">10,000,000 SMILES</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify" id="S5.T3.4.1.8.7.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.8.7.4.1">
<span class="ltx_p" id="S5.T3.4.1.8.7.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://pubchem.ncbi.nlm.nih.gov/" title="">https://pubchem.ncbi.nlm.nih.gov/</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.9.8">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.9.8.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">MolBERT</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.9.8.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">ZINC15, ChEMBL27</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.9.8.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">4,000,000 SMILES</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify" id="S5.T3.4.1.9.8.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.9.8.4.1">
<span class="ltx_p" id="S5.T3.4.1.9.8.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://zinc15.docking.org" title="">https://zinc15.docking.org</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.10.9">
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.10.9.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">AlphaFold</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.10.9.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">PDB</td>
<td class="ltx_td ltx_align_left" id="S5.T3.4.1.10.9.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">170,000</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify" id="S5.T3.4.1.10.9.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.10.9.4.1">
<span class="ltx_p" id="S5.T3.4.1.10.9.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.rcsb.org/" title="">https://www.rcsb.org/</a></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S5.T3.4.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.1.11.10.1" style="padding-top:2.5pt;padding-bottom:2.5pt;">TransConv</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.1.11.10.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">NetSurf, ProteinNet</td>
<td class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.1.11.10.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">11,000</td>
<td class="ltx_td ltx_nopad_r ltx_align_justify ltx_border_bb" id="S5.T3.4.1.11.10.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">
<span class="ltx_inline-block ltx_align_top" id="S5.T3.4.1.11.10.4.1">
<span class="ltx_p" id="S5.T3.4.1.11.10.4.1.1"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sayantanDs/transconv/tree/main/attsc_dataset" title="">https://github.com/sayantanDs/transconv/tree/main/attsc_dataset</a></span>
</span>
</td>
</tr>
</tbody>
</table>
</span></div>
</figure>
<div class="ltx_para" id="S5.p12">
<p class="ltx_p" id="S5.p12.1"><span class="ltx_text ltx_font_bold" id="S5.p12.1.1">3. Model Interpretability and Explainable AI</span></p>
</div>
<div class="ltx_para" id="S5.p13">
<p class="ltx_p" id="S5.p13.1">1). Attention Mechanisms for Model Interpretability:One potential way to improve interpretability is by utilizing the attention mechanism inherent in Transformer models. By visualizing the attention weights, researchers can gain insights into which parts of the protein sequence or structure the model is focusing on when making predictions. These attention maps can help explain why a model predicts a certain protein interaction, function, or structure. For example, attention could focus on regions in the protein sequence that are critical for binding interactions or areas of structural importance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib118" title="">118</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p14">
<p class="ltx_p" id="S5.p14.1">2). Explainable AI Techniques:Methods such as SHAP (SHapley Additive exPlanations) and LIME (Local Interpretable Model-agnostic Explanations) can be applied to interpret model predictions, offering a way to understand feature contributions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib119" title="">119</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p15">
<p class="ltx_p" id="S5.p15.1">3). Incorporating Domain Knowledge: Embedding biological knowledge into models, such as known functional domains or structural constraints, can enhance interpretability and guide the learning process. Hybrid models that combine machine learning with mechanistic understanding are promising in this regard <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib120" title="">120</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p16">
<p class="ltx_p" id="S5.p16.1"><span class="ltx_text ltx_font_bold" id="S5.p16.1.1">4. Expansion of Training Data and Benchmarks</span></p>
</div>
<div class="ltx_para" id="S5.p17">
<p class="ltx_p" id="S5.p17.1">Comprehensive and diverse datasets are foundational to model performance. Future work should focus on expanding protein complex databases with more multimeric structures, transient interactions, and annotated functional states. Emphasis should be placed on underrepresented protein families, such as membrane-associated or signaling proteins. The development of standardized benchmarks and evaluation protocols for quaternary structure prediction would also enable more rigorous comparisons across models and facilitate reproducibility <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib61" title="">61</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p18">
<p class="ltx_p" id="S5.p18.1"><span class="ltx_text ltx_font_bold" id="S5.p18.1.1">5. Efficient and Scalable Model Architectures</span></p>
</div>
<div class="ltx_para" id="S5.p19">
<p class="ltx_p" id="S5.p19.1">Although Transformer models have shown remarkable success in protein informatics, their computational cost remains a significant challenge. Training large-scale Transformer models typically requires vast amounts of computational resources, including powerful GPUs and distributed computing systems, making them inaccessible to many researchers. Reducing computational costs without sacrificing performance is a priority for future research.</p>
</div>
<div class="ltx_para" id="S5.p20">
<p class="ltx_p" id="S5.p20.1">1). Sparse Attention Mechanisms: To reduce computational complexity, researchers are exploring sparse attention mechanisms that limit the number of interactions modeled between tokens. Models like Longformer and Performer introduce approximations to the standard attention mechanism, enabling the handling of longer sequences with reduced computational load <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib121" title="">121</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p21">
<p class="ltx_p" id="S5.p21.1">2). Recurrent Neural Networks and Memory Mechanisms:Incorporating recurrence or memory mechanisms into Transformer architectures could help capture long-range dependencies without processing the entire sequence simultaneously, thus saving computational resources <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib122" title="">122</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p22">
<p class="ltx_p" id="S5.p22.1">3). Knowledge Distillation: Techniques like knowledge distillation can be used to transfer knowledge from large, cumbersome models to smaller, more efficient ones without significant loss in performance. This approach can make models more accessible and deployable in resource-limited settings <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib123" title="">123</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p23">
<p class="ltx_p" id="S5.p23.1">4). Hardware Acceleration: Advances in hardware, such as the development of specialized AI chips and neuromorphic computing, could alleviate computational constraints. Utilizing cloud-based resources and distributed computing can also make high-performance modeling more accessible <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib124" title="">124</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S5.p24">
<p class="ltx_p" id="S5.p24.1"><span class="ltx_text ltx_font_bold" id="S5.p24.1.1">6. Interdisciplinary and Collaborative Research</span></p>
</div>
<div class="ltx_para" id="S5.p25">
<p class="ltx_p" id="S5.p25.1">The complex nature of protein systems necessitates cross-disciplinary collaboration among computational biologists, structural biologists, machine learning experts, and domain scientists<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib125" title="">125</a>]</cite><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#bib.bib40" title="">40</a>]</cite>. Such collaboration will accelerate the translation of Transformer models into practical workflows for drug discovery, synthetic biology, and personalized medicine. Furthermore, co-development of models and wet-lab validation pipelines will ensure that computational predictions have real-world biological relevance.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">CONCLUSION</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">Transformer models have rapidly become a cornerstone in protein informatics, offering robust mechanisms for learning long-range dependencies and capturing complex biological patterns. This survey has presented a systematic review of Transformer-based architectures and their applications across critical domains, including protein structure prediction, function annotation, protein-protein interactions, and computational drug discovery. Starting from foundational principles, we examined leading models such as AlphaFold, ESM, ProtTrans, and RoseTTAFold, highlighting their design innovations and contributions to protein science.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">Beyond summarizing current advances, this work also identified key limitations associated with Transformer models, particularly in terms of computational scalability, data requirements, and biological interpretability. We emphasized the importance of future directions such as multimodal data integration, hybrid modeling with experimental constraints, improved model efficiency, and enhanced domain generalization. Moreover, we advocated for interdisciplinary collaboration as a driver for methodological breakthroughs and real-world applicability.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">By focusing specifically on protein-related tasks, this survey provides a unique perspective on the intersection of deep learning and molecular biology. It is our hope that this work will guide future research in developing more efficient, interpretable, and biologically grounded Transformer models, ultimately advancing the frontiers of bioinformatics, structural biology, and personalized medicine.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Resources and Reproducibility</span>
</h2>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS1.5.1.1">VII-A</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS1.6.2">Open Source Implementation</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Open-source implementations of Transformer-based models in protein informatics have played a crucial role in advancing the field, enabling researchers to conduct baseline experiments, benchmark new methods, and foster reproducibility. Publicly available codebases not only facilitate rapid experimentation but also promote transparency and collaboration across the community. TABLE <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S5.T2" title="TABLE II ‣ V FUTURE DIRECTIONS ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">II</span></a> summarizes representative open-source implementations related to protein Transformer models, detailing their publication year and repository links.</p>
</div>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S7.SS2.5.1.1">VII-B</span> </span><span class="ltx_text ltx_font_italic" id="S7.SS2.6.2">Datasets Used</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">To support the development and evaluation of Transformer models in protein informatics, researchers have employed a diverse range of large-scale biological datasets. These datasets cover protein sequences, structures, molecular properties, and chemical compounds, offering essential training material for pretraining, fine-tuning, and benchmarking. The use of well-curated and standardized datasets has greatly enhanced model generalizability and robustness in real-world bioinformatics scenarios.</p>
</div>
<div class="ltx_para" id="S7.SS2.p2">
<p class="ltx_p" id="S7.SS2.p2.1">Most datasets fall into one of the following categories: (1) large-scale protein sequence corpora such as UniRef, UR50, and BFD; (2) structure-related datasets from PDB or derived sources used for modeling or generative design; and (3) chemical and molecular datasets like SMILES strings for tasks involving drug discovery.</p>
</div>
<div class="ltx_para" id="S7.SS2.p3">
<p class="ltx_p" id="S7.SS2.p3.1">For example, models such as ProtTrans and ProteinBERT were pretrained on billions of protein sequences from UniRef50, UniRef100, and BFD, enabling them to capture both local motifs and global evolutionary patterns. ESM-Fold and ProtGPT2 utilized subsets like UR50 for efficient pretraining. Structure-based generative models such as RFDiffusion further incorporated manually curated datasets for training backbone diffusion networks. In the field of chemical modeling, ChemBERTa and MolBERT used millions of molecular SMILES representations from PubChem and ZINC databases to enable protein-ligand interaction prediction.</p>
</div>
<div class="ltx_para" id="S7.SS2.p4">
<p class="ltx_p" id="S7.SS2.p4.1">TABLE <a class="ltx_ref" href="https://arxiv.org/html/2505.20098v1#S5.T3" title="TABLE III ‣ V FUTURE DIRECTIONS ‣ Transformer in Protein: A Survey"><span class="ltx_text ltx_ref_tag">III</span></a> summarizes the major datasets adopted in the literature, including the dataset name, size, and access method.</p>
</div>
</section>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Vaswani, “Attention is all you need,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pretraining of deep bidirectional transformers for language understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT 2019)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
T. B. Brown, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">arXiv preprint arXiv:2005.14165</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
R. Liu, Y. Wang, H. Xu, Z. Qin, F. Zhang, Y. Liu, and Z. Cao, “Pmanet: Malicious url detection via post-trained language model guided multi-level feature attention network,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Information Fusion</em>, vol. 113, p. 102638, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
R. Liu, Y. Wang, H. Xu, J. Sun, F. Zhang, P. Li, and Z. Guo, “Vul-lmgnns: Fusing language models and online-distilled graph neural networks for code vulnerability detection,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Information Fusion</em>, vol. 115, p. 102748, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
J. Sun, Y. Jia, Y. Wang, Y. Tian, and S. Zhang, “Ethereum fraud detection via joint transaction language model and graph representation learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Information Fusion</em>, vol. 120, p. 103074, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>, “Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">Proceedings of the National Academy of Sciences</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>, “Highly accurate protein structure prediction with alphafold,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">nature</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
R. Rao, N. Bhattacharya, N. Thomas, Y. Duan, P. Chen, J. Canny, P. Abbeel, and Y. Song, “Evaluating protein transfer learning with tape,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Advances in neural information processing systems</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
B. L. Hie, K. K. Yang, and P. S. Kim, “Evolutionary velocity with protein language models predicts evolutionary dynamics of diverse proteins,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Cell Systems</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">et al.</em>, “Language models of protein sequences at the scale of evolution enable accurate structure prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2">BioRxiv</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
S. Min, B. Lee, and S. Yoon, “Deep learning in bioinformatics,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Briefings in bioinformatics</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
R. Casadio, P. L. Martelli, and C. Savojardo, “Machine learning solutions for predicting protein–protein interactions,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Wiley Interdisciplinary Reviews: Computational Molecular Science</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
E. C. Alley, G. Khimulya, S. Biswas, M. AlQuraishi, and G. M. Church, “Unified rational protein engineering with sequence-based deep representation learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Nature methods</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
N. Brandes, D. Ofer, Y. Peleg, N. Rappoport, and M. Linial, “Proteinbert: a universal deep-learning model of protein sequence and function,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Bioinformatics</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
B. Wen, W.-F. Zeng, Y. Liao, Z. Shi, S. R. Savage, W. Jiang, and B. Zhang, “Deep learning in proteomics,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proteomics</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
A. Chandra, L. Tünnermann, T. Löfstedt, and R. Gratz, “Transformer-based deep learning for predicting protein properties in the life sciences,” <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Elife</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
M. Hesami, M. Alizadeh, A. M. P. Jones, and D. Torkamaneh, “Machine learning: its challenges and opportunities in plant system biology,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Applied Microbiology and Biotechnology</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
A. Dhakal, C. McKay, J. J. Tanner, and J. Cheng, “Artificial intelligence in the prediction of protein–ligand interactions: recent advances and future directions,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">Briefings in Bioinformatics</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Liu, M. Ott, N. Goyal, J. Du <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">et al.</em>, “Roberta: A robustly optimized bert pretraining approach,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">arXiv preprint</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-attention with linear complexity,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">arXiv preprint</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, “An image is worth 16x16 words: Transformers for image recognition at scale,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2010.11929</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
J. Perez, J. Marinkovic, and P. Barcelo, “On the turing completeness of modern neural network architectures,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">International Conference on Learning Representations</em>, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
J.-B. Cordonnier, A. Loukas, and M. Jaggi, “On the relationship between self-attention and convolutional layers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">International Conference on Learning Representations</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
J. Dai, H. Qi, Y. Xiong, Z. Li, G. Zhang, H. Hu, and Y. Wei, “Deformable convolutional networks,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, W. Yu, L. Jones, T. Gibbs, T. Feher, C. Angerer, M. Steinegger <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>, “Prottrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et al.</em>, “Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">Proceedings of the National Academy of Sciences</em>, vol. 118, no. 15, p. e2016239118, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X. Li, X. Zhang, S. Wang, X. Yu, S. Pan, J. Wu, and et al., “Peptidebert: A pre-trained model for peptide representation learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">arXiv preprint arXiv:2309.03099</em>, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2309.03099" title="">https://arxiv.org/abs/2309.03099</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, T. Sercu, and A. Rives, “Language models of protein sequences at the scale of evolution enable accurate structure prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">bioRxiv</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
R. Rao, J. Liu, R. Verkuil, J. Meier, J. Canny, P. Abbeel, T. Sercu, and A. Rives, “Msa transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">bioRxiv</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
C. Lu, L. Xu, Y. Wu, Y. Liu, and et al., “Protein representation learning with cross-modal contrastive pretraining,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">arXiv preprint arXiv:2401.14819</em>, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2401.14819" title="">https://arxiv.org/abs/2401.14819</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
B. Alberts, D. Bray, K. Hopkin, A. D. Johnson, J. Lewis, M. Raff, K. Roberts, and P. Walter, <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">Essential cell biology</em>.   Garland Science, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
P. Y. Chou and G. D. Fasman, “Prediction of protein conformation,” <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Biochemistry</em>, vol. 13, no. 2, pp. 222–245, 1974.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
B. Rost and C. Sander, “Prediction of protein secondary structure at better than 70% accuracy,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Journal of molecular biology</em>, vol. 232, no. 2, pp. 584–599, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
J. Garnier, D. J. Osguthorpe, and B. Robson, “Analysis of the accuracy and implications of simple methods for predicting the secondary structure of globular proteins,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Journal of molecular biology</em>, vol. 120, no. 1, pp. 97–120, 1978.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
M. Heinzinger, A. Elnaggar, Y. Wang, C. Dallago, D. Nechaev, F. Matthes, and B. Rost, “Modeling aspects of the language of life through transfer-learning protein sequences,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">BMC bioinformatics</em>, vol. 20, pp. 1–17, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">et al.</em>, “Highly accurate protein structure prediction with alphafold,” <em class="ltx_emph ltx_font_italic" id="bib.bib37.2.2">nature</em>, vol. 596, no. 7873, pp. 583–589, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong, L. N. Kinch, R. D. Schaeffer <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">et al.</em>, “Accurate prediction of protein structures and interactions using a three-track neural network,” <em class="ltx_emph ltx_font_italic" id="bib.bib38.2.2">Science</em>, vol. 373, no. 6557, pp. 871–876, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Zhang and J. Skolnick, “Tm-align: a protein structure alignment algorithm based on the tm-score,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Nucleic acids research</em>, vol. 33, no. 7, pp. 2302–2309, 2005.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones, T. Gibbs, T. Feher, C. Angerer, M. Steinegger <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">et al.</em>, “Prottrans: towards cracking the language of life’s code through self-supervised learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib40.2.2">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, vol. 44, pp. 7112–7127, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
K. A. Dill and J. L. MacCallum, “The protein-folding problem, 50 years on,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">science</em>, vol. 338, no. 6110, pp. 1042–1046, 2012.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
C. M. Dobson, “Protein folding and misfolding,” <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Nature</em>, vol. 426, no. 6968, pp. 884–890, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
M. Steinegger and J. Söding, “Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets,” <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Nature biotechnology</em>, vol. 35, no. 11, pp. 1026–1028, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
R. Koradi, M. Billeter, and K. Wüthrich, “Molmol: a program for display and analysis of macromolecular structures,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Journal of molecular graphics</em>, vol. 14, no. 1, pp. 51–55, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">et al.</em>, “Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.2.2">Proceedings of the National Academy of Sciences</em>, vol. 118, no. 15, p. e2016239118, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
Anonymous, “Uniprot: the universal protein knowledgebase in 2021,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Nucleic acids research</em>, vol. 49, no. D1, pp. D480–D489, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
B. Adhikari and J. Cheng, “Confold2: improved contact-driven ab initio protein structure modeling,” <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">BMC bioinformatics</em>, vol. 19, pp. 1–5, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
H. Zhou and J. Skolnick, “Goap: a generalized orientation-dependent, all-atom statistical potential for protein structure prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Biophysical journal</em>, vol. 101, no. 8, pp. 2043–2052, 2011.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
D. T. Jones, “Protein secondary structure prediction based on position-specific scoring matrices,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Journal of molecular biology</em>, vol. 292, no. 2, pp. 195–202, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
B. Rost and C. Sander, “Prediction of protein secondary structure at better than 70% accuracy,” <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Journal of molecular biology</em>, vol. 232, no. 2, pp. 584–599, 1993.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
H. A. Scheraga, M. Khalili, and A. Liwo, “Protein-folding dynamics: overview of molecular simulation techniques,” <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">Annu. Rev. Phys. Chem.</em>, vol. 58, no. 1, pp. 57–83, 2007.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
C. N. Magnan and P. Baldi, “Sspro/accpro 5: almost perfect prediction of protein secondary structure and relative solvent accessibility using profiles, machine learning and structural similarity,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Bioinformatics</em>, vol. 30, no. 18, pp. 2592–2597, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
M. C. Peitsch, “Protein modeling by e-mail,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Bio/technology</em>, vol. 13, no. 7, pp. 658–660, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">et al.</em>, “Highly accurate protein structure prediction with alphafold,” <em class="ltx_emph ltx_font_italic" id="bib.bib54.2.2">nature</em>, vol. 596, no. 7873, pp. 583–589, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones, T. Gibbs, T. Feher, C. Angerer, M. Steinegger <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">et al.</em>, “Prottrans: Toward understanding the language of life through self-supervised learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib55.2.2">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 44, no. 10, pp. 7112–7127, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong, L. N. Kinch, R. D. Schaeffer <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">et al.</em>, “Accurate prediction of protein structures and interactions using a three-track neural network,” <em class="ltx_emph ltx_font_italic" id="bib.bib56.2.2">Science</em>, vol. 373, no. 6557, pp. 871–876, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">et al.</em>, “Language models of protein sequences at the scale of evolution enable accurate structure prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib57.2.2">BioRxiv</em>, vol. 2022, p. 500902, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
L. Arana, M. Melcón, D. Kessel, S. Hoyos, J. Albert, L. Carretié, and A. Capilla, “Suppression of alpha-band power underlies exogenous attention to emotional distractors,” <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Psychophysiology</em>, vol. 59, no. 9, p. e14051, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">et al.</em>, “Highly accurate protein structure prediction with alphafold,” <em class="ltx_emph ltx_font_italic" id="bib.bib59.2.2">nature</em>, vol. 596, no. 7873, pp. 583–589, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
A. W. Senior, R. Evans, J. Jumper, J. Kirkpatrick, L. Sifre, T. Green, C. Qin, A. Žídek, A. W. Nelson, A. Bridgland <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">et al.</em>, “Improved protein structure prediction using potentials from deep learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib60.2.2">Nature</em>, vol. 577, no. 7792, pp. 706–710, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
R. Rao, N. Bhattacharya, N. Thomas, Y. Duan, P. Chen, J. Canny, P. Abbeel, and Y. Song, “Evaluating protein transfer learning with tape,” <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">Advances in neural information processing systems</em>, vol. 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
Y. Zhou, K. Tan, X. Shen, Z. He, and H. Zheng, “A protein structure prediction approach leveraging transformer and cnn integration,” <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">arXiv preprint arXiv:2402.19095</em>, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2402.19095" title="">https://arxiv.org/abs/2402.19095</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
N. Brandes, D. Ofer, Y. Peleg, N. Rappoport, and M. Linial, “Proteinbert: a universal deep-learning model of protein sequence and function,” <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">Bioinformatics</em>, vol. 38, no. 8, pp. 2102–2110, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
C. Meng, Y. Shi, X. Fu, Q. Zou, and W. Han, “Trans-morfs: A disordered protein predictor based on the transformer architecture,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">IEEE Journal of Biomedical and Health Informatics</em>, pp. 1–10, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
S. Chithrananda, G. Grand, and B. Ramsundar, “Chemberta: large-scale self-supervised pretraining for molecular property prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2010.09885</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
B. Ramsundar, P. Eastman, P. Walters, V. Pande, K. Leswing, and Z. Wu, <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Deep Learning for the Life Sciences: Applying Deep Learning to Genomics, Microscopy, Drug Discovery, and More</em>.   O’Reilly Media, Inc., 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
R. Wu, F. Ding, R. Wang, R. Shen, X. Zhang, S. Luo, C. Su, Z. Wu, Q. Xie, B. Berger, and J. Ma, “High-resolution de novo structure prediction from primary sequence,” <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">bioRxiv</em>, 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1101/2022.07.26.501554" title="">https://doi.org/10.1101/2022.07.26.501554</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
N. Ferruz, S. Schmidt, and B. Höcker, “Protgpt2 is a deep unsupervised language model for protein design,” <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Nature Communications</em>, vol. 13, no. 1, p. 4348, 2022. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.nature.com/articles/s41467-022-31900-5" title="">https://www.nature.com/articles/s41467-022-31900-5</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
A. Madani, B. Krause, E. R. Greene, S. Subramanian, B. P. Mohr, J. M. Holton, J. L. Olmos, C. Xiong, Z. Z. Sun, R. Socher <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">et al.</em>, “Large language models generate functional protein sequences across diverse families,” <em class="ltx_emph ltx_font_italic" id="bib.bib69.2.2">Nature Biotechnology</em>, vol. 41, no. 8, pp. 1099–1106, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
J. L. Watson, D. Juergens, N. R. Bennett, B. L. Trippe, J. Yim, H. E. Eisenach, W. Ahern, A. J. Borst, R. J. Ragotte, L. F. Milles <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">et al.</em>, “De novo design of protein structure and function with rfdiffusion,” <em class="ltx_emph ltx_font_italic" id="bib.bib70.2.2">Nature</em>, vol. 620, no. 7976, pp. 1089–1100, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
H. L. Carscadden, L. Machi, C. J. Kuhlman, D. Machi, and S. Ravi, “Graphtrans: a software system for network conversions for simulation, structural analysis, and graph operations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">2021 Winter Simulation Conference (WSC)</em>.   IEEE, 2021, pp. 1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
Y. Chen, G. Chen, and C. Y.-C. Chen, “Mftrans: A multi-feature transformer network for protein secondary structure prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib72.1.1">International Journal of Biological Macromolecules</em>, vol. 267, p. 131311, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
S. Das, S. Ghosh, and N. Jana, “Transconv: Convolution-infused transformer for protein secondary structure prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">Springer nature link</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
N. Giri and J. Cheng, “De novo atomic protein structure modeling for cryoem density maps using 3d transformer and hmm,” <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">Nature Communications</em>, vol. 15, no. 1, p. 5511, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1038/s41467-024-49647-6" title="">https://doi.org/10.1038/s41467-024-49647-6</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
J. Fang, “A critical review of five machine learning-based algorithms for predicting protein stability changes upon mutation,” <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">Briefings in Bioinformatics</em>, vol. 21, no. 4, pp. 1285–1292, 07 2019. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1093/bib/bbz071" title="">https://doi.org/10.1093/bib/bbz071</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
M. Bhasin and G. P. S. Raghava, “Gpcrpred: an svm-based method for prediction of families and subfamilies of g-protein coupled receptors,” <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Nucleic Acids Research</em>, vol. 32, no. suppl_2, pp. W383–W389, 07 2004. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1093/nar/gkh416" title="">https://doi.org/10.1093/nar/gkh416</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
F. Wang and L. Wei, “Multi-scale deep learning for the imbalanced multi-label protein subcellular localization prediction based on immunohistochemistry images,” <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">Bioinformatics</em>, vol. 38, no. 16, pp. 4019–4027, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
G. Li, N. Zhang, and L. Fan, “Prog-sol: Predicting protein solubility using protein embeddings and dual-graph convolutional networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">ACS Omega</em>, vol. 10, no. 4, pp. 3910–3916, 2025. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1021/acsomega.4c09688" title="">https://doi.org/10.1021/acsomega.4c09688</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
S. Khan, S. Noor, H. H. Awan, S. Iqbal <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">et al.</em>, “Deep-probind: Binding protein prediction with transformer-based deep learning model,” <em class="ltx_emph ltx_font_italic" id="bib.bib79.2.2">Springer nature link</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
M. Wenzel, E. Grüner, and N. Strodthoff, “Insights into the inner workings of transformer models for protein function prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">Bioinformatics</em>, vol. 40, no. 3, p. btae031, 01 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1093/bioinformatics/btae031" title="">https://doi.org/10.1093/bioinformatics/btae031</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Y. Wang, Y. Sun, B. Lin, and Others, “Segt-go: a graph transformer method based on ppi serialization and explanatory artificial intelligence for protein function prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">BMC Bioinformatics</em>, vol. 26, p. 46, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
G. B. de Oliveira, H. Pedrini, and Z. Dias, “Integrating transformers and automl for protein function prediction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">2024 46th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)</em>, 2024, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
X. Du, S. Sun, C. Hu, Y. Yao, Y. Yan, and Y. Zhang, “Deepppi: boosting prediction of protein–protein interactions with deep neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">Journal of chemical information and modeling</em>, vol. 57, no. 6, pp. 1499–1510, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
H. L. Carscadden, L. Machi, C. J. Kuhlman, D. Machi, and S. Ravi, “Graphtrans: a software system for network conversions for simulation, structural analysis, and graph operations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">2021 Winter Simulation Conference (WSC)</em>.   IEEE, 2021, pp. 1–12.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
L. Meng and H. Zhang, “Gact-ppis: Prediction of protein-protein interaction sites based on graph structure and transformer network,” <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">International Journal of Biological Macromolecules</em>, vol. 283, p. 137272, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S0141813024080814" title="">https://www.sciencedirect.com/science/article/pii/S0141813024080814</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
S. H. Khan, H. Tayara, and K. T. Chong, “Tranp-b-site: A transformer enhanced method for prediction of binding sites of protein-protein interactions,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">Measurement</em>, vol. 251, p. 117227, 2025. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S026322412500586X" title="">https://www.sciencedirect.com/science/article/pii/S026322412500586X</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
Y. S. Ko, J. Parkinson, C. Liu, and W. Wang, “Tuna: An uncertainty-aware transformer model for sequence-based protein–protein interaction prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">Briefings in Bioinformatics</em>, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.1093/bib/bbae359" title="">https://doi.org/10.1093/bib/bbae359</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
L. Chen, K. F. Ahmad Nasif, B. Deng, S. Niu, and C. Y. Xie, “Predicting protein-protein binding affinity with deep learning: A comparative analysis of cnn and transformer models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">2024 IEEE 36th International Conference on Tools with Artificial Intelligence (ICTAI)</em>, 2024, pp. 548–555.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
J. Jiang, L. Chen, L. Ke, B. Dou, C. Zhang, H. Feng, Y. Zhu, H. Qiu, B. Zhang, and G. Wei, “A review of transformers in drug discovery and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">Journal of Pharmaceutical Analysis</em>, p. 101081, 2024. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.sciencedirect.com/science/article/pii/S2095177924001783" title="">https://www.sciencedirect.com/science/article/pii/S2095177924001783</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
J. Li and X. Jiang, “Mol-bert: An effective molecular representation with bert for molecular property prediction,” <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">Wireless Communications and Mobile Computing</em>, vol. 2021, no. 1, p. 7181815, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
P. Bongini, M. Bianchini, and F. Scarselli, “Molecular generative graph neural networks for drug discovery,” <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">Neurocomputing</em>, vol. 450, pp. 242–252, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones, T. Gibbs, T. Feher, C. Angerer, M. Steinegger <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">et al.</em>, “Prottrans: Toward understanding the language of life through self-supervised learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib92.2.2">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 44, no. 10, pp. 7112–7127, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
R. R. Kotkondawar, S. R. Sutar, A. W. Kiwelekar, and V. J. Kadam, “Integrating transformer-based language model for drug discovery,” in <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">2024 11th International Conference on Computing for Sustainable Global Development (INDIACom)</em>, 2024, pp. 1096–1101.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
N. Aksamit, J. Hou, Y. Li, and Others, “Integrating transformers and many-objective optimization for drug design,” <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">BMC Bioinformatics</em>, vol. 25, p. 208, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
A. M. Bran and P. Schwaller, “Transformers and large language models for chemistry and drug discovery,” in <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">Drug Development Supported by Informatics</em>, H. Satoh, K. Funatsu, and H. Yamamoto, Eds.   Springer, Singapore, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
C. N. Magnan and P. Baldi, “Sspro/accpro 5: almost perfect prediction of protein secondary structure and relative solvent accessibility using profiles, machine learning and structural similarity,” <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">Bioinformatics</em>, vol. 30, no. 18, pp. 2592–2597, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
Anonymous, “Uniprot: the universal protein knowledgebase in 2021,” <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">Nucleic acids research</em>, vol. 49, no. D1, pp. D480–D489, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
M. Steinegger and J. Söding, “Mmseqs2 enables sensitive protein sequence searching for the analysis of massive data sets,” <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">Nature biotechnology</em>, vol. 35, no. 11, pp. 1026–1028, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
B. Adhikari and J. Cheng, “Confold2: improved contact-driven ab initio protein structure modeling,” <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">BMC bioinformatics</em>, vol. 19, pp. 1–5, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
R. Rao, J. Meier, T. Sercu, S. Ovchinnikov, and A. Rives, “Transformer protein language models are unsupervised structure learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">Biorxiv</em>, pp. 2020–12, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
J. Meier, R. Rao, R. Verkuil, J. Liu, T. Sercu, and A. Rives, “Language models enable zero-shot prediction of the effects of mutations on protein function,” <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">Advances in neural information processing systems</em>, vol. 34, pp. 29 287–29 303, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
E. Strubell, A. Ganesh, and A. McCallum, “Energy and policy considerations for modern deep learning research,” in <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">Proceedings of the AAAI conference on artificial intelligence</em>, vol. 34, no. 09, 2020, pp. 13 693–13 696.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
B. Wen, W.-F. Zeng, Y. Liao, Z. Shi, S. R. Savage, W. Jiang, and B. Zhang, “Deep learning in proteomics,” <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">Proteomics</em>, vol. 20, no. 21-22, p. 1900335, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
X. Yan, S. Hu, Y. Mao, Y. Ye, and H. Yu, “Deep multi-view learning methods: A review,” <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">Neurocomputing</em>, vol. 448, pp. 106–129, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">et al.</em>, “Highly accurate protein structure prediction with alphafold,” <em class="ltx_emph ltx_font_italic" id="bib.bib105.2.2">nature</em>, vol. 596, no. 7873, pp. 583–589, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
F. Doshi-Velez and B. Kim, “Towards a rigorous science of interpretable machine learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">arXiv preprint arXiv:1702.08608</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
W. Samek, “Explainable artificial intelligence: Understanding, visualizing and interpreting deep learning models,” <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">arXiv preprint arXiv:1708.08296</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
R. Rao, J. Meier, T. Sercu, S. Ovchinnikov, and A. Rives, “Transformer protein language models are unsupervised structure learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Biorxiv</em>, pp. 2020–12, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
I. Anishchenko, M. Baek, H. Park, N. Hiranuma, D. E. Kim, J. Dauparas, S. Mansoor, I. R. Humphreys, and D. Baker, “Protein tertiary structure prediction and refinement using deep learning and rosetta in casp14,” <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">Proteins: Structure, Function, and Bioinformatics</em>, vol. 89, no. 12, pp. 1722–1733, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
M. Karimi, D. Wu, Z. Wang, and Y. Shen, “Deepaffinity: interpretable deep learning of compound–protein affinity through unified recurrent and convolutional neural networks,” <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">Bioinformatics</em>, vol. 35, no. 18, pp. 3329–3338, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
O. Vinyals, I. Babuschkin, W. M. Czarnecki, M. Mathieu, A. Dudzik, J. Chung, D. H. Choi, R. Powell, T. Ewalds, P. Georgiev <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">et al.</em>, “Grandmaster level in starcraft ii using multi-agent reinforcement learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib111.2.2">nature</em>, vol. 575, no. 7782, pp. 350–354, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
R. L. Grossman, A. P. Heath, V. Ferretti, H. E. Varmus, D. R. Lowy, W. A. Kibbe, and L. M. Staudt, “Toward a shared vision for cancer genomic data,” <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">New England Journal of Medicine</em>, vol. 375, no. 12, pp. 1109–1112, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">et al.</em>, “Highly accurate protein structure prediction with alphafold,” <em class="ltx_emph ltx_font_italic" id="bib.bib113.2.2">nature</em>, vol. 596, no. 7873, pp. 583–589, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
A. Elnaggar, M. Heinzinger, C. Dallago, G. Rehawi, Y. Wang, L. Jones, T. Gibbs, T. Feher, C. Angerer, M. Steinegger <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">et al.</em>, “Prottrans: Toward understanding the language of life through self-supervised learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib114.2.2">IEEE transactions on pattern analysis and machine intelligence</em>, vol. 44, no. 10, pp. 7112–7127, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
A. Rives, J. Meier, T. Sercu, S. Goyal, Z. Lin, J. Liu, D. Guo, M. Ott, C. L. Zitnick, J. Ma <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">et al.</em>, “Biological structure and function emerge from scaling unsupervised learning to 250 million protein sequences,” <em class="ltx_emph ltx_font_italic" id="bib.bib115.2.2">Proceedings of the National Academy of Sciences</em>, vol. 118, no. 15, p. e2016239118, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
M. Baek, F. DiMaio, I. Anishchenko, J. Dauparas, S. Ovchinnikov, G. R. Lee, J. Wang, Q. Cong, L. N. Kinch, R. D. Schaeffer <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">et al.</em>, “Accurate prediction of protein structures and interactions using a three-track neural network,” <em class="ltx_emph ltx_font_italic" id="bib.bib116.2.2">Science</em>, vol. 373, no. 6557, pp. 871–876, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
R. Koradi, M. Billeter, and K. Wüthrich, “Molmol: a program for display and analysis of macromolecular structures,” <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">Journal of molecular graphics</em>, vol. 14, no. 1, pp. 51–55, 1996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
S. R. Choi and M. Lee, “Transformer architecture and attention mechanisms in genome data analysis: a comprehensive review,” <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">Biology</em>, vol. 12, no. 7, p. 1033, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
S. Lundberg, “A unified approach to interpreting model predictions,” <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">arXiv preprint arXiv:1705.07874</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
P. Sturmfels, J. Vig, A. Madani, and N. F. Rajani, “Profile prediction: An alignment-based pre-training task for protein sequence models,” <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">arXiv preprint arXiv:2012.00195</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
I. Beltagy, M. E. Peters, and A. Cohan, “Longformer: The long-document transformer,” <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">arXiv preprint arXiv:2004.05150</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Z. Dai, “Transformer-xl: Attentive language models beyond a fixed-length context,” <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">arXiv preprint arXiv:1901.02860</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
G. Hinton, “Distilling the knowledge in a neural network,” <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">arXiv preprint arXiv:1503.02531</em>, 2015.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
D. Patterson, J. Gonzalez, Q. Le, C. Liang, L.-M. Munguia, D. Rothchild, D. So, M. Texier, and J. Dean, “Carbon emissions and large neural network training,” <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">arXiv preprint arXiv:2104.10350</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
J. Jumper, R. Evans, A. Pritzel, T. Green, M. Figurnov, O. Ronneberger, K. Tunyasuvunakool, R. Bates, A. Žídek, A. Potapenko <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">et al.</em>, “Highly accurate protein structure prediction with alphafold,” <em class="ltx_emph ltx_font_italic" id="bib.bib125.2.2">nature</em>, vol. 596, no. 7873, pp. 583–589, 2021.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 26 14:57:45 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

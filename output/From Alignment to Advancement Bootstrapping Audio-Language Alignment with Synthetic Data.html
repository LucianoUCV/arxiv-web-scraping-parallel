<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data</title>
<!--Generated on Mon May 26 15:47:42 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Audio-aware large language models,  audio-language alignment
" lang="en" name="keywords"/>
<base href="/html/2505.20166v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S1" title="In From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S2" title="In From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Related Works</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3" title="In From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">Method</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.SS1" title="In III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.SS2" title="In III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">BALSa-MA: Bootstrapping Multi-Audio Learning via Difference Comparison and Joint Captioning</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.SS3" title="In III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-C</span> </span><span class="ltx_text ltx_font_italic">Model Design and Training Approach</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S4" title="In From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Experimental Setup</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S4.SS1" title="In IV Experimental Setup ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Training Datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S4.SS2" title="In IV Experimental Setup ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Model Selection and Training Setups</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S4.SS3" title="In IV Experimental Setup ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-C</span> </span><span class="ltx_text ltx_font_italic">Baseline Models and Evaluation Setups</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S4.SS4" title="In IV Experimental Setup ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-D</span> </span><span class="ltx_text ltx_font_italic">Ablation Study</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S5" title="In From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Evaluation Benchmarks</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S5.SS1" title="In V Evaluation Benchmarks ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-A</span> </span><span class="ltx_text ltx_font_italic">Audio Question Answering Benchmark</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S5.SS2" title="In V Evaluation Benchmarks ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-B</span> </span><span class="ltx_text ltx_font_italic">Audio Reasoning Benchmark</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S5.SS3" title="In V Evaluation Benchmarks ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">V-C</span> </span><span class="ltx_text ltx_font_italic">Reliability and Safety Benchmark</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S6" title="In From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Result</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S7" title="In From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S8" title="In From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VIII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Chun-Yi Kuan and Hung-yi Lee 
</span><span class="ltx_author_notes">Chun-Yi Kuan and Hung-yi Lee are with the Graduate
Institute of Communication Engineering, National Taiwan University,
Taipei 10617, Taiwan (e-mail: chunyi.kuan.tw@gmail.com;
hungyilee@ntu.edu.tw).</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Audio-aware large language models (ALLMs) have recently made great strides in understanding and processing audio inputs.
These models are typically adapted from text-based large language models (LLMs) through additional training on audio-related tasks.
However, this adaptation process presents two major limitations.
First, ALLMs often suffer from catastrophic forgetting, where important textual capabilities such as instruction-following are lost after training on audio data.
In some cases, models may even hallucinate sounds that are not present in the input audio, raising concerns about their reliability.
Second, achieving cross-modal alignment between audio and language typically relies on large collections of task-specific question–answer pairs for instruction tuning, making the process resource-intensive.
To address these issues, we leverage the backbone LLMs from ALLMs to synthesize general-purpose caption-style alignment data.
We refer to this process as bootstrapping audio-language alignment via synthetic data generation from backbone LLMs (BALSa).
Building on BALSa, we introduce LISTEN (Learning to Identify Sounds Through Extended Negative Samples), a contrastive-like training method designed to improve ALLMs’ ability to differentiate between present and absent sounds.
We further extend BALSa to multi-audio scenarios, where the model either explains the differences between audio inputs or produces a unified caption that describes them all, thereby enhancing audio-language alignment. Experimental results indicate that our method effectively mitigates audio hallucinations while reliably maintaining strong performance in audio understanding, reasoning benchmarks, and instruction-following skills.
Moreover, incorporating multi-audio training further enhances the model’s comprehension and reasoning capabilities.
Overall, BALSa offers an efficient and scalable approach to the development of ALLMs.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Audio-aware large language models, audio-language alignment

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The development of large language models (LLMs) has garnered increasing attention from the research community. Building on this foundation, researchers have explored integrating different modalities into LLMs beyond their strong textual capabilities.
For instance, incorporating visual or audio modalities enables these models to perform visual or auditory understanding, thereby extending their capabilities beyond text processing.
This multimodal integration allows LLMs to leverage their inherent linguistic proficiency alongside their understanding of other modalities to perform multimodal reasoning and question answering.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In the audio-language domain, audio-aware large language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib10" title="">10</a>]</cite> (ALLMs) refer to models that integrate audio processing capabilities into LLMs, enabling them to understand and generate responses based on auditory inputs.
A critical step in developing these models is cross-modal alignment, where textual and auditory information are integrated to ensure effective reasoning across modalities.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">To achieve such alignment in practice, the dominant approach relies on instruction tuning with large-scale audio-language datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>]</cite>, typically composed of task-specific question-answer pairs.
However, it is expensive to manually collect this kind of data, so some previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>]</cite> often turns to proprietary language models like ChatGPT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib11" title="">11</a>]</cite> to synthesize it using metadata from existing audio corpora.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Beyond the data curation effort, another fundamental issue with existing ALLMs is catastrophic forgetting.
Since most ALLMs are adapted from text-based LLMs, they inherently possess strong language understanding capabilities before being trained on audio tasks.
However, prior research has reported that currently developed ALLMs often lose their textual capabilities after being trained on audio data.
This includes a decline in instruction-following behavior <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib12" title="">12</a>]</cite>, overfitting to pre-defined tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>]</cite>, and limited generalization to unseen cross-modal reasoning tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>]</cite>.
In some cases, the models may even mistakenly identify sounds that are not present in the audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address both the challenge of catastrophic forgetting and the burden of data curation, we propose a distinct approach that leverages a backbone LLM to generate synthetic data for audio-language alignment.
Here, the backbone LLM refers to the large language model that serves as the core of ALLM architectures.
We term this approach <span class="ltx_text ltx_font_bold" id="S1.p5.1.1">BALSa</span>, short for <span class="ltx_text ltx_font_bold" id="S1.p5.1.2">B</span>ootstrapping <span class="ltx_text ltx_font_bold" id="S1.p5.1.3">A</span>udio-<span class="ltx_text ltx_font_bold" id="S1.p5.1.4">L</span>anguage Alignment via <span class="ltx_text ltx_font_bold" id="S1.p5.1.5">S</span>ynthetic Dat<span class="ltx_text ltx_font_bold" id="S1.p5.1.6">a</span> Generation from Backbone LLMs.
While prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib18" title="">18</a>]</cite> have explored similar conceptual directions, we are the first to apply this approach to general audio tasks, offering advantages in simplicity, flexibility, and scalability.
Instead of relying on manually crafted or task-specific question-answer pairs, often constructed using proprietary LLMs and complex generation rules, BALSa leverages the backbone LLM’s understanding of audio metadata like sound event tags to synthesize training data in a prompt-driven, automatic, and extensible manner.
Users only need to design simple generation prompts like <span class="ltx_text ltx_font_italic" id="S1.p5.1.7">Repeat the audio</span> or <span class="ltx_text ltx_font_italic" id="S1.p5.1.8">List non-existent sounds in the audio</span>, allowing BALSa to flexibly generate diverse audio-language data tailored to different learning objectives with minimal human effort.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">BALSa offers a flexible and generalizable pipeline that enables audio-language alignment by learning from the behavioral patterns of the backbone LLM.
For instance, it can generate not only descriptive data, such as natural language descriptions of events occurring in the audio, but also contrastive data that distinguish between sounds that are present and those that are absent, simply by varying the generation prompt.
This highlights BALSa’s potential as a general-purpose framework for synthesizing diverse alignment data across a wide range of audio understanding tasks.
As part of this framework, we refer to the incorporation of synthetic negative samples as <span class="ltx_text ltx_font_bold" id="S1.p6.1.1">LISTEN</span> <span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>LISTEN was introduced in our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib19" title="">19</a>]</cite>.
This paper extends that work with a unified training framework, multi-audio modeling, and additional evaluations and experiments.</span></span></span> (<span class="ltx_text ltx_font_bold" id="S1.p6.1.2">L</span>earning to <span class="ltx_text ltx_font_bold" id="S1.p6.1.3">I</span>dentify <span class="ltx_text ltx_font_bold" id="S1.p6.1.4">S</span>ounds <span class="ltx_text ltx_font_bold" id="S1.p6.1.5">T</span>hrough <span class="ltx_text ltx_font_bold" id="S1.p6.1.6">E</span>xtended <span class="ltx_text ltx_font_bold" id="S1.p6.1.7">N</span>egative Samples) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib19" title="">19</a>]</cite>, a contrastive-like method that helps models distinguish between present and absent sounds.
Building on the flexibility of BALSa to generate diverse types of alignment data, our findings indicate that synthetic data generated by backbone LLMs plays a crucial role in model training.
In particular, incorporating negative synthetic samples through LISTEN effectively mitigates hallucinations, thereby enhancing model reliability.</p>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Furthermore, BALSa can be adapted and extended to various application scenarios.
In this study, we introduce an extension of BALSa designed for multi-audio scenarios, where the backbone LLM independently generates explanations of the differences between multiple audio samples and a caption that describes both audio samples together, serving as training data.
The objective is to expand the original alignment process into a twofold approach.
First, a discrimination-based process that generates comparative explanations for differences between audio samples.
Second, a description-based process that generates separate captions for each audio sample within the same response.</p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">Our experimental results demonstrate that training the model on multi-audio data, whether through joint captioning or comparative discrimination, sufficiently enhances its overall performance.
Interestingly, the difference between generating a single caption that encompasses both audio samples and producing comparative explanations for their differences is minimal.
This suggests that simply learning to handle multiple audio samples at once is sufficient to improve the model’s comprehension, even without explicit comparative training.
These findings indicate that exposing the model to multiple audio inputs, whether for captioning (descriptive learning) or discrimination (comparative learning), is a key factor in boosting performance.
Furthermore, the BALSa-based approach leads to more efficient data usage.
Our method achieves comparable or even better performance while using only 12% of the training data duration required by the previous state-of-the-art model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">To comprehensively evaluate model performance, we collect and construct multiple evaluation benchmarks, categorized into three major types: audio question answering, audio reasoning, and reliability and safety benchmarks.
These benchmarks assess various aspects of the model’s capabilities, including basic audio understanding, advanced reasoning, and its reliability and safety in real-world applications.
Through these evaluations, we compare our proposed approach against popular baseline ALLMs in terms of audio-language alignment.
Moreover, we investigate catastrophic forgetting and hallucination issues to further assess model robustness and reliability.
We provide illustrative examples on our demo page. <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://kuan2jiu99.github.io/Balsa" title="">https://kuan2jiu99.github.io/Balsa</a></span></span></span></p>
</div>
<div class="ltx_para" id="S1.p10">
<p class="ltx_p" id="S1.p10.1">In conclusion, our contributions are outlined as follows:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose a data construction pipeline, BALSa (Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs), which synthesizes audio-language alignment data using a backbone LLM and simple prompts, eliminating the need for task-specific question-answer data collection.
Our study explores and validates the effectiveness of this approach, in which the backbone LLM analyzes audio metadata such as sound event tags and generates descriptions guided by a flexibly designed prompt.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">Under the BALSa framework, we introduce LISTEN, a contrastive-style training strategy that incorporates synthetic negative samples to help models distinguish between present and absent sounds, thereby reducing audio hallucination.
Experimental results demonstrate that synthetic data generated by the backbone LLM plays a crucial role in model training, and the LISTEN method effectively mitigates audio hallucinations, enhancing model reliability in real-world applications.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We extend BALSa to support multi-audio processing by introducing BALSa-MA (Multi-Audio), which applies BALSa to scenarios involving multiple audio samples.
In this approach, the backbone LLM both compares audio samples by generating descriptions of their differences and provides separate captions for each sample within the same response.
Experimental results indicate that exposing the model to multiple audio samples, whether through comparative explanations or joint captioning, enhances its audio understanding and improves its performance on evaluation benchmarks.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">BALSa demonstrates high data efficiency, achieving performance comparable to or better than a widely adopted strong model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>]</cite>, while using only 12% of the training data duration.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">5.</span>
<div class="ltx_para" id="S1.I1.i5.p1">
<p class="ltx_p" id="S1.I1.i5.p1.1">To comprehensively assess audio-aware large language models, we conduct experiments on existing benchmarks, reproduce missing evaluation sets, and introduce new test sets to explore previously unexamined aspects.
Our evaluation covers three key areas: audio question answering, which assesses fundamental audio understanding; audio reasoning; and reliability and safety.
Through these evaluations, we systematically compare BALSa with mainstream models, examining the effectiveness of audio-language alignment and addressing challenges like audio hallucinations and instruction adherence.</p>
</div>
</li>
</ol>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Related Works</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Large language models (LLMs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib21" title="">21</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib23" title="">23</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib24" title="">24</a>]</cite> can perform zero-shot learning through instruction tuning.
This capability is expected to emerge when strong cross-modal alignment is achieved, particularly in scenarios where a text-based LLM is integrated with multi-modal encoders.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">Following this trend, audio-aware large language models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib7" title="">7</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib8" title="">8</a>]</cite> extend instruction tuning to audio-related tasks by combining audio encoders with language models and training them in a multi-stage process.
Qwen-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>]</cite> adopts a two-stage training strategy.
In the first stage, multi-task pre-training is conducted using a manually curated dataset that includes tasks such as acoustic scene classification, sound event classification, and audio question answering.
To unify the training format across tasks, Qwen-Audio introduces task-specific tags that indicate the nature of each task.
This stage aims to build a foundational understanding of audio signals.
In the second stage, Qwen-Audio performs supervised instruction tuning to align the model more closely with human intent.
To create training data, proprietary LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib26" title="">26</a>]</cite> are used to generate question–answer pairs based on text metadata, while additional audio-dialogue datasets are constructed using human annotations.
Through this process, Qwen-Audio enhances the model’s ability in both audio understanding and reasoning.
Qwen2-Audio <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>]</cite> follows a similar approach, focusing on collecting high-quality supervised fine-tuning data through meticulous curation.
SALMONN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>]</cite> also employs the same two-stage pipeline.
During instruction tuning, it prioritizes tasks based on their relevance and manually collects task-specific datasets, including those for audio question answering.
Like Qwen-Audio, it leverages proprietary LLMs to generate question–answer pairs grounded in the captions of existing audio corpora.
LTU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>]</cite> constructs a large-scale audio question answering dataset, covering both closed-ended and open-ended formats.
The closed-ended set includes tasks such as sound classification, audio captioning, and temporal analysis. Questions are generated using proprietary LLMs, while answers follow a fixed structure derived from rule-based algorithms.
For open-ended data, which is more expensive and labor-intensive to create manually, LTU uses a rule-based pipeline known as audio instruction generation.
This method inputs metadata such as audio captions and sound event tags into carefully designed prompts, allowing proprietary LLMs to produce diverse and contextually appropriate audio QA pairs.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">On the other hand, a distinct strategy for developing audio-aware large language models focuses on leveraging the backbone LLM itself to generate audio–language alignment data.
These alignment data include general response-style text conditioned on audio, captions that describe the audio given textual conditions, or continuations of audio content based on textual prompts.
Several prior works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib17" title="">17</a>]</cite> have adopted this approach.
For instance, AudioChatLlama <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib14" title="">14</a>]</cite> and DiVA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib18" title="">18</a>]</cite> use the backbone language model to generate responses conditioned on textual transcriptions.
BLSP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib16" title="">16</a>]</cite> further extends this idea by generating continuations based on transcriptions, while BLSP-Emo <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib17" title="">17</a>]</cite> and DeSTA2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib27" title="">27</a>]</cite> incorporate additional paralinguistic information like emotion to produce more expressive continuations and captions.</p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Inspired by these methods and motivated by the lack of research applying this strategy to general audio settings, our work investigates how to bootstrap audio–language alignment through synthetic data generation using a backbone LLM.
We further explore various extensions of this idea to evaluate its generality and effectiveness across different scenarios.
For example, we exploit the flexibility and scalability of this framework to introduce negative samples and multi-audio scenario into the training process.</p>
</div>
<div class="ltx_para" id="S2.p5">
<p class="ltx_p" id="S2.p5.1">This distinction highlights the key difference between our work and prior studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>]</cite>: instead of relying on externally generated task-specific question-answer pairs, we leverage a backbone LLM to generate general-purpose alignment data directly, enabling a more streamlined and self-contained approach to integrating audio with language models.
This provides a practical way to train audio-aware LLMs without the need to manually collect or rely on proprietary models to generate a large amount of task-specific question-answer data.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Method</span>
</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">During the data construction phase, we aim to generate audio-language pairs while ensuring minimal textual discrepancies between the training data and the backbone LLM used in the ALLM.
This design allows the model to maintain consistency in textual representation without modifying the backbone LLM’s parameters during subsequent training.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.1">The rationale behind generating audio-language pairs using the backbone LLM is to enable the model to acquire audio understanding capabilities solely through an audio modality adapter.
This approach ensures that the audio-aware large language model aligns its comprehension of audio with the text-based metadata information understood by the backbone LLM, without requiring adjustments to the backbone LLM itself.
In this work, we refer to this approach as <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.1">B</span>ootstrapping <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.2">A</span>udio-<span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.3">L</span>anguage Alignment via <span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.4">S</span>ynthetic Dat<span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.5">a</span> Generation from Backbone LLMs (<span class="ltx_text ltx_font_bold" id="S3.SS1.p2.1.6">BALSa</span>).</p>
</div>
<figure class="ltx_figure" id="S3.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="247" id="S3.F1.g1" src="extracted/6480732/figures/Overview-BALSA-Unified-V2.png" width="538"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>
The left diagram illustrates the Data Construction Stage, where a backbone LLM generates audio-language aligned descriptions for audio clips.
The right diagram represents the Training Stage, where an audio modality adapter learns to align audio inputs while the backbone LLM remains frozen.
In the single-audio scenario, the process follows only the orange arrows, whereas in the multi-audio scenario, both orange and blue arrows are involved, indicating the simultaneous alignment of multiple audio inputs.
</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.p3">
<p class="ltx_p" id="S3.SS1.p3.4">Next, we provide a detailed explanation of how LLMs are leveraged to generate audio-language alignment data.
While LLMs can not directly process audio, they excel at understanding and analyzing textual metadata associated with audio, such as sound events and audio captions.
To construct audio-language alignment data, we utilize the metadata from original audio datasets, which typically include two common types of annotations: human-annotated captions (<math alttext="{D}_{{caption}}" class="ltx_Math" display="inline" id="S3.SS1.p3.1.m1.1"><semantics id="S3.SS1.p3.1.m1.1a"><msub id="S3.SS1.p3.1.m1.1.1" xref="S3.SS1.p3.1.m1.1.1.cmml"><mi id="S3.SS1.p3.1.m1.1.1.2" xref="S3.SS1.p3.1.m1.1.1.2.cmml">D</mi><mrow id="S3.SS1.p3.1.m1.1.1.3" xref="S3.SS1.p3.1.m1.1.1.3.cmml"><mi id="S3.SS1.p3.1.m1.1.1.3.2" xref="S3.SS1.p3.1.m1.1.1.3.2.cmml">c</mi><mo id="S3.SS1.p3.1.m1.1.1.3.1" xref="S3.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.1.m1.1.1.3.3" xref="S3.SS1.p3.1.m1.1.1.3.3.cmml">a</mi><mo id="S3.SS1.p3.1.m1.1.1.3.1a" xref="S3.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.1.m1.1.1.3.4" xref="S3.SS1.p3.1.m1.1.1.3.4.cmml">p</mi><mo id="S3.SS1.p3.1.m1.1.1.3.1b" xref="S3.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.1.m1.1.1.3.5" xref="S3.SS1.p3.1.m1.1.1.3.5.cmml">t</mi><mo id="S3.SS1.p3.1.m1.1.1.3.1c" xref="S3.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.1.m1.1.1.3.6" xref="S3.SS1.p3.1.m1.1.1.3.6.cmml">i</mi><mo id="S3.SS1.p3.1.m1.1.1.3.1d" xref="S3.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.1.m1.1.1.3.7" xref="S3.SS1.p3.1.m1.1.1.3.7.cmml">o</mi><mo id="S3.SS1.p3.1.m1.1.1.3.1e" xref="S3.SS1.p3.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.1.m1.1.1.3.8" xref="S3.SS1.p3.1.m1.1.1.3.8.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.1.m1.1b"><apply id="S3.SS1.p3.1.m1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.1.m1.1.1.1.cmml" xref="S3.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p3.1.m1.1.1.2.cmml" xref="S3.SS1.p3.1.m1.1.1.2">𝐷</ci><apply id="S3.SS1.p3.1.m1.1.1.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3"><times id="S3.SS1.p3.1.m1.1.1.3.1.cmml" xref="S3.SS1.p3.1.m1.1.1.3.1"></times><ci id="S3.SS1.p3.1.m1.1.1.3.2.cmml" xref="S3.SS1.p3.1.m1.1.1.3.2">𝑐</ci><ci id="S3.SS1.p3.1.m1.1.1.3.3.cmml" xref="S3.SS1.p3.1.m1.1.1.3.3">𝑎</ci><ci id="S3.SS1.p3.1.m1.1.1.3.4.cmml" xref="S3.SS1.p3.1.m1.1.1.3.4">𝑝</ci><ci id="S3.SS1.p3.1.m1.1.1.3.5.cmml" xref="S3.SS1.p3.1.m1.1.1.3.5">𝑡</ci><ci id="S3.SS1.p3.1.m1.1.1.3.6.cmml" xref="S3.SS1.p3.1.m1.1.1.3.6">𝑖</ci><ci id="S3.SS1.p3.1.m1.1.1.3.7.cmml" xref="S3.SS1.p3.1.m1.1.1.3.7">𝑜</ci><ci id="S3.SS1.p3.1.m1.1.1.3.8.cmml" xref="S3.SS1.p3.1.m1.1.1.3.8">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.1.m1.1c">{D}_{{caption}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_c italic_a italic_p italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT</annotation></semantics></math>) and sound event tags (<math alttext="{D}_{{tag}}" class="ltx_Math" display="inline" id="S3.SS1.p3.2.m2.1"><semantics id="S3.SS1.p3.2.m2.1a"><msub id="S3.SS1.p3.2.m2.1.1" xref="S3.SS1.p3.2.m2.1.1.cmml"><mi id="S3.SS1.p3.2.m2.1.1.2" xref="S3.SS1.p3.2.m2.1.1.2.cmml">D</mi><mrow id="S3.SS1.p3.2.m2.1.1.3" xref="S3.SS1.p3.2.m2.1.1.3.cmml"><mi id="S3.SS1.p3.2.m2.1.1.3.2" xref="S3.SS1.p3.2.m2.1.1.3.2.cmml">t</mi><mo id="S3.SS1.p3.2.m2.1.1.3.1" xref="S3.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.2.m2.1.1.3.3" xref="S3.SS1.p3.2.m2.1.1.3.3.cmml">a</mi><mo id="S3.SS1.p3.2.m2.1.1.3.1a" xref="S3.SS1.p3.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.2.m2.1.1.3.4" xref="S3.SS1.p3.2.m2.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.2.m2.1b"><apply id="S3.SS1.p3.2.m2.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.2.m2.1.1.1.cmml" xref="S3.SS1.p3.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.p3.2.m2.1.1.2.cmml" xref="S3.SS1.p3.2.m2.1.1.2">𝐷</ci><apply id="S3.SS1.p3.2.m2.1.1.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3"><times id="S3.SS1.p3.2.m2.1.1.3.1.cmml" xref="S3.SS1.p3.2.m2.1.1.3.1"></times><ci id="S3.SS1.p3.2.m2.1.1.3.2.cmml" xref="S3.SS1.p3.2.m2.1.1.3.2">𝑡</ci><ci id="S3.SS1.p3.2.m2.1.1.3.3.cmml" xref="S3.SS1.p3.2.m2.1.1.3.3">𝑎</ci><ci id="S3.SS1.p3.2.m2.1.1.3.4.cmml" xref="S3.SS1.p3.2.m2.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.2.m2.1c">{D}_{{tag}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_t italic_a italic_g end_POSTSUBSCRIPT</annotation></semantics></math>).
To further minimize textual discrepancies and eliminate the need for manually designing task-specific question-answer pairs for formatting training data, we introduce a structured format called the seed prompt (<math alttext="{P}_{seed}" class="ltx_Math" display="inline" id="S3.SS1.p3.3.m3.1"><semantics id="S3.SS1.p3.3.m3.1a"><msub id="S3.SS1.p3.3.m3.1.1" xref="S3.SS1.p3.3.m3.1.1.cmml"><mi id="S3.SS1.p3.3.m3.1.1.2" xref="S3.SS1.p3.3.m3.1.1.2.cmml">P</mi><mrow id="S3.SS1.p3.3.m3.1.1.3" xref="S3.SS1.p3.3.m3.1.1.3.cmml"><mi id="S3.SS1.p3.3.m3.1.1.3.2" xref="S3.SS1.p3.3.m3.1.1.3.2.cmml">s</mi><mo id="S3.SS1.p3.3.m3.1.1.3.1" xref="S3.SS1.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.3.m3.1.1.3.3" xref="S3.SS1.p3.3.m3.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p3.3.m3.1.1.3.1a" xref="S3.SS1.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.3.m3.1.1.3.4" xref="S3.SS1.p3.3.m3.1.1.3.4.cmml">e</mi><mo id="S3.SS1.p3.3.m3.1.1.3.1b" xref="S3.SS1.p3.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.3.m3.1.1.3.5" xref="S3.SS1.p3.3.m3.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.3.m3.1b"><apply id="S3.SS1.p3.3.m3.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.3.m3.1.1.1.cmml" xref="S3.SS1.p3.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.p3.3.m3.1.1.2.cmml" xref="S3.SS1.p3.3.m3.1.1.2">𝑃</ci><apply id="S3.SS1.p3.3.m3.1.1.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3"><times id="S3.SS1.p3.3.m3.1.1.3.1.cmml" xref="S3.SS1.p3.3.m3.1.1.3.1"></times><ci id="S3.SS1.p3.3.m3.1.1.3.2.cmml" xref="S3.SS1.p3.3.m3.1.1.3.2">𝑠</ci><ci id="S3.SS1.p3.3.m3.1.1.3.3.cmml" xref="S3.SS1.p3.3.m3.1.1.3.3">𝑒</ci><ci id="S3.SS1.p3.3.m3.1.1.3.4.cmml" xref="S3.SS1.p3.3.m3.1.1.3.4">𝑒</ci><ci id="S3.SS1.p3.3.m3.1.1.3.5.cmml" xref="S3.SS1.p3.3.m3.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.3.m3.1c">{P}_{seed}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.3.m3.1d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d end_POSTSUBSCRIPT</annotation></semantics></math>).
The seed prompt is derived from the original dataset’s annotations and can take the form of either audio captions or sound event tags, depending on the available metadata.
We then append generation prompts (<math alttext="{P}_{gen}" class="ltx_Math" display="inline" id="S3.SS1.p3.4.m4.1"><semantics id="S3.SS1.p3.4.m4.1a"><msub id="S3.SS1.p3.4.m4.1.1" xref="S3.SS1.p3.4.m4.1.1.cmml"><mi id="S3.SS1.p3.4.m4.1.1.2" xref="S3.SS1.p3.4.m4.1.1.2.cmml">P</mi><mrow id="S3.SS1.p3.4.m4.1.1.3" xref="S3.SS1.p3.4.m4.1.1.3.cmml"><mi id="S3.SS1.p3.4.m4.1.1.3.2" xref="S3.SS1.p3.4.m4.1.1.3.2.cmml">g</mi><mo id="S3.SS1.p3.4.m4.1.1.3.1" xref="S3.SS1.p3.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.4.m4.1.1.3.3" xref="S3.SS1.p3.4.m4.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p3.4.m4.1.1.3.1a" xref="S3.SS1.p3.4.m4.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p3.4.m4.1.1.3.4" xref="S3.SS1.p3.4.m4.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p3.4.m4.1b"><apply id="S3.SS1.p3.4.m4.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p3.4.m4.1.1.1.cmml" xref="S3.SS1.p3.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.p3.4.m4.1.1.2.cmml" xref="S3.SS1.p3.4.m4.1.1.2">𝑃</ci><apply id="S3.SS1.p3.4.m4.1.1.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3"><times id="S3.SS1.p3.4.m4.1.1.3.1.cmml" xref="S3.SS1.p3.4.m4.1.1.3.1"></times><ci id="S3.SS1.p3.4.m4.1.1.3.2.cmml" xref="S3.SS1.p3.4.m4.1.1.3.2">𝑔</ci><ci id="S3.SS1.p3.4.m4.1.1.3.3.cmml" xref="S3.SS1.p3.4.m4.1.1.3.3">𝑒</ci><ci id="S3.SS1.p3.4.m4.1.1.3.4.cmml" xref="S3.SS1.p3.4.m4.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p3.4.m4.1c">{P}_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p3.4.m4.1d">italic_P start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math>) to the seed prompt and feed them into the LLM, prompting it to generate descriptions aligned with the audio content.
There are three types of generation prompts:</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p4">
<p class="ltx_p" id="S3.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p4.1.1">Positive Samples Generation Prompt (<math alttext="{P}_{{pos}}" class="ltx_Math" display="inline" id="S3.SS1.p4.1.1.m1.1"><semantics id="S3.SS1.p4.1.1.m1.1a"><msub id="S3.SS1.p4.1.1.m1.1.1" xref="S3.SS1.p4.1.1.m1.1.1.cmml"><mi id="S3.SS1.p4.1.1.m1.1.1.2" xref="S3.SS1.p4.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.SS1.p4.1.1.m1.1.1.3" xref="S3.SS1.p4.1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p4.1.1.m1.1.1.3.2" xref="S3.SS1.p4.1.1.m1.1.1.3.2.cmml">p</mi><mo id="S3.SS1.p4.1.1.m1.1.1.3.1" xref="S3.SS1.p4.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.1.1.m1.1.1.3.3" xref="S3.SS1.p4.1.1.m1.1.1.3.3.cmml">o</mi><mo id="S3.SS1.p4.1.1.m1.1.1.3.1a" xref="S3.SS1.p4.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p4.1.1.m1.1.1.3.4" xref="S3.SS1.p4.1.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p4.1.1.m1.1b"><apply id="S3.SS1.p4.1.1.m1.1.1.cmml" xref="S3.SS1.p4.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p4.1.1.m1.1.1.1.cmml" xref="S3.SS1.p4.1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p4.1.1.m1.1.1.2.cmml" xref="S3.SS1.p4.1.1.m1.1.1.2">𝑃</ci><apply id="S3.SS1.p4.1.1.m1.1.1.3.cmml" xref="S3.SS1.p4.1.1.m1.1.1.3"><times id="S3.SS1.p4.1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p4.1.1.m1.1.1.3.1"></times><ci id="S3.SS1.p4.1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p4.1.1.m1.1.1.3.2">𝑝</ci><ci id="S3.SS1.p4.1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p4.1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS1.p4.1.1.m1.1.1.3.4.cmml" xref="S3.SS1.p4.1.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p4.1.1.m1.1c">{P}_{{pos}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p4.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT</annotation></semantics></math>)</span> aims to generate descriptions of sound events that actually occur in the audio.
For example, <span class="ltx_text ltx_font_italic" id="S3.SS1.p4.1.2">Replay the audio</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p5">
<p class="ltx_p" id="S3.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p5.1.1">Negative Samples Generation Prompt (<math alttext="{P}_{{neg}}" class="ltx_Math" display="inline" id="S3.SS1.p5.1.1.m1.1"><semantics id="S3.SS1.p5.1.1.m1.1a"><msub id="S3.SS1.p5.1.1.m1.1.1" xref="S3.SS1.p5.1.1.m1.1.1.cmml"><mi id="S3.SS1.p5.1.1.m1.1.1.2" xref="S3.SS1.p5.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.SS1.p5.1.1.m1.1.1.3" xref="S3.SS1.p5.1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p5.1.1.m1.1.1.3.2" xref="S3.SS1.p5.1.1.m1.1.1.3.2.cmml">n</mi><mo id="S3.SS1.p5.1.1.m1.1.1.3.1" xref="S3.SS1.p5.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p5.1.1.m1.1.1.3.3" xref="S3.SS1.p5.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p5.1.1.m1.1.1.3.1a" xref="S3.SS1.p5.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p5.1.1.m1.1.1.3.4" xref="S3.SS1.p5.1.1.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p5.1.1.m1.1b"><apply id="S3.SS1.p5.1.1.m1.1.1.cmml" xref="S3.SS1.p5.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p5.1.1.m1.1.1.1.cmml" xref="S3.SS1.p5.1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p5.1.1.m1.1.1.2.cmml" xref="S3.SS1.p5.1.1.m1.1.1.2">𝑃</ci><apply id="S3.SS1.p5.1.1.m1.1.1.3.cmml" xref="S3.SS1.p5.1.1.m1.1.1.3"><times id="S3.SS1.p5.1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p5.1.1.m1.1.1.3.1"></times><ci id="S3.SS1.p5.1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p5.1.1.m1.1.1.3.2">𝑛</ci><ci id="S3.SS1.p5.1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p5.1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS1.p5.1.1.m1.1.1.3.4.cmml" xref="S3.SS1.p5.1.1.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p5.1.1.m1.1c">{P}_{{neg}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p5.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_n italic_e italic_g end_POSTSUBSCRIPT</annotation></semantics></math>)</span> aims to generate descriptions of sound events that are not present in the audio.
For example, <span class="ltx_text ltx_font_italic" id="S3.SS1.p5.1.2">Identify sounds that are absent as contrasting examples</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS1.p6">
<p class="ltx_p" id="S3.SS1.p6.1"><span class="ltx_text ltx_font_bold" id="S3.SS1.p6.1.1">Combined Samples Generation Prompt (<math alttext="{P}_{{comb}}" class="ltx_Math" display="inline" id="S3.SS1.p6.1.1.m1.1"><semantics id="S3.SS1.p6.1.1.m1.1a"><msub id="S3.SS1.p6.1.1.m1.1.1" xref="S3.SS1.p6.1.1.m1.1.1.cmml"><mi id="S3.SS1.p6.1.1.m1.1.1.2" xref="S3.SS1.p6.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.SS1.p6.1.1.m1.1.1.3" xref="S3.SS1.p6.1.1.m1.1.1.3.cmml"><mi id="S3.SS1.p6.1.1.m1.1.1.3.2" xref="S3.SS1.p6.1.1.m1.1.1.3.2.cmml">c</mi><mo id="S3.SS1.p6.1.1.m1.1.1.3.1" xref="S3.SS1.p6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p6.1.1.m1.1.1.3.3" xref="S3.SS1.p6.1.1.m1.1.1.3.3.cmml">o</mi><mo id="S3.SS1.p6.1.1.m1.1.1.3.1a" xref="S3.SS1.p6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p6.1.1.m1.1.1.3.4" xref="S3.SS1.p6.1.1.m1.1.1.3.4.cmml">m</mi><mo id="S3.SS1.p6.1.1.m1.1.1.3.1b" xref="S3.SS1.p6.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p6.1.1.m1.1.1.3.5" xref="S3.SS1.p6.1.1.m1.1.1.3.5.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p6.1.1.m1.1b"><apply id="S3.SS1.p6.1.1.m1.1.1.cmml" xref="S3.SS1.p6.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p6.1.1.m1.1.1.1.cmml" xref="S3.SS1.p6.1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p6.1.1.m1.1.1.2.cmml" xref="S3.SS1.p6.1.1.m1.1.1.2">𝑃</ci><apply id="S3.SS1.p6.1.1.m1.1.1.3.cmml" xref="S3.SS1.p6.1.1.m1.1.1.3"><times id="S3.SS1.p6.1.1.m1.1.1.3.1.cmml" xref="S3.SS1.p6.1.1.m1.1.1.3.1"></times><ci id="S3.SS1.p6.1.1.m1.1.1.3.2.cmml" xref="S3.SS1.p6.1.1.m1.1.1.3.2">𝑐</ci><ci id="S3.SS1.p6.1.1.m1.1.1.3.3.cmml" xref="S3.SS1.p6.1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.SS1.p6.1.1.m1.1.1.3.4.cmml" xref="S3.SS1.p6.1.1.m1.1.1.3.4">𝑚</ci><ci id="S3.SS1.p6.1.1.m1.1.1.3.5.cmml" xref="S3.SS1.p6.1.1.m1.1.1.3.5">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p6.1.1.m1.1c">{P}_{{comb}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p6.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_c italic_o italic_m italic_b end_POSTSUBSCRIPT</annotation></semantics></math>)</span> integrates both of the above prompts.
It aims to generate descriptions of both the sound events that are present and those that are absent in the audio.
For example, <span class="ltx_text ltx_font_italic" id="S3.SS1.p6.1.2">Replay the audio and identify sounds that are absent as contrasting examples</span>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p7">
<p class="ltx_p" id="S3.SS1.p7.1">In summary, the final input prompt (<math alttext="{P}_{{final}}" class="ltx_Math" display="inline" id="S3.SS1.p7.1.m1.1"><semantics id="S3.SS1.p7.1.m1.1a"><msub id="S3.SS1.p7.1.m1.1.1" xref="S3.SS1.p7.1.m1.1.1.cmml"><mi id="S3.SS1.p7.1.m1.1.1.2" xref="S3.SS1.p7.1.m1.1.1.2.cmml">P</mi><mrow id="S3.SS1.p7.1.m1.1.1.3" xref="S3.SS1.p7.1.m1.1.1.3.cmml"><mi id="S3.SS1.p7.1.m1.1.1.3.2" xref="S3.SS1.p7.1.m1.1.1.3.2.cmml">f</mi><mo id="S3.SS1.p7.1.m1.1.1.3.1" xref="S3.SS1.p7.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p7.1.m1.1.1.3.3" xref="S3.SS1.p7.1.m1.1.1.3.3.cmml">i</mi><mo id="S3.SS1.p7.1.m1.1.1.3.1a" xref="S3.SS1.p7.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p7.1.m1.1.1.3.4" xref="S3.SS1.p7.1.m1.1.1.3.4.cmml">n</mi><mo id="S3.SS1.p7.1.m1.1.1.3.1b" xref="S3.SS1.p7.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p7.1.m1.1.1.3.5" xref="S3.SS1.p7.1.m1.1.1.3.5.cmml">a</mi><mo id="S3.SS1.p7.1.m1.1.1.3.1c" xref="S3.SS1.p7.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p7.1.m1.1.1.3.6" xref="S3.SS1.p7.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p7.1.m1.1b"><apply id="S3.SS1.p7.1.m1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p7.1.m1.1.1.1.cmml" xref="S3.SS1.p7.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p7.1.m1.1.1.2.cmml" xref="S3.SS1.p7.1.m1.1.1.2">𝑃</ci><apply id="S3.SS1.p7.1.m1.1.1.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3"><times id="S3.SS1.p7.1.m1.1.1.3.1.cmml" xref="S3.SS1.p7.1.m1.1.1.3.1"></times><ci id="S3.SS1.p7.1.m1.1.1.3.2.cmml" xref="S3.SS1.p7.1.m1.1.1.3.2">𝑓</ci><ci id="S3.SS1.p7.1.m1.1.1.3.3.cmml" xref="S3.SS1.p7.1.m1.1.1.3.3">𝑖</ci><ci id="S3.SS1.p7.1.m1.1.1.3.4.cmml" xref="S3.SS1.p7.1.m1.1.1.3.4">𝑛</ci><ci id="S3.SS1.p7.1.m1.1.1.3.5.cmml" xref="S3.SS1.p7.1.m1.1.1.3.5">𝑎</ci><ci id="S3.SS1.p7.1.m1.1.1.3.6.cmml" xref="S3.SS1.p7.1.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p7.1.m1.1c">{P}_{{final}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p7.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>) to the LLM can be expressed in Equation <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.E1" title="In III-A Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a> and the overall pipeline is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.F1" title="Figure 1 ‣ III-A Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS1.p8">
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E1">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P_{final}=" class="ltx_Math" display="inline" id="S3.E1X.2.1.1.m1.1"><semantics id="S3.E1X.2.1.1.m1.1a"><mrow id="S3.E1X.2.1.1.m1.1.1" xref="S3.E1X.2.1.1.m1.1.1.cmml"><msub id="S3.E1X.2.1.1.m1.1.1.2" xref="S3.E1X.2.1.1.m1.1.1.2.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.2.2" xref="S3.E1X.2.1.1.m1.1.1.2.2.cmml">P</mi><mrow id="S3.E1X.2.1.1.m1.1.1.2.3" xref="S3.E1X.2.1.1.m1.1.1.2.3.cmml"><mi id="S3.E1X.2.1.1.m1.1.1.2.3.2" xref="S3.E1X.2.1.1.m1.1.1.2.3.2.cmml">f</mi><mo id="S3.E1X.2.1.1.m1.1.1.2.3.1" xref="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E1X.2.1.1.m1.1.1.2.3.3" xref="S3.E1X.2.1.1.m1.1.1.2.3.3.cmml">i</mi><mo id="S3.E1X.2.1.1.m1.1.1.2.3.1a" xref="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E1X.2.1.1.m1.1.1.2.3.4" xref="S3.E1X.2.1.1.m1.1.1.2.3.4.cmml">n</mi><mo id="S3.E1X.2.1.1.m1.1.1.2.3.1b" xref="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E1X.2.1.1.m1.1.1.2.3.5" xref="S3.E1X.2.1.1.m1.1.1.2.3.5.cmml">a</mi><mo id="S3.E1X.2.1.1.m1.1.1.2.3.1c" xref="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S3.E1X.2.1.1.m1.1.1.2.3.6" xref="S3.E1X.2.1.1.m1.1.1.2.3.6.cmml">l</mi></mrow></msub><mo id="S3.E1X.2.1.1.m1.1.1.1" xref="S3.E1X.2.1.1.m1.1.1.1.cmml">=</mo><mi id="S3.E1X.2.1.1.m1.1.1.3" xref="S3.E1X.2.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.2.1.1.m1.1b"><apply id="S3.E1X.2.1.1.m1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1"><eq id="S3.E1X.2.1.1.m1.1.1.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.1"></eq><apply id="S3.E1X.2.1.1.m1.1.1.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E1X.2.1.1.m1.1.1.2.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.2">subscript</csymbol><ci id="S3.E1X.2.1.1.m1.1.1.2.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.2">𝑃</ci><apply id="S3.E1X.2.1.1.m1.1.1.2.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3"><times id="S3.E1X.2.1.1.m1.1.1.2.3.1.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.1"></times><ci id="S3.E1X.2.1.1.m1.1.1.2.3.2.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.2">𝑓</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.3">𝑖</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.4.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.4">𝑛</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.5.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.5">𝑎</ci><ci id="S3.E1X.2.1.1.m1.1.1.2.3.6.cmml" xref="S3.E1X.2.1.1.m1.1.1.2.3.6">𝑙</ci></apply></apply><csymbol cd="latexml" id="S3.E1X.2.1.1.m1.1.1.3.cmml" xref="S3.E1X.2.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.2.1.1.m1.1c">\displaystyle P_{final}=</annotation><annotation encoding="application/x-llamapun" id="S3.E1X.2.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT =</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\,[\textit{Begin of audio}]\,P_{seed}\,[\textit{End of audio}]\,P%
_{gen}" class="ltx_Math" display="inline" id="S3.E1X.3.2.2.m1.2"><semantics id="S3.E1X.3.2.2.m1.2a"><mrow id="S3.E1X.3.2.2.m1.2.3" xref="S3.E1X.3.2.2.m1.2.3.cmml"><mrow id="S3.E1X.3.2.2.m1.2.3.2.2" xref="S3.E1X.3.2.2.m1.2.3.2.1.cmml"><mo id="S3.E1X.3.2.2.m1.2.3.2.2.1" stretchy="false" xref="S3.E1X.3.2.2.m1.2.3.2.1.1.cmml">[</mo><mtext class="ltx_mathvariant_italic" id="S3.E1X.3.2.2.m1.1.1" xref="S3.E1X.3.2.2.m1.1.1a.cmml">Begin of audio</mtext><mo id="S3.E1X.3.2.2.m1.2.3.2.2.2" stretchy="false" xref="S3.E1X.3.2.2.m1.2.3.2.1.1.cmml">]</mo></mrow><mo id="S3.E1X.3.2.2.m1.2.3.1" lspace="0.170em" xref="S3.E1X.3.2.2.m1.2.3.1.cmml">⁢</mo><msub id="S3.E1X.3.2.2.m1.2.3.3" xref="S3.E1X.3.2.2.m1.2.3.3.cmml"><mi id="S3.E1X.3.2.2.m1.2.3.3.2" xref="S3.E1X.3.2.2.m1.2.3.3.2.cmml">P</mi><mrow id="S3.E1X.3.2.2.m1.2.3.3.3" xref="S3.E1X.3.2.2.m1.2.3.3.3.cmml"><mi id="S3.E1X.3.2.2.m1.2.3.3.3.2" xref="S3.E1X.3.2.2.m1.2.3.3.3.2.cmml">s</mi><mo id="S3.E1X.3.2.2.m1.2.3.3.3.1" xref="S3.E1X.3.2.2.m1.2.3.3.3.1.cmml">⁢</mo><mi id="S3.E1X.3.2.2.m1.2.3.3.3.3" xref="S3.E1X.3.2.2.m1.2.3.3.3.3.cmml">e</mi><mo id="S3.E1X.3.2.2.m1.2.3.3.3.1a" xref="S3.E1X.3.2.2.m1.2.3.3.3.1.cmml">⁢</mo><mi id="S3.E1X.3.2.2.m1.2.3.3.3.4" xref="S3.E1X.3.2.2.m1.2.3.3.3.4.cmml">e</mi><mo id="S3.E1X.3.2.2.m1.2.3.3.3.1b" xref="S3.E1X.3.2.2.m1.2.3.3.3.1.cmml">⁢</mo><mi id="S3.E1X.3.2.2.m1.2.3.3.3.5" xref="S3.E1X.3.2.2.m1.2.3.3.3.5.cmml">d</mi></mrow></msub><mo id="S3.E1X.3.2.2.m1.2.3.1a" xref="S3.E1X.3.2.2.m1.2.3.1.cmml">⁢</mo><mrow id="S3.E1X.3.2.2.m1.2.3.4.2" xref="S3.E1X.3.2.2.m1.2.3.4.1.cmml"><mo id="S3.E1X.3.2.2.m1.2.3.4.2.1" stretchy="false" xref="S3.E1X.3.2.2.m1.2.3.4.1.1.cmml">[</mo><mtext class="ltx_mathvariant_italic" id="S3.E1X.3.2.2.m1.2.2" xref="S3.E1X.3.2.2.m1.2.2a.cmml">End of audio</mtext><mo id="S3.E1X.3.2.2.m1.2.3.4.2.2" stretchy="false" xref="S3.E1X.3.2.2.m1.2.3.4.1.1.cmml">]</mo></mrow><mo id="S3.E1X.3.2.2.m1.2.3.1b" lspace="0.170em" xref="S3.E1X.3.2.2.m1.2.3.1.cmml">⁢</mo><msub id="S3.E1X.3.2.2.m1.2.3.5" xref="S3.E1X.3.2.2.m1.2.3.5.cmml"><mi id="S3.E1X.3.2.2.m1.2.3.5.2" xref="S3.E1X.3.2.2.m1.2.3.5.2.cmml">P</mi><mrow id="S3.E1X.3.2.2.m1.2.3.5.3" xref="S3.E1X.3.2.2.m1.2.3.5.3.cmml"><mi id="S3.E1X.3.2.2.m1.2.3.5.3.2" xref="S3.E1X.3.2.2.m1.2.3.5.3.2.cmml">g</mi><mo id="S3.E1X.3.2.2.m1.2.3.5.3.1" xref="S3.E1X.3.2.2.m1.2.3.5.3.1.cmml">⁢</mo><mi id="S3.E1X.3.2.2.m1.2.3.5.3.3" xref="S3.E1X.3.2.2.m1.2.3.5.3.3.cmml">e</mi><mo id="S3.E1X.3.2.2.m1.2.3.5.3.1a" xref="S3.E1X.3.2.2.m1.2.3.5.3.1.cmml">⁢</mo><mi id="S3.E1X.3.2.2.m1.2.3.5.3.4" xref="S3.E1X.3.2.2.m1.2.3.5.3.4.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.E1X.3.2.2.m1.2b"><apply id="S3.E1X.3.2.2.m1.2.3.cmml" xref="S3.E1X.3.2.2.m1.2.3"><times id="S3.E1X.3.2.2.m1.2.3.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.1"></times><apply id="S3.E1X.3.2.2.m1.2.3.2.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.2.2"><csymbol cd="latexml" id="S3.E1X.3.2.2.m1.2.3.2.1.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.2.2.1">delimited-[]</csymbol><ci id="S3.E1X.3.2.2.m1.1.1a.cmml" xref="S3.E1X.3.2.2.m1.1.1"><mtext class="ltx_mathvariant_italic" id="S3.E1X.3.2.2.m1.1.1.cmml" xref="S3.E1X.3.2.2.m1.1.1">Begin of audio</mtext></ci></apply><apply id="S3.E1X.3.2.2.m1.2.3.3.cmml" xref="S3.E1X.3.2.2.m1.2.3.3"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.2.3.3.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.3">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.2.3.3.2.cmml" xref="S3.E1X.3.2.2.m1.2.3.3.2">𝑃</ci><apply id="S3.E1X.3.2.2.m1.2.3.3.3.cmml" xref="S3.E1X.3.2.2.m1.2.3.3.3"><times id="S3.E1X.3.2.2.m1.2.3.3.3.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.3.3.1"></times><ci id="S3.E1X.3.2.2.m1.2.3.3.3.2.cmml" xref="S3.E1X.3.2.2.m1.2.3.3.3.2">𝑠</ci><ci id="S3.E1X.3.2.2.m1.2.3.3.3.3.cmml" xref="S3.E1X.3.2.2.m1.2.3.3.3.3">𝑒</ci><ci id="S3.E1X.3.2.2.m1.2.3.3.3.4.cmml" xref="S3.E1X.3.2.2.m1.2.3.3.3.4">𝑒</ci><ci id="S3.E1X.3.2.2.m1.2.3.3.3.5.cmml" xref="S3.E1X.3.2.2.m1.2.3.3.3.5">𝑑</ci></apply></apply><apply id="S3.E1X.3.2.2.m1.2.3.4.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.4.2"><csymbol cd="latexml" id="S3.E1X.3.2.2.m1.2.3.4.1.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.4.2.1">delimited-[]</csymbol><ci id="S3.E1X.3.2.2.m1.2.2a.cmml" xref="S3.E1X.3.2.2.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S3.E1X.3.2.2.m1.2.2.cmml" xref="S3.E1X.3.2.2.m1.2.2">End of audio</mtext></ci></apply><apply id="S3.E1X.3.2.2.m1.2.3.5.cmml" xref="S3.E1X.3.2.2.m1.2.3.5"><csymbol cd="ambiguous" id="S3.E1X.3.2.2.m1.2.3.5.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.5">subscript</csymbol><ci id="S3.E1X.3.2.2.m1.2.3.5.2.cmml" xref="S3.E1X.3.2.2.m1.2.3.5.2">𝑃</ci><apply id="S3.E1X.3.2.2.m1.2.3.5.3.cmml" xref="S3.E1X.3.2.2.m1.2.3.5.3"><times id="S3.E1X.3.2.2.m1.2.3.5.3.1.cmml" xref="S3.E1X.3.2.2.m1.2.3.5.3.1"></times><ci id="S3.E1X.3.2.2.m1.2.3.5.3.2.cmml" xref="S3.E1X.3.2.2.m1.2.3.5.3.2">𝑔</ci><ci id="S3.E1X.3.2.2.m1.2.3.5.3.3.cmml" xref="S3.E1X.3.2.2.m1.2.3.5.3.3">𝑒</ci><ci id="S3.E1X.3.2.2.m1.2.3.5.3.4.cmml" xref="S3.E1X.3.2.2.m1.2.3.5.3.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1X.3.2.2.m1.2c">\displaystyle\,[\textit{Begin of audio}]\,P_{seed}\,[\textit{End of audio}]\,P%
_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.E1X.3.2.2.m1.2d">[ Begin of audio ] italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d end_POSTSUBSCRIPT [ End of audio ] italic_P start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(1)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle P_{seed}\in\{D_{caption},D_{tag}\}" class="ltx_Math" display="inline" id="S3.E1Xa.2.1.1.m1.2"><semantics id="S3.E1Xa.2.1.1.m1.2a"><mrow id="S3.E1Xa.2.1.1.m1.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.cmml"><msub id="S3.E1Xa.2.1.1.m1.2.2.4" xref="S3.E1Xa.2.1.1.m1.2.2.4.cmml"><mi id="S3.E1Xa.2.1.1.m1.2.2.4.2" xref="S3.E1Xa.2.1.1.m1.2.2.4.2.cmml">P</mi><mrow id="S3.E1Xa.2.1.1.m1.2.2.4.3" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.cmml"><mi id="S3.E1Xa.2.1.1.m1.2.2.4.3.2" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.2.cmml">s</mi><mo id="S3.E1Xa.2.1.1.m1.2.2.4.3.1" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.2.2.4.3.3" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.3.cmml">e</mi><mo id="S3.E1Xa.2.1.1.m1.2.2.4.3.1a" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.2.2.4.3.4" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.4.cmml">e</mi><mo id="S3.E1Xa.2.1.1.m1.2.2.4.3.1b" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.2.2.4.3.5" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.5.cmml">d</mi></mrow></msub><mo id="S3.E1Xa.2.1.1.m1.2.2.3" xref="S3.E1Xa.2.1.1.m1.2.2.3.cmml">∈</mo><mrow id="S3.E1Xa.2.1.1.m1.2.2.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.2.3.cmml"><mo id="S3.E1Xa.2.1.1.m1.2.2.2.2.3" stretchy="false" xref="S3.E1Xa.2.1.1.m1.2.2.2.3.cmml">{</mo><msub id="S3.E1Xa.2.1.1.m1.1.1.1.1.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2.cmml">D</mi><mrow id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.2" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.2.cmml">c</mi><mo id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.3" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.3.cmml">a</mi><mo id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1a" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.4" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.4.cmml">p</mi><mo id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1b" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.5" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.5.cmml">t</mi><mo id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1c" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.6" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.6.cmml">i</mi><mo id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1d" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.7" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.7.cmml">o</mi><mo id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1e" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.8" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.8.cmml">n</mi></mrow></msub><mo id="S3.E1Xa.2.1.1.m1.2.2.2.2.4" xref="S3.E1Xa.2.1.1.m1.2.2.2.3.cmml">,</mo><msub id="S3.E1Xa.2.1.1.m1.2.2.2.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.2" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.2.cmml">D</mi><mrow id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.cmml"><mi id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.2" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml">t</mi><mo id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.1" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.3" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.3.cmml">a</mi><mo id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.1a" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.4" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.4.cmml">g</mi></mrow></msub><mo id="S3.E1Xa.2.1.1.m1.2.2.2.2.5" stretchy="false" xref="S3.E1Xa.2.1.1.m1.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xa.2.1.1.m1.2b"><apply id="S3.E1Xa.2.1.1.m1.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2"><in id="S3.E1Xa.2.1.1.m1.2.2.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.3"></in><apply id="S3.E1Xa.2.1.1.m1.2.2.4.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.2.2.4.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4">subscript</csymbol><ci id="S3.E1Xa.2.1.1.m1.2.2.4.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4.2">𝑃</ci><apply id="S3.E1Xa.2.1.1.m1.2.2.4.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4.3"><times id="S3.E1Xa.2.1.1.m1.2.2.4.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.1"></times><ci id="S3.E1Xa.2.1.1.m1.2.2.4.3.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.2">𝑠</ci><ci id="S3.E1Xa.2.1.1.m1.2.2.4.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.3">𝑒</ci><ci id="S3.E1Xa.2.1.1.m1.2.2.4.3.4.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.4">𝑒</ci><ci id="S3.E1Xa.2.1.1.m1.2.2.4.3.5.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.4.3.5">𝑑</ci></apply></apply><set id="S3.E1Xa.2.1.1.m1.2.2.2.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2"><apply id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.2">𝐷</ci><apply id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3"><times id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.1"></times><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.2">𝑐</ci><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.3">𝑎</ci><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.4.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.4">𝑝</ci><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.5.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.5">𝑡</ci><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.6.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.6">𝑖</ci><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.7.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.7">𝑜</ci><ci id="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.8.cmml" xref="S3.E1Xa.2.1.1.m1.1.1.1.1.1.3.8">𝑛</ci></apply></apply><apply id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.2">𝐷</ci><apply id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3"><times id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.1.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.1"></times><ci id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.2">𝑡</ci><ci id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.3.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.3">𝑎</ci><ci id="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.4.cmml" xref="S3.E1Xa.2.1.1.m1.2.2.2.2.2.3.4">𝑔</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xa.2.1.1.m1.2c">\displaystyle P_{seed}\in\{D_{caption},D_{tag}\}</annotation><annotation encoding="application/x-llamapun" id="S3.E1Xa.2.1.1.m1.2d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d end_POSTSUBSCRIPT ∈ { italic_D start_POSTSUBSCRIPT italic_c italic_a italic_p italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_t italic_a italic_g end_POSTSUBSCRIPT }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E1Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle P_{gen}\in\{P_{pos},P_{neg},P_{comb}\}." class="ltx_Math" display="inline" id="S3.E1Xb.2.1.1.m1.1"><semantics id="S3.E1Xb.2.1.1.m1.1a"><mrow id="S3.E1Xb.2.1.1.m1.1.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.cmml"><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.cmml"><msub id="S3.E1Xb.2.1.1.m1.1.1.1.1.5" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.2.cmml">P</mi><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.2.cmml">g</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.3.cmml">e</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.1a" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.4" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.4.cmml">n</mi></mrow></msub><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.4" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.4.cmml">∈</mo><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.4.cmml"><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.4" stretchy="false" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.4.cmml">{</mo><msub id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.2.cmml">P</mi><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.cmml">p</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3.cmml">o</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1a" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.cmml">s</mi></mrow></msub><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.5" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.2.cmml">P</mi><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.2.cmml">n</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.3.cmml">e</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.1a" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.4" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.4.cmml">g</mi></mrow></msub><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.6" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.4.cmml">,</mo><msub id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.2.cmml">P</mi><mrow id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.cmml"><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.2" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.2.cmml">c</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.1" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.3" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.3.cmml">o</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.1a" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.4" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.4.cmml">m</mi><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.1b" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.1.cmml">⁢</mo><mi id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.5" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.5.cmml">b</mi></mrow></msub><mo id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.7" stretchy="false" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.4.cmml">}</mo></mrow></mrow><mo id="S3.E1Xb.2.1.1.m1.1.1.1.2" lspace="0em" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1Xb.2.1.1.m1.1b"><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1"><in id="S3.E1Xb.2.1.1.m1.1.1.1.1.4.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.4"></in><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5"><csymbol cd="ambiguous" id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5">subscript</csymbol><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.2">𝑃</ci><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3"><times id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.1"></times><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.2">𝑔</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.3">𝑒</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.4.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.5.3.4">𝑛</ci></apply></apply><set id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.4.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3"><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.2">𝑃</ci><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3"><times id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1"></times><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2">𝑝</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3">𝑜</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4">𝑠</ci></apply></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2">subscript</csymbol><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.2">𝑃</ci><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3"><times id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.1"></times><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.2">𝑛</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.3">𝑒</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.4.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.2.2.2.3.4">𝑔</ci></apply></apply><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3"><csymbol cd="ambiguous" id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3">subscript</csymbol><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.2">𝑃</ci><apply id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3"><times id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.1.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.1"></times><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.2.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.2">𝑐</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.3.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.3">𝑜</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.4.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.4">𝑚</ci><ci id="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.5.cmml" xref="S3.E1Xb.2.1.1.m1.1.1.1.1.3.3.3.3.5">𝑏</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1Xb.2.1.1.m1.1c">\displaystyle P_{gen}\in\{P_{pos},P_{neg},P_{comb}\}.</annotation><annotation encoding="application/x-llamapun" id="S3.E1Xb.2.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT ∈ { italic_P start_POSTSUBSCRIPT italic_p italic_o italic_s end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_n italic_e italic_g end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_c italic_o italic_m italic_b end_POSTSUBSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.p9">
<p class="ltx_p" id="S3.SS1.p9.1">In Equation <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.E1" title="In III-A Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>, <span class="ltx_text ltx_font_italic" id="S3.SS1.p9.1.1">[Begin of audio]</span> and <span class="ltx_text ltx_font_italic" id="S3.SS1.p9.1.2">[End of audio]</span> are expressed in natural language format, with the purpose of indicating to the model that the information between them represents the content from the audio.
When training audio-aware large language models, the position of <math alttext="{P}_{{seed}}" class="ltx_Math" display="inline" id="S3.SS1.p9.1.m1.1"><semantics id="S3.SS1.p9.1.m1.1a"><msub id="S3.SS1.p9.1.m1.1.1" xref="S3.SS1.p9.1.m1.1.1.cmml"><mi id="S3.SS1.p9.1.m1.1.1.2" xref="S3.SS1.p9.1.m1.1.1.2.cmml">P</mi><mrow id="S3.SS1.p9.1.m1.1.1.3" xref="S3.SS1.p9.1.m1.1.1.3.cmml"><mi id="S3.SS1.p9.1.m1.1.1.3.2" xref="S3.SS1.p9.1.m1.1.1.3.2.cmml">s</mi><mo id="S3.SS1.p9.1.m1.1.1.3.1" xref="S3.SS1.p9.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p9.1.m1.1.1.3.3" xref="S3.SS1.p9.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.SS1.p9.1.m1.1.1.3.1a" xref="S3.SS1.p9.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p9.1.m1.1.1.3.4" xref="S3.SS1.p9.1.m1.1.1.3.4.cmml">e</mi><mo id="S3.SS1.p9.1.m1.1.1.3.1b" xref="S3.SS1.p9.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS1.p9.1.m1.1.1.3.5" xref="S3.SS1.p9.1.m1.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.p9.1.m1.1b"><apply id="S3.SS1.p9.1.m1.1.1.cmml" xref="S3.SS1.p9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.p9.1.m1.1.1.1.cmml" xref="S3.SS1.p9.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.p9.1.m1.1.1.2.cmml" xref="S3.SS1.p9.1.m1.1.1.2">𝑃</ci><apply id="S3.SS1.p9.1.m1.1.1.3.cmml" xref="S3.SS1.p9.1.m1.1.1.3"><times id="S3.SS1.p9.1.m1.1.1.3.1.cmml" xref="S3.SS1.p9.1.m1.1.1.3.1"></times><ci id="S3.SS1.p9.1.m1.1.1.3.2.cmml" xref="S3.SS1.p9.1.m1.1.1.3.2">𝑠</ci><ci id="S3.SS1.p9.1.m1.1.1.3.3.cmml" xref="S3.SS1.p9.1.m1.1.1.3.3">𝑒</ci><ci id="S3.SS1.p9.1.m1.1.1.3.4.cmml" xref="S3.SS1.p9.1.m1.1.1.3.4">𝑒</ci><ci id="S3.SS1.p9.1.m1.1.1.3.5.cmml" xref="S3.SS1.p9.1.m1.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p9.1.m1.1c">{P}_{{seed}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p9.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d end_POSTSUBSCRIPT</annotation></semantics></math> will be replaced with corresponding audio representation.
These generated responses to the input context are then used as our training targets.
Actual examples are demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.T1" title="TABLE I ‣ III-A Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">I</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Examples produced under different generation prompts</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.3.4.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T1.3.4.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.4.1.1.1">
<span class="ltx_p" id="S3.T1.3.4.1.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.3.4.1.1.1.1.1" style="font-size:90%;">Input Audio:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.5.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.5.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.5.2.1.1">
<span class="ltx_p" id="S3.T1.3.5.2.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.5.2.1.1.1.1" style="font-size:90%;color:#14248A;">Audio Caption: A woman talks nearby as water pours.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.1.1.1.1">
<span class="ltx_p" id="S3.T1.1.1.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1.1.1" style="font-size:90%;">Positive Samples Generation Prompt (<math alttext="\bm{P}_{\bm{pos}}" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.1.1.m1.1a"><msub id="S3.T1.1.1.1.1.1.1.m1.1.1" xref="S3.T1.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.1.1.1.1.1.1.m1.1.1.2" xref="S3.T1.1.1.1.1.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.T1.1.1.1.1.1.1.m1.1.1.3" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.cmml"><mi id="S3.T1.1.1.1.1.1.1.m1.1.1.3.2" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.2.cmml">p</mi><mo id="S3.T1.1.1.1.1.1.1.m1.1.1.3.1" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T1.1.1.1.1.1.1.m1.1.1.3.3" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.3.cmml">o</mi><mo id="S3.T1.1.1.1.1.1.1.m1.1.1.3.1a" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T1.1.1.1.1.1.1.m1.1.1.3.4" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.1.1.m1.1b"><apply id="S3.T1.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T1.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.2">𝑃</ci><apply id="S3.T1.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3"><times id="S3.T1.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.1"></times><ci id="S3.T1.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.2">𝑝</ci><ci id="S3.T1.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.T1.1.1.1.1.1.1.m1.1.1.3.4.cmml" xref="S3.T1.1.1.1.1.1.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.1.1.m1.1c">\bm{P}_{\bm{pos}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.1.1.m1.1d">bold_italic_P start_POSTSUBSCRIPT bold_italic_p bold_italic_o bold_italic_s end_POSTSUBSCRIPT</annotation></semantics></math>):</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.6.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.6.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.6.3.1.1">
<span class="ltx_p" id="S3.T1.3.6.3.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.6.3.1.1.1.1" style="font-size:90%;">Replay the audio.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.7.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.7.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.7.4.1.1">
<span class="ltx_p" id="S3.T1.3.7.4.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.3.7.4.1.1.1.1" style="font-size:90%;">Response:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.8.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.8.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.8.5.1.1">
<span class="ltx_p" id="S3.T1.3.8.5.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.8.5.1.1.1.1" style="font-size:90%;">Water pouring sounds, woman talking in the background.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.2.2.1.1">
<span class="ltx_p" id="S3.T1.2.2.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.1.1.1.1" style="font-size:90%;">Negative Samples Generation Prompt (<math alttext="\bm{P}_{\bm{neg}}" class="ltx_Math" display="inline" id="S3.T1.2.2.1.1.1.1.m1.1"><semantics id="S3.T1.2.2.1.1.1.1.m1.1a"><msub id="S3.T1.2.2.1.1.1.1.m1.1.1" xref="S3.T1.2.2.1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.2.2.1.1.1.1.m1.1.1.2" xref="S3.T1.2.2.1.1.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.T1.2.2.1.1.1.1.m1.1.1.3" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.cmml"><mi id="S3.T1.2.2.1.1.1.1.m1.1.1.3.2" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.2.cmml">n</mi><mo id="S3.T1.2.2.1.1.1.1.m1.1.1.3.1" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T1.2.2.1.1.1.1.m1.1.1.3.3" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.T1.2.2.1.1.1.1.m1.1.1.3.1a" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T1.2.2.1.1.1.1.m1.1.1.3.4" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.1.1.1.1.m1.1b"><apply id="S3.T1.2.2.1.1.1.1.m1.1.1.cmml" xref="S3.T1.2.2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.2.2.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.2.2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T1.2.2.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.2.2.1.1.1.1.m1.1.1.2">𝑃</ci><apply id="S3.T1.2.2.1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3"><times id="S3.T1.2.2.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.1"></times><ci id="S3.T1.2.2.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.2">𝑛</ci><ci id="S3.T1.2.2.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.T1.2.2.1.1.1.1.m1.1.1.3.4.cmml" xref="S3.T1.2.2.1.1.1.1.m1.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.1.1.1.1.m1.1c">\bm{P}_{\bm{neg}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.1.1.1.1.m1.1d">bold_italic_P start_POSTSUBSCRIPT bold_italic_n bold_italic_e bold_italic_g end_POSTSUBSCRIPT</annotation></semantics></math>):</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.9.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.9.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.9.6.1.1">
<span class="ltx_p" id="S3.T1.3.9.6.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.9.6.1.1.1.1" style="font-size:90%;">Identify sounds that are absent as contrasting examples.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.10.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.10.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.10.7.1.1">
<span class="ltx_p" id="S3.T1.3.10.7.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.3.10.7.1.1.1.1" style="font-size:90%;">Response:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.11.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.11.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.11.8.1.1">
<span class="ltx_p" id="S3.T1.3.11.8.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.11.8.1.1.1.1" style="font-size:90%;">Based on the provided audio, here are some specific sound events that are not present in the audio:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.12.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.12.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.12.9.1.1">
<span class="ltx_p" id="S3.T1.3.12.9.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.12.9.1.1.1.1" style="font-size:90%;">1. A car driving by   2. Birds chirping   3. A dog barking</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T1.3.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.3.1.1">
<span class="ltx_p" id="S3.T1.3.3.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.1.1.1.1" style="font-size:90%;">Combined Samples Generation Prompt (<math alttext="\bm{P}_{\bm{comb}}" class="ltx_Math" display="inline" id="S3.T1.3.3.1.1.1.1.m1.1"><semantics id="S3.T1.3.3.1.1.1.1.m1.1a"><msub id="S3.T1.3.3.1.1.1.1.m1.1.1" xref="S3.T1.3.3.1.1.1.1.m1.1.1.cmml"><mi id="S3.T1.3.3.1.1.1.1.m1.1.1.2" xref="S3.T1.3.3.1.1.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.T1.3.3.1.1.1.1.m1.1.1.3" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.cmml"><mi id="S3.T1.3.3.1.1.1.1.m1.1.1.3.2" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.2.cmml">c</mi><mo id="S3.T1.3.3.1.1.1.1.m1.1.1.3.1" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.1.1.1.1.m1.1.1.3.3" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.3.cmml">o</mi><mo id="S3.T1.3.3.1.1.1.1.m1.1.1.3.1a" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.1.1.1.1.m1.1.1.3.4" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.4.cmml">m</mi><mo id="S3.T1.3.3.1.1.1.1.m1.1.1.3.1b" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T1.3.3.1.1.1.1.m1.1.1.3.5" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.5.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.1.1.1.1.m1.1b"><apply id="S3.T1.3.3.1.1.1.1.m1.1.1.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.3.3.1.1.1.1.m1.1.1.1.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T1.3.3.1.1.1.1.m1.1.1.2.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1.2">𝑃</ci><apply id="S3.T1.3.3.1.1.1.1.m1.1.1.3.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3"><times id="S3.T1.3.3.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.1"></times><ci id="S3.T1.3.3.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.2">𝑐</ci><ci id="S3.T1.3.3.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.T1.3.3.1.1.1.1.m1.1.1.3.4.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.4">𝑚</ci><ci id="S3.T1.3.3.1.1.1.1.m1.1.1.3.5.cmml" xref="S3.T1.3.3.1.1.1.1.m1.1.1.3.5">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.1.1.1.1.m1.1c">\bm{P}_{\bm{comb}}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.1.1.1.1.m1.1d">bold_italic_P start_POSTSUBSCRIPT bold_italic_c bold_italic_o bold_italic_m bold_italic_b end_POSTSUBSCRIPT</annotation></semantics></math>):</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.13.10">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.13.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.13.10.1.1">
<span class="ltx_p" id="S3.T1.3.13.10.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.13.10.1.1.1.1" style="font-size:90%;">Replay the audio and identify sounds that are absent as contrasting examples.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.14.11">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.14.11.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.14.11.1.1">
<span class="ltx_p" id="S3.T1.3.14.11.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T1.3.14.11.1.1.1.1" style="font-size:90%;">Response:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.15.12">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.15.12.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.15.12.1.1">
<span class="ltx_p" id="S3.T1.3.15.12.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.15.12.1.1.1.1" style="font-size:90%;">Specific sound events detected in the provided audio:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.16.13">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.16.13.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.16.13.1.1">
<span class="ltx_p" id="S3.T1.3.16.13.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.16.13.1.1.1.1" style="font-size:90%;">1. A woman’s voice   2. Water pouring</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.17.14">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T1.3.17.14.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.17.14.1.1">
<span class="ltx_p" id="S3.T1.3.17.14.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.17.14.1.1.1.1" style="font-size:90%;">Contrastive examples of specific sound events not present in the provided audio:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T1.3.18.15">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T1.3.18.15.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T1.3.18.15.1.1">
<span class="ltx_p" id="S3.T1.3.18.15.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T1.3.18.15.1.1.1.1" style="font-size:90%;">1. A car driving by   2. A dog barking   3. The door bang</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">BALSa-MA: Bootstrapping Multi-Audio Learning via Difference Comparison and Joint Captioning</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">BALSa can be extended to multi-audio scenarios, enabling the backbone LLM to both generate comparative explanations of differences between audio samples and independently caption each sample within the same response for training purposes.
The objective is to move beyond single-audio alignment by introducing a multi-audio learning framework that incorporates both discrimination-based comparison and parallel description-based captioning.
In this process, the model must first independently comprehend each audio sample before either distinguishing their differences or simultaneously generating separate captions for both audio samples within a unified response.
This dual-task formulation increases the learning complexity compared to single-audio alignment.
We refer to this multi-audio extended approach as BALSa-MA (Multi-Audio).
</p>
</div>
<div class="ltx_para" id="S3.SS2.p2">
<p class="ltx_p" id="S3.SS2.p2.5">To implement this, we extend the generation prompt to accommodate multiple audio inputs. Here, we consider a two-audio scenario, denoted as first audio and second audio.
The seed prompt also can be either audio captions (<math alttext="{D}_{{caption}}" class="ltx_Math" display="inline" id="S3.SS2.p2.1.m1.1"><semantics id="S3.SS2.p2.1.m1.1a"><msub id="S3.SS2.p2.1.m1.1.1" xref="S3.SS2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.p2.1.m1.1.1.2" xref="S3.SS2.p2.1.m1.1.1.2.cmml">D</mi><mrow id="S3.SS2.p2.1.m1.1.1.3" xref="S3.SS2.p2.1.m1.1.1.3.cmml"><mi id="S3.SS2.p2.1.m1.1.1.3.2" xref="S3.SS2.p2.1.m1.1.1.3.2.cmml">c</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3.3" xref="S3.SS2.p2.1.m1.1.1.3.3.cmml">a</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1a" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3.4" xref="S3.SS2.p2.1.m1.1.1.3.4.cmml">p</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1b" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3.5" xref="S3.SS2.p2.1.m1.1.1.3.5.cmml">t</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1c" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3.6" xref="S3.SS2.p2.1.m1.1.1.3.6.cmml">i</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1d" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3.7" xref="S3.SS2.p2.1.m1.1.1.3.7.cmml">o</mi><mo id="S3.SS2.p2.1.m1.1.1.3.1e" xref="S3.SS2.p2.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.1.m1.1.1.3.8" xref="S3.SS2.p2.1.m1.1.1.3.8.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.1.m1.1b"><apply id="S3.SS2.p2.1.m1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.p2.1.m1.1.1.2">𝐷</ci><apply id="S3.SS2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3"><times id="S3.SS2.p2.1.m1.1.1.3.1.cmml" xref="S3.SS2.p2.1.m1.1.1.3.1"></times><ci id="S3.SS2.p2.1.m1.1.1.3.2.cmml" xref="S3.SS2.p2.1.m1.1.1.3.2">𝑐</ci><ci id="S3.SS2.p2.1.m1.1.1.3.3.cmml" xref="S3.SS2.p2.1.m1.1.1.3.3">𝑎</ci><ci id="S3.SS2.p2.1.m1.1.1.3.4.cmml" xref="S3.SS2.p2.1.m1.1.1.3.4">𝑝</ci><ci id="S3.SS2.p2.1.m1.1.1.3.5.cmml" xref="S3.SS2.p2.1.m1.1.1.3.5">𝑡</ci><ci id="S3.SS2.p2.1.m1.1.1.3.6.cmml" xref="S3.SS2.p2.1.m1.1.1.3.6">𝑖</ci><ci id="S3.SS2.p2.1.m1.1.1.3.7.cmml" xref="S3.SS2.p2.1.m1.1.1.3.7">𝑜</ci><ci id="S3.SS2.p2.1.m1.1.1.3.8.cmml" xref="S3.SS2.p2.1.m1.1.1.3.8">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.1.m1.1c">{D}_{{caption}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.1.m1.1d">italic_D start_POSTSUBSCRIPT italic_c italic_a italic_p italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT</annotation></semantics></math>) or sound event tags (<math alttext="{D}_{{tag}}" class="ltx_Math" display="inline" id="S3.SS2.p2.2.m2.1"><semantics id="S3.SS2.p2.2.m2.1a"><msub id="S3.SS2.p2.2.m2.1.1" xref="S3.SS2.p2.2.m2.1.1.cmml"><mi id="S3.SS2.p2.2.m2.1.1.2" xref="S3.SS2.p2.2.m2.1.1.2.cmml">D</mi><mrow id="S3.SS2.p2.2.m2.1.1.3" xref="S3.SS2.p2.2.m2.1.1.3.cmml"><mi id="S3.SS2.p2.2.m2.1.1.3.2" xref="S3.SS2.p2.2.m2.1.1.3.2.cmml">t</mi><mo id="S3.SS2.p2.2.m2.1.1.3.1" xref="S3.SS2.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.m2.1.1.3.3" xref="S3.SS2.p2.2.m2.1.1.3.3.cmml">a</mi><mo id="S3.SS2.p2.2.m2.1.1.3.1a" xref="S3.SS2.p2.2.m2.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.2.m2.1.1.3.4" xref="S3.SS2.p2.2.m2.1.1.3.4.cmml">g</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.2.m2.1b"><apply id="S3.SS2.p2.2.m2.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.2.m2.1.1.1.cmml" xref="S3.SS2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.p2.2.m2.1.1.2.cmml" xref="S3.SS2.p2.2.m2.1.1.2">𝐷</ci><apply id="S3.SS2.p2.2.m2.1.1.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3"><times id="S3.SS2.p2.2.m2.1.1.3.1.cmml" xref="S3.SS2.p2.2.m2.1.1.3.1"></times><ci id="S3.SS2.p2.2.m2.1.1.3.2.cmml" xref="S3.SS2.p2.2.m2.1.1.3.2">𝑡</ci><ci id="S3.SS2.p2.2.m2.1.1.3.3.cmml" xref="S3.SS2.p2.2.m2.1.1.3.3">𝑎</ci><ci id="S3.SS2.p2.2.m2.1.1.3.4.cmml" xref="S3.SS2.p2.2.m2.1.1.3.4">𝑔</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.2.m2.1c">{D}_{{tag}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.2.m2.1d">italic_D start_POSTSUBSCRIPT italic_t italic_a italic_g end_POSTSUBSCRIPT</annotation></semantics></math>), depending on the annotation format of the original dataset.
We denote them as <math alttext="{P}_{{seed,1}}" class="ltx_Math" display="inline" id="S3.SS2.p2.3.m3.2"><semantics id="S3.SS2.p2.3.m3.2a"><msub id="S3.SS2.p2.3.m3.2.3" xref="S3.SS2.p2.3.m3.2.3.cmml"><mi id="S3.SS2.p2.3.m3.2.3.2" xref="S3.SS2.p2.3.m3.2.3.2.cmml">P</mi><mrow id="S3.SS2.p2.3.m3.2.2.2.2" xref="S3.SS2.p2.3.m3.2.2.2.3.cmml"><mrow id="S3.SS2.p2.3.m3.2.2.2.2.1" xref="S3.SS2.p2.3.m3.2.2.2.2.1.cmml"><mi id="S3.SS2.p2.3.m3.2.2.2.2.1.2" xref="S3.SS2.p2.3.m3.2.2.2.2.1.2.cmml">s</mi><mo id="S3.SS2.p2.3.m3.2.2.2.2.1.1" xref="S3.SS2.p2.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.2.2.2.2.1.3" xref="S3.SS2.p2.3.m3.2.2.2.2.1.3.cmml">e</mi><mo id="S3.SS2.p2.3.m3.2.2.2.2.1.1a" xref="S3.SS2.p2.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.2.2.2.2.1.4" xref="S3.SS2.p2.3.m3.2.2.2.2.1.4.cmml">e</mi><mo id="S3.SS2.p2.3.m3.2.2.2.2.1.1b" xref="S3.SS2.p2.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.3.m3.2.2.2.2.1.5" xref="S3.SS2.p2.3.m3.2.2.2.2.1.5.cmml">d</mi></mrow><mo id="S3.SS2.p2.3.m3.2.2.2.2.2" xref="S3.SS2.p2.3.m3.2.2.2.3.cmml">,</mo><mn id="S3.SS2.p2.3.m3.1.1.1.1" xref="S3.SS2.p2.3.m3.1.1.1.1.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.3.m3.2b"><apply id="S3.SS2.p2.3.m3.2.3.cmml" xref="S3.SS2.p2.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.3.m3.2.3.1.cmml" xref="S3.SS2.p2.3.m3.2.3">subscript</csymbol><ci id="S3.SS2.p2.3.m3.2.3.2.cmml" xref="S3.SS2.p2.3.m3.2.3.2">𝑃</ci><list id="S3.SS2.p2.3.m3.2.2.2.3.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2"><apply id="S3.SS2.p2.3.m3.2.2.2.2.1.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2.1"><times id="S3.SS2.p2.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2.1.1"></times><ci id="S3.SS2.p2.3.m3.2.2.2.2.1.2.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2.1.2">𝑠</ci><ci id="S3.SS2.p2.3.m3.2.2.2.2.1.3.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2.1.3">𝑒</ci><ci id="S3.SS2.p2.3.m3.2.2.2.2.1.4.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2.1.4">𝑒</ci><ci id="S3.SS2.p2.3.m3.2.2.2.2.1.5.cmml" xref="S3.SS2.p2.3.m3.2.2.2.2.1.5">𝑑</ci></apply><cn id="S3.SS2.p2.3.m3.1.1.1.1.cmml" type="integer" xref="S3.SS2.p2.3.m3.1.1.1.1">1</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.3.m3.2c">{P}_{{seed,1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.3.m3.2d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d , 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="{P}_{{seed,2}}" class="ltx_Math" display="inline" id="S3.SS2.p2.4.m4.2"><semantics id="S3.SS2.p2.4.m4.2a"><msub id="S3.SS2.p2.4.m4.2.3" xref="S3.SS2.p2.4.m4.2.3.cmml"><mi id="S3.SS2.p2.4.m4.2.3.2" xref="S3.SS2.p2.4.m4.2.3.2.cmml">P</mi><mrow id="S3.SS2.p2.4.m4.2.2.2.2" xref="S3.SS2.p2.4.m4.2.2.2.3.cmml"><mrow id="S3.SS2.p2.4.m4.2.2.2.2.1" xref="S3.SS2.p2.4.m4.2.2.2.2.1.cmml"><mi id="S3.SS2.p2.4.m4.2.2.2.2.1.2" xref="S3.SS2.p2.4.m4.2.2.2.2.1.2.cmml">s</mi><mo id="S3.SS2.p2.4.m4.2.2.2.2.1.1" xref="S3.SS2.p2.4.m4.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.2.2.2.2.1.3" xref="S3.SS2.p2.4.m4.2.2.2.2.1.3.cmml">e</mi><mo id="S3.SS2.p2.4.m4.2.2.2.2.1.1a" xref="S3.SS2.p2.4.m4.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.2.2.2.2.1.4" xref="S3.SS2.p2.4.m4.2.2.2.2.1.4.cmml">e</mi><mo id="S3.SS2.p2.4.m4.2.2.2.2.1.1b" xref="S3.SS2.p2.4.m4.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p2.4.m4.2.2.2.2.1.5" xref="S3.SS2.p2.4.m4.2.2.2.2.1.5.cmml">d</mi></mrow><mo id="S3.SS2.p2.4.m4.2.2.2.2.2" xref="S3.SS2.p2.4.m4.2.2.2.3.cmml">,</mo><mn id="S3.SS2.p2.4.m4.1.1.1.1" xref="S3.SS2.p2.4.m4.1.1.1.1.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.4.m4.2b"><apply id="S3.SS2.p2.4.m4.2.3.cmml" xref="S3.SS2.p2.4.m4.2.3"><csymbol cd="ambiguous" id="S3.SS2.p2.4.m4.2.3.1.cmml" xref="S3.SS2.p2.4.m4.2.3">subscript</csymbol><ci id="S3.SS2.p2.4.m4.2.3.2.cmml" xref="S3.SS2.p2.4.m4.2.3.2">𝑃</ci><list id="S3.SS2.p2.4.m4.2.2.2.3.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2"><apply id="S3.SS2.p2.4.m4.2.2.2.2.1.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2.1"><times id="S3.SS2.p2.4.m4.2.2.2.2.1.1.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2.1.1"></times><ci id="S3.SS2.p2.4.m4.2.2.2.2.1.2.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2.1.2">𝑠</ci><ci id="S3.SS2.p2.4.m4.2.2.2.2.1.3.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2.1.3">𝑒</ci><ci id="S3.SS2.p2.4.m4.2.2.2.2.1.4.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2.1.4">𝑒</ci><ci id="S3.SS2.p2.4.m4.2.2.2.2.1.5.cmml" xref="S3.SS2.p2.4.m4.2.2.2.2.1.5">𝑑</ci></apply><cn id="S3.SS2.p2.4.m4.1.1.1.1.cmml" type="integer" xref="S3.SS2.p2.4.m4.1.1.1.1">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.4.m4.2c">{P}_{{seed,2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.4.m4.2d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d , 2 end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.
Next, we append the generation prompts (<math alttext="{P}_{gen}" class="ltx_Math" display="inline" id="S3.SS2.p2.5.m5.1"><semantics id="S3.SS2.p2.5.m5.1a"><msub id="S3.SS2.p2.5.m5.1.1" xref="S3.SS2.p2.5.m5.1.1.cmml"><mi id="S3.SS2.p2.5.m5.1.1.2" xref="S3.SS2.p2.5.m5.1.1.2.cmml">P</mi><mrow id="S3.SS2.p2.5.m5.1.1.3" xref="S3.SS2.p2.5.m5.1.1.3.cmml"><mi id="S3.SS2.p2.5.m5.1.1.3.2" xref="S3.SS2.p2.5.m5.1.1.3.2.cmml">g</mi><mo id="S3.SS2.p2.5.m5.1.1.3.1" xref="S3.SS2.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.5.m5.1.1.3.3" xref="S3.SS2.p2.5.m5.1.1.3.3.cmml">e</mi><mo id="S3.SS2.p2.5.m5.1.1.3.1a" xref="S3.SS2.p2.5.m5.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p2.5.m5.1.1.3.4" xref="S3.SS2.p2.5.m5.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p2.5.m5.1b"><apply id="S3.SS2.p2.5.m5.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p2.5.m5.1.1.1.cmml" xref="S3.SS2.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS2.p2.5.m5.1.1.2.cmml" xref="S3.SS2.p2.5.m5.1.1.2">𝑃</ci><apply id="S3.SS2.p2.5.m5.1.1.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3"><times id="S3.SS2.p2.5.m5.1.1.3.1.cmml" xref="S3.SS2.p2.5.m5.1.1.3.1"></times><ci id="S3.SS2.p2.5.m5.1.1.3.2.cmml" xref="S3.SS2.p2.5.m5.1.1.3.2">𝑔</ci><ci id="S3.SS2.p2.5.m5.1.1.3.3.cmml" xref="S3.SS2.p2.5.m5.1.1.3.3">𝑒</ci><ci id="S3.SS2.p2.5.m5.1.1.3.4.cmml" xref="S3.SS2.p2.5.m5.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p2.5.m5.1c">{P}_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p2.5.m5.1d">italic_P start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math>) to both seed prompts and feed them into the LLM, enabling it to generate discrimination-based descriptions that compare the two audio samples.</p>
</div>
<div class="ltx_para" id="S3.SS2.p3">
<p class="ltx_p" id="S3.SS2.p3.1">In this study, we employ the following generation prompt:
<span class="ltx_text ltx_font_bold" id="S3.SS2.p3.1.1">Discrimination Generation Prompt (<math alttext="{P}_{{diff}}" class="ltx_Math" display="inline" id="S3.SS2.p3.1.1.m1.1"><semantics id="S3.SS2.p3.1.1.m1.1a"><msub id="S3.SS2.p3.1.1.m1.1.1" xref="S3.SS2.p3.1.1.m1.1.1.cmml"><mi id="S3.SS2.p3.1.1.m1.1.1.2" xref="S3.SS2.p3.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.SS2.p3.1.1.m1.1.1.3" xref="S3.SS2.p3.1.1.m1.1.1.3.cmml"><mi id="S3.SS2.p3.1.1.m1.1.1.3.2" xref="S3.SS2.p3.1.1.m1.1.1.3.2.cmml">d</mi><mo id="S3.SS2.p3.1.1.m1.1.1.3.1" xref="S3.SS2.p3.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.1.m1.1.1.3.3" xref="S3.SS2.p3.1.1.m1.1.1.3.3.cmml">i</mi><mo id="S3.SS2.p3.1.1.m1.1.1.3.1a" xref="S3.SS2.p3.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.1.m1.1.1.3.4" xref="S3.SS2.p3.1.1.m1.1.1.3.4.cmml">f</mi><mo id="S3.SS2.p3.1.1.m1.1.1.3.1b" xref="S3.SS2.p3.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p3.1.1.m1.1.1.3.5" xref="S3.SS2.p3.1.1.m1.1.1.3.5.cmml">f</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p3.1.1.m1.1b"><apply id="S3.SS2.p3.1.1.m1.1.1.cmml" xref="S3.SS2.p3.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p3.1.1.m1.1.1.1.cmml" xref="S3.SS2.p3.1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p3.1.1.m1.1.1.2.cmml" xref="S3.SS2.p3.1.1.m1.1.1.2">𝑃</ci><apply id="S3.SS2.p3.1.1.m1.1.1.3.cmml" xref="S3.SS2.p3.1.1.m1.1.1.3"><times id="S3.SS2.p3.1.1.m1.1.1.3.1.cmml" xref="S3.SS2.p3.1.1.m1.1.1.3.1"></times><ci id="S3.SS2.p3.1.1.m1.1.1.3.2.cmml" xref="S3.SS2.p3.1.1.m1.1.1.3.2">𝑑</ci><ci id="S3.SS2.p3.1.1.m1.1.1.3.3.cmml" xref="S3.SS2.p3.1.1.m1.1.1.3.3">𝑖</ci><ci id="S3.SS2.p3.1.1.m1.1.1.3.4.cmml" xref="S3.SS2.p3.1.1.m1.1.1.3.4">𝑓</ci><ci id="S3.SS2.p3.1.1.m1.1.1.3.5.cmml" xref="S3.SS2.p3.1.1.m1.1.1.3.5">𝑓</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p3.1.1.m1.1c">{P}_{{diff}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p3.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_d italic_i italic_f italic_f end_POSTSUBSCRIPT</annotation></semantics></math>)</span>, which generates comparative descriptions highlighting the differences between two audio samples.
For instance, <span class="ltx_text ltx_font_italic" id="S3.SS2.p3.1.2">Explain the difference between the audio clips</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS2.p4">
<p class="ltx_p" id="S3.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS2.p4.1.1">The Both Audio Captioning Prompt (<math alttext="{P}_{{cap,both}}" class="ltx_Math" display="inline" id="S3.SS2.p4.1.1.m1.2"><semantics id="S3.SS2.p4.1.1.m1.2a"><msub id="S3.SS2.p4.1.1.m1.2.3" xref="S3.SS2.p4.1.1.m1.2.3.cmml"><mi id="S3.SS2.p4.1.1.m1.2.3.2" xref="S3.SS2.p4.1.1.m1.2.3.2.cmml">P</mi><mrow id="S3.SS2.p4.1.1.m1.2.2.2.2" xref="S3.SS2.p4.1.1.m1.2.2.2.3.cmml"><mrow id="S3.SS2.p4.1.1.m1.1.1.1.1.1" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.cmml"><mi id="S3.SS2.p4.1.1.m1.1.1.1.1.1.2" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.2.cmml">c</mi><mo id="S3.SS2.p4.1.1.m1.1.1.1.1.1.1" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p4.1.1.m1.1.1.1.1.1.3" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.3.cmml">a</mi><mo id="S3.SS2.p4.1.1.m1.1.1.1.1.1.1a" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.SS2.p4.1.1.m1.1.1.1.1.1.4" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.4.cmml">p</mi></mrow><mo id="S3.SS2.p4.1.1.m1.2.2.2.2.3" xref="S3.SS2.p4.1.1.m1.2.2.2.3.cmml">,</mo><mrow id="S3.SS2.p4.1.1.m1.2.2.2.2.2" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.cmml"><mi id="S3.SS2.p4.1.1.m1.2.2.2.2.2.2" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.2.cmml">b</mi><mo id="S3.SS2.p4.1.1.m1.2.2.2.2.2.1" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS2.p4.1.1.m1.2.2.2.2.2.3" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.3.cmml">o</mi><mo id="S3.SS2.p4.1.1.m1.2.2.2.2.2.1a" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS2.p4.1.1.m1.2.2.2.2.2.4" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.4.cmml">t</mi><mo id="S3.SS2.p4.1.1.m1.2.2.2.2.2.1b" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.SS2.p4.1.1.m1.2.2.2.2.2.5" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.5.cmml">h</mi></mrow></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p4.1.1.m1.2b"><apply id="S3.SS2.p4.1.1.m1.2.3.cmml" xref="S3.SS2.p4.1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.SS2.p4.1.1.m1.2.3.1.cmml" xref="S3.SS2.p4.1.1.m1.2.3">subscript</csymbol><ci id="S3.SS2.p4.1.1.m1.2.3.2.cmml" xref="S3.SS2.p4.1.1.m1.2.3.2">𝑃</ci><list id="S3.SS2.p4.1.1.m1.2.2.2.3.cmml" xref="S3.SS2.p4.1.1.m1.2.2.2.2"><apply id="S3.SS2.p4.1.1.m1.1.1.1.1.1.cmml" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1"><times id="S3.SS2.p4.1.1.m1.1.1.1.1.1.1.cmml" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.1"></times><ci id="S3.SS2.p4.1.1.m1.1.1.1.1.1.2.cmml" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.2">𝑐</ci><ci id="S3.SS2.p4.1.1.m1.1.1.1.1.1.3.cmml" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.3">𝑎</ci><ci id="S3.SS2.p4.1.1.m1.1.1.1.1.1.4.cmml" xref="S3.SS2.p4.1.1.m1.1.1.1.1.1.4">𝑝</ci></apply><apply id="S3.SS2.p4.1.1.m1.2.2.2.2.2.cmml" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2"><times id="S3.SS2.p4.1.1.m1.2.2.2.2.2.1.cmml" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.1"></times><ci id="S3.SS2.p4.1.1.m1.2.2.2.2.2.2.cmml" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.2">𝑏</ci><ci id="S3.SS2.p4.1.1.m1.2.2.2.2.2.3.cmml" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.3">𝑜</ci><ci id="S3.SS2.p4.1.1.m1.2.2.2.2.2.4.cmml" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.4">𝑡</ci><ci id="S3.SS2.p4.1.1.m1.2.2.2.2.2.5.cmml" xref="S3.SS2.p4.1.1.m1.2.2.2.2.2.5">ℎ</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p4.1.1.m1.2c">{P}_{{cap,both}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p4.1.1.m1.2d">italic_P start_POSTSUBSCRIPT italic_c italic_a italic_p , italic_b italic_o italic_t italic_h end_POSTSUBSCRIPT</annotation></semantics></math>)</span>, which generates separate captions for each audio sample, providing distinct descriptions for both inputs within a single response.</p>
</div>
<div class="ltx_para" id="S3.SS2.p5">
<p class="ltx_p" id="S3.SS2.p5.1">In summary, the final input prompt (<math alttext="{P}_{{final}}" class="ltx_Math" display="inline" id="S3.SS2.p5.1.m1.1"><semantics id="S3.SS2.p5.1.m1.1a"><msub id="S3.SS2.p5.1.m1.1.1" xref="S3.SS2.p5.1.m1.1.1.cmml"><mi id="S3.SS2.p5.1.m1.1.1.2" xref="S3.SS2.p5.1.m1.1.1.2.cmml">P</mi><mrow id="S3.SS2.p5.1.m1.1.1.3" xref="S3.SS2.p5.1.m1.1.1.3.cmml"><mi id="S3.SS2.p5.1.m1.1.1.3.2" xref="S3.SS2.p5.1.m1.1.1.3.2.cmml">f</mi><mo id="S3.SS2.p5.1.m1.1.1.3.1" xref="S3.SS2.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p5.1.m1.1.1.3.3" xref="S3.SS2.p5.1.m1.1.1.3.3.cmml">i</mi><mo id="S3.SS2.p5.1.m1.1.1.3.1a" xref="S3.SS2.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p5.1.m1.1.1.3.4" xref="S3.SS2.p5.1.m1.1.1.3.4.cmml">n</mi><mo id="S3.SS2.p5.1.m1.1.1.3.1b" xref="S3.SS2.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p5.1.m1.1.1.3.5" xref="S3.SS2.p5.1.m1.1.1.3.5.cmml">a</mi><mo id="S3.SS2.p5.1.m1.1.1.3.1c" xref="S3.SS2.p5.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.SS2.p5.1.m1.1.1.3.6" xref="S3.SS2.p5.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p5.1.m1.1b"><apply id="S3.SS2.p5.1.m1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p5.1.m1.1.1.1.cmml" xref="S3.SS2.p5.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.p5.1.m1.1.1.2.cmml" xref="S3.SS2.p5.1.m1.1.1.2">𝑃</ci><apply id="S3.SS2.p5.1.m1.1.1.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3"><times id="S3.SS2.p5.1.m1.1.1.3.1.cmml" xref="S3.SS2.p5.1.m1.1.1.3.1"></times><ci id="S3.SS2.p5.1.m1.1.1.3.2.cmml" xref="S3.SS2.p5.1.m1.1.1.3.2">𝑓</ci><ci id="S3.SS2.p5.1.m1.1.1.3.3.cmml" xref="S3.SS2.p5.1.m1.1.1.3.3">𝑖</ci><ci id="S3.SS2.p5.1.m1.1.1.3.4.cmml" xref="S3.SS2.p5.1.m1.1.1.3.4">𝑛</ci><ci id="S3.SS2.p5.1.m1.1.1.3.5.cmml" xref="S3.SS2.p5.1.m1.1.1.3.5">𝑎</ci><ci id="S3.SS2.p5.1.m1.1.1.3.6.cmml" xref="S3.SS2.p5.1.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p5.1.m1.1c">{P}_{{final}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p5.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>) to the LLM can be expressed in Equation <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.E2" title="In III-B BALSa-MA: Bootstrapping Multi-Audio Learning via Difference Comparison and Joint Captioning ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a> and the overall pipeline is illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.F1" title="Figure 1 ‣ III-A Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
<div class="ltx_para" id="S3.SS2.p6">
<table class="ltx_equationgroup ltx_eqn_table" id="S3.E2">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P_{final,1}" class="ltx_Math" display="inline" id="S3.E2X.2.1.1.m1.2"><semantics id="S3.E2X.2.1.1.m1.2a"><msub id="S3.E2X.2.1.1.m1.2.3" xref="S3.E2X.2.1.1.m1.2.3.cmml"><mi id="S3.E2X.2.1.1.m1.2.3.2" xref="S3.E2X.2.1.1.m1.2.3.2.cmml">P</mi><mrow id="S3.E2X.2.1.1.m1.2.2.2.2" xref="S3.E2X.2.1.1.m1.2.2.2.3.cmml"><mrow id="S3.E2X.2.1.1.m1.2.2.2.2.1" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.cmml"><mi id="S3.E2X.2.1.1.m1.2.2.2.2.1.2" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.2.cmml">f</mi><mo id="S3.E2X.2.1.1.m1.2.2.2.2.1.1" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2X.2.1.1.m1.2.2.2.2.1.3" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.3.cmml">i</mi><mo id="S3.E2X.2.1.1.m1.2.2.2.2.1.1a" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2X.2.1.1.m1.2.2.2.2.1.4" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.4.cmml">n</mi><mo id="S3.E2X.2.1.1.m1.2.2.2.2.1.1b" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2X.2.1.1.m1.2.2.2.2.1.5" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.5.cmml">a</mi><mo id="S3.E2X.2.1.1.m1.2.2.2.2.1.1c" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2X.2.1.1.m1.2.2.2.2.1.6" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.6.cmml">l</mi></mrow><mo id="S3.E2X.2.1.1.m1.2.2.2.2.2" xref="S3.E2X.2.1.1.m1.2.2.2.3.cmml">,</mo><mn id="S3.E2X.2.1.1.m1.1.1.1.1" xref="S3.E2X.2.1.1.m1.1.1.1.1.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E2X.2.1.1.m1.2b"><apply id="S3.E2X.2.1.1.m1.2.3.cmml" xref="S3.E2X.2.1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.E2X.2.1.1.m1.2.3.1.cmml" xref="S3.E2X.2.1.1.m1.2.3">subscript</csymbol><ci id="S3.E2X.2.1.1.m1.2.3.2.cmml" xref="S3.E2X.2.1.1.m1.2.3.2">𝑃</ci><list id="S3.E2X.2.1.1.m1.2.2.2.3.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2"><apply id="S3.E2X.2.1.1.m1.2.2.2.2.1.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.1"><times id="S3.E2X.2.1.1.m1.2.2.2.2.1.1.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.1"></times><ci id="S3.E2X.2.1.1.m1.2.2.2.2.1.2.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.2">𝑓</ci><ci id="S3.E2X.2.1.1.m1.2.2.2.2.1.3.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.3">𝑖</ci><ci id="S3.E2X.2.1.1.m1.2.2.2.2.1.4.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.4">𝑛</ci><ci id="S3.E2X.2.1.1.m1.2.2.2.2.1.5.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.5">𝑎</ci><ci id="S3.E2X.2.1.1.m1.2.2.2.2.1.6.cmml" xref="S3.E2X.2.1.1.m1.2.2.2.2.1.6">𝑙</ci></apply><cn id="S3.E2X.2.1.1.m1.1.1.1.1.cmml" type="integer" xref="S3.E2X.2.1.1.m1.1.1.1.1">1</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.2.1.1.m1.2c">\displaystyle P_{final,1}</annotation><annotation encoding="application/x-llamapun" id="S3.E2X.2.1.1.m1.2d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l , 1 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=[\textit{Begin of audio1}]\,P_{seed,1}\,[\textit{End of audio1}]" class="ltx_Math" display="inline" id="S3.E2X.3.2.2.m1.4"><semantics id="S3.E2X.3.2.2.m1.4a"><mrow id="S3.E2X.3.2.2.m1.4.5" xref="S3.E2X.3.2.2.m1.4.5.cmml"><mi id="S3.E2X.3.2.2.m1.4.5.2" xref="S3.E2X.3.2.2.m1.4.5.2.cmml"></mi><mo id="S3.E2X.3.2.2.m1.4.5.1" xref="S3.E2X.3.2.2.m1.4.5.1.cmml">=</mo><mrow id="S3.E2X.3.2.2.m1.4.5.3" xref="S3.E2X.3.2.2.m1.4.5.3.cmml"><mrow id="S3.E2X.3.2.2.m1.4.5.3.2.2" xref="S3.E2X.3.2.2.m1.4.5.3.2.1.cmml"><mo id="S3.E2X.3.2.2.m1.4.5.3.2.2.1" stretchy="false" xref="S3.E2X.3.2.2.m1.4.5.3.2.1.1.cmml">[</mo><mtext class="ltx_mathvariant_italic" id="S3.E2X.3.2.2.m1.3.3" xref="S3.E2X.3.2.2.m1.3.3a.cmml">Begin of audio1</mtext><mo id="S3.E2X.3.2.2.m1.4.5.3.2.2.2" stretchy="false" xref="S3.E2X.3.2.2.m1.4.5.3.2.1.1.cmml">]</mo></mrow><mo id="S3.E2X.3.2.2.m1.4.5.3.1" lspace="0.170em" xref="S3.E2X.3.2.2.m1.4.5.3.1.cmml">⁢</mo><msub id="S3.E2X.3.2.2.m1.4.5.3.3" xref="S3.E2X.3.2.2.m1.4.5.3.3.cmml"><mi id="S3.E2X.3.2.2.m1.4.5.3.3.2" xref="S3.E2X.3.2.2.m1.4.5.3.3.2.cmml">P</mi><mrow id="S3.E2X.3.2.2.m1.2.2.2.2" xref="S3.E2X.3.2.2.m1.2.2.2.3.cmml"><mrow id="S3.E2X.3.2.2.m1.2.2.2.2.1" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.cmml"><mi id="S3.E2X.3.2.2.m1.2.2.2.2.1.2" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.2.cmml">s</mi><mo id="S3.E2X.3.2.2.m1.2.2.2.2.1.1" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2X.3.2.2.m1.2.2.2.2.1.3" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.3.cmml">e</mi><mo id="S3.E2X.3.2.2.m1.2.2.2.2.1.1a" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2X.3.2.2.m1.2.2.2.2.1.4" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.4.cmml">e</mi><mo id="S3.E2X.3.2.2.m1.2.2.2.2.1.1b" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2X.3.2.2.m1.2.2.2.2.1.5" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.5.cmml">d</mi></mrow><mo id="S3.E2X.3.2.2.m1.2.2.2.2.2" xref="S3.E2X.3.2.2.m1.2.2.2.3.cmml">,</mo><mn id="S3.E2X.3.2.2.m1.1.1.1.1" xref="S3.E2X.3.2.2.m1.1.1.1.1.cmml">1</mn></mrow></msub><mo id="S3.E2X.3.2.2.m1.4.5.3.1a" xref="S3.E2X.3.2.2.m1.4.5.3.1.cmml">⁢</mo><mrow id="S3.E2X.3.2.2.m1.4.5.3.4.2" xref="S3.E2X.3.2.2.m1.4.5.3.4.1.cmml"><mo id="S3.E2X.3.2.2.m1.4.5.3.4.2.1" stretchy="false" xref="S3.E2X.3.2.2.m1.4.5.3.4.1.1.cmml">[</mo><mtext class="ltx_mathvariant_italic" id="S3.E2X.3.2.2.m1.4.4" xref="S3.E2X.3.2.2.m1.4.4a.cmml">End of audio1</mtext><mo id="S3.E2X.3.2.2.m1.4.5.3.4.2.2" stretchy="false" xref="S3.E2X.3.2.2.m1.4.5.3.4.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2X.3.2.2.m1.4b"><apply id="S3.E2X.3.2.2.m1.4.5.cmml" xref="S3.E2X.3.2.2.m1.4.5"><eq id="S3.E2X.3.2.2.m1.4.5.1.cmml" xref="S3.E2X.3.2.2.m1.4.5.1"></eq><csymbol cd="latexml" id="S3.E2X.3.2.2.m1.4.5.2.cmml" xref="S3.E2X.3.2.2.m1.4.5.2">absent</csymbol><apply id="S3.E2X.3.2.2.m1.4.5.3.cmml" xref="S3.E2X.3.2.2.m1.4.5.3"><times id="S3.E2X.3.2.2.m1.4.5.3.1.cmml" xref="S3.E2X.3.2.2.m1.4.5.3.1"></times><apply id="S3.E2X.3.2.2.m1.4.5.3.2.1.cmml" xref="S3.E2X.3.2.2.m1.4.5.3.2.2"><csymbol cd="latexml" id="S3.E2X.3.2.2.m1.4.5.3.2.1.1.cmml" xref="S3.E2X.3.2.2.m1.4.5.3.2.2.1">delimited-[]</csymbol><ci id="S3.E2X.3.2.2.m1.3.3a.cmml" xref="S3.E2X.3.2.2.m1.3.3"><mtext class="ltx_mathvariant_italic" id="S3.E2X.3.2.2.m1.3.3.cmml" xref="S3.E2X.3.2.2.m1.3.3">Begin of audio1</mtext></ci></apply><apply id="S3.E2X.3.2.2.m1.4.5.3.3.cmml" xref="S3.E2X.3.2.2.m1.4.5.3.3"><csymbol cd="ambiguous" id="S3.E2X.3.2.2.m1.4.5.3.3.1.cmml" xref="S3.E2X.3.2.2.m1.4.5.3.3">subscript</csymbol><ci id="S3.E2X.3.2.2.m1.4.5.3.3.2.cmml" xref="S3.E2X.3.2.2.m1.4.5.3.3.2">𝑃</ci><list id="S3.E2X.3.2.2.m1.2.2.2.3.cmml" xref="S3.E2X.3.2.2.m1.2.2.2.2"><apply id="S3.E2X.3.2.2.m1.2.2.2.2.1.cmml" xref="S3.E2X.3.2.2.m1.2.2.2.2.1"><times id="S3.E2X.3.2.2.m1.2.2.2.2.1.1.cmml" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.1"></times><ci id="S3.E2X.3.2.2.m1.2.2.2.2.1.2.cmml" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.2">𝑠</ci><ci id="S3.E2X.3.2.2.m1.2.2.2.2.1.3.cmml" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.3">𝑒</ci><ci id="S3.E2X.3.2.2.m1.2.2.2.2.1.4.cmml" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.4">𝑒</ci><ci id="S3.E2X.3.2.2.m1.2.2.2.2.1.5.cmml" xref="S3.E2X.3.2.2.m1.2.2.2.2.1.5">𝑑</ci></apply><cn id="S3.E2X.3.2.2.m1.1.1.1.1.cmml" type="integer" xref="S3.E2X.3.2.2.m1.1.1.1.1">1</cn></list></apply><apply id="S3.E2X.3.2.2.m1.4.5.3.4.1.cmml" xref="S3.E2X.3.2.2.m1.4.5.3.4.2"><csymbol cd="latexml" id="S3.E2X.3.2.2.m1.4.5.3.4.1.1.cmml" xref="S3.E2X.3.2.2.m1.4.5.3.4.2.1">delimited-[]</csymbol><ci id="S3.E2X.3.2.2.m1.4.4a.cmml" xref="S3.E2X.3.2.2.m1.4.4"><mtext class="ltx_mathvariant_italic" id="S3.E2X.3.2.2.m1.4.4.cmml" xref="S3.E2X.3.2.2.m1.4.4">End of audio1</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2X.3.2.2.m1.4c">\displaystyle=[\textit{Begin of audio1}]\,P_{seed,1}\,[\textit{End of audio1}]</annotation><annotation encoding="application/x-llamapun" id="S3.E2X.3.2.2.m1.4d">= [ Begin of audio1 ] italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d , 1 end_POSTSUBSCRIPT [ End of audio1 ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="5"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(2)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P_{final,2}" class="ltx_Math" display="inline" id="S3.E2Xa.2.1.1.m1.2"><semantics id="S3.E2Xa.2.1.1.m1.2a"><msub id="S3.E2Xa.2.1.1.m1.2.3" xref="S3.E2Xa.2.1.1.m1.2.3.cmml"><mi id="S3.E2Xa.2.1.1.m1.2.3.2" xref="S3.E2Xa.2.1.1.m1.2.3.2.cmml">P</mi><mrow id="S3.E2Xa.2.1.1.m1.2.2.2.2" xref="S3.E2Xa.2.1.1.m1.2.2.2.3.cmml"><mrow id="S3.E2Xa.2.1.1.m1.2.2.2.2.1" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.cmml"><mi id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.2" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.2.cmml">f</mi><mo id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.3" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.3.cmml">i</mi><mo id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1a" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.4" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.4.cmml">n</mi><mo id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1b" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.5" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.5.cmml">a</mi><mo id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1c" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.6" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.6.cmml">l</mi></mrow><mo id="S3.E2Xa.2.1.1.m1.2.2.2.2.2" xref="S3.E2Xa.2.1.1.m1.2.2.2.3.cmml">,</mo><mn id="S3.E2Xa.2.1.1.m1.1.1.1.1" xref="S3.E2Xa.2.1.1.m1.1.1.1.1.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E2Xa.2.1.1.m1.2b"><apply id="S3.E2Xa.2.1.1.m1.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.2.3"><csymbol cd="ambiguous" id="S3.E2Xa.2.1.1.m1.2.3.1.cmml" xref="S3.E2Xa.2.1.1.m1.2.3">subscript</csymbol><ci id="S3.E2Xa.2.1.1.m1.2.3.2.cmml" xref="S3.E2Xa.2.1.1.m1.2.3.2">𝑃</ci><list id="S3.E2Xa.2.1.1.m1.2.2.2.3.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2"><apply id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1"><times id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.1"></times><ci id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.2.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.2">𝑓</ci><ci id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.3.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.3">𝑖</ci><ci id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.4.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.4">𝑛</ci><ci id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.5.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.5">𝑎</ci><ci id="S3.E2Xa.2.1.1.m1.2.2.2.2.1.6.cmml" xref="S3.E2Xa.2.1.1.m1.2.2.2.2.1.6">𝑙</ci></apply><cn id="S3.E2Xa.2.1.1.m1.1.1.1.1.cmml" type="integer" xref="S3.E2Xa.2.1.1.m1.1.1.1.1">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xa.2.1.1.m1.2c">\displaystyle P_{final,2}</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xa.2.1.1.m1.2d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l , 2 end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=[\textit{Begin of audio2}]\,P_{seed,2}\,[\textit{End of audio2}]" class="ltx_Math" display="inline" id="S3.E2Xa.3.2.2.m1.4"><semantics id="S3.E2Xa.3.2.2.m1.4a"><mrow id="S3.E2Xa.3.2.2.m1.4.5" xref="S3.E2Xa.3.2.2.m1.4.5.cmml"><mi id="S3.E2Xa.3.2.2.m1.4.5.2" xref="S3.E2Xa.3.2.2.m1.4.5.2.cmml"></mi><mo id="S3.E2Xa.3.2.2.m1.4.5.1" xref="S3.E2Xa.3.2.2.m1.4.5.1.cmml">=</mo><mrow id="S3.E2Xa.3.2.2.m1.4.5.3" xref="S3.E2Xa.3.2.2.m1.4.5.3.cmml"><mrow id="S3.E2Xa.3.2.2.m1.4.5.3.2.2" xref="S3.E2Xa.3.2.2.m1.4.5.3.2.1.cmml"><mo id="S3.E2Xa.3.2.2.m1.4.5.3.2.2.1" stretchy="false" xref="S3.E2Xa.3.2.2.m1.4.5.3.2.1.1.cmml">[</mo><mtext class="ltx_mathvariant_italic" id="S3.E2Xa.3.2.2.m1.3.3" xref="S3.E2Xa.3.2.2.m1.3.3a.cmml">Begin of audio2</mtext><mo id="S3.E2Xa.3.2.2.m1.4.5.3.2.2.2" stretchy="false" xref="S3.E2Xa.3.2.2.m1.4.5.3.2.1.1.cmml">]</mo></mrow><mo id="S3.E2Xa.3.2.2.m1.4.5.3.1" lspace="0.170em" xref="S3.E2Xa.3.2.2.m1.4.5.3.1.cmml">⁢</mo><msub id="S3.E2Xa.3.2.2.m1.4.5.3.3" xref="S3.E2Xa.3.2.2.m1.4.5.3.3.cmml"><mi id="S3.E2Xa.3.2.2.m1.4.5.3.3.2" xref="S3.E2Xa.3.2.2.m1.4.5.3.3.2.cmml">P</mi><mrow id="S3.E2Xa.3.2.2.m1.2.2.2.2" xref="S3.E2Xa.3.2.2.m1.2.2.2.3.cmml"><mrow id="S3.E2Xa.3.2.2.m1.2.2.2.2.1" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.cmml"><mi id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.2" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.2.cmml">s</mi><mo id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.1" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.3" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.3.cmml">e</mi><mo id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.1a" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.4" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.4.cmml">e</mi><mo id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.1b" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.5" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.5.cmml">d</mi></mrow><mo id="S3.E2Xa.3.2.2.m1.2.2.2.2.2" xref="S3.E2Xa.3.2.2.m1.2.2.2.3.cmml">,</mo><mn id="S3.E2Xa.3.2.2.m1.1.1.1.1" xref="S3.E2Xa.3.2.2.m1.1.1.1.1.cmml">2</mn></mrow></msub><mo id="S3.E2Xa.3.2.2.m1.4.5.3.1a" xref="S3.E2Xa.3.2.2.m1.4.5.3.1.cmml">⁢</mo><mrow id="S3.E2Xa.3.2.2.m1.4.5.3.4.2" xref="S3.E2Xa.3.2.2.m1.4.5.3.4.1.cmml"><mo id="S3.E2Xa.3.2.2.m1.4.5.3.4.2.1" stretchy="false" xref="S3.E2Xa.3.2.2.m1.4.5.3.4.1.1.cmml">[</mo><mtext class="ltx_mathvariant_italic" id="S3.E2Xa.3.2.2.m1.4.4" xref="S3.E2Xa.3.2.2.m1.4.4a.cmml">End of audio2</mtext><mo id="S3.E2Xa.3.2.2.m1.4.5.3.4.2.2" stretchy="false" xref="S3.E2Xa.3.2.2.m1.4.5.3.4.1.1.cmml">]</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2Xa.3.2.2.m1.4b"><apply id="S3.E2Xa.3.2.2.m1.4.5.cmml" xref="S3.E2Xa.3.2.2.m1.4.5"><eq id="S3.E2Xa.3.2.2.m1.4.5.1.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.1"></eq><csymbol cd="latexml" id="S3.E2Xa.3.2.2.m1.4.5.2.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.2">absent</csymbol><apply id="S3.E2Xa.3.2.2.m1.4.5.3.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3"><times id="S3.E2Xa.3.2.2.m1.4.5.3.1.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3.1"></times><apply id="S3.E2Xa.3.2.2.m1.4.5.3.2.1.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3.2.2"><csymbol cd="latexml" id="S3.E2Xa.3.2.2.m1.4.5.3.2.1.1.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3.2.2.1">delimited-[]</csymbol><ci id="S3.E2Xa.3.2.2.m1.3.3a.cmml" xref="S3.E2Xa.3.2.2.m1.3.3"><mtext class="ltx_mathvariant_italic" id="S3.E2Xa.3.2.2.m1.3.3.cmml" xref="S3.E2Xa.3.2.2.m1.3.3">Begin of audio2</mtext></ci></apply><apply id="S3.E2Xa.3.2.2.m1.4.5.3.3.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3.3"><csymbol cd="ambiguous" id="S3.E2Xa.3.2.2.m1.4.5.3.3.1.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3.3">subscript</csymbol><ci id="S3.E2Xa.3.2.2.m1.4.5.3.3.2.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3.3.2">𝑃</ci><list id="S3.E2Xa.3.2.2.m1.2.2.2.3.cmml" xref="S3.E2Xa.3.2.2.m1.2.2.2.2"><apply id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.cmml" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1"><times id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.1.cmml" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.1"></times><ci id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.2.cmml" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.2">𝑠</ci><ci id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.3.cmml" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.3">𝑒</ci><ci id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.4.cmml" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.4">𝑒</ci><ci id="S3.E2Xa.3.2.2.m1.2.2.2.2.1.5.cmml" xref="S3.E2Xa.3.2.2.m1.2.2.2.2.1.5">𝑑</ci></apply><cn id="S3.E2Xa.3.2.2.m1.1.1.1.1.cmml" type="integer" xref="S3.E2Xa.3.2.2.m1.1.1.1.1">2</cn></list></apply><apply id="S3.E2Xa.3.2.2.m1.4.5.3.4.1.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3.4.2"><csymbol cd="latexml" id="S3.E2Xa.3.2.2.m1.4.5.3.4.1.1.cmml" xref="S3.E2Xa.3.2.2.m1.4.5.3.4.2.1">delimited-[]</csymbol><ci id="S3.E2Xa.3.2.2.m1.4.4a.cmml" xref="S3.E2Xa.3.2.2.m1.4.4"><mtext class="ltx_mathvariant_italic" id="S3.E2Xa.3.2.2.m1.4.4.cmml" xref="S3.E2Xa.3.2.2.m1.4.4">End of audio2</mtext></ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xa.3.2.2.m1.4c">\displaystyle=[\textit{Begin of audio2}]\,P_{seed,2}\,[\textit{End of audio2}]</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xa.3.2.2.m1.4d">= [ Begin of audio2 ] italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d , 2 end_POSTSUBSCRIPT [ End of audio2 ]</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P_{final}" class="ltx_Math" display="inline" id="S3.E2Xb.2.1.1.m1.1"><semantics id="S3.E2Xb.2.1.1.m1.1a"><msub id="S3.E2Xb.2.1.1.m1.1.1" xref="S3.E2Xb.2.1.1.m1.1.1.cmml"><mi id="S3.E2Xb.2.1.1.m1.1.1.2" xref="S3.E2Xb.2.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.E2Xb.2.1.1.m1.1.1.3" xref="S3.E2Xb.2.1.1.m1.1.1.3.cmml"><mi id="S3.E2Xb.2.1.1.m1.1.1.3.2" xref="S3.E2Xb.2.1.1.m1.1.1.3.2.cmml">f</mi><mo id="S3.E2Xb.2.1.1.m1.1.1.3.1" xref="S3.E2Xb.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xb.2.1.1.m1.1.1.3.3" xref="S3.E2Xb.2.1.1.m1.1.1.3.3.cmml">i</mi><mo id="S3.E2Xb.2.1.1.m1.1.1.3.1a" xref="S3.E2Xb.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xb.2.1.1.m1.1.1.3.4" xref="S3.E2Xb.2.1.1.m1.1.1.3.4.cmml">n</mi><mo id="S3.E2Xb.2.1.1.m1.1.1.3.1b" xref="S3.E2Xb.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xb.2.1.1.m1.1.1.3.5" xref="S3.E2Xb.2.1.1.m1.1.1.3.5.cmml">a</mi><mo id="S3.E2Xb.2.1.1.m1.1.1.3.1c" xref="S3.E2Xb.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xb.2.1.1.m1.1.1.3.6" xref="S3.E2Xb.2.1.1.m1.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E2Xb.2.1.1.m1.1b"><apply id="S3.E2Xb.2.1.1.m1.1.1.cmml" xref="S3.E2Xb.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E2Xb.2.1.1.m1.1.1.1.cmml" xref="S3.E2Xb.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.E2Xb.2.1.1.m1.1.1.2.cmml" xref="S3.E2Xb.2.1.1.m1.1.1.2">𝑃</ci><apply id="S3.E2Xb.2.1.1.m1.1.1.3.cmml" xref="S3.E2Xb.2.1.1.m1.1.1.3"><times id="S3.E2Xb.2.1.1.m1.1.1.3.1.cmml" xref="S3.E2Xb.2.1.1.m1.1.1.3.1"></times><ci id="S3.E2Xb.2.1.1.m1.1.1.3.2.cmml" xref="S3.E2Xb.2.1.1.m1.1.1.3.2">𝑓</ci><ci id="S3.E2Xb.2.1.1.m1.1.1.3.3.cmml" xref="S3.E2Xb.2.1.1.m1.1.1.3.3">𝑖</ci><ci id="S3.E2Xb.2.1.1.m1.1.1.3.4.cmml" xref="S3.E2Xb.2.1.1.m1.1.1.3.4">𝑛</ci><ci id="S3.E2Xb.2.1.1.m1.1.1.3.5.cmml" xref="S3.E2Xb.2.1.1.m1.1.1.3.5">𝑎</ci><ci id="S3.E2Xb.2.1.1.m1.1.1.3.6.cmml" xref="S3.E2Xb.2.1.1.m1.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xb.2.1.1.m1.1c">\displaystyle P_{final}</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xb.2.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle=P_{final,1}\,||\,P_{final,2}\,||\,P_{gen}" class="ltx_Math" display="inline" id="S3.E2Xb.3.2.2.m1.5"><semantics id="S3.E2Xb.3.2.2.m1.5a"><mrow id="S3.E2Xb.3.2.2.m1.5.5" xref="S3.E2Xb.3.2.2.m1.5.5.cmml"><mi id="S3.E2Xb.3.2.2.m1.5.5.3" xref="S3.E2Xb.3.2.2.m1.5.5.3.cmml"></mi><mo id="S3.E2Xb.3.2.2.m1.5.5.2" xref="S3.E2Xb.3.2.2.m1.5.5.2.cmml">=</mo><mrow id="S3.E2Xb.3.2.2.m1.5.5.1" xref="S3.E2Xb.3.2.2.m1.5.5.1.cmml"><msub id="S3.E2Xb.3.2.2.m1.5.5.1.3" xref="S3.E2Xb.3.2.2.m1.5.5.1.3.cmml"><mi id="S3.E2Xb.3.2.2.m1.5.5.1.3.2" xref="S3.E2Xb.3.2.2.m1.5.5.1.3.2.cmml">P</mi><mrow id="S3.E2Xb.3.2.2.m1.2.2.2.2" xref="S3.E2Xb.3.2.2.m1.2.2.2.3.cmml"><mrow id="S3.E2Xb.3.2.2.m1.2.2.2.2.1" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.cmml"><mi id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.2" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.2.cmml">f</mi><mo id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.3" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.3.cmml">i</mi><mo id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1a" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.4" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.4.cmml">n</mi><mo id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1b" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.5" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.5.cmml">a</mi><mo id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1c" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.6" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.6.cmml">l</mi></mrow><mo id="S3.E2Xb.3.2.2.m1.2.2.2.2.2" xref="S3.E2Xb.3.2.2.m1.2.2.2.3.cmml">,</mo><mn id="S3.E2Xb.3.2.2.m1.1.1.1.1" xref="S3.E2Xb.3.2.2.m1.1.1.1.1.cmml">1</mn></mrow></msub><mo id="S3.E2Xb.3.2.2.m1.5.5.1.2" xref="S3.E2Xb.3.2.2.m1.5.5.1.2.cmml">⁢</mo><mrow id="S3.E2Xb.3.2.2.m1.5.5.1.1.1" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.2.cmml"><mo id="S3.E2Xb.3.2.2.m1.5.5.1.1.1.2" stretchy="false" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.2.1.cmml">‖</mo><msub id="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1.cmml"><mi id="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1.2" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1.2.cmml">P</mi><mrow id="S3.E2Xb.3.2.2.m1.4.4.2.2" xref="S3.E2Xb.3.2.2.m1.4.4.2.3.cmml"><mrow id="S3.E2Xb.3.2.2.m1.4.4.2.2.1" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.cmml"><mi id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.2" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.2.cmml">f</mi><mo id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.3" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.3.cmml">i</mi><mo id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1a" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.4" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.4.cmml">n</mi><mo id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1b" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.5" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.5.cmml">a</mi><mo id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1c" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.6" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.6.cmml">l</mi></mrow><mo id="S3.E2Xb.3.2.2.m1.4.4.2.2.2" xref="S3.E2Xb.3.2.2.m1.4.4.2.3.cmml">,</mo><mn id="S3.E2Xb.3.2.2.m1.3.3.1.1" xref="S3.E2Xb.3.2.2.m1.3.3.1.1.cmml">2</mn></mrow></msub><mo id="S3.E2Xb.3.2.2.m1.5.5.1.1.1.3" stretchy="false" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.2.1.cmml">‖</mo></mrow><mo id="S3.E2Xb.3.2.2.m1.5.5.1.2a" xref="S3.E2Xb.3.2.2.m1.5.5.1.2.cmml">⁢</mo><msub id="S3.E2Xb.3.2.2.m1.5.5.1.4" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.cmml"><mi id="S3.E2Xb.3.2.2.m1.5.5.1.4.2" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.2.cmml">P</mi><mrow id="S3.E2Xb.3.2.2.m1.5.5.1.4.3" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.cmml"><mi id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.2" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.2.cmml">g</mi><mo id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.1" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.3" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.3.cmml">e</mi><mo id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.1a" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.1.cmml">⁢</mo><mi id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.4" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.4.cmml">n</mi></mrow></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2Xb.3.2.2.m1.5b"><apply id="S3.E2Xb.3.2.2.m1.5.5.cmml" xref="S3.E2Xb.3.2.2.m1.5.5"><eq id="S3.E2Xb.3.2.2.m1.5.5.2.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.2"></eq><csymbol cd="latexml" id="S3.E2Xb.3.2.2.m1.5.5.3.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.3">absent</csymbol><apply id="S3.E2Xb.3.2.2.m1.5.5.1.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1"><times id="S3.E2Xb.3.2.2.m1.5.5.1.2.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.2"></times><apply id="S3.E2Xb.3.2.2.m1.5.5.1.3.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.3"><csymbol cd="ambiguous" id="S3.E2Xb.3.2.2.m1.5.5.1.3.1.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.3">subscript</csymbol><ci id="S3.E2Xb.3.2.2.m1.5.5.1.3.2.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.3.2">𝑃</ci><list id="S3.E2Xb.3.2.2.m1.2.2.2.3.cmml" xref="S3.E2Xb.3.2.2.m1.2.2.2.2"><apply id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.cmml" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1"><times id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1.cmml" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.1"></times><ci id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.2.cmml" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.2">𝑓</ci><ci id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.3.cmml" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.3">𝑖</ci><ci id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.4.cmml" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.4">𝑛</ci><ci id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.5.cmml" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.5">𝑎</ci><ci id="S3.E2Xb.3.2.2.m1.2.2.2.2.1.6.cmml" xref="S3.E2Xb.3.2.2.m1.2.2.2.2.1.6">𝑙</ci></apply><cn id="S3.E2Xb.3.2.2.m1.1.1.1.1.cmml" type="integer" xref="S3.E2Xb.3.2.2.m1.1.1.1.1">1</cn></list></apply><apply id="S3.E2Xb.3.2.2.m1.5.5.1.1.2.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.1"><csymbol cd="latexml" id="S3.E2Xb.3.2.2.m1.5.5.1.1.2.1.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.1.2">norm</csymbol><apply id="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1.1.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1">subscript</csymbol><ci id="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1.2.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.1.1.1.2">𝑃</ci><list id="S3.E2Xb.3.2.2.m1.4.4.2.3.cmml" xref="S3.E2Xb.3.2.2.m1.4.4.2.2"><apply id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.cmml" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1"><times id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1.cmml" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.1"></times><ci id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.2.cmml" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.2">𝑓</ci><ci id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.3.cmml" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.3">𝑖</ci><ci id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.4.cmml" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.4">𝑛</ci><ci id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.5.cmml" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.5">𝑎</ci><ci id="S3.E2Xb.3.2.2.m1.4.4.2.2.1.6.cmml" xref="S3.E2Xb.3.2.2.m1.4.4.2.2.1.6">𝑙</ci></apply><cn id="S3.E2Xb.3.2.2.m1.3.3.1.1.cmml" type="integer" xref="S3.E2Xb.3.2.2.m1.3.3.1.1">2</cn></list></apply></apply><apply id="S3.E2Xb.3.2.2.m1.5.5.1.4.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.4"><csymbol cd="ambiguous" id="S3.E2Xb.3.2.2.m1.5.5.1.4.1.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.4">subscript</csymbol><ci id="S3.E2Xb.3.2.2.m1.5.5.1.4.2.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.2">𝑃</ci><apply id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3"><times id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.1.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.1"></times><ci id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.2.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.2">𝑔</ci><ci id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.3.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.3">𝑒</ci><ci id="S3.E2Xb.3.2.2.m1.5.5.1.4.3.4.cmml" xref="S3.E2Xb.3.2.2.m1.5.5.1.4.3.4">𝑛</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xb.3.2.2.m1.5c">\displaystyle=P_{final,1}\,||\,P_{final,2}\,||\,P_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xb.3.2.2.m1.5d">= italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l , 1 end_POSTSUBSCRIPT | | italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l , 2 end_POSTSUBSCRIPT | | italic_P start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2Xc">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P_{seed}" class="ltx_Math" display="inline" id="S3.E2Xc.2.1.1.m1.1"><semantics id="S3.E2Xc.2.1.1.m1.1a"><msub id="S3.E2Xc.2.1.1.m1.1.1" xref="S3.E2Xc.2.1.1.m1.1.1.cmml"><mi id="S3.E2Xc.2.1.1.m1.1.1.2" xref="S3.E2Xc.2.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.E2Xc.2.1.1.m1.1.1.3" xref="S3.E2Xc.2.1.1.m1.1.1.3.cmml"><mi id="S3.E2Xc.2.1.1.m1.1.1.3.2" xref="S3.E2Xc.2.1.1.m1.1.1.3.2.cmml">s</mi><mo id="S3.E2Xc.2.1.1.m1.1.1.3.1" xref="S3.E2Xc.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.2.1.1.m1.1.1.3.3" xref="S3.E2Xc.2.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.E2Xc.2.1.1.m1.1.1.3.1a" xref="S3.E2Xc.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.2.1.1.m1.1.1.3.4" xref="S3.E2Xc.2.1.1.m1.1.1.3.4.cmml">e</mi><mo id="S3.E2Xc.2.1.1.m1.1.1.3.1b" xref="S3.E2Xc.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.2.1.1.m1.1.1.3.5" xref="S3.E2Xc.2.1.1.m1.1.1.3.5.cmml">d</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E2Xc.2.1.1.m1.1b"><apply id="S3.E2Xc.2.1.1.m1.1.1.cmml" xref="S3.E2Xc.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E2Xc.2.1.1.m1.1.1.1.cmml" xref="S3.E2Xc.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.E2Xc.2.1.1.m1.1.1.2.cmml" xref="S3.E2Xc.2.1.1.m1.1.1.2">𝑃</ci><apply id="S3.E2Xc.2.1.1.m1.1.1.3.cmml" xref="S3.E2Xc.2.1.1.m1.1.1.3"><times id="S3.E2Xc.2.1.1.m1.1.1.3.1.cmml" xref="S3.E2Xc.2.1.1.m1.1.1.3.1"></times><ci id="S3.E2Xc.2.1.1.m1.1.1.3.2.cmml" xref="S3.E2Xc.2.1.1.m1.1.1.3.2">𝑠</ci><ci id="S3.E2Xc.2.1.1.m1.1.1.3.3.cmml" xref="S3.E2Xc.2.1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.E2Xc.2.1.1.m1.1.1.3.4.cmml" xref="S3.E2Xc.2.1.1.m1.1.1.3.4">𝑒</ci><ci id="S3.E2Xc.2.1.1.m1.1.1.3.5.cmml" xref="S3.E2Xc.2.1.1.m1.1.1.3.5">𝑑</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xc.2.1.1.m1.1c">\displaystyle P_{seed}</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xc.2.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\in\{D_{caption},D_{tag}\}" class="ltx_Math" display="inline" id="S3.E2Xc.3.2.2.m1.2"><semantics id="S3.E2Xc.3.2.2.m1.2a"><mrow id="S3.E2Xc.3.2.2.m1.2.2" xref="S3.E2Xc.3.2.2.m1.2.2.cmml"><mi id="S3.E2Xc.3.2.2.m1.2.2.4" xref="S3.E2Xc.3.2.2.m1.2.2.4.cmml"></mi><mo id="S3.E2Xc.3.2.2.m1.2.2.3" xref="S3.E2Xc.3.2.2.m1.2.2.3.cmml">∈</mo><mrow id="S3.E2Xc.3.2.2.m1.2.2.2.2" xref="S3.E2Xc.3.2.2.m1.2.2.2.3.cmml"><mo id="S3.E2Xc.3.2.2.m1.2.2.2.2.3" stretchy="false" xref="S3.E2Xc.3.2.2.m1.2.2.2.3.cmml">{</mo><msub id="S3.E2Xc.3.2.2.m1.1.1.1.1.1" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.cmml"><mi id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.2" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.2.cmml">D</mi><mrow id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.cmml"><mi id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.2" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.2.cmml">c</mi><mo id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.3" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.3.cmml">a</mi><mo id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1a" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.4" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.4.cmml">p</mi><mo id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1b" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.5" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.5.cmml">t</mi><mo id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1c" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.6" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.6.cmml">i</mi><mo id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1d" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.7" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.7.cmml">o</mi><mo id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1e" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.8" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.8.cmml">n</mi></mrow></msub><mo id="S3.E2Xc.3.2.2.m1.2.2.2.2.4" xref="S3.E2Xc.3.2.2.m1.2.2.2.3.cmml">,</mo><msub id="S3.E2Xc.3.2.2.m1.2.2.2.2.2" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.cmml"><mi id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.2" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.2.cmml">D</mi><mrow id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.cmml"><mi id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.2" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.2.cmml">t</mi><mo id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.1" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.3" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.3.cmml">a</mi><mo id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.1a" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.4" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.4.cmml">g</mi></mrow></msub><mo id="S3.E2Xc.3.2.2.m1.2.2.2.2.5" stretchy="false" xref="S3.E2Xc.3.2.2.m1.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2Xc.3.2.2.m1.2b"><apply id="S3.E2Xc.3.2.2.m1.2.2.cmml" xref="S3.E2Xc.3.2.2.m1.2.2"><in id="S3.E2Xc.3.2.2.m1.2.2.3.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.3"></in><csymbol cd="latexml" id="S3.E2Xc.3.2.2.m1.2.2.4.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.4">absent</csymbol><set id="S3.E2Xc.3.2.2.m1.2.2.2.3.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2"><apply id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.1.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.2.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.2">𝐷</ci><apply id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3"><times id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.1"></times><ci id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.2.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.2">𝑐</ci><ci id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.3.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.3">𝑎</ci><ci id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.4.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.4">𝑝</ci><ci id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.5.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.5">𝑡</ci><ci id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.6.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.6">𝑖</ci><ci id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.7.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.7">𝑜</ci><ci id="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.8.cmml" xref="S3.E2Xc.3.2.2.m1.1.1.1.1.1.3.8">𝑛</ci></apply></apply><apply id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.1.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2">subscript</csymbol><ci id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.2.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.2">𝐷</ci><apply id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3"><times id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.1.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.1"></times><ci id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.2.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.2">𝑡</ci><ci id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.3.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.3">𝑎</ci><ci id="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.4.cmml" xref="S3.E2Xc.3.2.2.m1.2.2.2.2.2.3.4">𝑔</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xc.3.2.2.m1.2c">\displaystyle\in\{D_{caption},D_{tag}\}</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xc.3.2.2.m1.2d">∈ { italic_D start_POSTSUBSCRIPT italic_c italic_a italic_p italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_t italic_a italic_g end_POSTSUBSCRIPT }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S3.E2Xd">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P_{gen}" class="ltx_Math" display="inline" id="S3.E2Xd.2.1.1.m1.1"><semantics id="S3.E2Xd.2.1.1.m1.1a"><msub id="S3.E2Xd.2.1.1.m1.1.1" xref="S3.E2Xd.2.1.1.m1.1.1.cmml"><mi id="S3.E2Xd.2.1.1.m1.1.1.2" xref="S3.E2Xd.2.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.E2Xd.2.1.1.m1.1.1.3" xref="S3.E2Xd.2.1.1.m1.1.1.3.cmml"><mi id="S3.E2Xd.2.1.1.m1.1.1.3.2" xref="S3.E2Xd.2.1.1.m1.1.1.3.2.cmml">g</mi><mo id="S3.E2Xd.2.1.1.m1.1.1.3.1" xref="S3.E2Xd.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xd.2.1.1.m1.1.1.3.3" xref="S3.E2Xd.2.1.1.m1.1.1.3.3.cmml">e</mi><mo id="S3.E2Xd.2.1.1.m1.1.1.3.1a" xref="S3.E2Xd.2.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xd.2.1.1.m1.1.1.3.4" xref="S3.E2Xd.2.1.1.m1.1.1.3.4.cmml">n</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.E2Xd.2.1.1.m1.1b"><apply id="S3.E2Xd.2.1.1.m1.1.1.cmml" xref="S3.E2Xd.2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.E2Xd.2.1.1.m1.1.1.1.cmml" xref="S3.E2Xd.2.1.1.m1.1.1">subscript</csymbol><ci id="S3.E2Xd.2.1.1.m1.1.1.2.cmml" xref="S3.E2Xd.2.1.1.m1.1.1.2">𝑃</ci><apply id="S3.E2Xd.2.1.1.m1.1.1.3.cmml" xref="S3.E2Xd.2.1.1.m1.1.1.3"><times id="S3.E2Xd.2.1.1.m1.1.1.3.1.cmml" xref="S3.E2Xd.2.1.1.m1.1.1.3.1"></times><ci id="S3.E2Xd.2.1.1.m1.1.1.3.2.cmml" xref="S3.E2Xd.2.1.1.m1.1.1.3.2">𝑔</ci><ci id="S3.E2Xd.2.1.1.m1.1.1.3.3.cmml" xref="S3.E2Xd.2.1.1.m1.1.1.3.3">𝑒</ci><ci id="S3.E2Xd.2.1.1.m1.1.1.3.4.cmml" xref="S3.E2Xd.2.1.1.m1.1.1.3.4">𝑛</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xd.2.1.1.m1.1c">\displaystyle P_{gen}</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xd.2.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\in\{P_{diff},P_{cap,both}\}." class="ltx_Math" display="inline" id="S3.E2Xd.3.2.2.m1.3"><semantics id="S3.E2Xd.3.2.2.m1.3a"><mrow id="S3.E2Xd.3.2.2.m1.3.3.1" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.cmml"><mrow id="S3.E2Xd.3.2.2.m1.3.3.1.1" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.cmml"><mi id="S3.E2Xd.3.2.2.m1.3.3.1.1.4" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.4.cmml"></mi><mo id="S3.E2Xd.3.2.2.m1.3.3.1.1.3" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.3.cmml">∈</mo><mrow id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.3.cmml"><mo id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.3" stretchy="false" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.3.cmml">{</mo><msub id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.cmml"><mi id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.2" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.2.cmml">P</mi><mrow id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.cmml"><mi id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.2" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.2.cmml">d</mi><mo id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.1" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.3" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.3.cmml">i</mi><mo id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.1a" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.4" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.4.cmml">f</mi><mo id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.1b" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.5" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.5.cmml">f</mi></mrow></msub><mo id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.4" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.3.cmml">,</mo><msub id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2.cmml"><mi id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2.2" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2.2.cmml">P</mi><mrow id="S3.E2Xd.3.2.2.m1.2.2.2.2" xref="S3.E2Xd.3.2.2.m1.2.2.2.3.cmml"><mrow id="S3.E2Xd.3.2.2.m1.1.1.1.1.1" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.cmml"><mi id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.2" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.2.cmml">c</mi><mo id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.1" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.3" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.3.cmml">a</mi><mo id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.1a" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.4" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.4.cmml">p</mi></mrow><mo id="S3.E2Xd.3.2.2.m1.2.2.2.2.3" xref="S3.E2Xd.3.2.2.m1.2.2.2.3.cmml">,</mo><mrow id="S3.E2Xd.3.2.2.m1.2.2.2.2.2" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.cmml"><mi id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.2" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.2.cmml">b</mi><mo id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.1" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.3" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.3.cmml">o</mi><mo id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.1a" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.4" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.4.cmml">t</mi><mo id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.1b" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.1.cmml">⁢</mo><mi id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.5" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.5.cmml">h</mi></mrow></mrow></msub><mo id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.5" stretchy="false" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.3.cmml">}</mo></mrow></mrow><mo id="S3.E2Xd.3.2.2.m1.3.3.1.2" lspace="0em" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E2Xd.3.2.2.m1.3b"><apply id="S3.E2Xd.3.2.2.m1.3.3.1.1.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1"><in id="S3.E2Xd.3.2.2.m1.3.3.1.1.3.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.3"></in><csymbol cd="latexml" id="S3.E2Xd.3.2.2.m1.3.3.1.1.4.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.4">absent</csymbol><set id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.3.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2"><apply id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.1.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1">subscript</csymbol><ci id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.2.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.2">𝑃</ci><apply id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3"><times id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.1.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.1"></times><ci id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.2.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.2">𝑑</ci><ci id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.3.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.3">𝑖</ci><ci id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.4.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.4">𝑓</ci><ci id="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.5.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.1.1.1.3.5">𝑓</ci></apply></apply><apply id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2"><csymbol cd="ambiguous" id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2.1.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2">subscript</csymbol><ci id="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2.2.cmml" xref="S3.E2Xd.3.2.2.m1.3.3.1.1.2.2.2.2">𝑃</ci><list id="S3.E2Xd.3.2.2.m1.2.2.2.3.cmml" xref="S3.E2Xd.3.2.2.m1.2.2.2.2"><apply id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.cmml" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1"><times id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.1.cmml" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.1"></times><ci id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.2.cmml" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.2">𝑐</ci><ci id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.3.cmml" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.3">𝑎</ci><ci id="S3.E2Xd.3.2.2.m1.1.1.1.1.1.4.cmml" xref="S3.E2Xd.3.2.2.m1.1.1.1.1.1.4">𝑝</ci></apply><apply id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.cmml" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2"><times id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.1.cmml" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.1"></times><ci id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.2.cmml" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.2">𝑏</ci><ci id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.3.cmml" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.3">𝑜</ci><ci id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.4.cmml" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.4">𝑡</ci><ci id="S3.E2Xd.3.2.2.m1.2.2.2.2.2.5.cmml" xref="S3.E2Xd.3.2.2.m1.2.2.2.2.2.5">ℎ</ci></apply></list></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2Xd.3.2.2.m1.3c">\displaystyle\in\{P_{diff},P_{cap,both}\}.</annotation><annotation encoding="application/x-llamapun" id="S3.E2Xd.3.2.2.m1.3d">∈ { italic_P start_POSTSUBSCRIPT italic_d italic_i italic_f italic_f end_POSTSUBSCRIPT , italic_P start_POSTSUBSCRIPT italic_c italic_a italic_p , italic_b italic_o italic_t italic_h end_POSTSUBSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS2.p7">
<p class="ltx_p" id="S3.SS2.p7.3">In Equation <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.E2" title="In III-B BALSa-MA: Bootstrapping Multi-Audio Learning via Difference Comparison and Joint Captioning ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">2</span></a>, <math alttext="\parallel" class="ltx_Math" display="inline" id="S3.SS2.p7.1.m1.1"><semantics id="S3.SS2.p7.1.m1.1a"><mo id="S3.SS2.p7.1.m1.1.1" xref="S3.SS2.p7.1.m1.1.1.cmml">∥</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.1.m1.1b"><csymbol cd="latexml" id="S3.SS2.p7.1.m1.1.1.cmml" xref="S3.SS2.p7.1.m1.1.1">parallel-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.1.m1.1c">\parallel</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.1.m1.1d">∥</annotation></semantics></math> denotes concatenation, while <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.3.1">[Begin of audio]</span> and <span class="ltx_text ltx_font_italic" id="S3.SS2.p7.3.2">[End of audio]</span> are expressed in natural language format to serve two main purposes.
First, they indicate to the model that the enclosed content corresponds to the audio.
Second, they provide flexibility for adaptation to multiple-audio formats.
When training audio-aware large language models, the positions of <math alttext="{P}_{{seed,1}}" class="ltx_Math" display="inline" id="S3.SS2.p7.2.m2.2"><semantics id="S3.SS2.p7.2.m2.2a"><msub id="S3.SS2.p7.2.m2.2.3" xref="S3.SS2.p7.2.m2.2.3.cmml"><mi id="S3.SS2.p7.2.m2.2.3.2" xref="S3.SS2.p7.2.m2.2.3.2.cmml">P</mi><mrow id="S3.SS2.p7.2.m2.2.2.2.2" xref="S3.SS2.p7.2.m2.2.2.2.3.cmml"><mrow id="S3.SS2.p7.2.m2.2.2.2.2.1" xref="S3.SS2.p7.2.m2.2.2.2.2.1.cmml"><mi id="S3.SS2.p7.2.m2.2.2.2.2.1.2" xref="S3.SS2.p7.2.m2.2.2.2.2.1.2.cmml">s</mi><mo id="S3.SS2.p7.2.m2.2.2.2.2.1.1" xref="S3.SS2.p7.2.m2.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p7.2.m2.2.2.2.2.1.3" xref="S3.SS2.p7.2.m2.2.2.2.2.1.3.cmml">e</mi><mo id="S3.SS2.p7.2.m2.2.2.2.2.1.1a" xref="S3.SS2.p7.2.m2.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p7.2.m2.2.2.2.2.1.4" xref="S3.SS2.p7.2.m2.2.2.2.2.1.4.cmml">e</mi><mo id="S3.SS2.p7.2.m2.2.2.2.2.1.1b" xref="S3.SS2.p7.2.m2.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p7.2.m2.2.2.2.2.1.5" xref="S3.SS2.p7.2.m2.2.2.2.2.1.5.cmml">d</mi></mrow><mo id="S3.SS2.p7.2.m2.2.2.2.2.2" xref="S3.SS2.p7.2.m2.2.2.2.3.cmml">,</mo><mn id="S3.SS2.p7.2.m2.1.1.1.1" xref="S3.SS2.p7.2.m2.1.1.1.1.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.2.m2.2b"><apply id="S3.SS2.p7.2.m2.2.3.cmml" xref="S3.SS2.p7.2.m2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p7.2.m2.2.3.1.cmml" xref="S3.SS2.p7.2.m2.2.3">subscript</csymbol><ci id="S3.SS2.p7.2.m2.2.3.2.cmml" xref="S3.SS2.p7.2.m2.2.3.2">𝑃</ci><list id="S3.SS2.p7.2.m2.2.2.2.3.cmml" xref="S3.SS2.p7.2.m2.2.2.2.2"><apply id="S3.SS2.p7.2.m2.2.2.2.2.1.cmml" xref="S3.SS2.p7.2.m2.2.2.2.2.1"><times id="S3.SS2.p7.2.m2.2.2.2.2.1.1.cmml" xref="S3.SS2.p7.2.m2.2.2.2.2.1.1"></times><ci id="S3.SS2.p7.2.m2.2.2.2.2.1.2.cmml" xref="S3.SS2.p7.2.m2.2.2.2.2.1.2">𝑠</ci><ci id="S3.SS2.p7.2.m2.2.2.2.2.1.3.cmml" xref="S3.SS2.p7.2.m2.2.2.2.2.1.3">𝑒</ci><ci id="S3.SS2.p7.2.m2.2.2.2.2.1.4.cmml" xref="S3.SS2.p7.2.m2.2.2.2.2.1.4">𝑒</ci><ci id="S3.SS2.p7.2.m2.2.2.2.2.1.5.cmml" xref="S3.SS2.p7.2.m2.2.2.2.2.1.5">𝑑</ci></apply><cn id="S3.SS2.p7.2.m2.1.1.1.1.cmml" type="integer" xref="S3.SS2.p7.2.m2.1.1.1.1">1</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.2.m2.2c">{P}_{{seed,1}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.2.m2.2d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d , 1 end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="{P}_{{seed,2}}" class="ltx_Math" display="inline" id="S3.SS2.p7.3.m3.2"><semantics id="S3.SS2.p7.3.m3.2a"><msub id="S3.SS2.p7.3.m3.2.3" xref="S3.SS2.p7.3.m3.2.3.cmml"><mi id="S3.SS2.p7.3.m3.2.3.2" xref="S3.SS2.p7.3.m3.2.3.2.cmml">P</mi><mrow id="S3.SS2.p7.3.m3.2.2.2.2" xref="S3.SS2.p7.3.m3.2.2.2.3.cmml"><mrow id="S3.SS2.p7.3.m3.2.2.2.2.1" xref="S3.SS2.p7.3.m3.2.2.2.2.1.cmml"><mi id="S3.SS2.p7.3.m3.2.2.2.2.1.2" xref="S3.SS2.p7.3.m3.2.2.2.2.1.2.cmml">s</mi><mo id="S3.SS2.p7.3.m3.2.2.2.2.1.1" xref="S3.SS2.p7.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p7.3.m3.2.2.2.2.1.3" xref="S3.SS2.p7.3.m3.2.2.2.2.1.3.cmml">e</mi><mo id="S3.SS2.p7.3.m3.2.2.2.2.1.1a" xref="S3.SS2.p7.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p7.3.m3.2.2.2.2.1.4" xref="S3.SS2.p7.3.m3.2.2.2.2.1.4.cmml">e</mi><mo id="S3.SS2.p7.3.m3.2.2.2.2.1.1b" xref="S3.SS2.p7.3.m3.2.2.2.2.1.1.cmml">⁢</mo><mi id="S3.SS2.p7.3.m3.2.2.2.2.1.5" xref="S3.SS2.p7.3.m3.2.2.2.2.1.5.cmml">d</mi></mrow><mo id="S3.SS2.p7.3.m3.2.2.2.2.2" xref="S3.SS2.p7.3.m3.2.2.2.3.cmml">,</mo><mn id="S3.SS2.p7.3.m3.1.1.1.1" xref="S3.SS2.p7.3.m3.1.1.1.1.cmml">2</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p7.3.m3.2b"><apply id="S3.SS2.p7.3.m3.2.3.cmml" xref="S3.SS2.p7.3.m3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p7.3.m3.2.3.1.cmml" xref="S3.SS2.p7.3.m3.2.3">subscript</csymbol><ci id="S3.SS2.p7.3.m3.2.3.2.cmml" xref="S3.SS2.p7.3.m3.2.3.2">𝑃</ci><list id="S3.SS2.p7.3.m3.2.2.2.3.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2"><apply id="S3.SS2.p7.3.m3.2.2.2.2.1.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.1"><times id="S3.SS2.p7.3.m3.2.2.2.2.1.1.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.1.1"></times><ci id="S3.SS2.p7.3.m3.2.2.2.2.1.2.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.1.2">𝑠</ci><ci id="S3.SS2.p7.3.m3.2.2.2.2.1.3.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.1.3">𝑒</ci><ci id="S3.SS2.p7.3.m3.2.2.2.2.1.4.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.1.4">𝑒</ci><ci id="S3.SS2.p7.3.m3.2.2.2.2.1.5.cmml" xref="S3.SS2.p7.3.m3.2.2.2.2.1.5">𝑑</ci></apply><cn id="S3.SS2.p7.3.m3.1.1.1.1.cmml" type="integer" xref="S3.SS2.p7.3.m3.1.1.1.1">2</cn></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p7.3.m3.2c">{P}_{{seed,2}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p7.3.m3.2d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d , 2 end_POSTSUBSCRIPT</annotation></semantics></math> will be replaced with the corresponding audio representations.
The generated responses based on the input context are then used as training targets.
Actual examples are presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.T2" title="TABLE II ‣ III-B BALSa-MA: Bootstrapping Multi-Audio Learning via Difference Comparison and Joint Captioning ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">II</span></a>.</p>
</div>
<figure class="ltx_table" id="S3.T2">
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Examples corresponding to different generation prompts.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.2">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.2.3.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="S3.T2.2.3.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.3.1.1.1">
<span class="ltx_p" id="S3.T2.2.3.1.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.3.1.1.1.1.1" style="font-size:90%;">Input Audio:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.4.2">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.2.4.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.4.2.1.1">
<span class="ltx_p" id="S3.T2.2.4.2.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T2.2.4.2.1.1.1.1" style="font-size:90%;color:#14248A;">Audio1 Caption: A woman talks nearby while water pours.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.5.3">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.2.5.3.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.5.3.1.1">
<span class="ltx_p" id="S3.T2.2.5.3.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T2.2.5.3.1.1.1.1" style="font-size:90%;color:#14248A;">Audio2 Caption: A dog barks and a man speaks.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.1.1.1.1">
<span class="ltx_p" id="S3.T2.1.1.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1.1.1" style="font-size:90%;">Discrimination Generation Prompt (<math alttext="\bm{P}_{\bm{pos}}" class="ltx_Math" display="inline" id="S3.T2.1.1.1.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.1.1.1.m1.1a"><msub id="S3.T2.1.1.1.1.1.1.m1.1.1" xref="S3.T2.1.1.1.1.1.1.m1.1.1.cmml"><mi id="S3.T2.1.1.1.1.1.1.m1.1.1.2" xref="S3.T2.1.1.1.1.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.T2.1.1.1.1.1.1.m1.1.1.3" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.cmml"><mi id="S3.T2.1.1.1.1.1.1.m1.1.1.3.2" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.2.cmml">p</mi><mo id="S3.T2.1.1.1.1.1.1.m1.1.1.3.1" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T2.1.1.1.1.1.1.m1.1.1.3.3" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.3.cmml">o</mi><mo id="S3.T2.1.1.1.1.1.1.m1.1.1.3.1a" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T2.1.1.1.1.1.1.m1.1.1.3.4" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.4.cmml">s</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.1.1.m1.1b"><apply id="S3.T2.1.1.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.1.1.1.1.1.1.m1.1.1.1.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T2.1.1.1.1.1.1.m1.1.1.2.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1.2">𝑃</ci><apply id="S3.T2.1.1.1.1.1.1.m1.1.1.3.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3"><times id="S3.T2.1.1.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.1"></times><ci id="S3.T2.1.1.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.2">𝑝</ci><ci id="S3.T2.1.1.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.T2.1.1.1.1.1.1.m1.1.1.3.4.cmml" xref="S3.T2.1.1.1.1.1.1.m1.1.1.3.4">𝑠</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.1.1.m1.1c">\bm{P}_{\bm{pos}}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.1.1.m1.1d">bold_italic_P start_POSTSUBSCRIPT bold_italic_p bold_italic_o bold_italic_s end_POSTSUBSCRIPT</annotation></semantics></math>):</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.6.4">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.2.6.4.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.6.4.1.1">
<span class="ltx_p" id="S3.T2.2.6.4.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T2.2.6.4.1.1.1.1" style="font-size:90%;">Explain the difference between two audio.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.7.5">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.2.7.5.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.7.5.1.1">
<span class="ltx_p" id="S3.T2.2.7.5.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.7.5.1.1.1.1" style="font-size:90%;">Response:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.8.6">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.2.8.6.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.8.6.1.1">
<span class="ltx_p" id="S3.T2.2.8.6.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T2.2.8.6.1.1.1.1" style="font-size:90%;">Audio1 features a woman speaking with the background sound of pouring water, while Audio2 contains a dog barking alongside a man’s speech, differing in both speaker identity and background sound events.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="S3.T2.2.2.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.2.1.1">
<span class="ltx_p" id="S3.T2.2.2.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.2.1.1.1.1" style="font-size:90%;">Both Audio Captioning Prompt (<math alttext="\bm{P}_{\bm{comb}}" class="ltx_Math" display="inline" id="S3.T2.2.2.1.1.1.1.m1.1"><semantics id="S3.T2.2.2.1.1.1.1.m1.1a"><msub id="S3.T2.2.2.1.1.1.1.m1.1.1" xref="S3.T2.2.2.1.1.1.1.m1.1.1.cmml"><mi id="S3.T2.2.2.1.1.1.1.m1.1.1.2" xref="S3.T2.2.2.1.1.1.1.m1.1.1.2.cmml">P</mi><mrow id="S3.T2.2.2.1.1.1.1.m1.1.1.3" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.cmml"><mi id="S3.T2.2.2.1.1.1.1.m1.1.1.3.2" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.2.cmml">c</mi><mo id="S3.T2.2.2.1.1.1.1.m1.1.1.3.1" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T2.2.2.1.1.1.1.m1.1.1.3.3" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.3.cmml">o</mi><mo id="S3.T2.2.2.1.1.1.1.m1.1.1.3.1a" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T2.2.2.1.1.1.1.m1.1.1.3.4" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.4.cmml">m</mi><mo id="S3.T2.2.2.1.1.1.1.m1.1.1.3.1b" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S3.T2.2.2.1.1.1.1.m1.1.1.3.5" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.5.cmml">b</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.1.1.1.1.m1.1b"><apply id="S3.T2.2.2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.2.2.1.1.1.1.m1.1.1.1.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1">subscript</csymbol><ci id="S3.T2.2.2.1.1.1.1.m1.1.1.2.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1.2">𝑃</ci><apply id="S3.T2.2.2.1.1.1.1.m1.1.1.3.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3"><times id="S3.T2.2.2.1.1.1.1.m1.1.1.3.1.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.1"></times><ci id="S3.T2.2.2.1.1.1.1.m1.1.1.3.2.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.2">𝑐</ci><ci id="S3.T2.2.2.1.1.1.1.m1.1.1.3.3.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.3">𝑜</ci><ci id="S3.T2.2.2.1.1.1.1.m1.1.1.3.4.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.4">𝑚</ci><ci id="S3.T2.2.2.1.1.1.1.m1.1.1.3.5.cmml" xref="S3.T2.2.2.1.1.1.1.m1.1.1.3.5">𝑏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.1.1.1.1.m1.1c">\bm{P}_{\bm{comb}}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.1.1.1.1.m1.1d">bold_italic_P start_POSTSUBSCRIPT bold_italic_c bold_italic_o bold_italic_m bold_italic_b end_POSTSUBSCRIPT</annotation></semantics></math>):</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.9.7">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.2.9.7.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.9.7.1.1">
<span class="ltx_p" id="S3.T2.2.9.7.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T2.2.9.7.1.1.1.1" style="font-size:90%;">Repeat the both audio.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.10.8">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.2.10.8.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.10.8.1.1">
<span class="ltx_p" id="S3.T2.2.10.8.1.1.1" style="width:207.0pt;"><span class="ltx_text ltx_font_bold" id="S3.T2.2.10.8.1.1.1.1" style="font-size:90%;">Response:</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.11.9">
<td class="ltx_td ltx_align_justify ltx_align_top" id="S3.T2.2.11.9.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.11.9.1.1">
<span class="ltx_p" id="S3.T2.2.11.9.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T2.2.11.9.1.1.1.1" style="font-size:90%;">Audio1: The sound of water pouring accompanies a woman speaking nearby.</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="S3.T2.2.12.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_bb" id="S3.T2.2.12.10.1">
<span class="ltx_inline-block ltx_align_top" id="S3.T2.2.12.10.1.1">
<span class="ltx_p" id="S3.T2.2.12.10.1.1.1" style="width:207.0pt;"><span class="ltx_text" id="S3.T2.2.12.10.1.1.1.1" style="font-size:90%;">Audio2: A man talks while a dog barks in the background.</span></span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS3.5.1.1">III-C</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS3.6.2">Model Design and Training Approach</span>
</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3.F1" title="Figure 1 ‣ III-A Bootstrapping Audio-Language Alignment via Synthetic Data Generation from Backbone LLMs ‣ III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">1</span></a> illustrates the architecture of our model based on BALSa, applied to single-audio and multi-audio scenarios, respectively.
Our model consists of three main components: audio encoder, backbone large language model, and audio modality adapter.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p2">
<p class="ltx_p" id="S3.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p2.1.1">Audio Encoder</span>.
We employ <span class="ltx_text ltx_font_italic" id="S3.SS3.p2.1.2">Whisper</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib28" title="">28</a>]</cite>, a foundation model, as the audio encoder.
Previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib29" title="">29</a>]</cite> have demonstrated Whisper’s strong performance across various audio-related tasks, despite its original design being primarily for automatic speech recognition and speech translation.
Since Whisper follows an encoder-decoder architecture, we utilize only its encoder component.
To preserve the advantages of the pre-trained model, the parameters of the audio encoder remain frozen.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p3">
<p class="ltx_p" id="S3.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p3.1.1">Backbone Large Language Model</span>.
This study adopts <span class="ltx_text ltx_font_italic" id="S3.SS3.p3.1.2">LLaMA-3.1-8B-Instruct</span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib21" title="">21</a>]</cite>, an instruction-tuned model, as the backbone large language model.
To maintain its original text-processing capabilities, we do not apply LoRA or fine-tune any of its parameters.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS3.p4">
<p class="ltx_p" id="S3.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.SS3.p4.1.1">Audio Modality Adapter</span>.
The only trainable component is the audio modality adapter, which is randomly initialized.
This adapter projects the output representations extracted by the audio encoder into the input space of the backbone large language model.
Specifically, we employ a Qformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib30" title="">30</a>]</cite> architecture to extract audio features from the intermediate layers of the Whisper encoder.
These extracted features are aggregated using a weighted sum with learnable weights, followed by a linear projection layer that aligns the aggregated features with the input dimensions of the backbone large language model.
We optimize the entire architecture end-to-end with a next-token prediction loss, using the model’s own generated responses as learning targets.
To improve efficiency, we freeze the parameters of both the backbone LLM and the audio encoder.
Instead, we adopt a lightweight adapter-based training strategy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib18" title="">18</a>]</cite>, in which only the modality adapter is trained.
This design not only conserves computational resources but also preserves the LLM’s original linguistic knowledge while enabling it to process audio through the adapter.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Experimental Setup</span>
</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Training Datasets</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The training datasets include AudioSet-20K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib32" title="">32</a>]</cite>, AudioCaps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib33" title="">33</a>]</cite>, FSD50K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib34" title="">34</a>]</cite>, MACS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib35" title="">35</a>]</cite>, ESC-50 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib36" title="">36</a>]</cite>, UrbanSound8K <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib37" title="">37</a>]</cite>, Clotho <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib38" title="">38</a>]</cite>, and VocalSound <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib39" title="">39</a>]</cite>.
Among these datasets, AudioCaps, Clotho, and MACS provide ground-truth labels in the form of human-annotated audio captions, while the rest use sound event tags.
Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S4.T3" title="TABLE III ‣ IV-A Training Datasets ‣ IV Experimental Setup ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">III</span></a> summarizes the total duration of the training datasets used in our work and compares it with the training data durations reported in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>]</cite>.
The “Duration” column represents the original total duration of unique audio in each dataset, measured in hours and excluding any duplicate segments.
The “Equi. Duration” column accounts for the effect of generating multiple aligned text descriptions per audio clip, leading to an increased effective training duration, also measured in hours.
The blue numbers indicate the amount of data used in the multi-audio scenario. The number of samples represents the total number of data instances used during training.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE III: </span>
Summary of training datasets.
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.3.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.1.1.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.1.1" style="font-size:80%;">Dataset</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.2.1" style="font-size:80%;">Duration</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.3.1" style="font-size:80%;">Equi. Duration</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S4.T3.3.1.1.4.1" style="font-size:80%;"># Samples</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.2.2.1">
<span class="ltx_text" id="S4.T3.3.2.2.1.1" style="font-size:80%;">AudioSet-20K</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.2.2.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib32" title="">32</a><span class="ltx_text" id="S4.T3.3.2.2.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.2.2"><span class="ltx_text" id="S4.T3.3.2.2.2.1" style="font-size:80%;">60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.2.3"><span class="ltx_text" id="S4.T3.3.2.2.3.1" style="font-size:80%;">121</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.2.2.4"><span class="ltx_text" id="S4.T3.3.2.2.4.1" style="font-size:80%;">42K</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.3.3.1">
<span class="ltx_text" id="S4.T3.3.3.3.1.1" style="font-size:80%;">AudioCaps</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.3.3.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib33" title="">33</a><span class="ltx_text" id="S4.T3.3.3.3.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.2"><span class="ltx_text" id="S4.T3.3.3.3.2.1" style="font-size:80%;">138</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.3">
<span class="ltx_text" id="S4.T3.3.3.3.3.1" style="font-size:80%;">276+</span><span class="ltx_text" id="S4.T3.3.3.3.3.2" style="font-size:80%;color:#0000FF;">269</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.3.3.4">
<span class="ltx_text" id="S4.T3.3.3.3.4.1" style="font-size:80%;">96K+</span><span class="ltx_text" id="S4.T3.3.3.3.4.2" style="font-size:80%;color:#0000FF;">49K</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.4.4.1">
<span class="ltx_text" id="S4.T3.3.4.4.1.1" style="font-size:80%;">Clotho</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.4.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib38" title="">38</a><span class="ltx_text" id="S4.T3.3.4.4.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.4.2"><span class="ltx_text" id="S4.T3.3.4.4.2.1" style="font-size:80%;">18</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.4.3">
<span class="ltx_text" id="S4.T3.3.4.4.3.1" style="font-size:80%;">36+</span><span class="ltx_text" id="S4.T3.3.4.4.3.2" style="font-size:80%;color:#0000FF;">160</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.4.4.4">
<span class="ltx_text" id="S4.T3.3.4.4.4.1" style="font-size:80%;">27K+</span><span class="ltx_text" id="S4.T3.3.4.4.4.2" style="font-size:80%;color:#0000FF;">13K</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.5.5.1">
<span class="ltx_text" id="S4.T3.3.5.5.1.1" style="font-size:80%;">MACS</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.5.5.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib35" title="">35</a><span class="ltx_text" id="S4.T3.3.5.5.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.5.2"><span class="ltx_text" id="S4.T3.3.5.5.2.1" style="font-size:80%;">11</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.5.3"><span class="ltx_text" id="S4.T3.3.5.5.3.1" style="font-size:80%;">22</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.5.5.4"><span class="ltx_text" id="S4.T3.3.5.5.4.1" style="font-size:80%;">33K</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.6.6.1">
<span class="ltx_text" id="S4.T3.3.6.6.1.1" style="font-size:80%;">FSD50K</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.6.6.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib34" title="">34</a><span class="ltx_text" id="S4.T3.3.6.6.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.6.2"><span class="ltx_text" id="S4.T3.3.6.6.2.1" style="font-size:80%;">56</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.6.3">
<span class="ltx_text" id="S4.T3.3.6.6.3.1" style="font-size:80%;">113+</span><span class="ltx_text" id="S4.T3.3.6.6.3.2" style="font-size:80%;color:#0000FF;">75</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.6.6.4">
<span class="ltx_text" id="S4.T3.3.6.6.4.1" style="font-size:80%;">56K+</span><span class="ltx_text" id="S4.T3.3.6.6.4.2" style="font-size:80%;color:#0000FF;">19K</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.7.7.1">
<span class="ltx_text" id="S4.T3.3.7.7.1.1" style="font-size:80%;">ESC50</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.7.7.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib36" title="">36</a><span class="ltx_text" id="S4.T3.3.7.7.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.7.2"><span class="ltx_text" id="S4.T3.3.7.7.2.1" style="font-size:80%;">3</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.7.3">
<span class="ltx_text" id="S4.T3.3.7.7.3.1" style="font-size:80%;">6+</span><span class="ltx_text" id="S4.T3.3.7.7.3.2" style="font-size:80%;color:#0000FF;">28</span>
</td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.7.7.4">
<span class="ltx_text" id="S4.T3.3.7.7.4.1" style="font-size:80%;">4K+</span><span class="ltx_text" id="S4.T3.3.7.7.4.2" style="font-size:80%;color:#0000FF;">10K</span>
</td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.8.8.1">
<span class="ltx_text" id="S4.T3.3.8.8.1.1" style="font-size:80%;">UrbanSound8K</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.8.8.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib37" title="">37</a><span class="ltx_text" id="S4.T3.3.8.8.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.8.2"><span class="ltx_text" id="S4.T3.3.8.8.2.1" style="font-size:80%;">10</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.8.3"><span class="ltx_text" id="S4.T3.3.8.8.3.1" style="font-size:80%;">19</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.8.8.4"><span class="ltx_text" id="S4.T3.3.8.8.4.1" style="font-size:80%;">17K</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.9.9.1">
<span class="ltx_text" id="S4.T3.3.9.9.1.1" style="font-size:80%;">VocalSound</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.T3.3.9.9.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib39" title="">39</a><span class="ltx_text" id="S4.T3.3.9.9.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.9.9.2"><span class="ltx_text" id="S4.T3.3.9.9.2.1" style="font-size:80%;">20</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.9.9.3"><span class="ltx_text" id="S4.T3.3.9.9.3.1" style="font-size:80%;">40</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.9.9.4"><span class="ltx_text" id="S4.T3.3.9.9.4.1" style="font-size:80%;">34K</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.10.10.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.10.10.1.1" style="font-size:80%;">Total (Ours)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.10.10.2"><span class="ltx_text" id="S4.T3.3.10.10.2.1" style="font-size:80%;">316</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.10.10.3"><span class="ltx_text" id="S4.T3.3.10.10.3.1" style="font-size:80%;">1164</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.10.10.4"><span class="ltx_text" id="S4.T3.3.10.10.4.1" style="font-size:80%;">400K</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T3.3.11.11.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.11.11.1.1" style="font-size:80%;">Qwen2-Audio<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_medium" id="S4.T3.3.11.11.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a><span class="ltx_text ltx_font_medium" id="S4.T3.3.11.11.1.1.2.2">]</span></cite></span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.11.11.2"><span class="ltx_text" id="S4.T3.3.11.11.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.11.11.3"><span class="ltx_text" id="S4.T3.3.11.11.3.1" style="font-size:80%;">10000</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.3.11.11.4"><span class="ltx_text" id="S4.T3.3.11.11.4.1" style="font-size:80%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T3.3.12.12.1"><span class="ltx_text ltx_font_bold" id="S4.T3.3.12.12.1.1" style="font-size:80%;">SALMONN<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_medium" id="S4.T3.3.12.12.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a><span class="ltx_text ltx_font_medium" id="S4.T3.3.12.12.1.1.2.2">]</span></cite></span></th>
<td class="ltx_td ltx_align_center" id="S4.T3.3.12.12.2"><span class="ltx_text" id="S4.T3.3.12.12.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.12.12.3"><span class="ltx_text" id="S4.T3.3.12.12.3.1" style="font-size:80%;">1044</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.3.12.12.4"><span class="ltx_text" id="S4.T3.3.12.12.4.1" style="font-size:80%;">370K</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.3.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_b" id="S4.T3.3.13.13.1">
<span class="ltx_text ltx_font_bold" id="S4.T3.3.13.13.1.1" style="font-size:80%;">LTU<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_medium" id="S4.T3.3.13.13.1.1.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a><span class="ltx_text ltx_font_medium" id="S4.T3.3.13.13.1.1.2.2">]</span></cite></span><span class="ltx_text" id="S4.T3.3.13.13.1.2" style="font-size:80%;">, </span><span class="ltx_text ltx_font_bold" id="S4.T3.3.13.13.1.3" style="font-size:80%;">LTU-AS<cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text ltx_font_medium" id="S4.T3.3.13.13.1.3.1.1">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a><span class="ltx_text ltx_font_medium" id="S4.T3.3.13.13.1.3.2.2">]</span></cite></span>
</th>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.13.13.2"><span class="ltx_text" id="S4.T3.3.13.13.2.1" style="font-size:80%;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.13.13.3"><span class="ltx_text" id="S4.T3.3.13.13.3.1" style="font-size:80%;">1784</span></td>
<td class="ltx_td ltx_align_center ltx_border_b" id="S4.T3.3.13.13.4"><span class="ltx_text" id="S4.T3.3.13.13.4.1" style="font-size:80%;">5682K</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Model Selection and Training Setups</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">Our framework integrates two key foundation models: Llama-3.1-8B-Instruct<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="huggingface.co/meta-llama/Llama-3.1-8B-Instruct" style="font-size:70%;" title="">huggingface.co/meta-llama/Llama-3.1-8B-Instruct</a></span></span></span><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib21" title="">21</a>]</cite> and the compact Whisper-small<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="huggingface.co/openai/whisper-small" style="font-size:70%;" title="">huggingface.co/openai/whisper-small</a></span></span></span><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib28" title="">28</a>]</cite>, which contains about 240 million parameters.
To connect these components, followed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib31" title="">31</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib15" title="">15</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib18" title="">18</a>]</cite>, we developed a specialized modality adapter, featuring a two-layer transformer decoder architecture.
This adapter uses 64 specialized vectors to capture and process audio features from the encoder’s hidden states.
In our implementation strategy, we maintain the original architecture of both LLaMA and Whisper unchanged.
Instead, we focused our training efforts solely on the modality adapter, which introduces approximately 22 million trainable parameters.
The system processes information by first converting text instructions through LLaMA’s embedding layer, then combining them with the processed audio features.
</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">To enhance the model’s ability to handle multi-audio inputs, a progressive learning strategy is adopted, where the model is first trained on single-audio tasks before transitioning to multi-audio scenarios, following a curriculum learning approach.
For the technical implementation, we built our system using the Hugging Face Transformers<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib40" title="">40</a>]</cite> framework.
Following our progressive learning strategy, the training process consisted of two stages: first, we pretrained the model on single-audio tasks for five epochs to establish a strong baseline;
second, we fine-tuned it under both positive and negative data settings on multi-audio data for two epochs to enhance its ability to process multiple inputs.
We used the Adam optimization algorithm with a cosine annealing learning schedule and 2,000 warm-up steps.
All experiments were conducted on two NVIDIA RTX 6000 Ada Generation GPUs, with a global batch size of 16 and a learning rate of 1e-4.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS3.5.1.1">IV-C</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS3.6.2">Baseline Models and Evaluation Setups</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">In this study, we incorporate widely used, well-documented, and fully open-source models as baselines for comparison. These include Qwen-Audio-Chat <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>]</cite>, Qwen2-Audio-Instruct <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>]</cite>, SALMONN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>]</cite> (including 7B and 13B versions), LTU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>]</cite>, and LTU-AS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>]</cite>, as well as proprietary commercial models like Gemini-Pro-1.5 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib26" title="">26</a>]</cite>.
Since most ALLMs are instruction-tuned for open-ended responses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>]</cite>, they generate answers based on user queries rather than selecting from predefined choices.
To align with this design, we extract relevant answers from model outputs using a structured evaluation approach.
All experiments adopt a consistent greedy decoding strategy with a maximum output length of 512.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1">To facilitate the extraction of relevant answers during evaluation, we implement robust regular expressions to extract key information from model responses.
The extracted content is then matched to the provided options via string-based comparison.
To minimize potential biases, the answer options in the original question are randomized before evaluation.
To ensure concise questions, we randomly select three incorrect options along with the correct answer and then shuffle their order in the multiple-choice format.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1">Once the relevant answers are extracted and mapped to the given options, we proceed with the evaluation process using standardized metrics.
If a model’s response cannot be parsed using the aforementioned approach, it is considered incorrect.
Our evaluation strategy is structured as follows.
We report the weighted F1 score for multi-class classification tasks.
For other tasks with their own evaluation protocols, we follow established methodologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib13" title="">13</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib43" title="">43</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib44" title="">44</a>]</cite>.
In our experiment results, bold text indicates the best performance, while underlined text represents the second-best.
We further detail the evaluation methods for each benchmark.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p4">
<p class="ltx_p" id="S4.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p4.1.1">Audio Question Answering Benchmark</span>.
In ClothoAQA, which includes both binary and non-binary classification tasks, we report accuracy as the primary metric.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p5">
<p class="ltx_p" id="S4.SS3.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p5.1.1">Audio Reasoning Benchmark</span>.
We report weighted F1 score for multi-class classification tasks.
For Synonym-Hypernym Test, we also report weighted precision and recall.
In addition, the original MMAU benchmark uses micro-averaged accuracy as its evaluation metric.
The SAKURA test set follows a multiple-choice format, and we evaluate model performance using three key metrics.
We report overall accuracy across all questions, as well as separate accuracy scores for single-hop and multi-hop questions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p6">
<p class="ltx_p" id="S4.SS3.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p6.1.1">Audio Hallucination Benchmark</span>.
Following previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib13" title="">13</a>]</cite>, we compute overall accuracy and F1 scores for questions where the correct answer is “Yes” or “No”.
As an additional reference, we also report the proportion of cases where the model responds with “Yes”.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS3.p7">
<p class="ltx_p" id="S4.SS3.p7.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p7.1.1">Instruction-Following Benchmark</span>.
We report the accuracy of each instruction-following task, the overall average accuracy, and the performance gap between the evaluated model and its backbone LLM.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS4.5.1.1">IV-D</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS4.6.2">Ablation Study</span>
</h3>
<div class="ltx_para" id="S4.SS4.p1">
<p class="ltx_p" id="S4.SS4.p1.1">First, in the single-audio training scenario, to ensure a fair evaluation of contrastive-like methods using BALSa, we conducted the following experiments to investigate the role of negative samples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p2">
<p class="ltx_p" id="S4.SS4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p2.1.1">Positive-only Training Data:</span>
The training set contains only positive samples, with a total of <math alttext="2N" class="ltx_Math" display="inline" id="S4.SS4.p2.1.m1.1"><semantics id="S4.SS4.p2.1.m1.1a"><mrow id="S4.SS4.p2.1.m1.1.1" xref="S4.SS4.p2.1.m1.1.1.cmml"><mn id="S4.SS4.p2.1.m1.1.1.2" xref="S4.SS4.p2.1.m1.1.1.2.cmml">2</mn><mo id="S4.SS4.p2.1.m1.1.1.1" xref="S4.SS4.p2.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p2.1.m1.1.1.3" xref="S4.SS4.p2.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p2.1.m1.1b"><apply id="S4.SS4.p2.1.m1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1"><times id="S4.SS4.p2.1.m1.1.1.1.cmml" xref="S4.SS4.p2.1.m1.1.1.1"></times><cn id="S4.SS4.p2.1.m1.1.1.2.cmml" type="integer" xref="S4.SS4.p2.1.m1.1.1.2">2</cn><ci id="S4.SS4.p2.1.m1.1.1.3.cmml" xref="S4.SS4.p2.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p2.1.m1.1c">2N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p2.1.m1.1d">2 italic_N</annotation></semantics></math> data points.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p3">
<p class="ltx_p" id="S4.SS4.p3.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.p3.2.1">Positive and Negative Training Data:</span>
The training set includes both positive and negative samples, each with <math alttext="N" class="ltx_Math" display="inline" id="S4.SS4.p3.1.m1.1"><semantics id="S4.SS4.p3.1.m1.1a"><mi id="S4.SS4.p3.1.m1.1.1" xref="S4.SS4.p3.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.1.m1.1b"><ci id="S4.SS4.p3.1.m1.1.1.cmml" xref="S4.SS4.p3.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.1.m1.1d">italic_N</annotation></semantics></math> data points, resulting in a total of <math alttext="2N" class="ltx_Math" display="inline" id="S4.SS4.p3.2.m2.1"><semantics id="S4.SS4.p3.2.m2.1a"><mrow id="S4.SS4.p3.2.m2.1.1" xref="S4.SS4.p3.2.m2.1.1.cmml"><mn id="S4.SS4.p3.2.m2.1.1.2" xref="S4.SS4.p3.2.m2.1.1.2.cmml">2</mn><mo id="S4.SS4.p3.2.m2.1.1.1" xref="S4.SS4.p3.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p3.2.m2.1.1.3" xref="S4.SS4.p3.2.m2.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p3.2.m2.1b"><apply id="S4.SS4.p3.2.m2.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1"><times id="S4.SS4.p3.2.m2.1.1.1.cmml" xref="S4.SS4.p3.2.m2.1.1.1"></times><cn id="S4.SS4.p3.2.m2.1.1.2.cmml" type="integer" xref="S4.SS4.p3.2.m2.1.1.2">2</cn><ci id="S4.SS4.p3.2.m2.1.1.3.cmml" xref="S4.SS4.p3.2.m2.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p3.2.m2.1c">2N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p3.2.m2.1d">2 italic_N</annotation></semantics></math> data points.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p4">
<p class="ltx_p" id="S4.SS4.p4.2"><span class="ltx_text ltx_font_bold" id="S4.SS4.p4.2.1">Combined Training Data:</span>
The training set consists of combined samples, as described in Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S3" title="III Method ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">III</span></a>.
A combined sample includes both sound events that are present and those that are absent within a single sample.
In contrast, positive samples only contain present sound events, while negative samples only contain absent sound events.
Under the combined setup, there are <math alttext="N" class="ltx_Math" display="inline" id="S4.SS4.p4.1.m1.1"><semantics id="S4.SS4.p4.1.m1.1a"><mi id="S4.SS4.p4.1.m1.1.1" xref="S4.SS4.p4.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.1.m1.1b"><ci id="S4.SS4.p4.1.m1.1.1.cmml" xref="S4.SS4.p4.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.1.m1.1d">italic_N</annotation></semantics></math> data points.
Since each data point carries information about both present and absent sound events, it is equivalent to having a total of <math alttext="2N" class="ltx_Math" display="inline" id="S4.SS4.p4.2.m2.1"><semantics id="S4.SS4.p4.2.m2.1a"><mrow id="S4.SS4.p4.2.m2.1.1" xref="S4.SS4.p4.2.m2.1.1.cmml"><mn id="S4.SS4.p4.2.m2.1.1.2" xref="S4.SS4.p4.2.m2.1.1.2.cmml">2</mn><mo id="S4.SS4.p4.2.m2.1.1.1" xref="S4.SS4.p4.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p4.2.m2.1.1.3" xref="S4.SS4.p4.2.m2.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p4.2.m2.1b"><apply id="S4.SS4.p4.2.m2.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1"><times id="S4.SS4.p4.2.m2.1.1.1.cmml" xref="S4.SS4.p4.2.m2.1.1.1"></times><cn id="S4.SS4.p4.2.m2.1.1.2.cmml" type="integer" xref="S4.SS4.p4.2.m2.1.1.2">2</cn><ci id="S4.SS4.p4.2.m2.1.1.3.cmml" xref="S4.SS4.p4.2.m2.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p4.2.m2.1c">2N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p4.2.m2.1d">2 italic_N</annotation></semantics></math> data points.</p>
</div>
<div class="ltx_para" id="S4.SS4.p5">
<p class="ltx_p" id="S4.SS4.p5.1">Second, to validate that data generated by the backbone LLM outperforms data generated by an external LLM under this setting, we also examine a scenario where a stronger LLM is used to generate synthetic data via our proposed pipeline, BALSa.
Specifically, we select Gemini-1.5-Pro <span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>gemini-1.5-pro-latest</span></span></span> to synthesize the positive-only, positive and negative, and combined training data.
The same methodology is applied, but a more advanced model is used to generate the data.</p>
</div>
<div class="ltx_para" id="S4.SS4.p6">
<p class="ltx_p" id="S4.SS4.p6.3">Third, we compare two approaches for generating training data for audio-language alignment: our method, which allows the backbone LLM to freely generate captions based on meta-information, and another that provides it with collected questions and asks it to generate corresponding answers.
To investigate this, we leverage the questions (<math alttext="{Q}_{{OpenAQA}}" class="ltx_Math" display="inline" id="S4.SS4.p6.1.m1.1"><semantics id="S4.SS4.p6.1.m1.1a"><msub id="S4.SS4.p6.1.m1.1.1" xref="S4.SS4.p6.1.m1.1.1.cmml"><mi id="S4.SS4.p6.1.m1.1.1.2" xref="S4.SS4.p6.1.m1.1.1.2.cmml">Q</mi><mrow id="S4.SS4.p6.1.m1.1.1.3" xref="S4.SS4.p6.1.m1.1.1.3.cmml"><mi id="S4.SS4.p6.1.m1.1.1.3.2" xref="S4.SS4.p6.1.m1.1.1.3.2.cmml">O</mi><mo id="S4.SS4.p6.1.m1.1.1.3.1" xref="S4.SS4.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.1.m1.1.1.3.3" xref="S4.SS4.p6.1.m1.1.1.3.3.cmml">p</mi><mo id="S4.SS4.p6.1.m1.1.1.3.1a" xref="S4.SS4.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.1.m1.1.1.3.4" xref="S4.SS4.p6.1.m1.1.1.3.4.cmml">e</mi><mo id="S4.SS4.p6.1.m1.1.1.3.1b" xref="S4.SS4.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.1.m1.1.1.3.5" xref="S4.SS4.p6.1.m1.1.1.3.5.cmml">n</mi><mo id="S4.SS4.p6.1.m1.1.1.3.1c" xref="S4.SS4.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.1.m1.1.1.3.6" xref="S4.SS4.p6.1.m1.1.1.3.6.cmml">A</mi><mo id="S4.SS4.p6.1.m1.1.1.3.1d" xref="S4.SS4.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.1.m1.1.1.3.7" xref="S4.SS4.p6.1.m1.1.1.3.7.cmml">Q</mi><mo id="S4.SS4.p6.1.m1.1.1.3.1e" xref="S4.SS4.p6.1.m1.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.1.m1.1.1.3.8" xref="S4.SS4.p6.1.m1.1.1.3.8.cmml">A</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.1.m1.1b"><apply id="S4.SS4.p6.1.m1.1.1.cmml" xref="S4.SS4.p6.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS4.p6.1.m1.1.1.1.cmml" xref="S4.SS4.p6.1.m1.1.1">subscript</csymbol><ci id="S4.SS4.p6.1.m1.1.1.2.cmml" xref="S4.SS4.p6.1.m1.1.1.2">𝑄</ci><apply id="S4.SS4.p6.1.m1.1.1.3.cmml" xref="S4.SS4.p6.1.m1.1.1.3"><times id="S4.SS4.p6.1.m1.1.1.3.1.cmml" xref="S4.SS4.p6.1.m1.1.1.3.1"></times><ci id="S4.SS4.p6.1.m1.1.1.3.2.cmml" xref="S4.SS4.p6.1.m1.1.1.3.2">𝑂</ci><ci id="S4.SS4.p6.1.m1.1.1.3.3.cmml" xref="S4.SS4.p6.1.m1.1.1.3.3">𝑝</ci><ci id="S4.SS4.p6.1.m1.1.1.3.4.cmml" xref="S4.SS4.p6.1.m1.1.1.3.4">𝑒</ci><ci id="S4.SS4.p6.1.m1.1.1.3.5.cmml" xref="S4.SS4.p6.1.m1.1.1.3.5">𝑛</ci><ci id="S4.SS4.p6.1.m1.1.1.3.6.cmml" xref="S4.SS4.p6.1.m1.1.1.3.6">𝐴</ci><ci id="S4.SS4.p6.1.m1.1.1.3.7.cmml" xref="S4.SS4.p6.1.m1.1.1.3.7">𝑄</ci><ci id="S4.SS4.p6.1.m1.1.1.3.8.cmml" xref="S4.SS4.p6.1.m1.1.1.3.8">𝐴</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.1.m1.1c">{Q}_{{OpenAQA}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p6.1.m1.1d">italic_Q start_POSTSUBSCRIPT italic_O italic_p italic_e italic_n italic_A italic_Q italic_A end_POSTSUBSCRIPT</annotation></semantics></math>) from the OpenAQA dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>]</cite> and apply the BALSa pipeline to generate corresponding answers using the backbone LLM.
Specifically, OpenAQA contains 5 million instances, from which we select a subset that aligns with the audio data used in this work.
As a result, the final training dataset also consists of <math alttext="2N" class="ltx_Math" display="inline" id="S4.SS4.p6.2.m2.1"><semantics id="S4.SS4.p6.2.m2.1a"><mrow id="S4.SS4.p6.2.m2.1.1" xref="S4.SS4.p6.2.m2.1.1.cmml"><mn id="S4.SS4.p6.2.m2.1.1.2" xref="S4.SS4.p6.2.m2.1.1.2.cmml">2</mn><mo id="S4.SS4.p6.2.m2.1.1.1" xref="S4.SS4.p6.2.m2.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p6.2.m2.1.1.3" xref="S4.SS4.p6.2.m2.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.2.m2.1b"><apply id="S4.SS4.p6.2.m2.1.1.cmml" xref="S4.SS4.p6.2.m2.1.1"><times id="S4.SS4.p6.2.m2.1.1.1.cmml" xref="S4.SS4.p6.2.m2.1.1.1"></times><cn id="S4.SS4.p6.2.m2.1.1.2.cmml" type="integer" xref="S4.SS4.p6.2.m2.1.1.2">2</cn><ci id="S4.SS4.p6.2.m2.1.1.3.cmml" xref="S4.SS4.p6.2.m2.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.2.m2.1c">2N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p6.2.m2.1d">2 italic_N</annotation></semantics></math> data points.
In summary, the final input prompt (<math alttext="{P}_{{final}}" class="ltx_Math" display="inline" id="S4.SS4.p6.3.m3.1"><semantics id="S4.SS4.p6.3.m3.1a"><msub id="S4.SS4.p6.3.m3.1.1" xref="S4.SS4.p6.3.m3.1.1.cmml"><mi id="S4.SS4.p6.3.m3.1.1.2" xref="S4.SS4.p6.3.m3.1.1.2.cmml">P</mi><mrow id="S4.SS4.p6.3.m3.1.1.3" xref="S4.SS4.p6.3.m3.1.1.3.cmml"><mi id="S4.SS4.p6.3.m3.1.1.3.2" xref="S4.SS4.p6.3.m3.1.1.3.2.cmml">f</mi><mo id="S4.SS4.p6.3.m3.1.1.3.1" xref="S4.SS4.p6.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.3.m3.1.1.3.3" xref="S4.SS4.p6.3.m3.1.1.3.3.cmml">i</mi><mo id="S4.SS4.p6.3.m3.1.1.3.1a" xref="S4.SS4.p6.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.3.m3.1.1.3.4" xref="S4.SS4.p6.3.m3.1.1.3.4.cmml">n</mi><mo id="S4.SS4.p6.3.m3.1.1.3.1b" xref="S4.SS4.p6.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.3.m3.1.1.3.5" xref="S4.SS4.p6.3.m3.1.1.3.5.cmml">a</mi><mo id="S4.SS4.p6.3.m3.1.1.3.1c" xref="S4.SS4.p6.3.m3.1.1.3.1.cmml">⁢</mo><mi id="S4.SS4.p6.3.m3.1.1.3.6" xref="S4.SS4.p6.3.m3.1.1.3.6.cmml">l</mi></mrow></msub><annotation-xml encoding="MathML-Content" id="S4.SS4.p6.3.m3.1b"><apply id="S4.SS4.p6.3.m3.1.1.cmml" xref="S4.SS4.p6.3.m3.1.1"><csymbol cd="ambiguous" id="S4.SS4.p6.3.m3.1.1.1.cmml" xref="S4.SS4.p6.3.m3.1.1">subscript</csymbol><ci id="S4.SS4.p6.3.m3.1.1.2.cmml" xref="S4.SS4.p6.3.m3.1.1.2">𝑃</ci><apply id="S4.SS4.p6.3.m3.1.1.3.cmml" xref="S4.SS4.p6.3.m3.1.1.3"><times id="S4.SS4.p6.3.m3.1.1.3.1.cmml" xref="S4.SS4.p6.3.m3.1.1.3.1"></times><ci id="S4.SS4.p6.3.m3.1.1.3.2.cmml" xref="S4.SS4.p6.3.m3.1.1.3.2">𝑓</ci><ci id="S4.SS4.p6.3.m3.1.1.3.3.cmml" xref="S4.SS4.p6.3.m3.1.1.3.3">𝑖</ci><ci id="S4.SS4.p6.3.m3.1.1.3.4.cmml" xref="S4.SS4.p6.3.m3.1.1.3.4">𝑛</ci><ci id="S4.SS4.p6.3.m3.1.1.3.5.cmml" xref="S4.SS4.p6.3.m3.1.1.3.5">𝑎</ci><ci id="S4.SS4.p6.3.m3.1.1.3.6.cmml" xref="S4.SS4.p6.3.m3.1.1.3.6">𝑙</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p6.3.m3.1c">{P}_{{final}}</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p6.3.m3.1d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT</annotation></semantics></math>) to the backbone LLM is formulated in Equation <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S4.E3" title="In IV-D Ablation Study ‣ IV Experimental Setup ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">3</span></a>. This setting refers to BALSa-OpenAQA.</p>
</div>
<div class="ltx_para" id="S4.SS4.p7">
<table class="ltx_equationgroup ltx_eqn_table" id="S4.E3">
<tbody>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3X">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle P_{final}=" class="ltx_Math" display="inline" id="S4.E3X.2.1.1.m1.1"><semantics id="S4.E3X.2.1.1.m1.1a"><mrow id="S4.E3X.2.1.1.m1.1.1" xref="S4.E3X.2.1.1.m1.1.1.cmml"><msub id="S4.E3X.2.1.1.m1.1.1.2" xref="S4.E3X.2.1.1.m1.1.1.2.cmml"><mi id="S4.E3X.2.1.1.m1.1.1.2.2" xref="S4.E3X.2.1.1.m1.1.1.2.2.cmml">P</mi><mrow id="S4.E3X.2.1.1.m1.1.1.2.3" xref="S4.E3X.2.1.1.m1.1.1.2.3.cmml"><mi id="S4.E3X.2.1.1.m1.1.1.2.3.2" xref="S4.E3X.2.1.1.m1.1.1.2.3.2.cmml">f</mi><mo id="S4.E3X.2.1.1.m1.1.1.2.3.1" xref="S4.E3X.2.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.E3X.2.1.1.m1.1.1.2.3.3" xref="S4.E3X.2.1.1.m1.1.1.2.3.3.cmml">i</mi><mo id="S4.E3X.2.1.1.m1.1.1.2.3.1a" xref="S4.E3X.2.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.E3X.2.1.1.m1.1.1.2.3.4" xref="S4.E3X.2.1.1.m1.1.1.2.3.4.cmml">n</mi><mo id="S4.E3X.2.1.1.m1.1.1.2.3.1b" xref="S4.E3X.2.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.E3X.2.1.1.m1.1.1.2.3.5" xref="S4.E3X.2.1.1.m1.1.1.2.3.5.cmml">a</mi><mo id="S4.E3X.2.1.1.m1.1.1.2.3.1c" xref="S4.E3X.2.1.1.m1.1.1.2.3.1.cmml">⁢</mo><mi id="S4.E3X.2.1.1.m1.1.1.2.3.6" xref="S4.E3X.2.1.1.m1.1.1.2.3.6.cmml">l</mi></mrow></msub><mo id="S4.E3X.2.1.1.m1.1.1.1" xref="S4.E3X.2.1.1.m1.1.1.1.cmml">=</mo><mi id="S4.E3X.2.1.1.m1.1.1.3" xref="S4.E3X.2.1.1.m1.1.1.3.cmml"></mi></mrow><annotation-xml encoding="MathML-Content" id="S4.E3X.2.1.1.m1.1b"><apply id="S4.E3X.2.1.1.m1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1"><eq id="S4.E3X.2.1.1.m1.1.1.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.1"></eq><apply id="S4.E3X.2.1.1.m1.1.1.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S4.E3X.2.1.1.m1.1.1.2.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.2">subscript</csymbol><ci id="S4.E3X.2.1.1.m1.1.1.2.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.2.2">𝑃</ci><apply id="S4.E3X.2.1.1.m1.1.1.2.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.2.3"><times id="S4.E3X.2.1.1.m1.1.1.2.3.1.cmml" xref="S4.E3X.2.1.1.m1.1.1.2.3.1"></times><ci id="S4.E3X.2.1.1.m1.1.1.2.3.2.cmml" xref="S4.E3X.2.1.1.m1.1.1.2.3.2">𝑓</ci><ci id="S4.E3X.2.1.1.m1.1.1.2.3.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.2.3.3">𝑖</ci><ci id="S4.E3X.2.1.1.m1.1.1.2.3.4.cmml" xref="S4.E3X.2.1.1.m1.1.1.2.3.4">𝑛</ci><ci id="S4.E3X.2.1.1.m1.1.1.2.3.5.cmml" xref="S4.E3X.2.1.1.m1.1.1.2.3.5">𝑎</ci><ci id="S4.E3X.2.1.1.m1.1.1.2.3.6.cmml" xref="S4.E3X.2.1.1.m1.1.1.2.3.6">𝑙</ci></apply></apply><csymbol cd="latexml" id="S4.E3X.2.1.1.m1.1.1.3.cmml" xref="S4.E3X.2.1.1.m1.1.1.3">absent</csymbol></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3X.2.1.1.m1.1c">\displaystyle P_{final}=</annotation><annotation encoding="application/x-llamapun" id="S4.E3X.2.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT =</annotation></semantics></math></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle\,[\textit{Begin of audio}]\,P_{seed}\,[\textit{End of audio}]\,P%
_{gen}" class="ltx_Math" display="inline" id="S4.E3X.3.2.2.m1.2"><semantics id="S4.E3X.3.2.2.m1.2a"><mrow id="S4.E3X.3.2.2.m1.2.3" xref="S4.E3X.3.2.2.m1.2.3.cmml"><mrow id="S4.E3X.3.2.2.m1.2.3.2.2" xref="S4.E3X.3.2.2.m1.2.3.2.1.cmml"><mo id="S4.E3X.3.2.2.m1.2.3.2.2.1" stretchy="false" xref="S4.E3X.3.2.2.m1.2.3.2.1.1.cmml">[</mo><mtext class="ltx_mathvariant_italic" id="S4.E3X.3.2.2.m1.1.1" xref="S4.E3X.3.2.2.m1.1.1a.cmml">Begin of audio</mtext><mo id="S4.E3X.3.2.2.m1.2.3.2.2.2" stretchy="false" xref="S4.E3X.3.2.2.m1.2.3.2.1.1.cmml">]</mo></mrow><mo id="S4.E3X.3.2.2.m1.2.3.1" lspace="0.170em" xref="S4.E3X.3.2.2.m1.2.3.1.cmml">⁢</mo><msub id="S4.E3X.3.2.2.m1.2.3.3" xref="S4.E3X.3.2.2.m1.2.3.3.cmml"><mi id="S4.E3X.3.2.2.m1.2.3.3.2" xref="S4.E3X.3.2.2.m1.2.3.3.2.cmml">P</mi><mrow id="S4.E3X.3.2.2.m1.2.3.3.3" xref="S4.E3X.3.2.2.m1.2.3.3.3.cmml"><mi id="S4.E3X.3.2.2.m1.2.3.3.3.2" xref="S4.E3X.3.2.2.m1.2.3.3.3.2.cmml">s</mi><mo id="S4.E3X.3.2.2.m1.2.3.3.3.1" xref="S4.E3X.3.2.2.m1.2.3.3.3.1.cmml">⁢</mo><mi id="S4.E3X.3.2.2.m1.2.3.3.3.3" xref="S4.E3X.3.2.2.m1.2.3.3.3.3.cmml">e</mi><mo id="S4.E3X.3.2.2.m1.2.3.3.3.1a" xref="S4.E3X.3.2.2.m1.2.3.3.3.1.cmml">⁢</mo><mi id="S4.E3X.3.2.2.m1.2.3.3.3.4" xref="S4.E3X.3.2.2.m1.2.3.3.3.4.cmml">e</mi><mo id="S4.E3X.3.2.2.m1.2.3.3.3.1b" xref="S4.E3X.3.2.2.m1.2.3.3.3.1.cmml">⁢</mo><mi id="S4.E3X.3.2.2.m1.2.3.3.3.5" xref="S4.E3X.3.2.2.m1.2.3.3.3.5.cmml">d</mi></mrow></msub><mo id="S4.E3X.3.2.2.m1.2.3.1a" xref="S4.E3X.3.2.2.m1.2.3.1.cmml">⁢</mo><mrow id="S4.E3X.3.2.2.m1.2.3.4.2" xref="S4.E3X.3.2.2.m1.2.3.4.1.cmml"><mo id="S4.E3X.3.2.2.m1.2.3.4.2.1" stretchy="false" xref="S4.E3X.3.2.2.m1.2.3.4.1.1.cmml">[</mo><mtext class="ltx_mathvariant_italic" id="S4.E3X.3.2.2.m1.2.2" xref="S4.E3X.3.2.2.m1.2.2a.cmml">End of audio</mtext><mo id="S4.E3X.3.2.2.m1.2.3.4.2.2" stretchy="false" xref="S4.E3X.3.2.2.m1.2.3.4.1.1.cmml">]</mo></mrow><mo id="S4.E3X.3.2.2.m1.2.3.1b" lspace="0.170em" xref="S4.E3X.3.2.2.m1.2.3.1.cmml">⁢</mo><msub id="S4.E3X.3.2.2.m1.2.3.5" xref="S4.E3X.3.2.2.m1.2.3.5.cmml"><mi id="S4.E3X.3.2.2.m1.2.3.5.2" xref="S4.E3X.3.2.2.m1.2.3.5.2.cmml">P</mi><mrow id="S4.E3X.3.2.2.m1.2.3.5.3" xref="S4.E3X.3.2.2.m1.2.3.5.3.cmml"><mi id="S4.E3X.3.2.2.m1.2.3.5.3.2" xref="S4.E3X.3.2.2.m1.2.3.5.3.2.cmml">g</mi><mo id="S4.E3X.3.2.2.m1.2.3.5.3.1" xref="S4.E3X.3.2.2.m1.2.3.5.3.1.cmml">⁢</mo><mi id="S4.E3X.3.2.2.m1.2.3.5.3.3" xref="S4.E3X.3.2.2.m1.2.3.5.3.3.cmml">e</mi><mo id="S4.E3X.3.2.2.m1.2.3.5.3.1a" xref="S4.E3X.3.2.2.m1.2.3.5.3.1.cmml">⁢</mo><mi id="S4.E3X.3.2.2.m1.2.3.5.3.4" xref="S4.E3X.3.2.2.m1.2.3.5.3.4.cmml">n</mi></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S4.E3X.3.2.2.m1.2b"><apply id="S4.E3X.3.2.2.m1.2.3.cmml" xref="S4.E3X.3.2.2.m1.2.3"><times id="S4.E3X.3.2.2.m1.2.3.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.1"></times><apply id="S4.E3X.3.2.2.m1.2.3.2.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.2.2"><csymbol cd="latexml" id="S4.E3X.3.2.2.m1.2.3.2.1.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.2.2.1">delimited-[]</csymbol><ci id="S4.E3X.3.2.2.m1.1.1a.cmml" xref="S4.E3X.3.2.2.m1.1.1"><mtext class="ltx_mathvariant_italic" id="S4.E3X.3.2.2.m1.1.1.cmml" xref="S4.E3X.3.2.2.m1.1.1">Begin of audio</mtext></ci></apply><apply id="S4.E3X.3.2.2.m1.2.3.3.cmml" xref="S4.E3X.3.2.2.m1.2.3.3"><csymbol cd="ambiguous" id="S4.E3X.3.2.2.m1.2.3.3.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.3">subscript</csymbol><ci id="S4.E3X.3.2.2.m1.2.3.3.2.cmml" xref="S4.E3X.3.2.2.m1.2.3.3.2">𝑃</ci><apply id="S4.E3X.3.2.2.m1.2.3.3.3.cmml" xref="S4.E3X.3.2.2.m1.2.3.3.3"><times id="S4.E3X.3.2.2.m1.2.3.3.3.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.3.3.1"></times><ci id="S4.E3X.3.2.2.m1.2.3.3.3.2.cmml" xref="S4.E3X.3.2.2.m1.2.3.3.3.2">𝑠</ci><ci id="S4.E3X.3.2.2.m1.2.3.3.3.3.cmml" xref="S4.E3X.3.2.2.m1.2.3.3.3.3">𝑒</ci><ci id="S4.E3X.3.2.2.m1.2.3.3.3.4.cmml" xref="S4.E3X.3.2.2.m1.2.3.3.3.4">𝑒</ci><ci id="S4.E3X.3.2.2.m1.2.3.3.3.5.cmml" xref="S4.E3X.3.2.2.m1.2.3.3.3.5">𝑑</ci></apply></apply><apply id="S4.E3X.3.2.2.m1.2.3.4.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.4.2"><csymbol cd="latexml" id="S4.E3X.3.2.2.m1.2.3.4.1.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.4.2.1">delimited-[]</csymbol><ci id="S4.E3X.3.2.2.m1.2.2a.cmml" xref="S4.E3X.3.2.2.m1.2.2"><mtext class="ltx_mathvariant_italic" id="S4.E3X.3.2.2.m1.2.2.cmml" xref="S4.E3X.3.2.2.m1.2.2">End of audio</mtext></ci></apply><apply id="S4.E3X.3.2.2.m1.2.3.5.cmml" xref="S4.E3X.3.2.2.m1.2.3.5"><csymbol cd="ambiguous" id="S4.E3X.3.2.2.m1.2.3.5.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.5">subscript</csymbol><ci id="S4.E3X.3.2.2.m1.2.3.5.2.cmml" xref="S4.E3X.3.2.2.m1.2.3.5.2">𝑃</ci><apply id="S4.E3X.3.2.2.m1.2.3.5.3.cmml" xref="S4.E3X.3.2.2.m1.2.3.5.3"><times id="S4.E3X.3.2.2.m1.2.3.5.3.1.cmml" xref="S4.E3X.3.2.2.m1.2.3.5.3.1"></times><ci id="S4.E3X.3.2.2.m1.2.3.5.3.2.cmml" xref="S4.E3X.3.2.2.m1.2.3.5.3.2">𝑔</ci><ci id="S4.E3X.3.2.2.m1.2.3.5.3.3.cmml" xref="S4.E3X.3.2.2.m1.2.3.5.3.3">𝑒</ci><ci id="S4.E3X.3.2.2.m1.2.3.5.3.4.cmml" xref="S4.E3X.3.2.2.m1.2.3.5.3.4">𝑛</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3X.3.2.2.m1.2c">\displaystyle\,[\textit{Begin of audio}]\,P_{seed}\,[\textit{End of audio}]\,P%
_{gen}</annotation><annotation encoding="application/x-llamapun" id="S4.E3X.3.2.2.m1.2d">[ Begin of audio ] italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d end_POSTSUBSCRIPT [ End of audio ] italic_P start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="3"><span class="ltx_tag ltx_tag_equationgroup ltx_align_right">(3)</span></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3Xa">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle P_{seed}\in\{D_{caption},D_{tag}\}" class="ltx_Math" display="inline" id="S4.E3Xa.2.1.1.m1.2"><semantics id="S4.E3Xa.2.1.1.m1.2a"><mrow id="S4.E3Xa.2.1.1.m1.2.2" xref="S4.E3Xa.2.1.1.m1.2.2.cmml"><msub id="S4.E3Xa.2.1.1.m1.2.2.4" xref="S4.E3Xa.2.1.1.m1.2.2.4.cmml"><mi id="S4.E3Xa.2.1.1.m1.2.2.4.2" xref="S4.E3Xa.2.1.1.m1.2.2.4.2.cmml">P</mi><mrow id="S4.E3Xa.2.1.1.m1.2.2.4.3" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.cmml"><mi id="S4.E3Xa.2.1.1.m1.2.2.4.3.2" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.2.cmml">s</mi><mo id="S4.E3Xa.2.1.1.m1.2.2.4.3.1" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.2.2.4.3.3" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.3.cmml">e</mi><mo id="S4.E3Xa.2.1.1.m1.2.2.4.3.1a" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.2.2.4.3.4" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.4.cmml">e</mi><mo id="S4.E3Xa.2.1.1.m1.2.2.4.3.1b" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.2.2.4.3.5" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.5.cmml">d</mi></mrow></msub><mo id="S4.E3Xa.2.1.1.m1.2.2.3" xref="S4.E3Xa.2.1.1.m1.2.2.3.cmml">∈</mo><mrow id="S4.E3Xa.2.1.1.m1.2.2.2.2" xref="S4.E3Xa.2.1.1.m1.2.2.2.3.cmml"><mo id="S4.E3Xa.2.1.1.m1.2.2.2.2.3" stretchy="false" xref="S4.E3Xa.2.1.1.m1.2.2.2.3.cmml">{</mo><msub id="S4.E3Xa.2.1.1.m1.1.1.1.1.1" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.cmml"><mi id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.2" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.2.cmml">D</mi><mrow id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.cmml"><mi id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.2" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.2.cmml">c</mi><mo id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.3" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.3.cmml">a</mi><mo id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1a" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.4" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.4.cmml">p</mi><mo id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1b" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.5" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.5.cmml">t</mi><mo id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1c" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.6" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.6.cmml">i</mi><mo id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1d" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.7" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.7.cmml">o</mi><mo id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1e" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.8" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.8.cmml">n</mi></mrow></msub><mo id="S4.E3Xa.2.1.1.m1.2.2.2.2.4" xref="S4.E3Xa.2.1.1.m1.2.2.2.3.cmml">,</mo><msub id="S4.E3Xa.2.1.1.m1.2.2.2.2.2" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.cmml"><mi id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.2" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.2.cmml">D</mi><mrow id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.cmml"><mi id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.2" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml">t</mi><mo id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.1" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.3" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.3.cmml">a</mi><mo id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.1a" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.1.cmml">⁢</mo><mi id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.4" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.4.cmml">g</mi></mrow></msub><mo id="S4.E3Xa.2.1.1.m1.2.2.2.2.5" stretchy="false" xref="S4.E3Xa.2.1.1.m1.2.2.2.3.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S4.E3Xa.2.1.1.m1.2b"><apply id="S4.E3Xa.2.1.1.m1.2.2.cmml" xref="S4.E3Xa.2.1.1.m1.2.2"><in id="S4.E3Xa.2.1.1.m1.2.2.3.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.3"></in><apply id="S4.E3Xa.2.1.1.m1.2.2.4.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4"><csymbol cd="ambiguous" id="S4.E3Xa.2.1.1.m1.2.2.4.1.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4">subscript</csymbol><ci id="S4.E3Xa.2.1.1.m1.2.2.4.2.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4.2">𝑃</ci><apply id="S4.E3Xa.2.1.1.m1.2.2.4.3.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4.3"><times id="S4.E3Xa.2.1.1.m1.2.2.4.3.1.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.1"></times><ci id="S4.E3Xa.2.1.1.m1.2.2.4.3.2.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.2">𝑠</ci><ci id="S4.E3Xa.2.1.1.m1.2.2.4.3.3.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.3">𝑒</ci><ci id="S4.E3Xa.2.1.1.m1.2.2.4.3.4.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.4">𝑒</ci><ci id="S4.E3Xa.2.1.1.m1.2.2.4.3.5.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.4.3.5">𝑑</ci></apply></apply><set id="S4.E3Xa.2.1.1.m1.2.2.2.3.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2"><apply id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.1.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.2">𝐷</ci><apply id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3"><times id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.1"></times><ci id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.2.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.2">𝑐</ci><ci id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.3.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.3">𝑎</ci><ci id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.4.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.4">𝑝</ci><ci id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.5.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.5">𝑡</ci><ci id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.6.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.6">𝑖</ci><ci id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.7.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.7">𝑜</ci><ci id="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.8.cmml" xref="S4.E3Xa.2.1.1.m1.1.1.1.1.1.3.8">𝑛</ci></apply></apply><apply id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2"><csymbol cd="ambiguous" id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.1.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2">subscript</csymbol><ci id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.2.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.2">𝐷</ci><apply id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3"><times id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.1.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.1"></times><ci id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.2.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.2">𝑡</ci><ci id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.3.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.3">𝑎</ci><ci id="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.4.cmml" xref="S4.E3Xa.2.1.1.m1.2.2.2.2.2.3.4">𝑔</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3Xa.2.1.1.m1.2c">\displaystyle P_{seed}\in\{D_{caption},D_{tag}\}</annotation><annotation encoding="application/x-llamapun" id="S4.E3Xa.2.1.1.m1.2d">italic_P start_POSTSUBSCRIPT italic_s italic_e italic_e italic_d end_POSTSUBSCRIPT ∈ { italic_D start_POSTSUBSCRIPT italic_c italic_a italic_p italic_t italic_i italic_o italic_n end_POSTSUBSCRIPT , italic_D start_POSTSUBSCRIPT italic_t italic_a italic_g end_POSTSUBSCRIPT }</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
<tr class="ltx_equation ltx_eqn_row ltx_align_baseline" id="S4.E3Xb">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_eqn_cell"></td>
<td class="ltx_td ltx_align_left ltx_eqn_cell"><math alttext="\displaystyle P_{gen}\in\{Q_{OpenAQA}\}." class="ltx_Math" display="inline" id="S4.E3Xb.2.1.1.m1.1"><semantics id="S4.E3Xb.2.1.1.m1.1a"><mrow id="S4.E3Xb.2.1.1.m1.1.1.1" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.cmml"><mrow id="S4.E3Xb.2.1.1.m1.1.1.1.1" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.cmml"><msub id="S4.E3Xb.2.1.1.m1.1.1.1.1.3" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.cmml"><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.2" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.2.cmml">P</mi><mrow id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.cmml"><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.2" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.2.cmml">g</mi><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.1" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.3" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.3.cmml">e</mi><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.1a" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.1.cmml">⁢</mo><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.4" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.4.cmml">n</mi></mrow></msub><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.2" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.2.cmml">∈</mo><mrow id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.2.cmml"><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.2.cmml">{</mo><msub id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.2" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.2.cmml">Q</mi><mrow id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.cmml"><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.cmml">O</mi><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3.cmml">p</mi><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1a" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.cmml">e</mi><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1b" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.5" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.5.cmml">n</mi><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1c" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.6" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.6.cmml">A</mi><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1d" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.7" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.7.cmml">Q</mi><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1e" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.8" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.8.cmml">A</mi></mrow></msub><mo id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow></mrow><mo id="S4.E3Xb.2.1.1.m1.1.1.1.2" lspace="0em" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S4.E3Xb.2.1.1.m1.1b"><apply id="S4.E3Xb.2.1.1.m1.1.1.1.1.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1"><in id="S4.E3Xb.2.1.1.m1.1.1.1.1.2.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.2"></in><apply id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3"><csymbol cd="ambiguous" id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.1.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3">subscript</csymbol><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.2.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.2">𝑃</ci><apply id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3"><times id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.1.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.1"></times><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.2.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.2">𝑔</ci><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.3.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.3">𝑒</ci><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.4.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.3.3.4">𝑛</ci></apply></apply><set id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.2.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1"><apply id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.2">𝑄</ci><apply id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3"><times id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.1"></times><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.2">𝑂</ci><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.3">𝑝</ci><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.4">𝑒</ci><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.5.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.5">𝑛</ci><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.6.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.6">𝐴</ci><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.7.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.7">𝑄</ci><ci id="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.8.cmml" xref="S4.E3Xb.2.1.1.m1.1.1.1.1.1.1.1.3.8">𝐴</ci></apply></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.E3Xb.2.1.1.m1.1c">\displaystyle P_{gen}\in\{Q_{OpenAQA}\}.</annotation><annotation encoding="application/x-llamapun" id="S4.E3Xb.2.1.1.m1.1d">italic_P start_POSTSUBSCRIPT italic_g italic_e italic_n end_POSTSUBSCRIPT ∈ { italic_Q start_POSTSUBSCRIPT italic_O italic_p italic_e italic_n italic_A italic_Q italic_A end_POSTSUBSCRIPT } .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr>
</tbody>
</table>
<p class="ltx_p" id="S4.SS4.p7.1">For comparison, we also directly use question-answer pairs derived from the OpenAQA dataset as our training data.</p>
</div>
<div class="ltx_para" id="S4.SS4.p8">
<p class="ltx_p" id="S4.SS4.p8.1">Fourth, we explore an alternative approach where, instead of using BALSa to generate captioning data, we employ pre-designed description-based prompts like <span class="ltx_text ltx_font_italic" id="S4.SS4.p8.1.1">”Describe the audio.”</span>, which is the rule-based templates.
In this setting, the training targets are the corresponding ground truth captions or sound event tags of the audio.
The total training dataset also consists of <math alttext="2N" class="ltx_Math" display="inline" id="S4.SS4.p8.1.m1.1"><semantics id="S4.SS4.p8.1.m1.1a"><mrow id="S4.SS4.p8.1.m1.1.1" xref="S4.SS4.p8.1.m1.1.1.cmml"><mn id="S4.SS4.p8.1.m1.1.1.2" xref="S4.SS4.p8.1.m1.1.1.2.cmml">2</mn><mo id="S4.SS4.p8.1.m1.1.1.1" xref="S4.SS4.p8.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p8.1.m1.1.1.3" xref="S4.SS4.p8.1.m1.1.1.3.cmml">N</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p8.1.m1.1b"><apply id="S4.SS4.p8.1.m1.1.1.cmml" xref="S4.SS4.p8.1.m1.1.1"><times id="S4.SS4.p8.1.m1.1.1.1.cmml" xref="S4.SS4.p8.1.m1.1.1.1"></times><cn id="S4.SS4.p8.1.m1.1.1.2.cmml" type="integer" xref="S4.SS4.p8.1.m1.1.1.2">2</cn><ci id="S4.SS4.p8.1.m1.1.1.3.cmml" xref="S4.SS4.p8.1.m1.1.1.3">𝑁</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p8.1.m1.1c">2N</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p8.1.m1.1d">2 italic_N</annotation></semantics></math> data points.</p>
</div>
<div class="ltx_para" id="S4.SS4.p9">
<p class="ltx_p" id="S4.SS4.p9.1">Beyond the single-audio scenario, we conduct ablation experiments on the multi-audio scenario, BALSa-MA, an extended version of BALSa.
In BALSa-MA, we aim to enable models to learn from multiple audio sources, including explaining the differences between two audio samples and captioning both simultaneously.
For conciseness, in our experiments, we adapt BALSa-MA to a two-audio-sample setting.</p>
</div>
<div class="ltx_para" id="S4.SS4.p10">
<p class="ltx_p" id="S4.SS4.p10.1">To ensure a fair evaluation of different methods for audio interpretation, we design the following experiments.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p11">
<p class="ltx_p" id="S4.SS4.p11.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p11.1.1">Discrimination-only Training Data:</span>
The training set contains only discrimination samples, with a total of <math alttext="2M" class="ltx_Math" display="inline" id="S4.SS4.p11.1.m1.1"><semantics id="S4.SS4.p11.1.m1.1a"><mrow id="S4.SS4.p11.1.m1.1.1" xref="S4.SS4.p11.1.m1.1.1.cmml"><mn id="S4.SS4.p11.1.m1.1.1.2" xref="S4.SS4.p11.1.m1.1.1.2.cmml">2</mn><mo id="S4.SS4.p11.1.m1.1.1.1" xref="S4.SS4.p11.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p11.1.m1.1.1.3" xref="S4.SS4.p11.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p11.1.m1.1b"><apply id="S4.SS4.p11.1.m1.1.1.cmml" xref="S4.SS4.p11.1.m1.1.1"><times id="S4.SS4.p11.1.m1.1.1.1.cmml" xref="S4.SS4.p11.1.m1.1.1.1"></times><cn id="S4.SS4.p11.1.m1.1.1.2.cmml" type="integer" xref="S4.SS4.p11.1.m1.1.1.2">2</cn><ci id="S4.SS4.p11.1.m1.1.1.3.cmml" xref="S4.SS4.p11.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p11.1.m1.1c">2M</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p11.1.m1.1d">2 italic_M</annotation></semantics></math> data points.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p12">
<p class="ltx_p" id="S4.SS4.p12.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p12.1.1">Captioning-only Training Data:</span>
The training set contains only captioning samples, with a total of <math alttext="2M" class="ltx_Math" display="inline" id="S4.SS4.p12.1.m1.1"><semantics id="S4.SS4.p12.1.m1.1a"><mrow id="S4.SS4.p12.1.m1.1.1" xref="S4.SS4.p12.1.m1.1.1.cmml"><mn id="S4.SS4.p12.1.m1.1.1.2" xref="S4.SS4.p12.1.m1.1.1.2.cmml">2</mn><mo id="S4.SS4.p12.1.m1.1.1.1" xref="S4.SS4.p12.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p12.1.m1.1.1.3" xref="S4.SS4.p12.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p12.1.m1.1b"><apply id="S4.SS4.p12.1.m1.1.1.cmml" xref="S4.SS4.p12.1.m1.1.1"><times id="S4.SS4.p12.1.m1.1.1.1.cmml" xref="S4.SS4.p12.1.m1.1.1.1"></times><cn id="S4.SS4.p12.1.m1.1.1.2.cmml" type="integer" xref="S4.SS4.p12.1.m1.1.1.2">2</cn><ci id="S4.SS4.p12.1.m1.1.1.3.cmml" xref="S4.SS4.p12.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p12.1.m1.1c">2M</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p12.1.m1.1d">2 italic_M</annotation></semantics></math> data points.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.SS4.p13">
<p class="ltx_p" id="S4.SS4.p13.1"><span class="ltx_text ltx_font_bold" id="S4.SS4.p13.1.1">Joint Discrimination-Captioning Training Data:</span>
This training set is a balanced combination of discrimination and captioning samples, with each type comprising 50% of the dataset, totaling <math alttext="2M" class="ltx_Math" display="inline" id="S4.SS4.p13.1.m1.1"><semantics id="S4.SS4.p13.1.m1.1a"><mrow id="S4.SS4.p13.1.m1.1.1" xref="S4.SS4.p13.1.m1.1.1.cmml"><mn id="S4.SS4.p13.1.m1.1.1.2" xref="S4.SS4.p13.1.m1.1.1.2.cmml">2</mn><mo id="S4.SS4.p13.1.m1.1.1.1" xref="S4.SS4.p13.1.m1.1.1.1.cmml">⁢</mo><mi id="S4.SS4.p13.1.m1.1.1.3" xref="S4.SS4.p13.1.m1.1.1.3.cmml">M</mi></mrow><annotation-xml encoding="MathML-Content" id="S4.SS4.p13.1.m1.1b"><apply id="S4.SS4.p13.1.m1.1.1.cmml" xref="S4.SS4.p13.1.m1.1.1"><times id="S4.SS4.p13.1.m1.1.1.1.cmml" xref="S4.SS4.p13.1.m1.1.1.1"></times><cn id="S4.SS4.p13.1.m1.1.1.2.cmml" type="integer" xref="S4.SS4.p13.1.m1.1.1.2">2</cn><ci id="S4.SS4.p13.1.m1.1.1.3.cmml" xref="S4.SS4.p13.1.m1.1.1.3">𝑀</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS4.p13.1.m1.1c">2M</annotation><annotation encoding="application/x-llamapun" id="S4.SS4.p13.1.m1.1d">2 italic_M</annotation></semantics></math> data points.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Evaluation Benchmarks</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this work, we categorize evaluation benchmarks into three types: audio question answering benchmark, audio reasoning benchmark, and reliability and safety benchmark.</p>
</div>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS1.5.1.1">V-A</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS1.6.2">Audio Question Answering Benchmark</span>
</h3>
<div class="ltx_para" id="S5.SS1.p1">
<p class="ltx_p" id="S5.SS1.p1.1">This benchmark focuses on assessing a model’s fundamental understanding of audio. It evaluates capabilities such as sound event recognition, acoustic scene classification, temporal comprehension, and audio attribute identification.
By designing diverse types of questions, this benchmark examines whether a model can accurately interpret various aspects of an audio signal and provide reasonable and correct answers based on the given auditory information.
This allows us to gauge the model’s core competence in audio understanding.
For this benchmark, we have collected and organized the following evaluation datasets.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p2">
<p class="ltx_p" id="S5.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p2.1.1">Clotho-AQA</span>:
Clotho-AQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib38" title="">38</a>]</cite> is a widely used dataset designed for audio question answering (AQA) tasks, where models generate natural language answers based on audio signals and corresponding questions.
The dataset is constructed through crowd-sourcing, with questions collected for each audio file and answers provided by multiple annotators.
To ensure data reliability, only questions where all annotators unanimously agree on the answer are included.
The final dataset consists of 789 instances.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p3">
<p class="ltx_p" id="S5.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p3.1.1">Nonspeech7k AQA</span>:
Nonspeech7k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib45" title="">45</a>]</cite> is a dataset of human non-speech sounds collected from real-life environments, covering seven sound categories: breath, cough, cry, laugh, scream, sneeze, and yawn.
We utilize all 725 audio instances from the test split of the Nonspeech7k dataset for evaluation.
To adapt the dataset to a question-answering (QA) format, we manually designed general questions such as: <span class="ltx_text ltx_font_italic" id="S5.SS1.p3.1.2">Classify the given sound as exactly one of: breath, cough, cry, laugh, scream, sneeze, or yawn.</span>
This ensures that the model follows explicit instructions when answering the questions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p4">
<p class="ltx_p" id="S5.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p4.1.1">CochlScene AQA</span>:
CochlScene <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib46" title="">46</a>]</cite> is a crowd-sourced dataset designed for acoustic scene classification, covering 13 different acoustic scenes.
The primary goal of this task is to determine the environment in which a given audio recording was captured based on its acoustic characteristics.
For evaluation, we sample 100 audio clips from each category in the test split, resulting in a total of 1,300 test instances.
We converted the dataset into a question-answering (QA) format through the manual design of general questions like <span class="ltx_text ltx_font_italic" id="S5.SS1.p4.1.2">Which acoustic scene does this audio clip belong to? The answer must be one of the four given options: Residential Area, Restaurant, Subway Station, Car.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS1.p5">
<p class="ltx_p" id="S5.SS1.p5.1"><span class="ltx_text ltx_font_bold" id="S5.SS1.p5.1.1">EDANSA-2019 AQA</span>:
The Ecoacoustic Dataset from Arctic North Slope Alaska (EDANSA-2019) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib47" title="">47</a>]</cite> is an ecoacoustic dataset collected from the North Slope of Alaska during the summer of 2019.
The dataset contains over 27 hours of labeled audio.
Since this dataset was recorded in the Arctic, most existing models may not have been trained on similar data.
Publicly available training datasets rarely include such recordings, making EDANSA-2019 an out-of-distribution (OOD) dataset.
This poses a challenge for models, as they may struggle to generalize to these unfamiliar acoustic environments.</p>
</div>
<div class="ltx_para" id="S5.SS1.p6">
<p class="ltx_p" id="S5.SS1.p6.1">For evaluation, we selected sound events with a sufficient number of samples, covering fly, bird, woof, wind, rain, aircraft, and machine.
Due to data limitations, fly (62 instances) and woof (15 instances) have fewer samples, while all other categories contain 200 randomly selected audio clips.
In total, the test set consists of 1,077 instances.
We reformulated the dataset into a question-answering (QA) format by manually crafting general questions, for example: <span class="ltx_text ltx_font_italic" id="S5.SS1.p6.1.1">Select the most appropriate category for the given sound from the following options: Wind, Machine, Rain, Fly.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS2.5.1.1">V-B</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS2.6.2">Audio Reasoning Benchmark</span>
</h3>
<div class="ltx_para" id="S5.SS2.p1">
<p class="ltx_p" id="S5.SS2.p1.1">This benchmark focuses on evaluating a model’s advanced reasoning capabilities in audio understanding.
Beyond recognizing and interpreting audio content, models must demonstrate the ability to reason based on their understanding to make logical inferences.
For this benchmark, we utilize the following dataset for this benchmark:</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p2">
<p class="ltx_p" id="S5.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p2.1.1">Synonym and Hypernym Test</span>:
A previous study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib48" title="">48</a>]</cite> introduced a synonym and hypernym test to assess whether models can understand audio and its semantic connections to text.
Synonyms refer to words with similar meanings, while hypernyms denote broader categories that encompass more specific terms.
These relationships enable the construction of text-based prompts that test a model’s understanding of similarity and hierarchy in audio reasoning.
For example, given an audio clip of a songbird chirping, a possible question could be: <span class="ltx_text ltx_font_italic" id="S5.SS2.p2.1.2">Is the sound from an object that is a type of chordate?</span> The expected answer is yes.</p>
</div>
<div class="ltx_para" id="S5.SS2.p3">
<p class="ltx_p" id="S5.SS2.p3.1">This benchmark evaluates whether audio-aware large language models genuinely comprehend audio content and semantic relationships, or if they merely rely on statistical associations between audio, keywords, and captions.
We reproduce the evaluation setup and data selection from the original study, sourcing data from the AudioCaps dataset, resulting in 636 evaluation instances.
To further analyze the model’s behavior, we separately compute weighted precision, recall, and F1 scores for questions where the correct answer is yes and no.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p4">
<p class="ltx_p" id="S5.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p4.1.1">Audio Entailment</span>:
A previous study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib49" title="">49</a>]</cite> introduced the task of audio entailment to assess a model’s ability to perform deductive reasoning. This task evaluates whether a textual description (hypothesis) can be logically inferred from an audio recording (premise).
The conclusion falls into one of three categories: entailment, neutral, or contradiction, depending on the available evidence.
Entailment occurs when the audio recording provides sufficient evidence to affirm the truth of the hypothesis.
Neutral applies when the audio recording does not contain enough information to either confirm or deny the hypothesis.
Contradiction is determined when the audio recording offers clear evidence that the hypothesis is false.</p>
</div>
<div class="ltx_para" id="S5.SS2.p5">
<p class="ltx_p" id="S5.SS2.p5.1">We format this task as a multiple-choice question, requiring the model to identify the correct conclusion type based on the given prompt.
For example, given an audio clip, the model is asked to select the hypothesis that corresponds to entailment: <span class="ltx_text ltx_font_italic" id="S5.SS2.p5.1.1">Based on what you hear, which of the following hypotheses is entailment? (A) A quiet room with no movement or wind. (B) A motorcycle is overtaking cars on a windy road. (C) Vehicles are moving on a road with wind noise present.</span>
In this setup, the order of the answer choices is randomly shuffled for each instance.
The dataset sources audio recordings from Clotho <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib38" title="">38</a>]</cite> and AudioCaps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib33" title="">33</a>]</cite>.
For evaluation, we report the overall accuracy.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p6">
<p class="ltx_p" id="S5.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p6.1.1">SAKURA</span>:
This benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib41" title="">41</a>]</cite> evaluates multi-hop reasoning in audio understanding.
We use the SAKURA test set from the audio track for this evaluation.
In this benchmark, each audio clip is paired with two types of questions: single-hop and multi-hop.
Single-hop questions directly ask about attributes of the sound event in the audio, such as <span class="ltx_text ltx_font_italic" id="S5.SS2.p6.1.2">Out of the animals listed below, can you identify the one that most closely matches the sound in the audio?</span>
In contrast, multi-hop questions require multiple steps of reasoning, combining fundamental audio understanding with text-based inference to arrive at the correct answer.
For example, a multi-hop question might be: <span class="ltx_text ltx_font_italic" id="S5.SS2.p6.1.3">Taking into account the sound from the recording and the natural tendencies of this type of animal making the sound, which of the following behaviors is most representative of how it typically acts? (a) Wagging its tail (b) Jumping between branches (c) Hiding in shells (d) Flapping its wings</span>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS2.p7">
<p class="ltx_p" id="S5.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S5.SS2.p7.1.1">MMAU</span>:
This is a massive multi-task audio understanding and reasoning benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib42" title="">42</a>]</cite> designed to evaluate multimodal audio understanding models on tasks requiring expert-level knowledge and complex reasoning.
In this study, we focus on assessing model performance specifically along the audio dimension, using the test-mini split for evaluation.
The evaluation settings follow the original benchmark’s guidelines.</p>
</div>
</section>
<section class="ltx_subsection" id="S5.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S5.SS3.5.1.1">V-C</span> </span><span class="ltx_text ltx_font_italic" id="S5.SS3.6.2">Reliability and Safety Benchmark</span>
</h3>
<div class="ltx_para" id="S5.SS3.p1">
<p class="ltx_p" id="S5.SS3.p1.1">The Reliable and Safety Benchmark focuses on evaluating the trustworthiness and safety of audio-aware large language models.
We select the following evaluation benchmark.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p2">
<p class="ltx_p" id="S5.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p2.1.1">Audio Hallucination</span>:
Our previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib13" title="">13</a>]</cite> introduced benchmarks to evaluate object hallucination in audio—a phenomenon where models incorrectly detect or infer non-existent sound events.
In this task, given an audio clip, the model is tested on whether a specific sound event is present.
Inspired by Polling-based Object Probing Evaluation (POPE) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib50" title="">50</a>]</cite>, our prior work adopted a similar methodology to evaluate ALLMs.
We formulate object hallucination detection as a binary classification task, prompting ALLMs to respond with “Yes” or “No”.
To assess model robustness, we design four different prompts that ask whether a particular sound is present, including variations such as <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.2">Is there a sound of [object]?</span>, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.3">Does the audio contain the sound of [object]?</span>, <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.4">Have you noticed the sound of [object]?</span>, and <span class="ltx_text ltx_font_italic" id="S5.SS3.p2.1.5">Can you hear the sound of [object]?</span></p>
</div>
<div class="ltx_para" id="S5.SS3.p3">
<p class="ltx_p" id="S5.SS3.p3.1">This benchmark is constructed using AudioCaps, sampling audio clips that contain more than three sound events.
It consists of 407 distinct audio clips, where we treat all ground truth labels as positive samples.
To introduce negative samples, we apply a random negative sampling strategy, selecting three objects that are not present in the given audio.
In addition, in order to further analyze the model’s behavior, we separately compute precision, recall, and F1 scores for questions where the correct answer is yes and no.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.SS3.p4">
<p class="ltx_p" id="S5.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S5.SS3.p4.1.1">Instruction Following Ability Evaluation</span>:
Previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib12" title="">12</a>]</cite> has assessed instruction-following capabilities and quantified catastrophic forgetting in audio-aware large language models.
The findings highlight that most audio-aware large language models struggle with even basic instruction following, exhibiting significant performance degradation compared to text-based large language models.
To evaluate the model’s instruction-following ability, we utilize both close-ended and open-ended questions.
Close-ended questions require the model to adhere to specific formatting constraints, ensuring strict compliance with instructions.
In contrast, open-ended questions allow for more flexibility, evaluating the model’s ability to follow complex and creative instructions.
We report the instruction following rate (IFrate) and forgetting rate (<math alttext="\Delta" class="ltx_Math" display="inline" id="S5.SS3.p4.1.m1.1"><semantics id="S5.SS3.p4.1.m1.1a"><mi id="S5.SS3.p4.1.m1.1.1" mathvariant="normal" xref="S5.SS3.p4.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S5.SS3.p4.1.m1.1b"><ci id="S5.SS3.p4.1.m1.1.1.cmml" xref="S5.SS3.p4.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS3.p4.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S5.SS3.p4.1.m1.1d">roman_Δ</annotation></semantics></math>), which is the relative IFrate difference between ALLMs and their LLM counterpart.
A smaller forgetting rate indicates a lower degree of forgetting in ALLMs.
Note that since we cannot directly access proprietary models like Gemini, we are unable to calculate the forgetting rate.
Similarly, for the Qwen-Audio models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>]</cite>, which are finetuned from the pretrained Qwen-Audio base model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>]</cite>, it is not feasible to compute the IFrate difference between ALLMs and their text-only LLM.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Result</span>
</h2>
<figure class="ltx_table" id="S6.T4">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE IV: </span>
Evaluation results for the audio question answering and audio hallucination benchmark.
F1 (Y) and F1 (N) represent the F1 scores for cases where the correct answer is yes and no, respectively.
F1 (W) denotes the weighted F1 score.
Acc refers to accuracy, and Yes indicates the percentage of responses where the model answers yes.
(Unit: %)
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T4.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T4.3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T4.3.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.2.1" style="font-size:80%;">EDANSA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.3.1" style="font-size:80%;">ClothoAQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.4.1" style="font-size:80%;">CochlScene</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T4.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.5.1" style="font-size:80%;">NonSpeech</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S6.T4.3.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T4.3.1.1.6.1" style="font-size:80%;">Audio Hallucination</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.2.2.1"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.1.1" style="font-size:80%;">Models</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.2.1" style="font-size:80%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.2.2.3"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.3.1" style="font-size:80%;">Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.2.2.4"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.4.1" style="font-size:80%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.2.2.5"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.5.1" style="font-size:80%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.2.2.6"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.6.1" style="font-size:80%;">F1 (Y)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.2.2.7"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.7.1" style="font-size:80%;">F1 (N)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.2.2.8"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.8.1" style="font-size:80%;">F1 (W)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.2.2.9"><span class="ltx_text ltx_font_bold" id="S6.T4.3.2.2.9.1" style="font-size:80%;">Yes</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.3.3.3.1">
<span class="ltx_text" id="S6.T4.3.3.3.1.1" style="font-size:80%;">Qwen-Audio-Chat</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T4.3.3.3.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a><span class="ltx_text" id="S6.T4.3.3.3.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.3.2"><span class="ltx_text" id="S6.T4.3.3.3.2.1" style="font-size:80%;">34.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.3.3"><span class="ltx_text" id="S6.T4.3.3.3.3.1" style="font-size:80%;">74.90</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.3.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T4.3.3.3.4.1" style="font-size:80%;">55.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.3.5"><span class="ltx_text" id="S6.T4.3.3.3.5.1" style="font-size:80%;">34.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.3.6"><span class="ltx_text" id="S6.T4.3.3.3.6.1" style="font-size:80%;">75.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.3.7"><span class="ltx_text" id="S6.T4.3.3.3.7.1" style="font-size:80%;">52.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.3.8"><span class="ltx_text" id="S6.T4.3.3.3.8.1" style="font-size:80%;">64.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.3.3.9"><span class="ltx_text" id="S6.T4.3.3.3.9.1" style="font-size:80%;">81.04</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.4.4.1">
<span class="ltx_text" id="S6.T4.3.4.4.1.1" style="font-size:80%;">Qwen2-Audio-Instruct</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T4.3.4.4.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a><span class="ltx_text" id="S6.T4.3.4.4.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.4.4.2"><span class="ltx_text" id="S6.T4.3.4.4.2.1" style="font-size:80%;">41.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.4.4.3"><span class="ltx_text" id="S6.T4.3.4.4.3.1" style="font-size:80%;">75.55</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.4.4.4"><span class="ltx_text ltx_font_bold" id="S6.T4.3.4.4.4.1" style="font-size:80%;">60.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.4.4.5"><span class="ltx_text" id="S6.T4.3.4.4.5.1" style="font-size:80%;">54.84</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.4.4.6"><span class="ltx_text" id="S6.T4.3.4.4.6.1" style="font-size:80%;">72.89</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.4.4.7"><span class="ltx_text" id="S6.T4.3.4.4.7.1" style="font-size:80%;">54.44</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.4.4.8"><span class="ltx_text" id="S6.T4.3.4.4.8.1" style="font-size:80%;">63.67</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.4.4.9"><span class="ltx_text" id="S6.T4.3.4.4.9.1" style="font-size:80%;">75.39</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.5.5.1">
<span class="ltx_text" id="S6.T4.3.5.5.1.1" style="font-size:80%;">SALMONN-7B</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T4.3.5.5.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a><span class="ltx_text" id="S6.T4.3.5.5.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.5.5.2"><span class="ltx_text" id="S6.T4.3.5.5.2.1" style="font-size:80%;">26.36</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.5.5.3"><span class="ltx_text" id="S6.T4.3.5.5.3.1" style="font-size:80%;">76.56</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.5.5.4"><span class="ltx_text" id="S6.T4.3.5.5.4.1" style="font-size:80%;">37.99</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.5.5.5"><span class="ltx_text ltx_font_bold" id="S6.T4.3.5.5.5.1" style="font-size:80%;">78.29</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.5.5.6"><span class="ltx_text" id="S6.T4.3.5.5.6.1" style="font-size:80%;">70.06</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.5.5.7"><span class="ltx_text" id="S6.T4.3.5.5.7.1" style="font-size:80%;">26.48</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.5.5.8"><span class="ltx_text" id="S6.T4.3.5.5.8.1" style="font-size:80%;">48.27</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.5.5.9"><span class="ltx_text" id="S6.T4.3.5.5.9.1" style="font-size:80%;">92.14</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.6.6.1">
<span class="ltx_text" id="S6.T4.3.6.6.1.1" style="font-size:80%;">SALMONN-13B</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T4.3.6.6.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a><span class="ltx_text" id="S6.T4.3.6.6.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.6.6.2"><span class="ltx_text" id="S6.T4.3.6.6.2.1" style="font-size:80%;">29.96</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.6.6.3"><span class="ltx_text ltx_font_bold" id="S6.T4.3.6.6.3.1" style="font-size:80%;">83.99</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.6.6.4"><span class="ltx_text" id="S6.T4.3.6.6.4.1" style="font-size:80%;">31.06</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.6.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T4.3.6.6.5.1" style="font-size:80%;">77.24</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.6.6.6"><span class="ltx_text" id="S6.T4.3.6.6.6.1" style="font-size:80%;">76.94</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.6.6.7"><span class="ltx_text" id="S6.T4.3.6.6.7.1" style="font-size:80%;">60.07</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.6.6.8"><span class="ltx_text" id="S6.T4.3.6.6.8.1" style="font-size:80%;">68.50</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.6.6.9"><span class="ltx_text" id="S6.T4.3.6.6.9.1" style="font-size:80%;">76.78</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.7.7.1">
<span class="ltx_text" id="S6.T4.3.7.7.1.1" style="font-size:80%;">LTU</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T4.3.7.7.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a><span class="ltx_text" id="S6.T4.3.7.7.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.7.7.2"><span class="ltx_text" id="S6.T4.3.7.7.2.1" style="font-size:80%;">38.71</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.7.7.3"><span class="ltx_text" id="S6.T4.3.7.7.3.1" style="font-size:80%;">67.10</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.7.7.4"><span class="ltx_text" id="S6.T4.3.7.7.4.1" style="font-size:80%;">14.91</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.7.7.5"><span class="ltx_text" id="S6.T4.3.7.7.5.1" style="font-size:80%;">5.79</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.7.7.6"><span class="ltx_text" id="S6.T4.3.7.7.6.1" style="font-size:80%;">78.27</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.7.7.7"><span class="ltx_text" id="S6.T4.3.7.7.7.1" style="font-size:80%;">73.65</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.7.7.8"><span class="ltx_text" id="S6.T4.3.7.7.8.1" style="font-size:80%;">75.96</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.7.7.9"><span class="ltx_text" id="S6.T4.3.7.7.9.1" style="font-size:80%;">59.97</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.8.8.1">
<span class="ltx_text" id="S6.T4.3.8.8.1.1" style="font-size:80%;">LTU-AS</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T4.3.8.8.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a><span class="ltx_text" id="S6.T4.3.8.8.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.8.8.2"><span class="ltx_text" id="S6.T4.3.8.8.2.1" style="font-size:80%;">23.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.8.8.3"><span class="ltx_text" id="S6.T4.3.8.8.3.1" style="font-size:80%;">56.77</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.8.8.4"><span class="ltx_text" id="S6.T4.3.8.8.4.1" style="font-size:80%;">20.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.8.8.5"><span class="ltx_text" id="S6.T4.3.8.8.5.1" style="font-size:80%;">8.06</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.8.8.6"><span class="ltx_text" id="S6.T4.3.8.8.6.1" style="font-size:80%;">51.68</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.8.8.7"><span class="ltx_text" id="S6.T4.3.8.8.7.1" style="font-size:80%;">45.49</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.8.8.8"><span class="ltx_text" id="S6.T4.3.8.8.8.1" style="font-size:80%;">48.59</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.8.8.9"><span class="ltx_text" id="S6.T4.3.8.8.9.1" style="font-size:80%;">56.02</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.9.9.1">
<span class="ltx_text" id="S6.T4.3.9.9.1.1" style="font-size:80%;">Gemini-1.5-Pro</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T4.3.9.9.1.2.1" style="font-size:80%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib26" title="">26</a><span class="ltx_text" id="S6.T4.3.9.9.1.3.2" style="font-size:80%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.9.9.2"><span class="ltx_text" id="S6.T4.3.9.9.2.1" style="font-size:80%;">17.46</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.9.9.3"><span class="ltx_text" id="S6.T4.3.9.9.3.1" style="font-size:80%;">68.43</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.9.9.4"><span class="ltx_text" id="S6.T4.3.9.9.4.1" style="font-size:80%;">36.18</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.9.9.5"><span class="ltx_text" id="S6.T4.3.9.9.5.1" style="font-size:80%;">47.01</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.9.9.6"><span class="ltx_text" id="S6.T4.3.9.9.6.1" style="font-size:80%;">65.86</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.9.9.7"><span class="ltx_text" id="S6.T4.3.9.9.7.1" style="font-size:80%;">70.95</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.9.9.8"><span class="ltx_text" id="S6.T4.3.9.9.8.1" style="font-size:80%;">68.41</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.9.9.9"><span class="ltx_text" id="S6.T4.3.9.9.9.1" style="font-size:80%;">42.02</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.3.10.10.1"><span class="ltx_text" id="S6.T4.3.10.10.1.1" style="font-size:80%;">Ours (BALSa, only-positive)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.10.10.2"><span class="ltx_text" id="S6.T4.3.10.10.2.1" style="font-size:80%;">45.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.10.10.3"><span class="ltx_text" id="S6.T4.3.10.10.3.1" style="font-size:80%;">75.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.10.10.4"><span class="ltx_text" id="S6.T4.3.10.10.4.1" style="font-size:80%;">39.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.10.10.5"><span class="ltx_text" id="S6.T4.3.10.10.5.1" style="font-size:80%;">54.17</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.10.10.6"><span class="ltx_text" id="S6.T4.3.10.10.6.1" style="font-size:80%;">73.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.10.10.7"><span class="ltx_text" id="S6.T4.3.10.10.7.1" style="font-size:80%;">55.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.10.10.8"><span class="ltx_text" id="S6.T4.3.10.10.8.1" style="font-size:80%;">64.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.10.10.9"><span class="ltx_text" id="S6.T4.3.10.10.9.1" style="font-size:80%;">74.81</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.11.11.1"><span class="ltx_text" id="S6.T4.3.11.11.1.1" style="font-size:80%;">Ours (BALSa, positive-negative)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.11.11.2"><span class="ltx_text" id="S6.T4.3.11.11.2.1" style="font-size:80%;">42.06</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.11.11.3"><span class="ltx_text" id="S6.T4.3.11.11.3.1" style="font-size:80%;">74.96</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.11.11.4"><span class="ltx_text" id="S6.T4.3.11.11.4.1" style="font-size:80%;">42.87</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.11.11.5"><span class="ltx_text" id="S6.T4.3.11.11.5.1" style="font-size:80%;">53.66</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.11.11.6"><span class="ltx_text" id="S6.T4.3.11.11.6.1" style="font-size:80%;">80.18</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.11.11.7"><span class="ltx_text" id="S6.T4.3.11.11.7.1" style="font-size:80%;">74.04</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.11.11.8"><span class="ltx_text" id="S6.T4.3.11.11.8.1" style="font-size:80%;">77.1</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.11.11.9"><span class="ltx_text" id="S6.T4.3.11.11.9.1" style="font-size:80%;">63.69</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T4.3.12.12.1"><span class="ltx_text" id="S6.T4.3.12.12.1.1" style="font-size:80%;">Ours (BALSa, combined)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T4.3.12.12.2"><span class="ltx_text" id="S6.T4.3.12.12.2.1" style="font-size:80%;">48.56</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.12.12.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T4.3.12.12.3.1" style="font-size:80%;">80.93</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.12.12.4"><span class="ltx_text" id="S6.T4.3.12.12.4.1" style="font-size:80%;">42.48</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.12.12.5"><span class="ltx_text" id="S6.T4.3.12.12.5.1" style="font-size:80%;">52.17</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.12.12.6"><span class="ltx_text" id="S6.T4.3.12.12.6.1" style="font-size:80%;">77.21</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.12.12.7"><span class="ltx_text" id="S6.T4.3.12.12.7.1" style="font-size:80%;">71.23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.12.12.8"><span class="ltx_text" id="S6.T4.3.12.12.8.1" style="font-size:80%;">74.22</span></td>
<td class="ltx_td ltx_align_center" id="S6.T4.3.12.12.9"><span class="ltx_text" id="S6.T4.3.12.12.9.1" style="font-size:80%;">61.58</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.13.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T4.3.13.13.1"><span class="ltx_text" id="S6.T4.3.13.13.1.1" style="font-size:80%;">Ours (BALSa-MA, discrimination)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.13.13.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T4.3.13.13.2.1" style="font-size:80%;">50.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.13.13.3"><span class="ltx_text" id="S6.T4.3.13.13.3.1" style="font-size:80%;">77.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.13.13.4"><span class="ltx_text" id="S6.T4.3.13.13.4.1" style="font-size:80%;">44.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.13.13.5"><span class="ltx_text" id="S6.T4.3.13.13.5.1" style="font-size:80%;">68.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.13.13.6"><span class="ltx_text ltx_font_bold" id="S6.T4.3.13.13.6.1" style="font-size:80%;">85.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.13.13.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T4.3.13.13.7.1" style="font-size:80%;">84.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.13.13.8"><span class="ltx_text ltx_font_bold" id="S6.T4.3.13.13.8.1" style="font-size:80%;">85.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T4.3.13.13.9"><span class="ltx_text" id="S6.T4.3.13.13.9.1" style="font-size:80%;">52.29</span></td>
</tr>
<tr class="ltx_tr" id="S6.T4.3.14.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T4.3.14.14.1"><span class="ltx_text" id="S6.T4.3.14.14.1.1" style="font-size:80%;">Ours (BALSa-MA, captioning)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.14.14.2"><span class="ltx_text ltx_font_bold" id="S6.T4.3.14.14.2.1" style="font-size:80%;">53.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.14.14.3"><span class="ltx_text" id="S6.T4.3.14.14.3.1" style="font-size:80%;">78.75</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.14.14.4"><span class="ltx_text" id="S6.T4.3.14.14.4.1" style="font-size:80%;">44.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.14.14.5"><span class="ltx_text" id="S6.T4.3.14.14.5.1" style="font-size:80%;">70.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.14.14.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T4.3.14.14.6.1" style="font-size:80%;">84.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.14.14.7"><span class="ltx_text ltx_font_bold" id="S6.T4.3.14.14.7.1" style="font-size:80%;">84.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.14.14.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T4.3.14.14.8.1" style="font-size:80%;">84.47</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T4.3.14.14.9"><span class="ltx_text" id="S6.T4.3.14.14.9.1" style="font-size:80%;">47.01</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S6.T4" title="TABLE IV ‣ VI Result ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">IV</span></a>, compared to other publicly available baseline models, our proposed audio-aware large language models trained using the BALSa method (combined, positive-only, and positive-negative) achieve performance comparable to other public baseline models in audio question answering benchmarks.
For instance, in ClothoAQA, our models perform on par with Qwen2-Audio-Instruct and SALMONN-7B.
In NonSpeech AQA, our models attain results comparable to Qwen2-Audio-Instruct and Qwen-Audio-Chat.
In CochlScene AQA, our models outperform SALMONN-7B and SALMONN-13B.
Compared to Gemini-1.5-Pro, a proprietary model, our method remains competitive, leading by 6% to 25% on these evaluation benchmarks.
While SALMONN-13B achieves the best performance on ClothoAQA and NonSpeech AQA, our models surpass it in other evaluation benchmarks.
Despite falling behind Qwen-Audio-Chat and Qwen2-Audio-Instruct in tasks such as CochlScene AQA, our models demonstrate well-rounded performance across various benchmarks.
These results highlight the effectiveness of the BALSa framework.
By leveraging synthetic data to enhance audio-language alignment, BALSa delivers competitive results on traditional audio question answering benchmarks, often surpassing other baselines while maintaining consistent performance across different benchmarks.
Notably, our models achieve these results using only 12% of the equivalent training duration required by the widely adopted and powerful Qwen2-Audio-Instruct model, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S4.T3" title="TABLE III ‣ IV-A Training Datasets ‣ IV Experimental Setup ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">III</span></a>.</p>
</div>
<div class="ltx_para" id="S6.p2">
<p class="ltx_p" id="S6.p2.1">The EDANSA AQA tasks serve as zero-shot benchmarks, evaluating models on out-of-domain audio that is not encountered during training.
Our models, along with SALMONN and LTU-AS, are assessed under these conditions.
Compared to other baselines, our method demonstrates a significant performance advantage.
The results from this benchmark suggest that our approach exhibits a certain level of adaptability when applied to out-of-distribution data.
Note that the Qwen series does not disclose its training datasets, thereby making it impossible to determine whether its results are obtained under a true zero-shot setting.</p>
</div>
<div class="ltx_para" id="S6.p3">
<p class="ltx_p" id="S6.p3.1">In audio hallucination tasks, incorporating negative samples through the BALSa framework proves to be beneficial.
After adding negative samples, the positive-negative setting improves the weighted F1 score by 13%, representing an approximate 20% increase compared to the positive-only setting.
This suggests that a contrastive-like approach effectively help mitigate audio hallucinations while preserving performance on audio question answering.
Notably, in audio hallucination tasks, many baseline models struggle to distinguish nonexistent sounds when the ground truth is no, frequently misidentifying them as present.
In fact, most baseline models tend to answer yes regardless of whether the sound exists.
This issue raises concerns about model reliability in real-world applications.
Even models that perform well on other evaluation benchmarks, such as Qwen2-Audio-Instruct and Qwen-Audio-Chat, exhibit weaker performance in hallucination detection.
Our proposed method leverages BALSa to generate negative samples.
The addition of these samples, while simple, directly and effectively enhances performance on the audio hallucination benchmark.
At the same time, it maintains performance on other evaluation benchmarks and, in some cases, even leads to slight improvements.
This demonstrates that our approach not only mitigates hallucinations but also achieves a strong level of performance in fundamental audio understanding.
Regarding the incorporation of negative samples, our experimental results indicate that separating positive and negative samples yields better performance compared to combining them into a single sample.</p>
</div>
<figure class="ltx_table" id="S6.T5">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE V: </span>
Evaluation results for the audio reasoning and instruction-following benchmark.
P represents the weighted precision, R denotes the weighted recall, and F1 refers to the weighted F1 score.
Acc indicates overall accuracy, while S-Accuracy and M-Accuracy measure accuracy for single-hop and multi-hop questions, respectively.
Acc (C) and Acc (A) represent accuracy when the source datasets are Clotho and AudioCaps, respectively.
(Unit: %)
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T5.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T5.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T5.1.2.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T5.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.2.1.2.1" style="font-size:70%;">Synonym-Hypernym</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T5.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.2.1.3.1" style="font-size:70%;">SAKURA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T5.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.2.1.4.1" style="font-size:70%;">MMAU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S6.T5.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.2.1.5.1" style="font-size:70%;">Audio Entailment</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S6.T5.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.2.1.6.1" style="font-size:70%;">Speech-IFEval</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.2.1" style="font-size:70%;">Models</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.3.1" style="font-size:70%;">P</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.4.1" style="font-size:70%;">R</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.5.1" style="font-size:70%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.6.1" style="font-size:70%;">Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.7"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.7.1" style="font-size:70%;">S-Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.8"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.8.1" style="font-size:70%;">M-Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.9"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.9.1" style="font-size:70%;">Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.10.1" style="font-size:70%;">Acc (C)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.11"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.11.1" style="font-size:70%;">Acc (A)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.12"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.12.1" style="font-size:70%;">Close</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.13"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.13.1" style="font-size:70%;">Open</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.14"><span class="ltx_text ltx_font_bold" id="S6.T5.1.1.14.1" style="font-size:70%;">IFRate</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="S6.T5.1.1.1.m1.1"><semantics id="S6.T5.1.1.1.m1.1a"><mi id="S6.T5.1.1.1.m1.1.1" mathsize="70%" mathvariant="normal" xref="S6.T5.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S6.T5.1.1.1.m1.1b"><ci id="S6.T5.1.1.1.m1.1.1.cmml" xref="S6.T5.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T5.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S6.T5.1.1.1.m1.1d">roman_Δ</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.1.3.2.1">
<span class="ltx_text" id="S6.T5.1.3.2.1.1" style="font-size:70%;">Qwen-Audio-Chat</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T5.1.3.2.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a><span class="ltx_text" id="S6.T5.1.3.2.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.2"><span class="ltx_text ltx_font_bold" id="S6.T5.1.3.2.2.1" style="font-size:70%;">86.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.3"><span class="ltx_text" id="S6.T5.1.3.2.3.1" style="font-size:70%;">79.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.4"><span class="ltx_text" id="S6.T5.1.3.2.4.1" style="font-size:70%;">79.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.5"><span class="ltx_text" id="S6.T5.1.3.2.5.1" style="font-size:70%;">78.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.3.2.6.1" style="font-size:70%;">89.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.7"><span class="ltx_text" id="S6.T5.1.3.2.7.1" style="font-size:70%;">67.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.8"><span class="ltx_text" id="S6.T5.1.3.2.8.1" style="font-size:70%;">38.74</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.9"><span class="ltx_text" id="S6.T5.1.3.2.9.1" style="font-size:70%;">36.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.10"><span class="ltx_text" id="S6.T5.1.3.2.10.1" style="font-size:70%;">34.04</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.11"><span class="ltx_text" id="S6.T5.1.3.2.11.1" style="font-size:70%;">10.93</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.12"><span class="ltx_text" id="S6.T5.1.3.2.12.1" style="font-size:70%;">56.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.13"><span class="ltx_text" id="S6.T5.1.3.2.13.1" style="font-size:70%;">32.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.3.2.14"><span class="ltx_text" id="S6.T5.1.3.2.14.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.4.3.1">
<span class="ltx_text" id="S6.T5.1.4.3.1.1" style="font-size:70%;">Qwen2-Audio-Instruct</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T5.1.4.3.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a><span class="ltx_text" id="S6.T5.1.4.3.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.2"><span class="ltx_text" id="S6.T5.1.4.3.2.1" style="font-size:70%;">82.52</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.3"><span class="ltx_text" id="S6.T5.1.4.3.3.1" style="font-size:70%;">68.71</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.4"><span class="ltx_text" id="S6.T5.1.4.3.4.1" style="font-size:70%;">67.67</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.5"><span class="ltx_text" id="S6.T5.1.4.3.5.1" style="font-size:70%;">76.90</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.6"><span class="ltx_text ltx_font_bold" id="S6.T5.1.4.3.6.1" style="font-size:70%;">92.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.7"><span class="ltx_text" id="S6.T5.1.4.3.7.1" style="font-size:70%;">62.83</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.8"><span class="ltx_text ltx_font_bold" id="S6.T5.1.4.3.8.1" style="font-size:70%;">57.96</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.9"><span class="ltx_text" id="S6.T5.1.4.3.9.1" style="font-size:70%;">42.97</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.10"><span class="ltx_text" id="S6.T5.1.4.3.10.1" style="font-size:70%;">46.19</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.11"><span class="ltx_text" id="S6.T5.1.4.3.11.1" style="font-size:70%;">41.59</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.12"><span class="ltx_text" id="S6.T5.1.4.3.12.1" style="font-size:70%;">32.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.13"><span class="ltx_text" id="S6.T5.1.4.3.13.1" style="font-size:70%;">47.11</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.4.3.14"><span class="ltx_text" id="S6.T5.1.4.3.14.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.5.4.1">
<span class="ltx_text" id="S6.T5.1.5.4.1.1" style="font-size:70%;">SALMONN-7B</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T5.1.5.4.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a><span class="ltx_text" id="S6.T5.1.5.4.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.2"><span class="ltx_text" id="S6.T5.1.5.4.2.1" style="font-size:70%;">61.72</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.3"><span class="ltx_text" id="S6.T5.1.5.4.3.1" style="font-size:70%;">54.71</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.4"><span class="ltx_text" id="S6.T5.1.5.4.4.1" style="font-size:70%;">54.04</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.5"><span class="ltx_text" id="S6.T5.1.5.4.5.1" style="font-size:70%;">52.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.6"><span class="ltx_text" id="S6.T5.1.5.4.6.1" style="font-size:70%;">62.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.7"><span class="ltx_text" id="S6.T5.1.5.4.7.1" style="font-size:70%;">44.23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.8"><span class="ltx_text" id="S6.T5.1.5.4.8.1" style="font-size:70%;">51.95</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.9"><span class="ltx_text" id="S6.T5.1.5.4.9.1" style="font-size:70%;">33.24</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.10"><span class="ltx_text" id="S6.T5.1.5.4.10.1" style="font-size:70%;">34.10</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.11"><span class="ltx_text" id="S6.T5.1.5.4.11.1" style="font-size:70%;">63.02</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.12"><span class="ltx_text" id="S6.T5.1.5.4.12.1" style="font-size:70%;">46.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.13"><span class="ltx_text" id="S6.T5.1.5.4.13.1" style="font-size:70%;">54.51</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.5.4.14"><span class="ltx_text" id="S6.T5.1.5.4.14.1" style="font-size:70%;">-16.27</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.6.5.1">
<span class="ltx_text" id="S6.T5.1.6.5.1.1" style="font-size:70%;">SALMONN-13B</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T5.1.6.5.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a><span class="ltx_text" id="S6.T5.1.6.5.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.2"><span class="ltx_text" id="S6.T5.1.6.5.2.1" style="font-size:70%;">76.55</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.3"><span class="ltx_text" id="S6.T5.1.6.5.3.1" style="font-size:70%;">61.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.4"><span class="ltx_text" id="S6.T5.1.6.5.4.1" style="font-size:70%;">59.04</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.5"><span class="ltx_text" id="S6.T5.1.6.5.5.1" style="font-size:70%;">58.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.6"><span class="ltx_text" id="S6.T5.1.6.5.6.1" style="font-size:70%;">70.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.7"><span class="ltx_text" id="S6.T5.1.6.5.7.1" style="font-size:70%;">46.86</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.8"><span class="ltx_text" id="S6.T5.1.6.5.8.1" style="font-size:70%;">48.95</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.9"><span class="ltx_text" id="S6.T5.1.6.5.9.1" style="font-size:70%;">35.25</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.10"><span class="ltx_text" id="S6.T5.1.6.5.10.1" style="font-size:70%;">30.81</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.11"><span class="ltx_text" id="S6.T5.1.6.5.11.1" style="font-size:70%;">37.41</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.12"><span class="ltx_text" id="S6.T5.1.6.5.12.1" style="font-size:70%;">61.25</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.13"><span class="ltx_text" id="S6.T5.1.6.5.13.1" style="font-size:70%;">36.89</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.6.5.14"><span class="ltx_text" id="S6.T5.1.6.5.14.1" style="font-size:70%;">-34.53</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.7.6.1">
<span class="ltx_text" id="S6.T5.1.7.6.1.1" style="font-size:70%;">LTU</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T5.1.7.6.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a><span class="ltx_text" id="S6.T5.1.7.6.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.2"><span class="ltx_text" id="S6.T5.1.7.6.2.1" style="font-size:70%;">73.82</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.3"><span class="ltx_text" id="S6.T5.1.7.6.3.1" style="font-size:70%;">72.64</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.4"><span class="ltx_text" id="S6.T5.1.7.6.4.1" style="font-size:70%;">70.47</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.5"><span class="ltx_text" id="S6.T5.1.7.6.5.1" style="font-size:70%;">51.10</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.6"><span class="ltx_text" id="S6.T5.1.7.6.6.1" style="font-size:70%;">62.80</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.7"><span class="ltx_text" id="S6.T5.1.7.6.7.1" style="font-size:70%;">38.85</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.8"><span class="ltx_text" id="S6.T5.1.7.6.8.1" style="font-size:70%;">22.22</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.9"><span class="ltx_text" id="S6.T5.1.7.6.9.1" style="font-size:70%;">18.88</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.10"><span class="ltx_text" id="S6.T5.1.7.6.10.1" style="font-size:70%;">14.42</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.11"><span class="ltx_text" id="S6.T5.1.7.6.11.1" style="font-size:70%;">9.97</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.12"><span class="ltx_text" id="S6.T5.1.7.6.12.1" style="font-size:70%;">38.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.13"><span class="ltx_text" id="S6.T5.1.7.6.13.1" style="font-size:70%;">24.36</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.7.6.14"><span class="ltx_text" id="S6.T5.1.7.6.14.1" style="font-size:70%;">-62.58</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.8.7.1">
<span class="ltx_text" id="S6.T5.1.8.7.1.1" style="font-size:70%;">LTU-AS</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T5.1.8.7.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a><span class="ltx_text" id="S6.T5.1.8.7.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.2"><span class="ltx_text" id="S6.T5.1.8.7.2.1" style="font-size:70%;">47.91</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.3"><span class="ltx_text" id="S6.T5.1.8.7.3.1" style="font-size:70%;">50.63</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.4"><span class="ltx_text" id="S6.T5.1.8.7.4.1" style="font-size:70%;">48.71</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.5"><span class="ltx_text" id="S6.T5.1.8.7.5.1" style="font-size:70%;">24.70</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.6"><span class="ltx_text" id="S6.T5.1.8.7.6.1" style="font-size:70%;">30.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.7"><span class="ltx_text" id="S6.T5.1.8.7.7.1" style="font-size:70%;">19.74</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.8"><span class="ltx_text" id="S6.T5.1.8.7.8.1" style="font-size:70%;">16.82</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.9"><span class="ltx_text" id="S6.T5.1.8.7.9.1" style="font-size:70%;">26.95</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.10"><span class="ltx_text" id="S6.T5.1.8.7.10.1" style="font-size:70%;">28.64</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.11"><span class="ltx_text" id="S6.T5.1.8.7.11.1" style="font-size:70%;">28.83</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.12"><span class="ltx_text" id="S6.T5.1.8.7.12.1" style="font-size:70%;">47.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.13"><span class="ltx_text" id="S6.T5.1.8.7.13.1" style="font-size:70%;">29.19</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.8.7.14"><span class="ltx_text" id="S6.T5.1.8.7.14.1" style="font-size:70%;">-41.18</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.9.8.1">
<span class="ltx_text" id="S6.T5.1.9.8.1.1" style="font-size:70%;">Gemini-1.5-Pro</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S6.T5.1.9.8.1.2.1" style="font-size:70%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib26" title="">26</a><span class="ltx_text" id="S6.T5.1.9.8.1.3.2" style="font-size:70%;">]</span></cite>
</th>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.2"><span class="ltx_text" id="S6.T5.1.9.8.2.1" style="font-size:70%;">73.84</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.3"><span class="ltx_text" id="S6.T5.1.9.8.3.1" style="font-size:70%;">73.42</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.4"><span class="ltx_text" id="S6.T5.1.9.8.4.1" style="font-size:70%;">73.35</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.5"><span class="ltx_text" id="S6.T5.1.9.8.5.1" style="font-size:70%;">44.70</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.6"><span class="ltx_text" id="S6.T5.1.9.8.6.1" style="font-size:70%;">41.20</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.7"><span class="ltx_text" id="S6.T5.1.9.8.7.1" style="font-size:70%;">55.34</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.8"><span class="ltx_text" id="S6.T5.1.9.8.8.1" style="font-size:70%;">48.65</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.9"><span class="ltx_text" id="S6.T5.1.9.8.9.1" style="font-size:70%;">23.32</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.10"><span class="ltx_text" id="S6.T5.1.9.8.10.1" style="font-size:70%;">22.87</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.11"><span class="ltx_text ltx_font_bold" id="S6.T5.1.9.8.11.1" style="font-size:70%;">99.14</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.12"><span class="ltx_text" id="S6.T5.1.9.8.12.1" style="font-size:70%;">89.50</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.13"><span class="ltx_text ltx_font_bold" id="S6.T5.1.9.8.13.1" style="font-size:70%;">94.32</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.9.8.14"><span class="ltx_text" id="S6.T5.1.9.8.14.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.1.10.9.1"><span class="ltx_text" id="S6.T5.1.10.9.1.1" style="font-size:70%;">Ours (BALSa, only-positive)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.2"><span class="ltx_text" id="S6.T5.1.10.9.2.1" style="font-size:70%;">82.15</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.3"><span class="ltx_text" id="S6.T5.1.10.9.3.1" style="font-size:70%;">78.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.4"><span class="ltx_text" id="S6.T5.1.10.9.4.1" style="font-size:70%;">78.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.5"><span class="ltx_text" id="S6.T5.1.10.9.5.1" style="font-size:70%;">71.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.6"><span class="ltx_text" id="S6.T5.1.10.9.6.1" style="font-size:70%;">79.20</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.7"><span class="ltx_text" id="S6.T5.1.10.9.7.1" style="font-size:70%;">64.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.8"><span class="ltx_text" id="S6.T5.1.10.9.8.1" style="font-size:70%;">55.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.9"><span class="ltx_text" id="S6.T5.1.10.9.9.1" style="font-size:70%;">50.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.10"><span class="ltx_text" id="S6.T5.1.10.9.10.1" style="font-size:70%;">52.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.11"><span class="ltx_text" id="S6.T5.1.10.9.11.1" style="font-size:70%;">90.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.12"><span class="ltx_text" id="S6.T5.1.10.9.12.1" style="font-size:70%;">90.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.13"><span class="ltx_text" id="S6.T5.1.10.9.13.1" style="font-size:70%;">90.41</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.10.9.14"><span class="ltx_text" id="S6.T5.1.10.9.14.1" style="font-size:70%;">-0.68</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.11.10.1"><span class="ltx_text" id="S6.T5.1.11.10.1.1" style="font-size:70%;">Ours (BALSa, positive-negative)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.2"><span class="ltx_text" id="S6.T5.1.11.10.2.1" style="font-size:70%;">77.94</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.3"><span class="ltx_text" id="S6.T5.1.11.10.3.1" style="font-size:70%;">77.04</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.4"><span class="ltx_text" id="S6.T5.1.11.10.4.1" style="font-size:70%;">77.25</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.5"><span class="ltx_text" id="S6.T5.1.11.10.5.1" style="font-size:70%;">70.70</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.6"><span class="ltx_text" id="S6.T5.1.11.10.6.1" style="font-size:70%;">76.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.7"><span class="ltx_text" id="S6.T5.1.11.10.7.1" style="font-size:70%;">65.71</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.8"><span class="ltx_text" id="S6.T5.1.11.10.8.1" style="font-size:70%;">55.26</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.11.10.9.1" style="font-size:70%;">56.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.10"><span class="ltx_text ltx_font_bold" id="S6.T5.1.11.10.10.1" style="font-size:70%;">57.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.11.10.11.1" style="font-size:70%;">90.78</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.11.10.12.1" style="font-size:70%;">91.25</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.11.10.13.1" style="font-size:70%;">91.02</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.11.10.14"><span class="ltx_text" id="S6.T5.1.11.10.14.1" style="font-size:70%;">-0.02</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T5.1.12.11.1"><span class="ltx_text" id="S6.T5.1.12.11.1.1" style="font-size:70%;">Ours (BALSa, combined)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.2"><span class="ltx_text" id="S6.T5.1.12.11.2.1" style="font-size:70%;">75.12</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.3"><span class="ltx_text" id="S6.T5.1.12.11.3.1" style="font-size:70%;">75.31</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.4"><span class="ltx_text" id="S6.T5.1.12.11.4.1" style="font-size:70%;">74.76</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.5"><span class="ltx_text" id="S6.T5.1.12.11.5.1" style="font-size:70%;">65.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.6"><span class="ltx_text" id="S6.T5.1.12.11.6.1" style="font-size:70%;">68.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.7"><span class="ltx_text" id="S6.T5.1.12.11.7.1" style="font-size:70%;">61.99</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.8"><span class="ltx_text" id="S6.T5.1.12.11.8.1" style="font-size:70%;">54.95</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.9"><span class="ltx_text" id="S6.T5.1.12.11.9.1" style="font-size:70%;">46.92</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.10"><span class="ltx_text" id="S6.T5.1.12.11.10.1" style="font-size:70%;">49.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.11"><span class="ltx_text" id="S6.T5.1.12.11.11.1" style="font-size:70%;">88.32</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.12"><span class="ltx_text ltx_font_bold" id="S6.T5.1.12.11.12.1" style="font-size:70%;">91.50</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.13"><span class="ltx_text" id="S6.T5.1.12.11.13.1" style="font-size:70%;">89.91</span></td>
<td class="ltx_td ltx_align_center" id="S6.T5.1.12.11.14"><span class="ltx_text" id="S6.T5.1.12.11.14.1" style="font-size:70%;">-1.23</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T5.1.13.12.1"><span class="ltx_text" id="S6.T5.1.13.12.1.1" style="font-size:70%;">Ours (BALSa-MA, discrimination)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.2"><span class="ltx_text" id="S6.T5.1.13.12.2.1" style="font-size:70%;">81.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.13.12.3.1" style="font-size:70%;">80.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.13.12.4.1" style="font-size:70%;">80.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.13.12.5.1" style="font-size:70%;">79.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.6"><span class="ltx_text" id="S6.T5.1.13.12.6.1" style="font-size:70%;">86.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.7"><span class="ltx_text ltx_font_bold" id="S6.T5.1.13.12.7.1" style="font-size:70%;">73.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.8"><span class="ltx_text" id="S6.T5.1.13.12.8.1" style="font-size:70%;">56.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.9"><span class="ltx_text" id="S6.T5.1.13.12.9.1" style="font-size:70%;">54.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.10"><span class="ltx_text" id="S6.T5.1.13.12.10.1" style="font-size:70%;">55.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.11"><span class="ltx_text" id="S6.T5.1.13.12.11.1" style="font-size:70%;">88.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.12"><span class="ltx_text" id="S6.T5.1.13.12.12.1" style="font-size:70%;">87.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.13"><span class="ltx_text" id="S6.T5.1.13.12.13.1" style="font-size:70%;">88.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T5.1.13.12.14"><span class="ltx_text" id="S6.T5.1.13.12.14.1" style="font-size:70%;">-3.27</span></td>
</tr>
<tr class="ltx_tr" id="S6.T5.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T5.1.14.13.1"><span class="ltx_text" id="S6.T5.1.14.13.1.1" style="font-size:70%;">Ours (BALSa-MA, captioning)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.14.13.2.1" style="font-size:70%;">82.99</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.3"><span class="ltx_text ltx_font_bold" id="S6.T5.1.14.13.3.1" style="font-size:70%;">82.86</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.4"><span class="ltx_text ltx_font_bold" id="S6.T5.1.14.13.4.1" style="font-size:70%;">82.89</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.5"><span class="ltx_text ltx_font_bold" id="S6.T5.1.14.13.5.1" style="font-size:70%;">80.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.6"><span class="ltx_text" id="S6.T5.1.14.13.6.1" style="font-size:70%;">88.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.14.13.7.1" style="font-size:70%;">71.72</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.14.13.8.1" style="font-size:70%;">57.06</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.9"><span class="ltx_text ltx_font_bold" id="S6.T5.1.14.13.9.1" style="font-size:70%;">56.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T5.1.14.13.10.1" style="font-size:70%;">56.59</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.11"><span class="ltx_text" id="S6.T5.1.14.13.11.1" style="font-size:70%;">89.07</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.12"><span class="ltx_text" id="S6.T5.1.14.13.12.1" style="font-size:70%;">84.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.13"><span class="ltx_text" id="S6.T5.1.14.13.13.1" style="font-size:70%;">86.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T5.1.14.13.14"><span class="ltx_text" id="S6.T5.1.14.13.14.1" style="font-size:70%;">-4.80</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.p4">
<p class="ltx_p" id="S6.p4.1">In the audio reasoning benchmark, which is demonstrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S6.T5" title="TABLE V ‣ VI Result ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">V</span></a>, models are required to rely on their fundamental audio understanding and apply reasoning skills to determine the correct answer.
One of the key objectives of audio-language alignment is to integrate audio information, which belongs to a different modality than text, into the text space of large language models.
However, few studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib48" title="">48</a>]</cite> have systematically and explicitly explored this issue.
As an initial exploration, we build upon previous work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib41" title="">41</a>]</cite> and evaluate our model, along with other baselines, on several public benchmarks, including Synonym-Hypernym Test and SAKURA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib41" title="">41</a>]</cite>.
These evaluation aim to determine whether these models can genuinely comprehend audio content and utilize it for reasoning about semantic relationships.
In the Synonym-Hypernym Test, our proposed models achieve impressive performance, demonstrating stronger and more robust audio-text alignment compared to other baselines.
The only-positive setting attains an F1 score of 78.98, trailing Qwen-Audio-Chat by only a marginal difference of 1.0 point, while surpassing all other public baseline models by 8% to 31%.
Although Qwen-Audio-Chat achieves comparable performance, our method requires only 12% of their dataset’s total hours and does not involve any modifications to the audio foundation model or the backbone large language model.
</p>
</div>
<div class="ltx_para" id="S6.p5">
<p class="ltx_p" id="S6.p5.1">Furthermore, SAKURA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib41" title="">41</a>]</cite> is a multi-hop reasoning benchmark designed to assess whether a model can first comprehend information within audio and then perform multi-hop reasoning based on that information.
If a model merely memorizes the correspondence between text and audio during training, it may fail to flexibly apply its learned knowledge in reasoning tasks.
Although our models initially achieve lower overall accuracy compared to Qwen2-Audio-Instruct, they exhibit comparable performance in multi-hop question accuracy.
Our analysis suggests that the weaker performance on the SAKURA benchmark is primarily due to lower accuracy in single-hop questions, which in turn affects performance on multi-hop questions.
To address this limitation, we later incorporate multi-audio training, effectively enhancing the model’s performance.</p>
</div>
<div class="ltx_para" id="S6.p6">
<p class="ltx_p" id="S6.p6.1">In audio entailment tasks, models are evaluated on their ability to perform deductive reasoning for audio understanding.
Experimental results show that our proposed models, including both the only-positive and positive and negative settings, outperform other baseline models by at least 10% in audio entailment (Clotho).
While the performance gap in audio entailment (AudioCaps) is smaller, our models still achieve better results compared to other baselines.
In MMAU, compared to the state-of-the-art model on the sound track of MMAU test-mini, Qwen2-Audio-Instruct, our models lag by only 2.7%, demonstrating a comparable level of performance.
Furthermore, across all other comparisons, our models surpass other baselines by at least 3% to 20%.
The results of audio reasoning benchmark indicate that BALSa achieves an audio-language alignment performance comparable to other models.
Notably, our approach does not require constructing an extensive task-specific question-answering dataset, making it a practical and feasible alternative for future training of audio-language alignment capabilities.</p>
</div>
<div class="ltx_para" id="S6.p7">
<p class="ltx_p" id="S6.p7.1">For the evaluation of instruction-following abilities, excluding the proprietary model Gemini-1.5-Pro, our models achieve the highest instruction-following rate and the lowest forgetting rate.
Note that the performance of the baseline models is sourced from <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib12" title="">12</a>]</cite>.
This benchmark reassesses the fundamental aspects of developing ALLMs by separately examining their instruction-following capabilities and their understanding of audio content.
Models with strong audio understanding, such as Qwen2-Audio-Instruct and SALMONN-13B, struggle with even basic instruction-following.
They fail to retain their textual capabilities after audio-language alignment training.
This issue is evident in Qwen2-Audio-Instruct, which achieves an instruction-following rate of only 47.1%, while SALMONN-13B suffers from a forgetting rate as high as 34%.
During training, if chasing audio-language alignment leads to a significant loss of existing abilities, such as following basic instructions, it might reduce the model’s effectiveness in real-world applications.</p>
</div>
<div class="ltx_para" id="S6.p8">
<p class="ltx_p" id="S6.p8.1">In summary, our experimental results demonstrate that our models achieve a balance between audio understanding, reasoning, and fundamental instruction-following abilities.
They effectively combine strong audio understanding capabilities with robust instruction-following performance.</p>
</div>
<figure class="ltx_table" id="S6.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:80%;"><span class="ltx_tag ltx_tag_table">TABLE VI: </span>
Evaluation results of our ablation study on the audio question answering and audio hallucination benchmark.
(Unit: %)
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T6.3">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T6.3.1.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T6.3.1.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.3.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T6.3.1.1.2.1" style="font-size:80%;">EDANSA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.3.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T6.3.1.1.3.1" style="font-size:80%;">ClothoAQA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.3.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T6.3.1.1.4.1" style="font-size:80%;">CochlScene</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T6.3.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T6.3.1.1.5.1" style="font-size:80%;">NonSpeech</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S6.T6.3.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T6.3.1.1.6.1" style="font-size:80%;">Audio Hallucination</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.2.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.2.2.1"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.1.1" style="font-size:80%;">Models</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.2.2.2"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.2.1" style="font-size:80%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.2.2.3"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.3.1" style="font-size:80%;">Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.2.2.4"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.4.1" style="font-size:80%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.2.2.5"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.5.1" style="font-size:80%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.2.2.6"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.6.1" style="font-size:80%;">F1 (Y)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.2.2.7"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.7.1" style="font-size:80%;">F1 (N)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.2.2.8"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.8.1" style="font-size:80%;">F1 (W)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.2.2.9"><span class="ltx_text ltx_font_bold" id="S6.T6.3.2.2.9.1" style="font-size:80%;">Yes</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T6.3.3.3.1"><span class="ltx_text" id="S6.T6.3.3.3.1.1" style="font-size:80%;">Ours (Gemini, only-positive)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.3.3.2"><span class="ltx_text" id="S6.T6.3.3.3.2.1" style="font-size:80%;">38.39</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.3.3.3"><span class="ltx_text" id="S6.T6.3.3.3.3.1" style="font-size:80%;">46.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.3.3.4"><span class="ltx_text" id="S6.T6.3.3.3.4.1" style="font-size:80%;">31.21</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.3.3.5"><span class="ltx_text" id="S6.T6.3.3.3.5.1" style="font-size:80%;">40.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.3.3.6"><span class="ltx_text" id="S6.T6.3.3.3.6.1" style="font-size:80%;">66.58</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.3.3.7"><span class="ltx_text" id="S6.T6.3.3.3.7.1" style="font-size:80%;">3.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.3.3.8"><span class="ltx_text" id="S6.T6.3.3.3.8.1" style="font-size:80%;">34.96</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.3.3.9"><span class="ltx_text" id="S6.T6.3.3.3.9.1" style="font-size:80%;">98.61</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.4.4.1"><span class="ltx_text" id="S6.T6.3.4.4.1.1" style="font-size:80%;">Ours (Gemini, positive-negative)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T6.3.4.4.2"><span class="ltx_text" id="S6.T6.3.4.4.2.1" style="font-size:80%;">16.09</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.4.4.3"><span class="ltx_text" id="S6.T6.3.4.4.3.1" style="font-size:80%;">52.98</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.4.4.4"><span class="ltx_text" id="S6.T6.3.4.4.4.1" style="font-size:80%;">29.26</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.4.4.5"><span class="ltx_text" id="S6.T6.3.4.4.5.1" style="font-size:80%;">18.99</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.4.4.6"><span class="ltx_text" id="S6.T6.3.4.4.6.1" style="font-size:80%;">63.92</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.4.4.7"><span class="ltx_text" id="S6.T6.3.4.4.7.1" style="font-size:80%;">32.90</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.4.4.8"><span class="ltx_text" id="S6.T6.3.4.4.8.1" style="font-size:80%;">48.41</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.4.4.9"><span class="ltx_text" id="S6.T6.3.4.4.9.1" style="font-size:80%;">80.06</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.5.5.1"><span class="ltx_text" id="S6.T6.3.5.5.1.1" style="font-size:80%;">Ours (Gemini, combined)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T6.3.5.5.2"><span class="ltx_text" id="S6.T6.3.5.5.2.1" style="font-size:80%;">48.53</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.5.5.3"><span class="ltx_text" id="S6.T6.3.5.5.3.1" style="font-size:80%;">66.81</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.5.5.4"><span class="ltx_text" id="S6.T6.3.5.5.4.1" style="font-size:80%;">35.68</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.5.5.5"><span class="ltx_text" id="S6.T6.3.5.5.5.1" style="font-size:80%;">26.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.5.5.6"><span class="ltx_text" id="S6.T6.3.5.5.6.1" style="font-size:80%;">58.76</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.5.5.7"><span class="ltx_text" id="S6.T6.3.5.5.7.1" style="font-size:80%;">66.89</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.5.5.8"><span class="ltx_text" id="S6.T6.3.5.5.8.1" style="font-size:80%;">62.83</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.5.5.9"><span class="ltx_text" id="S6.T6.3.5.5.9.1" style="font-size:80%;">39.07</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T6.3.6.6.1"><span class="ltx_text" id="S6.T6.3.6.6.1.1" style="font-size:80%;">Ours (Rule-based Template)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.6.6.2"><span class="ltx_text" id="S6.T6.3.6.6.2.1" style="font-size:80%;">22.40</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.6.6.3"><span class="ltx_text" id="S6.T6.3.6.6.3.1" style="font-size:80%;">38.57</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.6.6.4"><span class="ltx_text" id="S6.T6.3.6.6.4.1" style="font-size:80%;">15.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.6.6.5"><span class="ltx_text" id="S6.T6.3.6.6.5.1" style="font-size:80%;">22.29</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.6.6.6"><span class="ltx_text" id="S6.T6.3.6.6.6.1" style="font-size:80%;">66.14</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.6.6.7"><span class="ltx_text" id="S6.T6.3.6.6.7.1" style="font-size:80%;">1.24</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.6.6.8"><span class="ltx_text" id="S6.T6.3.6.6.8.1" style="font-size:80%;">33.69</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.6.6.9"><span class="ltx_text" id="S6.T6.3.6.6.9.1" style="font-size:80%;">77.44</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.7.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.7.7.1"><span class="ltx_text" id="S6.T6.3.7.7.1.1" style="font-size:80%;">Ours (OpenAQA)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T6.3.7.7.2"><span class="ltx_text" id="S6.T6.3.7.7.2.1" style="font-size:80%;">18.76</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.7.7.3"><span class="ltx_text" id="S6.T6.3.7.7.3.1" style="font-size:80%;">68.70</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.7.7.4"><span class="ltx_text" id="S6.T6.3.7.7.4.1" style="font-size:80%;">33.42</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.7.7.5"><span class="ltx_text" id="S6.T6.3.7.7.5.1" style="font-size:80%;">46.39</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.7.7.6"><span class="ltx_text" id="S6.T6.3.7.7.6.1" style="font-size:80%;">78.89</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.7.7.7"><span class="ltx_text" id="S6.T6.3.7.7.7.1" style="font-size:80%;">71.90</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.7.7.8"><span class="ltx_text" id="S6.T6.3.7.7.8.1" style="font-size:80%;">75.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.7.7.9"><span class="ltx_text" id="S6.T6.3.7.7.9.1" style="font-size:80%;">63.88</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.8.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.8.8.1"><span class="ltx_text" id="S6.T6.3.8.8.1.1" style="font-size:80%;">Ours (BALSa-OpenAQA)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T6.3.8.8.2"><span class="ltx_text" id="S6.T6.3.8.8.2.1" style="font-size:80%;">45.77</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.8.8.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T6.3.8.8.3.1" style="font-size:80%;">78.31</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.8.8.4"><span class="ltx_text" id="S6.T6.3.8.8.4.1" style="font-size:80%;">41.69</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.8.8.5"><span class="ltx_text" id="S6.T6.3.8.8.5.1" style="font-size:80%;">56.22</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.8.8.6"><span class="ltx_text" id="S6.T6.3.8.8.6.1" style="font-size:80%;">78.67</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.8.8.7"><span class="ltx_text" id="S6.T6.3.8.8.7.1" style="font-size:80%;">77.65</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.8.8.8"><span class="ltx_text" id="S6.T6.3.8.8.8.1" style="font-size:80%;">78.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.8.8.9"><span class="ltx_text" id="S6.T6.3.8.8.9.1" style="font-size:80%;">52.33</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.9.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.9.9.1"><span class="ltx_text" id="S6.T6.3.9.9.1.1" style="font-size:80%;color:#808080;">Ours (BALSa, positive-negative)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T6.3.9.9.2"><span class="ltx_text" id="S6.T6.3.9.9.2.1" style="font-size:80%;">42.06</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.9.9.3"><span class="ltx_text" id="S6.T6.3.9.9.3.1" style="font-size:80%;">74.96</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.9.9.4"><span class="ltx_text" id="S6.T6.3.9.9.4.1" style="font-size:80%;">42.87</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.9.9.5"><span class="ltx_text" id="S6.T6.3.9.9.5.1" style="font-size:80%;">53.66</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.9.9.6"><span class="ltx_text" id="S6.T6.3.9.9.6.1" style="font-size:80%;">80.18</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.9.9.7"><span class="ltx_text" id="S6.T6.3.9.9.7.1" style="font-size:80%;">74.04</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.9.9.8"><span class="ltx_text" id="S6.T6.3.9.9.8.1" style="font-size:80%;">77.1</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.9.9.9"><span class="ltx_text" id="S6.T6.3.9.9.9.1" style="font-size:80%;">63.69</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.10.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T6.3.10.10.1"><span class="ltx_text" id="S6.T6.3.10.10.1.1" style="font-size:80%;color:#808080;">Ours (BALSa-MA, discrimination)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.10.10.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T6.3.10.10.2.1" style="font-size:80%;">50.97</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.10.10.3"><span class="ltx_text" id="S6.T6.3.10.10.3.1" style="font-size:80%;">77.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.10.10.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T6.3.10.10.4.1" style="font-size:80%;">44.77</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.10.10.5"><span class="ltx_text" id="S6.T6.3.10.10.5.1" style="font-size:80%;">68.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.10.10.6"><span class="ltx_text ltx_font_bold" id="S6.T6.3.10.10.6.1" style="font-size:80%;">85.35</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.10.10.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T6.3.10.10.7.1" style="font-size:80%;">84.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.10.10.8"><span class="ltx_text ltx_font_bold" id="S6.T6.3.10.10.8.1" style="font-size:80%;">85.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T6.3.10.10.9"><span class="ltx_text" id="S6.T6.3.10.10.9.1" style="font-size:80%;">52.29</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.11.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T6.3.11.11.1"><span class="ltx_text" id="S6.T6.3.11.11.1.1" style="font-size:80%;color:#808080;">Ours (BALSa-MA, captioning)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T6.3.11.11.2"><span class="ltx_text ltx_font_bold" id="S6.T6.3.11.11.2.1" style="font-size:80%;">53.80</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.11.11.3"><span class="ltx_text ltx_font_bold" id="S6.T6.3.11.11.3.1" style="font-size:80%;">78.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.11.11.4"><span class="ltx_text" id="S6.T6.3.11.11.4.1" style="font-size:80%;">44.47</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.11.11.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T6.3.11.11.5.1" style="font-size:80%;">70.03</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.11.11.6"><span class="ltx_text" id="S6.T6.3.11.11.6.1" style="font-size:80%;">84.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.11.11.7"><span class="ltx_text ltx_font_bold" id="S6.T6.3.11.11.7.1" style="font-size:80%;">84.93</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.11.11.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T6.3.11.11.8.1" style="font-size:80%;">84.47</span></td>
<td class="ltx_td ltx_align_center" id="S6.T6.3.11.11.9"><span class="ltx_text" id="S6.T6.3.11.11.9.1" style="font-size:80%;">47.01</span></td>
</tr>
<tr class="ltx_tr" id="S6.T6.3.12.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T6.3.12.12.1"><span class="ltx_text" id="S6.T6.3.12.12.1.1" style="font-size:80%;">Ours (BALSa-MA, joint)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.12.12.2"><span class="ltx_text" id="S6.T6.3.12.12.2.1" style="font-size:80%;">48.02</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.12.12.3"><span class="ltx_text" id="S6.T6.3.12.12.3.1" style="font-size:80%;">74.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.12.12.4"><span class="ltx_text ltx_font_bold" id="S6.T6.3.12.12.4.1" style="font-size:80%;">46.03</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.12.12.5"><span class="ltx_text ltx_font_bold" id="S6.T6.3.12.12.5.1" style="font-size:80%;">70.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.12.12.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T6.3.12.12.6.1" style="font-size:80%;">84.19</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.12.12.7"><span class="ltx_text" id="S6.T6.3.12.12.7.1" style="font-size:80%;">83.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.12.12.8"><span class="ltx_text" id="S6.T6.3.12.12.8.1" style="font-size:80%;">83.98</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T6.3.12.12.9"><span class="ltx_text" id="S6.T6.3.12.12.9.1" style="font-size:80%;">51.27</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="S6.T7">
<figcaption class="ltx_caption ltx_centering" style="font-size:70%;"><span class="ltx_tag ltx_tag_table">TABLE VII: </span>
Evaluation results of our ablation study on the audio reasoning and instruction-following benchmark.
(Unit: %)
</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S6.T7.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S6.T7.1.2.1">
<th class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S6.T7.1.2.1.1"></th>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T7.1.2.1.2"><span class="ltx_text ltx_font_bold" id="S6.T7.1.2.1.2.1" style="font-size:70%;">Synonym-Hypernym</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="3" id="S6.T7.1.2.1.3"><span class="ltx_text ltx_font_bold" id="S6.T7.1.2.1.3.1" style="font-size:70%;">SAKURA</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S6.T7.1.2.1.4"><span class="ltx_text ltx_font_bold" id="S6.T7.1.2.1.4.1" style="font-size:70%;">MMAU</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="2" id="S6.T7.1.2.1.5"><span class="ltx_text ltx_font_bold" id="S6.T7.1.2.1.5.1" style="font-size:70%;">Audio Entailment</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" colspan="4" id="S6.T7.1.2.1.6"><span class="ltx_text ltx_font_bold" id="S6.T7.1.2.1.6.1" style="font-size:70%;">Speech-IFEval</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.1.2"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.2.1" style="font-size:70%;">Models</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.3"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.3.1" style="font-size:70%;">P</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.4"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.4.1" style="font-size:70%;">R</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.5"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.5.1" style="font-size:70%;">F1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.6"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.6.1" style="font-size:70%;">Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.7"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.7.1" style="font-size:70%;">S-Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.8"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.8.1" style="font-size:70%;">M-Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.9"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.9.1" style="font-size:70%;">Acc</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.10"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.10.1" style="font-size:70%;">Acc (C)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.11"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.11.1" style="font-size:70%;">Acc (A)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.12"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.12.1" style="font-size:70%;">Close</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.13"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.13.1" style="font-size:70%;">Open</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.14"><span class="ltx_text ltx_font_bold" id="S6.T7.1.1.14.1" style="font-size:70%;">IFRate</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="S6.T7.1.1.1.m1.1"><semantics id="S6.T7.1.1.1.m1.1a"><mi id="S6.T7.1.1.1.m1.1.1" mathsize="70%" mathvariant="normal" xref="S6.T7.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S6.T7.1.1.1.m1.1b"><ci id="S6.T7.1.1.1.m1.1.1.cmml" xref="S6.T7.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.T7.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S6.T7.1.1.1.m1.1d">roman_Δ</annotation></semantics></math></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.3.2.1"><span class="ltx_text" id="S6.T7.1.3.2.1.1" style="font-size:70%;">Ours (Gemini, only-positive)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.2"><span class="ltx_text" id="S6.T7.1.3.2.2.1" style="font-size:70%;">31.94</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.3"><span class="ltx_text" id="S6.T7.1.3.2.3.1" style="font-size:70%;">36.32</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.4"><span class="ltx_text" id="S6.T7.1.3.2.4.1" style="font-size:70%;">32.88</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.5"><span class="ltx_text" id="S6.T7.1.3.2.5.1" style="font-size:70%;">47.10</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.6"><span class="ltx_text" id="S6.T7.1.3.2.6.1" style="font-size:70%;">54.60</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.7"><span class="ltx_text" id="S6.T7.1.3.2.7.1" style="font-size:70%;">40.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.8"><span class="ltx_text" id="S6.T7.1.3.2.8.1" style="font-size:70%;">45.65</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.9"><span class="ltx_text" id="S6.T7.1.3.2.9.1" style="font-size:70%;">35.82</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.10"><span class="ltx_text" id="S6.T7.1.3.2.10.1" style="font-size:70%;">33.91</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.11"><span class="ltx_text" id="S6.T7.1.3.2.11.1" style="font-size:70%;">52.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.12"><span class="ltx_text" id="S6.T7.1.3.2.12.1" style="font-size:70%;">63.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.13"><span class="ltx_text" id="S6.T7.1.3.2.13.1" style="font-size:70%;">57.87</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.3.2.14"><span class="ltx_text" id="S6.T7.1.3.2.14.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.4.3.1"><span class="ltx_text" id="S6.T7.1.4.3.1.1" style="font-size:70%;">Ours (Gemini, positive-negative)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.2"><span class="ltx_text" id="S6.T7.1.4.3.2.1" style="font-size:70%;">48.67</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.3"><span class="ltx_text" id="S6.T7.1.4.3.3.1" style="font-size:70%;">48.59</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.4"><span class="ltx_text" id="S6.T7.1.4.3.4.1" style="font-size:70%;">48.23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.5"><span class="ltx_text" id="S6.T7.1.4.3.5.1" style="font-size:70%;">43.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.6"><span class="ltx_text" id="S6.T7.1.4.3.6.1" style="font-size:70%;">48.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.7"><span class="ltx_text" id="S6.T7.1.4.3.7.1" style="font-size:70%;">38.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.8"><span class="ltx_text" id="S6.T7.1.4.3.8.1" style="font-size:70%;">39.64</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.9"><span class="ltx_text" id="S6.T7.1.4.3.9.1" style="font-size:70%;">35.02</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.10"><span class="ltx_text" id="S6.T7.1.4.3.10.1" style="font-size:70%;">23.92</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.11"><span class="ltx_text" id="S6.T7.1.4.3.11.1" style="font-size:70%;">77.92</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.12"><span class="ltx_text" id="S6.T7.1.4.3.12.1" style="font-size:70%;">77.50</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.13"><span class="ltx_text" id="S6.T7.1.4.3.13.1" style="font-size:70%;">77.71</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.4.3.14"><span class="ltx_text" id="S6.T7.1.4.3.14.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.5.4.1"><span class="ltx_text" id="S6.T7.1.5.4.1.1" style="font-size:70%;">Ours (Gemini, combined)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.2"><span class="ltx_text" id="S6.T7.1.5.4.2.1" style="font-size:70%;">48.72</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.3"><span class="ltx_text" id="S6.T7.1.5.4.3.1" style="font-size:70%;">47.64</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.4"><span class="ltx_text" id="S6.T7.1.5.4.4.1" style="font-size:70%;">47.93</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.5"><span class="ltx_text" id="S6.T7.1.5.4.5.1" style="font-size:70%;">49.90</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.6"><span class="ltx_text" id="S6.T7.1.5.4.6.1" style="font-size:70%;">49.80</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.7"><span class="ltx_text" id="S6.T7.1.5.4.7.1" style="font-size:70%;">51.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.8"><span class="ltx_text" id="S6.T7.1.5.4.8.1" style="font-size:70%;">51.65</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.9"><span class="ltx_text" id="S6.T7.1.5.4.9.1" style="font-size:70%;">32.44</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.10"><span class="ltx_text" id="S6.T7.1.5.4.10.1" style="font-size:70%;">32.25</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.11"><span class="ltx_text" id="S6.T7.1.5.4.11.1" style="font-size:70%;">88.85</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.12"><span class="ltx_text" id="S6.T7.1.5.4.12.1" style="font-size:70%;">75.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.13"><span class="ltx_text" id="S6.T7.1.5.4.13.1" style="font-size:70%;">82.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.5.4.14"><span class="ltx_text" id="S6.T7.1.5.4.14.1" style="font-size:70%;">-</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.6.5.1"><span class="ltx_text" id="S6.T7.1.6.5.1.1" style="font-size:70%;">Ours (Rule-based Template)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.2"><span class="ltx_text" id="S6.T7.1.6.5.2.1" style="font-size:70%;">16.26</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.3"><span class="ltx_text" id="S6.T7.1.6.5.3.1" style="font-size:70%;">23.11</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.4"><span class="ltx_text" id="S6.T7.1.6.5.4.1" style="font-size:70%;">18.78</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.5"><span class="ltx_text" id="S6.T7.1.6.5.5.1" style="font-size:70%;">31.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.6"><span class="ltx_text" id="S6.T7.1.6.5.6.1" style="font-size:70%;">54.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.7"><span class="ltx_text" id="S6.T7.1.6.5.7.1" style="font-size:70%;">7.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.8"><span class="ltx_text" id="S6.T7.1.6.5.8.1" style="font-size:70%;">10.51</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.9"><span class="ltx_text" id="S6.T7.1.6.5.9.1" style="font-size:70%;">3.70</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.10"><span class="ltx_text" id="S6.T7.1.6.5.10.1" style="font-size:70%;">0.73</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.11"><span class="ltx_text" id="S6.T7.1.6.5.11.1" style="font-size:70%;">0.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.12"><span class="ltx_text" id="S6.T7.1.6.5.12.1" style="font-size:70%;">29.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.13"><span class="ltx_text" id="S6.T7.1.6.5.13.1" style="font-size:70%;">14.63</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.6.5.14"><span class="ltx_text" id="S6.T7.1.6.5.14.1" style="font-size:70%;">-83.93</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.7.6.1"><span class="ltx_text" id="S6.T7.1.7.6.1.1" style="font-size:70%;">Ours (OpenAQA)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.2"><span class="ltx_text" id="S6.T7.1.7.6.2.1" style="font-size:70%;">66.21</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.3"><span class="ltx_text" id="S6.T7.1.7.6.3.1" style="font-size:70%;">66.98</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.4"><span class="ltx_text" id="S6.T7.1.7.6.4.1" style="font-size:70%;">65.99</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.5"><span class="ltx_text" id="S6.T7.1.7.6.5.1" style="font-size:70%;">33.70</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.6"><span class="ltx_text" id="S6.T7.1.7.6.6.1" style="font-size:70%;">34.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.7"><span class="ltx_text" id="S6.T7.1.7.6.7.1" style="font-size:70%;">39.41</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.8"><span class="ltx_text" id="S6.T7.1.7.6.8.1" style="font-size:70%;">51.05</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.9"><span class="ltx_text" id="S6.T7.1.7.6.9.1" style="font-size:70%;">45.74</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.10"><span class="ltx_text" id="S6.T7.1.7.6.10.1" style="font-size:70%;">44.11</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.11"><span class="ltx_text" id="S6.T7.1.7.6.11.1" style="font-size:70%;">57.88</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.12"><span class="ltx_text" id="S6.T7.1.7.6.12.1" style="font-size:70%;">65.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.13"><span class="ltx_text" id="S6.T7.1.7.6.13.1" style="font-size:70%;">61.81</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.7.6.14"><span class="ltx_text" id="S6.T7.1.7.6.14.1" style="font-size:70%;">-32.10</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.8.7.1"><span class="ltx_text" id="S6.T7.1.8.7.1.1" style="font-size:70%;">Ours (BALSa-OpenAQA)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.2"><span class="ltx_text" id="S6.T7.1.8.7.2.1" style="font-size:70%;">74.42</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.3"><span class="ltx_text" id="S6.T7.1.8.7.3.1" style="font-size:70%;">73.74</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.4"><span class="ltx_text" id="S6.T7.1.8.7.4.1" style="font-size:70%;">73.92</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.5"><span class="ltx_text" id="S6.T7.1.8.7.5.1" style="font-size:70%;">64.10</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.6"><span class="ltx_text" id="S6.T7.1.8.7.6.1" style="font-size:70%;">67.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.7"><span class="ltx_text" id="S6.T7.1.8.7.7.1" style="font-size:70%;">66.47</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.8"><span class="ltx_text" id="S6.T7.1.8.7.8.1" style="font-size:70%;">53.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.9"><span class="ltx_text" id="S6.T7.1.8.7.9.1" style="font-size:70%;">54.23</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.10"><span class="ltx_text" id="S6.T7.1.8.7.10.1" style="font-size:70%;">56.14</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.11"><span class="ltx_text" id="S6.T7.1.8.7.11.1" style="font-size:70%;">88.75</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.12"><span class="ltx_text" id="S6.T7.1.8.7.12.1" style="font-size:70%;">90.00</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.13"><span class="ltx_text" id="S6.T7.1.8.7.13.1" style="font-size:70%;">89.37</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.8.7.14"><span class="ltx_text" id="S6.T7.1.8.7.14.1" style="font-size:70%;">-1.82</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.9.8.1"><span class="ltx_text" id="S6.T7.1.9.8.1.1" style="font-size:70%;color:#808080;">Ours (BALSa, positive-negative)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.2"><span class="ltx_text" id="S6.T7.1.9.8.2.1" style="font-size:70%;">77.94</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.3"><span class="ltx_text" id="S6.T7.1.9.8.3.1" style="font-size:70%;">77.04</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.4"><span class="ltx_text" id="S6.T7.1.9.8.4.1" style="font-size:70%;">77.25</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.5"><span class="ltx_text" id="S6.T7.1.9.8.5.1" style="font-size:70%;">70.70</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.6"><span class="ltx_text" id="S6.T7.1.9.8.6.1" style="font-size:70%;">76.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.7"><span class="ltx_text" id="S6.T7.1.9.8.7.1" style="font-size:70%;">65.71</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.8"><span class="ltx_text" id="S6.T7.1.9.8.8.1" style="font-size:70%;">55.26</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.9.8.9.1" style="font-size:70%;">56.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.10"><span class="ltx_text ltx_font_bold" id="S6.T7.1.9.8.10.1" style="font-size:70%;">57.16</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.11"><span class="ltx_text" id="S6.T7.1.9.8.11.1" style="font-size:70%;">90.78</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.12"><span class="ltx_text ltx_font_bold" id="S6.T7.1.9.8.12.1" style="font-size:70%;">91.25</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.13"><span class="ltx_text ltx_font_bold" id="S6.T7.1.9.8.13.1" style="font-size:70%;">91.02</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.9.8.14"><span class="ltx_text" id="S6.T7.1.9.8.14.1" style="font-size:70%;">-0.02</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S6.T7.1.10.9.1"><span class="ltx_text" id="S6.T7.1.10.9.1.1" style="font-size:70%;color:#808080;">Ours (BALSa-MA, discrimination)</span></th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.10.9.2.1" style="font-size:70%;">81.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.10.9.3.1" style="font-size:70%;">80.50</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.10.9.4.1" style="font-size:70%;">80.55</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.10.9.5.1" style="font-size:70%;">79.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.10.9.6.1" style="font-size:70%;">86.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.7"><span class="ltx_text ltx_font_bold" id="S6.T7.1.10.9.7.1" style="font-size:70%;">73.49</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.10.9.8.1" style="font-size:70%;">56.76</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.9"><span class="ltx_text" id="S6.T7.1.10.9.9.1" style="font-size:70%;">54.67</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.10"><span class="ltx_text" id="S6.T7.1.10.9.10.1" style="font-size:70%;">55.66</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.11"><span class="ltx_text" id="S6.T7.1.10.9.11.1" style="font-size:70%;">88.85</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.12"><span class="ltx_text" id="S6.T7.1.10.9.12.1" style="font-size:70%;">87.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.13"><span class="ltx_text" id="S6.T7.1.10.9.13.1" style="font-size:70%;">88.05</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S6.T7.1.10.9.14"><span class="ltx_text" id="S6.T7.1.10.9.14.1" style="font-size:70%;">-3.27</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S6.T7.1.11.10.1"><span class="ltx_text" id="S6.T7.1.11.10.1.1" style="font-size:70%;color:#808080;">Ours (BALSa-MA, captioning)</span></th>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.2"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.10.2.1" style="font-size:70%;">82.99</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.3"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.10.3.1" style="font-size:70%;">82.86</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.4"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.10.4.1" style="font-size:70%;">82.89</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.5"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.10.5.1" style="font-size:70%;">80.30</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.6"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.10.6.1" style="font-size:70%;">88.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.7"><span class="ltx_text" id="S6.T7.1.11.10.7.1" style="font-size:70%;">71.72</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.8"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.10.8.1" style="font-size:70%;">57.06</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.9"><span class="ltx_text ltx_font_bold" id="S6.T7.1.11.10.9.1" style="font-size:70%;">56.40</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.11.10.10.1" style="font-size:70%;">56.59</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.11"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.11.10.11.1" style="font-size:70%;">89.07</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.12"><span class="ltx_text" id="S6.T7.1.11.10.12.1" style="font-size:70%;">84.25</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.13"><span class="ltx_text" id="S6.T7.1.11.10.13.1" style="font-size:70%;">86.66</span></td>
<td class="ltx_td ltx_align_center" id="S6.T7.1.11.10.14"><span class="ltx_text" id="S6.T7.1.11.10.14.1" style="font-size:70%;">-4.80</span></td>
</tr>
<tr class="ltx_tr" id="S6.T7.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S6.T7.1.12.11.1"><span class="ltx_text" id="S6.T7.1.12.11.1.1" style="font-size:70%;">Ours (BALSa-MA, joint)</span></th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.2"><span class="ltx_text" id="S6.T7.1.12.11.2.1" style="font-size:70%;">78.43</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.3"><span class="ltx_text" id="S6.T7.1.12.11.3.1" style="font-size:70%;">78.46</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.4"><span class="ltx_text" id="S6.T7.1.12.11.4.1" style="font-size:70%;">78.28</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.5"><span class="ltx_text" id="S6.T7.1.12.11.5.1" style="font-size:70%;">79.30</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.12.11.6.1" style="font-size:70%;">86.00</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.12.11.7.1" style="font-size:70%;">72.79</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.8"><span class="ltx_text" id="S6.T7.1.12.11.8.1" style="font-size:70%;">56.16</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.9"><span class="ltx_text" id="S6.T7.1.12.11.9.1" style="font-size:70%;">48.92</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.10"><span class="ltx_text" id="S6.T7.1.12.11.10.1" style="font-size:70%;">54.80</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.11"><span class="ltx_text ltx_font_bold" id="S6.T7.1.12.11.11.1" style="font-size:70%;">90.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.12"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.12.11.12.1" style="font-size:70%;">88.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.13"><span class="ltx_text ltx_framed ltx_framed_underline" id="S6.T7.1.12.11.13.1" style="font-size:70%;">89.25</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S6.T7.1.12.11.14"><span class="ltx_text" id="S6.T7.1.12.11.14.1" style="font-size:70%;">-1.96</span></td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S6.p9">
<p class="ltx_p" id="S6.p9.1">While our models demonstrate a balanced integration of audio understanding, reasoning, and instruction-following abilities, it is essential to analyze the factors that contribute to this performance.
To further investigate the role of different components, we conduct an ablation study to examine their impact on model effectiveness.
First, we replace the original backbone LLM with a stronger LLM like Gemini <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib26" title="">26</a>]</cite>, while applying the same BALSa pipeline to generate training data.
As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S6.T6" title="TABLE VI ‣ VI Result ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">VI</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S6.T7" title="TABLE VII ‣ VI Result ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">VII</span></a>, we observe that across all evaluation benchmarks, performance significantly declines regardless of whether the setting is positive-only, positive-negative, or combined.
This result highlights the importance of using the original backbone LLM for generating audio-language alignment training data.</p>
</div>
<div class="ltx_para" id="S6.p10">
<p class="ltx_p" id="S6.p10.1">Second, we explore an alternative approach by providing the model with a set of pre-collected questions source from OpenAQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>]</cite> and generating corresponding answers using the BALSa pipeline.
We refer to this variant as BALSa-OpenAQA.
Experimental results indicate that BALSa-OpenAQA, in most cases, perform slightly worse than those trained with BALSa-generated positive and negative samples.
However, the performance gap is not substantial overall.
On the other hand, if we directly train the model using the original question-answer pairs provided in OpenAQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>]</cite>, rather than generating answers through the BALSa approach, the experimental results show that models trained with BALSa-OpenAQA consistently outperform those trained directly on the original OpenAQA dataset across all evaluation benchmarks.
This finding further reinforces the importance of using the original backbone LLM for generating training data, highlighting its crucial role in enhancing model performance.
Third, we explore an approach that utilizes pre-defined rule-based templates to construct paired training data.
However, models trained with this method not only perform poorly across evaluation benchmarks but also exhibit a parroting effect, where responses frequently fail to follow instructions and instead generate only audio captions.</p>
</div>
<div class="ltx_para" id="S6.p11">
<p class="ltx_p" id="S6.p11.1">Based on the experimental results, we report the following findings and observations:
The BALSa framework enables the generation of audio-language alignment training data using only simple generation prompts, such as basic captioning-based instructions like <span class="ltx_text ltx_font_italic" id="S6.p11.1.1">Repeat the audio</span>.
If additional pre-collected question-answer data like OpenAQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>]</cite> is available, it can also be processed through the BALSa pipeline to generate training data, with the pre-collected questions serving to increase the diversity of generation prompts.
However, this approach requires additional data collection and preparation efforts.
The results highlight that the key factor in the BALSa framework is the use of the backbone LLM for data generation.
Furthermore, BALSa can be extended to multi-audio scenarios, leveraging explanations between two audio inputs to enhance the model’s audio understanding capabilities.</p>
</div>
<div class="ltx_para" id="S6.p12">
<p class="ltx_p" id="S6.p12.1">Building on this, we extend BALSa to the multi-audio setting.
Experimental results indicate that this extension leads to performance improvements across multiple evaluation benchmarks.
By comparing the positive-negative setting in the single-audio scenario with the discrimination setting in the multi-audio scenario, we observe an 8% improvement in audio hallucination performance, representing an approximately 10% increase.
On the fundamental audio question-answering benchmarks, our method achieves performance improvements of 8% and 15% on the EDANSA and NonSpeech tasks, respectively.
On SAKURA, we observe a nearly 10% increase in overall accuracy, along with an 8% improvement specifically on multi-hop questions.
In contrast, improvements in Audio Entailment are relatively smaller.
Overall, we find that BALSa-MA, which enables the model to learn from two audio inputs simultaneously, enhances its basic audio understanding abilities, thereby benefiting more advanced reasoning tasks.
</p>
</div>
<div class="ltx_para" id="S6.p13">
<p class="ltx_p" id="S6.p13.1">Following these findings, we investigate whether learning through discrimination-based training—where the model explains the differences between two audio inputs—or learning through complete description—where the model captions both audio inputs—is sufficient for improving performance.
As presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S6.T6" title="TABLE VI ‣ VI Result ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">VI</span></a> and Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#S6.T7" title="TABLE VII ‣ VI Result ‣ From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data"><span class="ltx_text ltx_ref_tag">VII</span></a>, we observe that the performance difference between Both Captions Only (BALSa-MA, captioning) and Comparison Only (BALSa-MA, discrimination) is minimal.
These findings suggest that simply learning to process two audio inputs simultaneously is sufficient to enhance the model’s overall understanding, without requiring explicit comparative training.
Furthermore, we experimented with a mixed dataset, combining Both Captions Only and Discrimination-Only data in equal proportions.
However, the results indicate that this hybrid approach does not provide any significant benefits.
Therefore, within the BALSa-MA framework, enabling the model to jointly learn from two audio inputs is sufficient to improve its audio understanding and reasoning capabilities.</p>
</div>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Prior work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib1" title="">1</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib3" title="">3</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib4" title="">4</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib5" title="">5</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20166v1#bib.bib7" title="">7</a>]</cite> has primarily focused on achieving audio-language alignment by collecting and constructing large-scale, task-specific question-answer datasets for model training.
In contrast, we adopt a different strategy in BALSa by leveraging backbone LLMs to generate general captioning-based alignment data.
Experimental results demonstrate that BALSa achieves comparable performance to previous methods and even outperforms them on certain benchmarks, while also maintaining a balance with fundamental instruction-following abilities.
It effectively combines strong audio understanding capabilities with robust instruction-following performance, and shows reliable behavior in mitigating audio hallucinations, highlighting its trustworthiness in practical scenarios.
</p>
</div>
<div class="ltx_para" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">In audio-language alignment, the most fundamental objective is to train models to understand audio content.
We achieve this through a captioning-based alignment strategy.
Meanwhile, reasoning ability is inherently provided by the backbone LLM, as we do not modify any of its parameters.
The motivation behind this approach is that, given a sufficiently capable text-based LLM as the foundation, it is only necessary to train a modality adaptation module to enable the model to understand general audio inputs without modifying the parameters of the original language model.
Such a design also reduces computational demands, allowing the LLM to process audio information while preserving its extensive pre-trained textual and instruction-tuned capabilities.
On the other hand, our experimental results further suggest that training audio understanding capabilities can also be achieved using question-answering pair data.
However, the key factor remains that the backbone LLM must be used to generate corresponding answers, ensuring effective audio-language alignment.
</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VIII </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">In this work, we introduce BALSa, a framework that uses backbone LLMs to generate synthetic data for alignment, reducing the reliance on large collections of task-specific QA datasets.
By leveraging a captioning-based alignment strategy, BALSa achieves strong audio understanding and reasoning while preserving the instruction-following capabilities of large language models.
Our experimental results demonstrate that BALSa achieves comparable or superior performance to previous approaches while improving data and computational efficiency.
These findings also highlights the importance of utilizing the original backbone LLM for generating audio-language alignment data.
In addition, by incorporating LISTEN (Learning to Identify Sounds Through Extended Negative Samples), BALSa effectively mitigates audio hallucinations, further reinforcing its robustness in real-world applications.</p>
</div>
<div class="ltx_para" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">Beyond single-audio alignment, we extend BALSa to more complex audio understanding settings.
In multi-audio scenarios, this extension further enhances modality alignment and leads to better performance on audio understanding and reasoning tasks.
We further examine the role of discrimination-based training and observe that learning to handle multiple audio inputs alone is sufficient to improve overall understanding, without requiring explicit comparative objectives.
Overall, BALSa provides a scalable and efficient pathway for developing and advancing audio-aware large language models.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
Y. Chu, J. Xu, X. Zhou, Q. Yang <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">et al.</em>, “Qwen-audio: Advancing universal audio understanding via unified large-scale audio-language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.2.2">arXiv preprint arXiv:2311.07919</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
Y. Chu, J. Xu, Q. Yang, H. Wei <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “Qwen2-audio technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">arXiv preprint arXiv:2407.10759</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
C. Tang <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">et al.</em>, “Salmonn: Towards generic hearing abilities for large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.2.2">arXiv preprint arXiv:2310.13289</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
Y. Gong <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">et al.</em>, “Listen, think, and understand,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">arXiv preprint arXiv:2305.10790</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
Y. Gong, A. H. Liu, H. Luo, L. Karlinsky, and J. Glass, “Joint audio and speech understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)</em>.   IEEE, 2023, pp. 1–8.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
S. Ghosh, S. Kumar, A. Seth, C. K. R. Evuru <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">et al.</em>, “Gama: A large audio-language model with advanced audio understanding and complex reasoning abilities,” in <em class="ltx_emph ltx_font_italic" id="bib.bib6.2.2">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, 2024, pp. 6288–6313.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
Z. Kong, A. Goel, R. Badlani, W. Ping, R. Valle, and B. Catanzaro, “Audio flamingo: a novel audio language model with few-shot learning and dialogue abilities,” in <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 41st International Conference on Machine Learning</em>, 2024, pp. 25 125–25 148.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
S. Deshmukh, B. Elizalde, R. Singh, and H. Wang, “Pengi: An audio language model for audio tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Advances in Neural Information Processing Systems</em>, vol. 36, pp. 18 090–18 108, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C.-Y. Kuan, C.-K. Yang, W.-P. Huang, K.-H. Lu, and H.-y. Lee, “Speech-copilot: Leveraging large language models for speech processing via task decomposition, modularization, and program generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">arXiv preprint arXiv:2407.09886</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
C.-K. Yang, N. S. Ho, and H.-y. Lee, “Towards holistic evaluation of large audio-language models: A comprehensive survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2505.15957</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
J. Achiam <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">et al.</em>, “Gpt-4 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.2.2">arXiv preprint arXiv:2303.08774</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
K.-H. Lu, C.-Y. Kuan, and H. yi Lee, “Speech-ifeval: Evaluating instruction-following and quantifying catastrophic forgetting in speech-aware language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Interspeech 2025</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
C.-Y. Kuan, W.-P. Huang, and H.-y. Lee, “Understanding sounds, missing the questions: The challenge of object hallucination in large audio-language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">arXiv preprint arXiv:2406.08402</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
Y. Fathullah <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">et al.</em>, “Towards general-purpose speech abilities for large language models using unpaired data,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">arXiv preprint arXiv:2311.06753</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
K.-H. Lu <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>, “Desta: Enhancing speech language models through descriptive speech-text alignment,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">arXiv preprint arXiv:2406.18871</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
C. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">et al.</em>, “Blsp: Bootstrapping language-speech pre-training via behavior alignment of continuation writing,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">arXiv preprint arXiv:2309.00916</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
C. Wang, M. Liao, Z. Huang, J. Wu, C. Zong, and J. Zhang, “Blsp-emo: Towards empathetic large speech-language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, 2024, pp. 19 186–19 199.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
W. Held, E. Li, M. Ryan, W. Shi, Y. Zhang, and D. Yang, “Distilling an end-to-end voice assistant without instruction training data,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2410.02678</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
C.-Y. Kuan and H.-y. Lee, “Teaching audio-aware large language models what does not hear: Mitigating hallucinations through synthesized negative samples,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2505.14518</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
H. Touvron <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">et al.</em>, “Llama 2: Open foundation and fine-tuned chat models,” <em class="ltx_emph ltx_font_italic" id="bib.bib20.2.2">arXiv preprint arXiv:2307.09288</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
A. Dubey <em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">et al.</em>, “The llama 3 herd of models,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.2.2">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
B. Peng, C. Li, P. He, M. Galley, and J. Gao, “Instruction tuning with gpt-4,” <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">arXiv preprint arXiv:2304.03277</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
A. Yang, B. Yang, B. Zhang, B. Hui, B. Zheng, B. Yu, C. Li, D. Liu, F. Huang, H. Wei <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">et al.</em>, “Qwen2. 5 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib23.2.2">arXiv preprint arXiv:2412.15115</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min, B. Zhang, J. Zhang, Z. Dong <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">et al.</em>, “A survey of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.2.2">arXiv preprint arXiv:2303.18223</em>, vol. 1, no. 2, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">et al.</em>, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib25.2.2">Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
G. Team, P. Georgiev, V. I. Lei <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">et al.</em>, “Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.2.2">arXiv preprint arXiv:2403.05530</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
K.-H. Lu <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">et al.</em>, “Developing instruction-following speech language model without speech instruction-tuning data,” in <em class="ltx_emph ltx_font_italic" id="bib.bib27.2.2">ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
A. Radford <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">et al.</em>, “Robust speech recognition via large-scale weak supervision,” in <em class="ltx_emph ltx_font_italic" id="bib.bib28.2.2">International conference on machine learning</em>.   PMLR, 2023, pp. 28 492–28 518.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Y. Gong, S. Khurana, L. Karlinsky, and J. Glass, “Whisper-at: Noise-robust automatic speech recognizers are also strong audio event taggers,” in <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proc. Interspeech 2023</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
J. Li <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">et al.</em>, “Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib30.2.2">International conference on machine learning</em>.   PMLR, 2023, pp. 19 730–19 742.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
D. Zhu <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">et al.</em>, “Minigpt-4: Enhancing vision-language understanding with advanced large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib31.2.2">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
J. F. Gemmeke, D. P. W. Ellis, D. Freedman, A. Jansen <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">et al.</em>, “Audio set: An ontology and human-labeled dataset for audio events,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2">2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2017, pp. 776–780.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
C. D. Kim, B. Kim, H. Lee, and G. Kim, “Audiocaps: Generating captions for audios in the wild,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, 2019, pp. 119–132.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, “Fsd50k: an open dataset of human-labeled sound events,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">IEEE/ACM Transactions on Audio, Speech, and Language Processing</em>, vol. 30, pp. 829–852, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
I. Martín-Morató <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">et al.</em>, “Sound event envelope estimation in polyphonic mixtures,” in <em class="ltx_emph ltx_font_italic" id="bib.bib35.2.2">ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2019, pp. 935–939.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
K. J. Piczak, “ESC: Dataset for Environmental Sound Classification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Proceedings of the 23rd Annual ACM Conference on Multimedia</em>.   ACM Press, 2015, pp. 1015–1018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
J. Salamon, C. Jacoby, and J. P. Bello, “A dataset and taxonomy for urban sound research,” in <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">22nd ACM International Conference on Multimedia (ACM-MM’14)</em>, Orlando, FL, USA, Nov. 2014, pp. 1041–1044.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
S. Lipping, P. Sudarsanam, K. Drossos, and T. Virtanen, “Clotho-aqa: A crowdsourced dataset for audio question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">2022 30th European Signal Processing Conference (EUSIPCO)</em>.   IEEE, 2022, pp. 1140–1144.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Gong, J. Yu, and J. Glass, “Vocalsound: A dataset for improving human vocal sounds recognition,” in <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>, 2022, pp. 151–155.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
T. Wolf, L. Debut, V. Sanh, and et al., “Huggingface’s transformers: State-of-the-art natural language processing,” 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
C.-K. Yang, N. Ho, Y.-T. Piao, and H.-y. Lee, “Sakura: On the multi-hop reasoning of large audio-language models based on speech and audio information,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2505.13237</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Sakshi, U. Tyagi, S. Kumar, A. Seth, R. Selvakumar, O. Nieto, R. Duraiswami, S. Ghosh, and D. Manocha, “Mmau: A massive multi-task audio understanding and reasoning benchmark,” in <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">The Thirteenth International Conference on Learning Representations</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
C.-y. Huang <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">et al.</em>, “Dynamic-superb: Towards a dynamic, collaborative, and comprehensive instruction-tuning benchmark for speech,” in <em class="ltx_emph ltx_font_italic" id="bib.bib43.2.2">ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2024, pp. 12 136–12 140.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
C.-y. Huang, W.-C. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">et al.</em>, “Dynamic-superb phase-2: A collaboratively expanding benchmark for measuring the capabilities of spoken language models with 180 tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.2.2">arXiv preprint arXiv:2411.05361</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
M. M. Rashid, G. Li, and C. Du, “Nonspeech7k dataset: Classification and analysis of human non-speech sound,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">IET Signal Processing</em>, vol. 17, no. 6, p. e12233, 2023. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12233" title="">https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.1049/sil2.12233</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
I.-Y. Jeong and J. Park, “Cochlscene: Acquisition of acoustic scene data using crowdsourcing,” in <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">2022 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)</em>.   IEEE, 2022, pp. 17–21.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
E. B. Çoban, M. Perra, D. Pir, and M. I. Mandel, “Edansa-2019: The ecoacoustic dataset from arctic north slope alaska,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Workshop on the Detection and Classification of Acoustic Scenes and Events</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
E. B. Çoban <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">et al.</em>, “What do mllms hear? examining the interaction between llm and audio encoder components in multimodal large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib48.2.2">Audio Imagination: NeurIPS 2024 Workshop AI-Driven Speech, Music, and Sound Generation</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
S. Deshmukh, S. Han, H. Bukhari, B. Elizalde <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">et al.</em>, “Audio entailment: Assessing deductive reasoning for audio understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.2.2">arXiv preprint arXiv:2407.18062</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
Y. Li, Y. Du, K. Zhou, J. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">et al.</em>, “Evaluating object hallucination in large vision-language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib50.2.2">arXiv preprint arXiv:2305.10355</em>, 2023.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 26 15:47:42 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

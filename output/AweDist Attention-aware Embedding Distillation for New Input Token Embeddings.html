<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings</title>
<!--Generated on Mon May 26 15:30:42 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2505.20133v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S1" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S2" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Background</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S3" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Method: <span class="ltx_text ltx_font_smallcaps">AweDist</span> – <span class="ltx_text ltx_framed ltx_framed_underline">A</span>ttention-a<span class="ltx_text ltx_framed ltx_framed_underline">w</span>are <span class="ltx_text ltx_framed ltx_framed_underline">E</span>mbedding <span class="ltx_text ltx_framed ltx_framed_underline">Dist</span>illation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S3.SS1" title="In 3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Motivation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S3.SS2" title="In 3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Method Description</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S3.SS3" title="In 3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Further Details</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S3.SS3.SSS0.Px1" title="In 3.3 Further Details ‣ 3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Retrieving relevant contexts for new tokens.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S3.SS3.SSS0.Px2" title="In 3.3 Further Details ‣ 3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Output embeddings.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S3.SS3.SSS0.Px3" title="In 3.3 Further Details ‣ 3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Hyperparameters.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S4" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Experimental Setup</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S4.SS1" title="In 4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Evaluation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S4.SS2" title="In 4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>Modeling</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S4.SS2.SSS0.Px1" title="In 4.2 Modeling ‣ 4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Considered base models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S4.SS2.SSS0.Px2" title="In 4.2 Modeling ‣ 4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Baselines.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S4.SS2.SSS0.Px3" title="In 4.2 Modeling ‣ 4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Our method.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS1" title="In 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.1 </span>Main Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS1.SSS0.Px1" title="In 5.1 Main Experiments ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Main results.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS1.SSS0.Px2" title="In 5.1 Main Experiments ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Freezing original embeddings.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS1.SSS0.Px3" title="In 5.1 Main Experiments ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Hypernetworks.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" title="In 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5.2 </span>In-Depth Analysis</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS2.SSS0.Px1" title="In 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Multi-word tokens.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS2.SSS0.Px2" title="In 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Retrieved vs. generated data.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS2.SSS0.Px3" title="In 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Choice of initial embedding for <span class="ltx_text ltx_font_smallcaps">AweDist</span>.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS2.SSS0.Px4" title="In 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Different distillation objectives.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S5.SS2.SSS0.Px5" title="In 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Target layer.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#S6" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A1" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Implementation Details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS1" title="In Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Token Selection for Domain Adaptation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS2" title="In Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Multi-Word Token Generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS3" title="In Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3 </span>Open Medical-LLM Leaderboard Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS3.SSS1" title="In B.3 Open Medical-LLM Leaderboard Experiments ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.3.1 </span>Compute Budget &amp; Runtime</span></a>
<ol class="ltx_toclist ltx_toclist_subsubsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS3.SSS1.Px1" title="In B.3.1 Compute Budget &amp; Runtime ‣ B.3 Open Medical-LLM Leaderboard Experiments ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title">Additional computational budget.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS3.SSS1.Px2" title="In B.3.1 Compute Budget &amp; Runtime ‣ B.3 Open Medical-LLM Leaderboard Experiments ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_text ltx_font_smallcaps">AweDist</span> versus hyper-networks.</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS4" title="In Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.4 </span>LLM-as-a-Judge Experiments</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS5" title="In Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.5 </span>Generation of Relevant Contexts</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS6" title="In Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.6 </span>Isolated Per-Token Optimization vs. Joint Optimization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS7" title="In Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.7 </span>Hyperparameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A2.SS8" title="In Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.8 </span>Models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A3" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">C</span> </span><span class="ltx_text" style="font-size:90%;">Additional Results</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#A4" title="In AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text" style="font-size:90%;">D</span> </span><span class="ltx_text" style="font-size:90%;">Use of LLMs</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
<span class="ltx_text ltx_font_smallcaps" id="id1.id1">AweDist</span>: Attention-aware Embedding Distillation for New Input Token Embeddings</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Konstantin Dobler
<br class="ltx_break"/>Hasso Plattner Institute
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id2.1.id1">konstantin.dobler@hpi.de</span> &amp;Desmond Elliott
<br class="ltx_break"/>University of Copenhagen
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id3.2.id2">de@di.ku.dk</span> &amp;Gerard de Melo
<br class="ltx_break"/>Hasso Plattner Institute
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id4.3.id3">gerard.demelo@hpi.de</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id5.id1">Current language models rely on static vocabularies determined at pretraining time, which can lead to decreased performance and increased computational cost for domains underrepresented in the original vocabulary.
New tokens can be added to solve this problem, when coupled with a good initialization for their new embeddings. However, existing embedding initialization methods either require expensive further training or pretraining of additional modules.
In this paper, we propose <span class="ltx_text ltx_font_smallcaps" id="id5.id1.1">AweDist</span> and show that by distilling representations obtained using the original tokenization, we can quickly learn high-quality input embeddings for new tokens.
Experimental results with a wide range of open-weight models show that <span class="ltx_text ltx_font_smallcaps" id="id5.id1.2">AweDist</span> is able to outperform even strong baselines<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Our code is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/konstantinjdobler/awedist" title="">https://github.com/konstantinjdobler/awedist</a>.</span></span></span>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Pretrained language models are built upon static vocabularies that often fail to represent novel or domain-specific terminology, resulting in broken tokens, longer input sequences, and diluted semantic representations. This excessive tokenization not only leads to reduced performance on downstream tasks <cite class="ltx_cite ltx_citemacro_citep">(Rust et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib47" title="">2021</a>)</cite> but also increases the computational (and therefore also financial) cost <cite class="ltx_cite ltx_citemacro_citep">(Ahia et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib1" title="">2023</a>)</cite>.
While adding new tokens to a model’s vocabulary can reduce over-tokenization, it is crucial to choose a good initialization for the new embeddings <cite class="ltx_cite ltx_citemacro_citep">(Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>; Dobler and de Melo, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib8" title="">2023</a>; Yamaguchi et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib60" title="">2024a</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">In this paper, we argue that many recent methods for embedding initialization are fundamentally limited. Whenever we wish to add a new token to a pretrained model’s vocabulary, this new token may be split up into multiple <span class="ltx_text ltx_font_italic" id="S1.p2.1.1">subtokens</span> in the original model’s tokenization. However, these subtokens might not be individually informative about the semantics of the entire new token (consider e.g. <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.2">&lt;_pal&gt;</span> <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.3">&lt;at&gt;</span> <span class="ltx_text ltx_font_typewriter" id="S1.p2.1.4">&lt;able&gt;</span>).
The semantic processing of these subtokens as a sequence will largely not be represented in their input embeddings at all – but rather in the attention mechanism and feed-forward network layer weights of the Transformer <cite class="ltx_cite ltx_citemacro_citep">(Elhage et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib10" title="">2022</a>; Lad et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib27" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Therefore, methods not exploiting information encoded in Transformer layer weights are at a serious disadvantage.
Motivated by this insight, we propose a novel method for input embedding initialization that captures information stored in all Transformer layers in addition to the existing input embeddings. Our method optimizes new token embeddings with a distillation-based objective to match the model’s behavior compared to when a new token is split up into its original subtokens. We demonstrate the efficacy of our method, dubbed <span class="ltx_text ltx_font_smallcaps" id="S1.p3.1.1">AweDist</span> for <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.2">A</span>ttention-a<span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.3">w</span>are <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.4">E</span>mbedding <span class="ltx_text ltx_framed ltx_framed_underline" id="S1.p3.1.5">Dist</span>illation, in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5" title="5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5</span></a>.
We illustrate our method in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> and describe it in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S3" title="3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 3</span></a>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">We compare against strong baselines, including standard embedding initialization procedures, training of the embedding matrices with causal language modeling, as well as pretrained hyper-networks that predict new token embeddings. Extensive experiments on a wide range of open-weight models including question-answering benchmarks and the generation of definitions for the newly added tokens confirm the effectiveness of our approach.
Our experimental setup is detailed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S4" title="4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 4</span></a>. Additionally, we describe related work in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S2" title="2 Background ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 2</span></a> and explicitly discuss limitations of our method in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A1" title="Appendix A Limitations ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Appendix A</span></a>.
In summary, our contributions are:</p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We propose <span class="ltx_text ltx_font_smallcaps" id="S1.I1.i1.p1.1.1">AweDist</span>, a novel method for providing high-quality input embeddings when adding new tokens to pretrained language models.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We motivate our proposed method by describing fundamental limitations of current embedding initialization methods and empirically verify our claims.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">Extensive experiments and in-depth analyses of our method using a wide range of open-weight model checkpoints and strong baselines confirm its effectiveness.</p>
</div>
</li>
</ul>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="320" id="S1.F1.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Illustration of <span class="ltx_text ltx_font_smallcaps" id="S1.F1.2.1">AweDist</span> – Given a sequence containing our new target token, we first obtain the model’s hidden states on that sequence using the original tokenization and then quickly learn a new embedding by reducing the mean squared error (MSE) between the original hidden states and the hidden states of the model when using a single token embedding to replace the original subtokens.
</figcaption>
</figure>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Background</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">Most state-of-the-art Large Language Models (LLMs) are trained using a static tokenizer, usually derived by a byte-pair encoding scheme before model training <cite class="ltx_cite ltx_citemacro_citep">(Sennrich et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib51" title="">2016</a>)</cite>. However, a single tokenizer predetermined before model training might not be equally well-suited for the many different use cases that a model will need to address later on. Suboptimal tokenization of new languages or domain-specific text increases the cost of further training <cite class="ltx_cite ltx_citemacro_citep">(Ahia et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib1" title="">2023</a>; Yamaguchi et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib60" title="">2024a</a>)</cite> and has also been tied to reduced performance <cite class="ltx_cite ltx_citemacro_citep">(Rust et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib47" title="">2021</a>; Ali et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib3" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S2.p2">
<p class="ltx_p" id="S2.p2.1">A solution to this problem is to modify the existing vocabulary to suit the specific needs. However, simply modifying the vocabulary is insufficient –
We also need suitable embeddings for the new tokens. A common approach is simple random initialization followed by a phase of training only the new embedding layers on the target corpus to “warm up” the new embedding weights.
However, random initialization of new token embeddings has been widely shown to underperform better methods <cite class="ltx_cite ltx_citemacro_citep">(e.g., Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>; Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib34" title="">2022</a>; Dobler and de Melo, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib8" title="">2023</a>)</cite>. This has led to notable interest in devising ways of obtaining more informative embeddings for new tokens.</p>
</div>
<div class="ltx_para" id="S2.p3">
<p class="ltx_p" id="S2.p3.1">A common alternative is to initialize a new token’s embedding as the mean of its subtoken embeddings <cite class="ltx_cite ltx_citemacro_citep">(Sachidananda et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib48" title="">2021</a>; Koto et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib25" title="">2021</a>; Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>)</cite> – potentially weighted based on the subtoken position in the new token <cite class="ltx_cite ltx_citemacro_citep">(Nakash et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib38" title="">2025</a>)</cite> – or based on other heuristics <cite class="ltx_cite ltx_citemacro_citep">(Downey et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib9" title="">2023</a>)</cite>.
Alternatively, a weighted mean of existing embeddings can also be computed based on similarity scores in auxiliary embedding spaces <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib58" title="">2019</a>; Tran, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib55" title="">2020</a>; Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib34" title="">2022</a>; Ostendorff and Rehm, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib40" title="">2023</a>; Dobler and de Melo, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib8" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib31" title="">2024</a>, <span class="ltx_text ltx_font_italic" id="S2.p3.1.1.1">inter alia</span>)</cite>.
In fact, most recent research into embedding initialization for new tokens proposes some variation of a weighted mean or copy of existing embedding vectors <cite class="ltx_cite ltx_citemacro_citep">(Mosin et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib36" title="">2023</a>; Zeng et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib62" title="">2023</a>; Mundra et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib37" title="">2024</a>; Yamaguchi et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib60" title="">2024a</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib61" title="">b</a>; Remy et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib45" title="">2024</a>)</cite>.
Nonetheless, the resulting embeddings still require further tuning to achieve good results <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite>. We refer to this category of initialization methods as “model-gradient free”, as they do not consider any training signal from the model.<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>While auxiliary embedding spaces can be trained, they do not backpropagate through the target model.</span></span></span></p>
</div>
<div class="ltx_para" id="S2.p4">
<p class="ltx_p" id="S2.p4.1">Only a few methods based on access to model gradients have been investigated in recent years. Model gradients can be used by directly optimizing just the new token embeddings to minimize a cross-entropy loss on relevant contexts <cite class="ltx_cite ltx_citemacro_citep">(Lampinen and McClelland, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib28" title="">2018</a>)</cite>,
the common practice of training all embeddings while freezing other parts of the model <cite class="ltx_cite ltx_citemacro_citep">(Artetxe et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib4" title="">2020</a>; de Vries and Nissim, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib7" title="">2021</a>)</cite> – potentially only backpropagating through parts of the model <cite class="ltx_cite ltx_citemacro_citep">(Marchisio et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib33" title="">2023</a>)</cite> – or via (pre-)training a hyper-network <cite class="ltx_cite ltx_citemacro_citep">(Ha et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib15" title="">2017</a>)</cite> that is able to predict embeddings for new tokens in the target embedding space <cite class="ltx_cite ltx_citemacro_citep">(Pinter et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib43" title="">2017</a>; Schick and Schütze, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib49" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib50" title="">2020</a>; Teehan et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib54" title="">2024</a>; Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite>.
These methods – hyper-networks in particular – have been shown to yield immediately useful representations but require additional compute for (pre-)training. In the case of hyper-networks, it additionally might be necessary to train different hyper-networks for different target domains, further increasing the complexity.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><cite class="ltx_cite ltx_citemacro_citet">Minixhofer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite>
train separate hyper-networks for <span class="ltx_text ltx_font_typewriter" id="footnote3.1">mistralai/Mistral-7b-v0.1</span> for English+Code and multilingual target tokenizers.</span></span></span>
Motivated by these difficulties, we propose an alternative method in this paper that does not require extensive pretraining of an embedding prediction hyper-network but still achieves competitive results.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Method: <span class="ltx_text ltx_font_smallcaps" id="S3.1.1">AweDist</span> – <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.2.2">A</span>ttention-a<span class="ltx_text ltx_framed ltx_framed_underline" id="S3.3.3">w</span>are <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.4.4">E</span>mbedding <span class="ltx_text ltx_framed ltx_framed_underline" id="S3.5.5">Dist</span>illation</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.4">Our goal is to initialize input embeddings for new tokens such that, when inserted into a frozen pretrained Transformer, they faithfully reproduce its original behavior – now using a single new token instead of multiple subtokens. Specifically, we seek an embedding <math alttext="\mathbf{e^{\star}}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><msup id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">𝐞</mi><mo id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">superscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">𝐞</ci><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">\mathbf{e^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> for a new token <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><msup id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">t</mi><mo id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">superscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">𝑡</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> such that all downstream hidden states match those obtained when the model would instead have seen the original subtokens <math alttext="[t_{1},\dots,t_{n}]" class="ltx_Math" display="inline" id="S3.p1.3.m3.3"><semantics id="S3.p1.3.m3.3a"><mrow id="S3.p1.3.m3.3.3.2" xref="S3.p1.3.m3.3.3.3.cmml"><mo id="S3.p1.3.m3.3.3.2.3" stretchy="false" xref="S3.p1.3.m3.3.3.3.cmml">[</mo><msub id="S3.p1.3.m3.2.2.1.1" xref="S3.p1.3.m3.2.2.1.1.cmml"><mi id="S3.p1.3.m3.2.2.1.1.2" xref="S3.p1.3.m3.2.2.1.1.2.cmml">t</mi><mn id="S3.p1.3.m3.2.2.1.1.3" xref="S3.p1.3.m3.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.p1.3.m3.3.3.2.4" xref="S3.p1.3.m3.3.3.3.cmml">,</mo><mi id="S3.p1.3.m3.1.1" mathvariant="normal" xref="S3.p1.3.m3.1.1.cmml">…</mi><mo id="S3.p1.3.m3.3.3.2.5" xref="S3.p1.3.m3.3.3.3.cmml">,</mo><msub id="S3.p1.3.m3.3.3.2.2" xref="S3.p1.3.m3.3.3.2.2.cmml"><mi id="S3.p1.3.m3.3.3.2.2.2" xref="S3.p1.3.m3.3.3.2.2.2.cmml">t</mi><mi id="S3.p1.3.m3.3.3.2.2.3" xref="S3.p1.3.m3.3.3.2.2.3.cmml">n</mi></msub><mo id="S3.p1.3.m3.3.3.2.6" stretchy="false" xref="S3.p1.3.m3.3.3.3.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p1.3.m3.3b"><list id="S3.p1.3.m3.3.3.3.cmml" xref="S3.p1.3.m3.3.3.2"><apply id="S3.p1.3.m3.2.2.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1"><csymbol cd="ambiguous" id="S3.p1.3.m3.2.2.1.1.1.cmml" xref="S3.p1.3.m3.2.2.1.1">subscript</csymbol><ci id="S3.p1.3.m3.2.2.1.1.2.cmml" xref="S3.p1.3.m3.2.2.1.1.2">𝑡</ci><cn id="S3.p1.3.m3.2.2.1.1.3.cmml" type="integer" xref="S3.p1.3.m3.2.2.1.1.3">1</cn></apply><ci id="S3.p1.3.m3.1.1.cmml" xref="S3.p1.3.m3.1.1">…</ci><apply id="S3.p1.3.m3.3.3.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2"><csymbol cd="ambiguous" id="S3.p1.3.m3.3.3.2.2.1.cmml" xref="S3.p1.3.m3.3.3.2.2">subscript</csymbol><ci id="S3.p1.3.m3.3.3.2.2.2.cmml" xref="S3.p1.3.m3.3.3.2.2.2">𝑡</ci><ci id="S3.p1.3.m3.3.3.2.2.3.cmml" xref="S3.p1.3.m3.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.m3.3c">[t_{1},\dots,t_{n}]</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.m3.3d">[ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>, which <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.p1.4.m4.1"><semantics id="S3.p1.4.m4.1a"><msup id="S3.p1.4.m4.1.1" xref="S3.p1.4.m4.1.1.cmml"><mi id="S3.p1.4.m4.1.1.2" xref="S3.p1.4.m4.1.1.2.cmml">t</mi><mo id="S3.p1.4.m4.1.1.3" xref="S3.p1.4.m4.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.p1.4.m4.1b"><apply id="S3.p1.4.m4.1.1.cmml" xref="S3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p1.4.m4.1.1.1.cmml" xref="S3.p1.4.m4.1.1">superscript</csymbol><ci id="S3.p1.4.m4.1.1.2.cmml" xref="S3.p1.4.m4.1.1.2">𝑡</ci><ci id="S3.p1.4.m4.1.1.3.cmml" xref="S3.p1.4.m4.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.m4.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.m4.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> would have been tokenized into according to the original vocabulary. In what follows, we (1) explain why existing methods fall short and motivate our method, (2) formalize our method, and (3) describe implementation details.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Motivation</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.3">As illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S1" title="1 Introduction ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 1</span></a>, individual subtoken embeddings <math alttext="t_{1},\dots,t_{n}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.3"><semantics id="S3.SS1.p1.1.m1.3a"><mrow id="S3.SS1.p1.1.m1.3.3.2" xref="S3.SS1.p1.1.m1.3.3.3.cmml"><msub id="S3.SS1.p1.1.m1.2.2.1.1" xref="S3.SS1.p1.1.m1.2.2.1.1.cmml"><mi id="S3.SS1.p1.1.m1.2.2.1.1.2" xref="S3.SS1.p1.1.m1.2.2.1.1.2.cmml">t</mi><mn id="S3.SS1.p1.1.m1.2.2.1.1.3" xref="S3.SS1.p1.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p1.1.m1.3.3.2.3" xref="S3.SS1.p1.1.m1.3.3.3.cmml">,</mo><mi id="S3.SS1.p1.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p1.1.m1.1.1.cmml">…</mi><mo id="S3.SS1.p1.1.m1.3.3.2.4" xref="S3.SS1.p1.1.m1.3.3.3.cmml">,</mo><msub id="S3.SS1.p1.1.m1.3.3.2.2" xref="S3.SS1.p1.1.m1.3.3.2.2.cmml"><mi id="S3.SS1.p1.1.m1.3.3.2.2.2" xref="S3.SS1.p1.1.m1.3.3.2.2.2.cmml">t</mi><mi id="S3.SS1.p1.1.m1.3.3.2.2.3" xref="S3.SS1.p1.1.m1.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.3b"><list id="S3.SS1.p1.1.m1.3.3.3.cmml" xref="S3.SS1.p1.1.m1.3.3.2"><apply id="S3.SS1.p1.1.m1.2.2.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p1.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p1.1.m1.2.2.1.1.2">𝑡</ci><cn id="S3.SS1.p1.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p1.1.m1.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">…</ci><apply id="S3.SS1.p1.1.m1.3.3.2.2.cmml" xref="S3.SS1.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p1.1.m1.3.3.2.2.1.cmml" xref="S3.SS1.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p1.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.p1.1.m1.3.3.2.2.2">𝑡</ci><ci id="S3.SS1.p1.1.m1.3.3.2.2.3.cmml" xref="S3.SS1.p1.1.m1.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.3c">t_{1},\dots,t_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.3d">italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> (e.g., <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.3.1">&lt;_pal&gt;</span> <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.3.2">&lt;at&gt;</span> <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.3.3">&lt;able&gt;</span>) of a new token <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS1.p1.2.m2.1"><semantics id="S3.SS1.p1.2.m2.1a"><msup id="S3.SS1.p1.2.m2.1.1" xref="S3.SS1.p1.2.m2.1.1.cmml"><mi id="S3.SS1.p1.2.m2.1.1.2" xref="S3.SS1.p1.2.m2.1.1.2.cmml">t</mi><mo id="S3.SS1.p1.2.m2.1.1.3" xref="S3.SS1.p1.2.m2.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.2.m2.1b"><apply id="S3.SS1.p1.2.m2.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.2.m2.1.1.1.cmml" xref="S3.SS1.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p1.2.m2.1.1.2.cmml" xref="S3.SS1.p1.2.m2.1.1.2">𝑡</ci><ci id="S3.SS1.p1.2.m2.1.1.3.cmml" xref="S3.SS1.p1.2.m2.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.2.m2.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.2.m2.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> (e.g., <span class="ltx_text ltx_font_typewriter" id="S3.SS1.p1.3.4">&lt;_palatable&gt;</span>) do not necessarily store the semantics of the specific concatenated form <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS1.p1.3.m3.1"><semantics id="S3.SS1.p1.3.m3.1a"><msup id="S3.SS1.p1.3.m3.1.1" xref="S3.SS1.p1.3.m3.1.1.cmml"><mi id="S3.SS1.p1.3.m3.1.1.2" xref="S3.SS1.p1.3.m3.1.1.2.cmml">t</mi><mo id="S3.SS1.p1.3.m3.1.1.3" xref="S3.SS1.p1.3.m3.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.3.m3.1b"><apply id="S3.SS1.p1.3.m3.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p1.3.m3.1.1.1.cmml" xref="S3.SS1.p1.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p1.3.m3.1.1.2.cmml" xref="S3.SS1.p1.3.m3.1.1.2">𝑡</ci><ci id="S3.SS1.p1.3.m3.1.1.3.cmml" xref="S3.SS1.p1.3.m3.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.3.m3.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.3.m3.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>. Instead, their contexualized representation is gradually constructed in the Transformer layers via a neural <span class="ltx_text ltx_font_italic" id="S3.SS1.p1.3.5">detokenization</span> process <cite class="ltx_cite ltx_citemacro_citep">(Elhage et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib10" title="">2022</a>; Lad et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib27" title="">2024</a>)</cite> and then attended to by other token positions in the sequence, which are influenced by this contextualized representation.
The various prior approaches that solely rely on information encoded in the embedding tables of a model thus ignore the crucial functional knowledge stored in the attention and feed-forward weights of Transformer layers, which imposes a fundamental limitation on their performance.</p>
</div>
<div class="ltx_para" id="S3.SS1.p2">
<p class="ltx_p" id="S3.SS1.p2.10">Precisely pinpointing the specifically involved attention heads is difficult and can require manual inspection <cite class="ltx_cite ltx_citemacro_citep">(Elhage et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib10" title="">2022</a>)</cite>.
Instead, we propose to <em class="ltx_emph ltx_font_italic" id="S3.SS1.p2.10.1">distill</em> <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib19" title="">2015</a>; Snell et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib53" title="">2022</a>)</cite> the impact that the multiple subtokens <math alttext="t_{1},\dots,t_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.1.m1.3"><semantics id="S3.SS1.p2.1.m1.3a"><mrow id="S3.SS1.p2.1.m1.3.3.2" xref="S3.SS1.p2.1.m1.3.3.3.cmml"><msub id="S3.SS1.p2.1.m1.2.2.1.1" xref="S3.SS1.p2.1.m1.2.2.1.1.cmml"><mi id="S3.SS1.p2.1.m1.2.2.1.1.2" xref="S3.SS1.p2.1.m1.2.2.1.1.2.cmml">t</mi><mn id="S3.SS1.p2.1.m1.2.2.1.1.3" xref="S3.SS1.p2.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.1.m1.3.3.2.3" xref="S3.SS1.p2.1.m1.3.3.3.cmml">,</mo><mi id="S3.SS1.p2.1.m1.1.1" mathvariant="normal" xref="S3.SS1.p2.1.m1.1.1.cmml">…</mi><mo id="S3.SS1.p2.1.m1.3.3.2.4" xref="S3.SS1.p2.1.m1.3.3.3.cmml">,</mo><msub id="S3.SS1.p2.1.m1.3.3.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.cmml"><mi id="S3.SS1.p2.1.m1.3.3.2.2.2" xref="S3.SS1.p2.1.m1.3.3.2.2.2.cmml">t</mi><mi id="S3.SS1.p2.1.m1.3.3.2.2.3" xref="S3.SS1.p2.1.m1.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.1.m1.3b"><list id="S3.SS1.p2.1.m1.3.3.3.cmml" xref="S3.SS1.p2.1.m1.3.3.2"><apply id="S3.SS1.p2.1.m1.2.2.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.2.2.1.1.1.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.1.m1.2.2.1.1.2.cmml" xref="S3.SS1.p2.1.m1.2.2.1.1.2">𝑡</ci><cn id="S3.SS1.p2.1.m1.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.1.m1.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p2.1.m1.1.1.cmml" xref="S3.SS1.p2.1.m1.1.1">…</ci><apply id="S3.SS1.p2.1.m1.3.3.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.1.m1.3.3.2.2.1.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.1.m1.3.3.2.2.2.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2.2">𝑡</ci><ci id="S3.SS1.p2.1.m1.3.3.2.2.3.cmml" xref="S3.SS1.p2.1.m1.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.1.m1.3c">t_{1},\dots,t_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.1.m1.3d">italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> have on other tokens attending to them into a single token embedding <math alttext="\mathbf{e^{\star}}" class="ltx_Math" display="inline" id="S3.SS1.p2.2.m2.1"><semantics id="S3.SS1.p2.2.m2.1a"><msup id="S3.SS1.p2.2.m2.1.1" xref="S3.SS1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.p2.2.m2.1.1.2" xref="S3.SS1.p2.2.m2.1.1.2.cmml">𝐞</mi><mo id="S3.SS1.p2.2.m2.1.1.3" xref="S3.SS1.p2.2.m2.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.2.m2.1b"><apply id="S3.SS1.p2.2.m2.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.p2.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.p2.2.m2.1.1.2">𝐞</ci><ci id="S3.SS1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.p2.2.m2.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.2.m2.1c">\mathbf{e^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.2.m2.1d">bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>. Our intuition is as follows:
If we identify an embedding <math alttext="\mathbf{e^{\star}}" class="ltx_Math" display="inline" id="S3.SS1.p2.3.m3.1"><semantics id="S3.SS1.p2.3.m3.1a"><msup id="S3.SS1.p2.3.m3.1.1" xref="S3.SS1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.p2.3.m3.1.1.2" xref="S3.SS1.p2.3.m3.1.1.2.cmml">𝐞</mi><mo id="S3.SS1.p2.3.m3.1.1.3" xref="S3.SS1.p2.3.m3.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.3.m3.1b"><apply id="S3.SS1.p2.3.m3.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.p2.3.m3.1.1">superscript</csymbol><ci id="S3.SS1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.p2.3.m3.1.1.2">𝐞</ci><ci id="S3.SS1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.p2.3.m3.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.3.m3.1c">\mathbf{e^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.3.m3.1d">bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> for <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS1.p2.4.m4.1"><semantics id="S3.SS1.p2.4.m4.1a"><msup id="S3.SS1.p2.4.m4.1.1" xref="S3.SS1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.p2.4.m4.1.1.2" xref="S3.SS1.p2.4.m4.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.4.m4.1.1.3" xref="S3.SS1.p2.4.m4.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.4.m4.1b"><apply id="S3.SS1.p2.4.m4.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.p2.4.m4.1.1">superscript</csymbol><ci id="S3.SS1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.p2.4.m4.1.1.2">𝑡</ci><ci id="S3.SS1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.p2.4.m4.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.4.m4.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.4.m4.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> such that the model produces similar hidden states in the succeeding positions after seeing <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS1.p2.5.m5.1"><semantics id="S3.SS1.p2.5.m5.1a"><msup id="S3.SS1.p2.5.m5.1.1" xref="S3.SS1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.p2.5.m5.1.1.2" xref="S3.SS1.p2.5.m5.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.5.m5.1.1.3" xref="S3.SS1.p2.5.m5.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.5.m5.1b"><apply id="S3.SS1.p2.5.m5.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.p2.5.m5.1.1">superscript</csymbol><ci id="S3.SS1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.p2.5.m5.1.1.2">𝑡</ci><ci id="S3.SS1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.p2.5.m5.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.5.m5.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.5.m5.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS1.p2.6.m6.1"><semantics id="S3.SS1.p2.6.m6.1a"><msup id="S3.SS1.p2.6.m6.1.1" xref="S3.SS1.p2.6.m6.1.1.cmml"><mi id="S3.SS1.p2.6.m6.1.1.2" xref="S3.SS1.p2.6.m6.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.6.m6.1.1.3" xref="S3.SS1.p2.6.m6.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.6.m6.1b"><apply id="S3.SS1.p2.6.m6.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.6.m6.1.1.1.cmml" xref="S3.SS1.p2.6.m6.1.1">superscript</csymbol><ci id="S3.SS1.p2.6.m6.1.1.2.cmml" xref="S3.SS1.p2.6.m6.1.1.2">𝑡</ci><ci id="S3.SS1.p2.6.m6.1.1.3.cmml" xref="S3.SS1.p2.6.m6.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.6.m6.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.6.m6.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>’s original subtokens <math alttext="t_{1},\dots,t_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.7.m7.3"><semantics id="S3.SS1.p2.7.m7.3a"><mrow id="S3.SS1.p2.7.m7.3.3.2" xref="S3.SS1.p2.7.m7.3.3.3.cmml"><msub id="S3.SS1.p2.7.m7.2.2.1.1" xref="S3.SS1.p2.7.m7.2.2.1.1.cmml"><mi id="S3.SS1.p2.7.m7.2.2.1.1.2" xref="S3.SS1.p2.7.m7.2.2.1.1.2.cmml">t</mi><mn id="S3.SS1.p2.7.m7.2.2.1.1.3" xref="S3.SS1.p2.7.m7.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.7.m7.3.3.2.3" xref="S3.SS1.p2.7.m7.3.3.3.cmml">,</mo><mi id="S3.SS1.p2.7.m7.1.1" mathvariant="normal" xref="S3.SS1.p2.7.m7.1.1.cmml">…</mi><mo id="S3.SS1.p2.7.m7.3.3.2.4" xref="S3.SS1.p2.7.m7.3.3.3.cmml">,</mo><msub id="S3.SS1.p2.7.m7.3.3.2.2" xref="S3.SS1.p2.7.m7.3.3.2.2.cmml"><mi id="S3.SS1.p2.7.m7.3.3.2.2.2" xref="S3.SS1.p2.7.m7.3.3.2.2.2.cmml">t</mi><mi id="S3.SS1.p2.7.m7.3.3.2.2.3" xref="S3.SS1.p2.7.m7.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.7.m7.3b"><list id="S3.SS1.p2.7.m7.3.3.3.cmml" xref="S3.SS1.p2.7.m7.3.3.2"><apply id="S3.SS1.p2.7.m7.2.2.1.1.cmml" xref="S3.SS1.p2.7.m7.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.2.2.1.1.1.cmml" xref="S3.SS1.p2.7.m7.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.7.m7.2.2.1.1.2.cmml" xref="S3.SS1.p2.7.m7.2.2.1.1.2">𝑡</ci><cn id="S3.SS1.p2.7.m7.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.7.m7.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p2.7.m7.1.1.cmml" xref="S3.SS1.p2.7.m7.1.1">…</ci><apply id="S3.SS1.p2.7.m7.3.3.2.2.cmml" xref="S3.SS1.p2.7.m7.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.7.m7.3.3.2.2.1.cmml" xref="S3.SS1.p2.7.m7.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.7.m7.3.3.2.2.2.cmml" xref="S3.SS1.p2.7.m7.3.3.2.2.2">𝑡</ci><ci id="S3.SS1.p2.7.m7.3.3.2.2.3.cmml" xref="S3.SS1.p2.7.m7.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.7.m7.3c">t_{1},\dots,t_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.7.m7.3d">italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, we have extracted the relevant information stored in the model’s Transformer layers without requiring a specific localization within the model weights.
Thus, we propose to optimize <math alttext="\mathbf{e^{\star}}" class="ltx_Math" display="inline" id="S3.SS1.p2.8.m8.1"><semantics id="S3.SS1.p2.8.m8.1a"><msup id="S3.SS1.p2.8.m8.1.1" xref="S3.SS1.p2.8.m8.1.1.cmml"><mi id="S3.SS1.p2.8.m8.1.1.2" xref="S3.SS1.p2.8.m8.1.1.2.cmml">𝐞</mi><mo id="S3.SS1.p2.8.m8.1.1.3" xref="S3.SS1.p2.8.m8.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.8.m8.1b"><apply id="S3.SS1.p2.8.m8.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.8.m8.1.1.1.cmml" xref="S3.SS1.p2.8.m8.1.1">superscript</csymbol><ci id="S3.SS1.p2.8.m8.1.1.2.cmml" xref="S3.SS1.p2.8.m8.1.1.2">𝐞</ci><ci id="S3.SS1.p2.8.m8.1.1.3.cmml" xref="S3.SS1.p2.8.m8.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.8.m8.1c">\mathbf{e^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.8.m8.1d">bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> by minimizing the mean-squared error between hidden states produced by the input sequences containing <math alttext="t_{1},\dots,t_{n}" class="ltx_Math" display="inline" id="S3.SS1.p2.9.m9.3"><semantics id="S3.SS1.p2.9.m9.3a"><mrow id="S3.SS1.p2.9.m9.3.3.2" xref="S3.SS1.p2.9.m9.3.3.3.cmml"><msub id="S3.SS1.p2.9.m9.2.2.1.1" xref="S3.SS1.p2.9.m9.2.2.1.1.cmml"><mi id="S3.SS1.p2.9.m9.2.2.1.1.2" xref="S3.SS1.p2.9.m9.2.2.1.1.2.cmml">t</mi><mn id="S3.SS1.p2.9.m9.2.2.1.1.3" xref="S3.SS1.p2.9.m9.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS1.p2.9.m9.3.3.2.3" xref="S3.SS1.p2.9.m9.3.3.3.cmml">,</mo><mi id="S3.SS1.p2.9.m9.1.1" mathvariant="normal" xref="S3.SS1.p2.9.m9.1.1.cmml">…</mi><mo id="S3.SS1.p2.9.m9.3.3.2.4" xref="S3.SS1.p2.9.m9.3.3.3.cmml">,</mo><msub id="S3.SS1.p2.9.m9.3.3.2.2" xref="S3.SS1.p2.9.m9.3.3.2.2.cmml"><mi id="S3.SS1.p2.9.m9.3.3.2.2.2" xref="S3.SS1.p2.9.m9.3.3.2.2.2.cmml">t</mi><mi id="S3.SS1.p2.9.m9.3.3.2.2.3" xref="S3.SS1.p2.9.m9.3.3.2.2.3.cmml">n</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.9.m9.3b"><list id="S3.SS1.p2.9.m9.3.3.3.cmml" xref="S3.SS1.p2.9.m9.3.3.2"><apply id="S3.SS1.p2.9.m9.2.2.1.1.cmml" xref="S3.SS1.p2.9.m9.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.2.2.1.1.1.cmml" xref="S3.SS1.p2.9.m9.2.2.1.1">subscript</csymbol><ci id="S3.SS1.p2.9.m9.2.2.1.1.2.cmml" xref="S3.SS1.p2.9.m9.2.2.1.1.2">𝑡</ci><cn id="S3.SS1.p2.9.m9.2.2.1.1.3.cmml" type="integer" xref="S3.SS1.p2.9.m9.2.2.1.1.3">1</cn></apply><ci id="S3.SS1.p2.9.m9.1.1.cmml" xref="S3.SS1.p2.9.m9.1.1">…</ci><apply id="S3.SS1.p2.9.m9.3.3.2.2.cmml" xref="S3.SS1.p2.9.m9.3.3.2.2"><csymbol cd="ambiguous" id="S3.SS1.p2.9.m9.3.3.2.2.1.cmml" xref="S3.SS1.p2.9.m9.3.3.2.2">subscript</csymbol><ci id="S3.SS1.p2.9.m9.3.3.2.2.2.cmml" xref="S3.SS1.p2.9.m9.3.3.2.2.2">𝑡</ci><ci id="S3.SS1.p2.9.m9.3.3.2.2.3.cmml" xref="S3.SS1.p2.9.m9.3.3.2.2.3">𝑛</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.9.m9.3c">t_{1},\dots,t_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.9.m9.3d">italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> and their counterparts using <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS1.p2.10.m10.1"><semantics id="S3.SS1.p2.10.m10.1a"><msup id="S3.SS1.p2.10.m10.1.1" xref="S3.SS1.p2.10.m10.1.1.cmml"><mi id="S3.SS1.p2.10.m10.1.1.2" xref="S3.SS1.p2.10.m10.1.1.2.cmml">t</mi><mo id="S3.SS1.p2.10.m10.1.1.3" xref="S3.SS1.p2.10.m10.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.p2.10.m10.1b"><apply id="S3.SS1.p2.10.m10.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS1.p2.10.m10.1.1.1.cmml" xref="S3.SS1.p2.10.m10.1.1">superscript</csymbol><ci id="S3.SS1.p2.10.m10.1.1.2.cmml" xref="S3.SS1.p2.10.m10.1.1.2">𝑡</ci><ci id="S3.SS1.p2.10.m10.1.1.3.cmml" xref="S3.SS1.p2.10.m10.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p2.10.m10.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p2.10.m10.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>.
We will show in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5" title="5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5</span></a> that this empirically outperforms existing training-free embedding initialization methods, embedding tuning with a next-token prediction objective, as well as pretrained embedding prediction hyper-networks.
Next, we formally describe our method.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Method Description</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.36">Let <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mi id="S3.SS2.p1.1.m1.1.1" xref="S3.SS2.p1.1.m1.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><ci id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">italic_τ</annotation></semantics></math> be the tokenizer of a pretrained Transformer. Given a new token string <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><msup id="S3.SS2.p1.2.m2.1.1" xref="S3.SS2.p1.2.m2.1.1.cmml"><mi id="S3.SS2.p1.2.m2.1.1.2" xref="S3.SS2.p1.2.m2.1.1.2.cmml">t</mi><mo id="S3.SS2.p1.2.m2.1.1.3" xref="S3.SS2.p1.2.m2.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><apply id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.2.m2.1.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">superscript</csymbol><ci id="S3.SS2.p1.2.m2.1.1.2.cmml" xref="S3.SS2.p1.2.m2.1.1.2">𝑡</ci><ci id="S3.SS2.p1.2.m2.1.1.3.cmml" xref="S3.SS2.p1.2.m2.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> with original subtokens <math alttext="\tau(t^{\star})=[t_{1},\dots,t_{n}]" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.4"><semantics id="S3.SS2.p1.3.m3.4a"><mrow id="S3.SS2.p1.3.m3.4.4" xref="S3.SS2.p1.3.m3.4.4.cmml"><mrow id="S3.SS2.p1.3.m3.2.2.1" xref="S3.SS2.p1.3.m3.2.2.1.cmml"><mi id="S3.SS2.p1.3.m3.2.2.1.3" xref="S3.SS2.p1.3.m3.2.2.1.3.cmml">τ</mi><mo id="S3.SS2.p1.3.m3.2.2.1.2" xref="S3.SS2.p1.3.m3.2.2.1.2.cmml">⁢</mo><mrow id="S3.SS2.p1.3.m3.2.2.1.1.1" xref="S3.SS2.p1.3.m3.2.2.1.1.1.1.cmml"><mo id="S3.SS2.p1.3.m3.2.2.1.1.1.2" stretchy="false" xref="S3.SS2.p1.3.m3.2.2.1.1.1.1.cmml">(</mo><msup id="S3.SS2.p1.3.m3.2.2.1.1.1.1" xref="S3.SS2.p1.3.m3.2.2.1.1.1.1.cmml"><mi id="S3.SS2.p1.3.m3.2.2.1.1.1.1.2" xref="S3.SS2.p1.3.m3.2.2.1.1.1.1.2.cmml">t</mi><mo id="S3.SS2.p1.3.m3.2.2.1.1.1.1.3" xref="S3.SS2.p1.3.m3.2.2.1.1.1.1.3.cmml">⋆</mo></msup><mo id="S3.SS2.p1.3.m3.2.2.1.1.1.3" stretchy="false" xref="S3.SS2.p1.3.m3.2.2.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.SS2.p1.3.m3.4.4.4" xref="S3.SS2.p1.3.m3.4.4.4.cmml">=</mo><mrow id="S3.SS2.p1.3.m3.4.4.3.2" xref="S3.SS2.p1.3.m3.4.4.3.3.cmml"><mo id="S3.SS2.p1.3.m3.4.4.3.2.3" stretchy="false" xref="S3.SS2.p1.3.m3.4.4.3.3.cmml">[</mo><msub id="S3.SS2.p1.3.m3.3.3.2.1.1" xref="S3.SS2.p1.3.m3.3.3.2.1.1.cmml"><mi id="S3.SS2.p1.3.m3.3.3.2.1.1.2" xref="S3.SS2.p1.3.m3.3.3.2.1.1.2.cmml">t</mi><mn id="S3.SS2.p1.3.m3.3.3.2.1.1.3" xref="S3.SS2.p1.3.m3.3.3.2.1.1.3.cmml">1</mn></msub><mo id="S3.SS2.p1.3.m3.4.4.3.2.4" xref="S3.SS2.p1.3.m3.4.4.3.3.cmml">,</mo><mi id="S3.SS2.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS2.p1.3.m3.1.1.cmml">…</mi><mo id="S3.SS2.p1.3.m3.4.4.3.2.5" xref="S3.SS2.p1.3.m3.4.4.3.3.cmml">,</mo><msub id="S3.SS2.p1.3.m3.4.4.3.2.2" xref="S3.SS2.p1.3.m3.4.4.3.2.2.cmml"><mi id="S3.SS2.p1.3.m3.4.4.3.2.2.2" xref="S3.SS2.p1.3.m3.4.4.3.2.2.2.cmml">t</mi><mi id="S3.SS2.p1.3.m3.4.4.3.2.2.3" xref="S3.SS2.p1.3.m3.4.4.3.2.2.3.cmml">n</mi></msub><mo id="S3.SS2.p1.3.m3.4.4.3.2.6" stretchy="false" xref="S3.SS2.p1.3.m3.4.4.3.3.cmml">]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.4b"><apply id="S3.SS2.p1.3.m3.4.4.cmml" xref="S3.SS2.p1.3.m3.4.4"><eq id="S3.SS2.p1.3.m3.4.4.4.cmml" xref="S3.SS2.p1.3.m3.4.4.4"></eq><apply id="S3.SS2.p1.3.m3.2.2.1.cmml" xref="S3.SS2.p1.3.m3.2.2.1"><times id="S3.SS2.p1.3.m3.2.2.1.2.cmml" xref="S3.SS2.p1.3.m3.2.2.1.2"></times><ci id="S3.SS2.p1.3.m3.2.2.1.3.cmml" xref="S3.SS2.p1.3.m3.2.2.1.3">𝜏</ci><apply id="S3.SS2.p1.3.m3.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p1.3.m3.2.2.1.1.1">superscript</csymbol><ci id="S3.SS2.p1.3.m3.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p1.3.m3.2.2.1.1.1.1.2">𝑡</ci><ci id="S3.SS2.p1.3.m3.2.2.1.1.1.1.3.cmml" xref="S3.SS2.p1.3.m3.2.2.1.1.1.1.3">⋆</ci></apply></apply><list id="S3.SS2.p1.3.m3.4.4.3.3.cmml" xref="S3.SS2.p1.3.m3.4.4.3.2"><apply id="S3.SS2.p1.3.m3.3.3.2.1.1.cmml" xref="S3.SS2.p1.3.m3.3.3.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.3.3.2.1.1.1.cmml" xref="S3.SS2.p1.3.m3.3.3.2.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.3.3.2.1.1.2.cmml" xref="S3.SS2.p1.3.m3.3.3.2.1.1.2">𝑡</ci><cn id="S3.SS2.p1.3.m3.3.3.2.1.1.3.cmml" type="integer" xref="S3.SS2.p1.3.m3.3.3.2.1.1.3">1</cn></apply><ci id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">…</ci><apply id="S3.SS2.p1.3.m3.4.4.3.2.2.cmml" xref="S3.SS2.p1.3.m3.4.4.3.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.4.4.3.2.2.1.cmml" xref="S3.SS2.p1.3.m3.4.4.3.2.2">subscript</csymbol><ci id="S3.SS2.p1.3.m3.4.4.3.2.2.2.cmml" xref="S3.SS2.p1.3.m3.4.4.3.2.2.2">𝑡</ci><ci id="S3.SS2.p1.3.m3.4.4.3.2.2.3.cmml" xref="S3.SS2.p1.3.m3.4.4.3.2.2.3">𝑛</ci></apply></list></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.4c">\tau(t^{\star})=[t_{1},\dots,t_{n}]</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.4d">italic_τ ( italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ) = [ italic_t start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_t start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ]</annotation></semantics></math>, we require a small corpus of example sequences <math alttext="s\in\mathcal{S}" class="ltx_Math" display="inline" id="S3.SS2.p1.4.m4.1"><semantics id="S3.SS2.p1.4.m4.1a"><mrow id="S3.SS2.p1.4.m4.1.1" xref="S3.SS2.p1.4.m4.1.1.cmml"><mi id="S3.SS2.p1.4.m4.1.1.2" xref="S3.SS2.p1.4.m4.1.1.2.cmml">s</mi><mo id="S3.SS2.p1.4.m4.1.1.1" xref="S3.SS2.p1.4.m4.1.1.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.4.m4.1.1.3" xref="S3.SS2.p1.4.m4.1.1.3.cmml">𝒮</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.4.m4.1b"><apply id="S3.SS2.p1.4.m4.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1"><in id="S3.SS2.p1.4.m4.1.1.1.cmml" xref="S3.SS2.p1.4.m4.1.1.1"></in><ci id="S3.SS2.p1.4.m4.1.1.2.cmml" xref="S3.SS2.p1.4.m4.1.1.2">𝑠</ci><ci id="S3.SS2.p1.4.m4.1.1.3.cmml" xref="S3.SS2.p1.4.m4.1.1.3">𝒮</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.4.m4.1c">s\in\mathcal{S}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.4.m4.1d">italic_s ∈ caligraphic_S</annotation></semantics></math> each containing the new token string <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS2.p1.5.m5.1"><semantics id="S3.SS2.p1.5.m5.1a"><msup id="S3.SS2.p1.5.m5.1.1" xref="S3.SS2.p1.5.m5.1.1.cmml"><mi id="S3.SS2.p1.5.m5.1.1.2" xref="S3.SS2.p1.5.m5.1.1.2.cmml">t</mi><mo id="S3.SS2.p1.5.m5.1.1.3" xref="S3.SS2.p1.5.m5.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.5.m5.1b"><apply id="S3.SS2.p1.5.m5.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.5.m5.1.1.1.cmml" xref="S3.SS2.p1.5.m5.1.1">superscript</csymbol><ci id="S3.SS2.p1.5.m5.1.1.2.cmml" xref="S3.SS2.p1.5.m5.1.1.2">𝑡</ci><ci id="S3.SS2.p1.5.m5.1.1.3.cmml" xref="S3.SS2.p1.5.m5.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.5.m5.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.5.m5.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>. We denote the new tokenizer, which includes <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS2.p1.6.m6.1"><semantics id="S3.SS2.p1.6.m6.1a"><msup id="S3.SS2.p1.6.m6.1.1" xref="S3.SS2.p1.6.m6.1.1.cmml"><mi id="S3.SS2.p1.6.m6.1.1.2" xref="S3.SS2.p1.6.m6.1.1.2.cmml">t</mi><mo id="S3.SS2.p1.6.m6.1.1.3" xref="S3.SS2.p1.6.m6.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.6.m6.1b"><apply id="S3.SS2.p1.6.m6.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.6.m6.1.1.1.cmml" xref="S3.SS2.p1.6.m6.1.1">superscript</csymbol><ci id="S3.SS2.p1.6.m6.1.1.2.cmml" xref="S3.SS2.p1.6.m6.1.1.2">𝑡</ci><ci id="S3.SS2.p1.6.m6.1.1.3.cmml" xref="S3.SS2.p1.6.m6.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.6.m6.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.6.m6.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>, as <math alttext="{\tau^{\star}}" class="ltx_Math" display="inline" id="S3.SS2.p1.7.m7.1"><semantics id="S3.SS2.p1.7.m7.1a"><msup id="S3.SS2.p1.7.m7.1.1" xref="S3.SS2.p1.7.m7.1.1.cmml"><mi id="S3.SS2.p1.7.m7.1.1.2" xref="S3.SS2.p1.7.m7.1.1.2.cmml">τ</mi><mo id="S3.SS2.p1.7.m7.1.1.3" xref="S3.SS2.p1.7.m7.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.7.m7.1b"><apply id="S3.SS2.p1.7.m7.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.7.m7.1.1.1.cmml" xref="S3.SS2.p1.7.m7.1.1">superscript</csymbol><ci id="S3.SS2.p1.7.m7.1.1.2.cmml" xref="S3.SS2.p1.7.m7.1.1.2">𝜏</ci><ci id="S3.SS2.p1.7.m7.1.1.3.cmml" xref="S3.SS2.p1.7.m7.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.7.m7.1c">{\tau^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.7.m7.1d">italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>, and the tokenization of <math alttext="s" class="ltx_Math" display="inline" id="S3.SS2.p1.8.m8.1"><semantics id="S3.SS2.p1.8.m8.1a"><mi id="S3.SS2.p1.8.m8.1.1" xref="S3.SS2.p1.8.m8.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.8.m8.1b"><ci id="S3.SS2.p1.8.m8.1.1.cmml" xref="S3.SS2.p1.8.m8.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.8.m8.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.8.m8.1d">italic_s</annotation></semantics></math> using <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS2.p1.9.m9.1"><semantics id="S3.SS2.p1.9.m9.1a"><mi id="S3.SS2.p1.9.m9.1.1" xref="S3.SS2.p1.9.m9.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.9.m9.1b"><ci id="S3.SS2.p1.9.m9.1.1.cmml" xref="S3.SS2.p1.9.m9.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.9.m9.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.9.m9.1d">italic_τ</annotation></semantics></math> or <math alttext="\tau^{\star}" class="ltx_Math" display="inline" id="S3.SS2.p1.10.m10.1"><semantics id="S3.SS2.p1.10.m10.1a"><msup id="S3.SS2.p1.10.m10.1.1" xref="S3.SS2.p1.10.m10.1.1.cmml"><mi id="S3.SS2.p1.10.m10.1.1.2" xref="S3.SS2.p1.10.m10.1.1.2.cmml">τ</mi><mo id="S3.SS2.p1.10.m10.1.1.3" xref="S3.SS2.p1.10.m10.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.10.m10.1b"><apply id="S3.SS2.p1.10.m10.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.10.m10.1.1.1.cmml" xref="S3.SS2.p1.10.m10.1.1">superscript</csymbol><ci id="S3.SS2.p1.10.m10.1.1.2.cmml" xref="S3.SS2.p1.10.m10.1.1.2">𝜏</ci><ci id="S3.SS2.p1.10.m10.1.1.3.cmml" xref="S3.SS2.p1.10.m10.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.10.m10.1c">\tau^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.10.m10.1d">italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> as <math alttext="s_{\tau}" class="ltx_Math" display="inline" id="S3.SS2.p1.11.m11.1"><semantics id="S3.SS2.p1.11.m11.1a"><msub id="S3.SS2.p1.11.m11.1.1" xref="S3.SS2.p1.11.m11.1.1.cmml"><mi id="S3.SS2.p1.11.m11.1.1.2" xref="S3.SS2.p1.11.m11.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.11.m11.1.1.3" xref="S3.SS2.p1.11.m11.1.1.3.cmml">τ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.11.m11.1b"><apply id="S3.SS2.p1.11.m11.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.11.m11.1.1.1.cmml" xref="S3.SS2.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS2.p1.11.m11.1.1.2.cmml" xref="S3.SS2.p1.11.m11.1.1.2">𝑠</ci><ci id="S3.SS2.p1.11.m11.1.1.3.cmml" xref="S3.SS2.p1.11.m11.1.1.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.11.m11.1c">s_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.11.m11.1d">italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="s_{\tau^{\star}}" class="ltx_Math" display="inline" id="S3.SS2.p1.12.m12.1"><semantics id="S3.SS2.p1.12.m12.1a"><msub id="S3.SS2.p1.12.m12.1.1" xref="S3.SS2.p1.12.m12.1.1.cmml"><mi id="S3.SS2.p1.12.m12.1.1.2" xref="S3.SS2.p1.12.m12.1.1.2.cmml">s</mi><msup id="S3.SS2.p1.12.m12.1.1.3" xref="S3.SS2.p1.12.m12.1.1.3.cmml"><mi id="S3.SS2.p1.12.m12.1.1.3.2" xref="S3.SS2.p1.12.m12.1.1.3.2.cmml">τ</mi><mo id="S3.SS2.p1.12.m12.1.1.3.3" xref="S3.SS2.p1.12.m12.1.1.3.3.cmml">⋆</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.12.m12.1b"><apply id="S3.SS2.p1.12.m12.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m12.1.1.1.cmml" xref="S3.SS2.p1.12.m12.1.1">subscript</csymbol><ci id="S3.SS2.p1.12.m12.1.1.2.cmml" xref="S3.SS2.p1.12.m12.1.1.2">𝑠</ci><apply id="S3.SS2.p1.12.m12.1.1.3.cmml" xref="S3.SS2.p1.12.m12.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.12.m12.1.1.3.1.cmml" xref="S3.SS2.p1.12.m12.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.12.m12.1.1.3.2.cmml" xref="S3.SS2.p1.12.m12.1.1.3.2">𝜏</ci><ci id="S3.SS2.p1.12.m12.1.1.3.3.cmml" xref="S3.SS2.p1.12.m12.1.1.3.3">⋆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.12.m12.1c">s_{\tau^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.12.m12.1d">italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.
The target for learning the embedding of <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS2.p1.13.m13.1"><semantics id="S3.SS2.p1.13.m13.1a"><msup id="S3.SS2.p1.13.m13.1.1" xref="S3.SS2.p1.13.m13.1.1.cmml"><mi id="S3.SS2.p1.13.m13.1.1.2" xref="S3.SS2.p1.13.m13.1.1.2.cmml">t</mi><mo id="S3.SS2.p1.13.m13.1.1.3" xref="S3.SS2.p1.13.m13.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.13.m13.1b"><apply id="S3.SS2.p1.13.m13.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.13.m13.1.1.1.cmml" xref="S3.SS2.p1.13.m13.1.1">superscript</csymbol><ci id="S3.SS2.p1.13.m13.1.1.2.cmml" xref="S3.SS2.p1.13.m13.1.1.2">𝑡</ci><ci id="S3.SS2.p1.13.m13.1.1.3.cmml" xref="S3.SS2.p1.13.m13.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.13.m13.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.13.m13.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> are the hidden states at a specified layer <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p1.14.m14.1"><semantics id="S3.SS2.p1.14.m14.1a"><mi id="S3.SS2.p1.14.m14.1.1" xref="S3.SS2.p1.14.m14.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.14.m14.1b"><ci id="S3.SS2.p1.14.m14.1.1.cmml" xref="S3.SS2.p1.14.m14.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.14.m14.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.14.m14.1d">italic_l</annotation></semantics></math> produced by the language model when processing <math alttext="s_{\tau}" class="ltx_Math" display="inline" id="S3.SS2.p1.15.m15.1"><semantics id="S3.SS2.p1.15.m15.1a"><msub id="S3.SS2.p1.15.m15.1.1" xref="S3.SS2.p1.15.m15.1.1.cmml"><mi id="S3.SS2.p1.15.m15.1.1.2" xref="S3.SS2.p1.15.m15.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.15.m15.1.1.3" xref="S3.SS2.p1.15.m15.1.1.3.cmml">τ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.15.m15.1b"><apply id="S3.SS2.p1.15.m15.1.1.cmml" xref="S3.SS2.p1.15.m15.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.15.m15.1.1.1.cmml" xref="S3.SS2.p1.15.m15.1.1">subscript</csymbol><ci id="S3.SS2.p1.15.m15.1.1.2.cmml" xref="S3.SS2.p1.15.m15.1.1.2">𝑠</ci><ci id="S3.SS2.p1.15.m15.1.1.3.cmml" xref="S3.SS2.p1.15.m15.1.1.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.15.m15.1c">s_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.15.m15.1d">italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT</annotation></semantics></math>, which we denote as <math alttext="\mathcal{H}^{(l)}\mathrm{(}s_{\tau}\mathrm{)}" class="ltx_Math" display="inline" id="S3.SS2.p1.16.m16.2"><semantics id="S3.SS2.p1.16.m16.2a"><mrow id="S3.SS2.p1.16.m16.2.2" xref="S3.SS2.p1.16.m16.2.2.cmml"><msup id="S3.SS2.p1.16.m16.2.2.3" xref="S3.SS2.p1.16.m16.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.16.m16.2.2.3.2" xref="S3.SS2.p1.16.m16.2.2.3.2.cmml">ℋ</mi><mrow id="S3.SS2.p1.16.m16.1.1.1.3" xref="S3.SS2.p1.16.m16.2.2.3.cmml"><mo id="S3.SS2.p1.16.m16.1.1.1.3.1" stretchy="false" xref="S3.SS2.p1.16.m16.2.2.3.cmml">(</mo><mi id="S3.SS2.p1.16.m16.1.1.1.1" xref="S3.SS2.p1.16.m16.1.1.1.1.cmml">l</mi><mo id="S3.SS2.p1.16.m16.1.1.1.3.2" stretchy="false" xref="S3.SS2.p1.16.m16.2.2.3.cmml">)</mo></mrow></msup><mo id="S3.SS2.p1.16.m16.2.2.2" xref="S3.SS2.p1.16.m16.2.2.2.cmml">⁢</mo><mrow id="S3.SS2.p1.16.m16.2.2.1.1" xref="S3.SS2.p1.16.m16.2.2.1.1.1.cmml"><mo id="S3.SS2.p1.16.m16.2.2.1.1.2" stretchy="false" xref="S3.SS2.p1.16.m16.2.2.1.1.1.cmml">(</mo><msub id="S3.SS2.p1.16.m16.2.2.1.1.1" xref="S3.SS2.p1.16.m16.2.2.1.1.1.cmml"><mi id="S3.SS2.p1.16.m16.2.2.1.1.1.2" xref="S3.SS2.p1.16.m16.2.2.1.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.16.m16.2.2.1.1.1.3" xref="S3.SS2.p1.16.m16.2.2.1.1.1.3.cmml">τ</mi></msub><mo id="S3.SS2.p1.16.m16.2.2.1.1.3" stretchy="false" xref="S3.SS2.p1.16.m16.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.16.m16.2b"><apply id="S3.SS2.p1.16.m16.2.2.cmml" xref="S3.SS2.p1.16.m16.2.2"><times id="S3.SS2.p1.16.m16.2.2.2.cmml" xref="S3.SS2.p1.16.m16.2.2.2"></times><apply id="S3.SS2.p1.16.m16.2.2.3.cmml" xref="S3.SS2.p1.16.m16.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.16.m16.2.2.3.1.cmml" xref="S3.SS2.p1.16.m16.2.2.3">superscript</csymbol><ci id="S3.SS2.p1.16.m16.2.2.3.2.cmml" xref="S3.SS2.p1.16.m16.2.2.3.2">ℋ</ci><ci id="S3.SS2.p1.16.m16.1.1.1.1.cmml" xref="S3.SS2.p1.16.m16.1.1.1.1">𝑙</ci></apply><apply id="S3.SS2.p1.16.m16.2.2.1.1.1.cmml" xref="S3.SS2.p1.16.m16.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.16.m16.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.16.m16.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p1.16.m16.2.2.1.1.1.2.cmml" xref="S3.SS2.p1.16.m16.2.2.1.1.1.2">𝑠</ci><ci id="S3.SS2.p1.16.m16.2.2.1.1.1.3.cmml" xref="S3.SS2.p1.16.m16.2.2.1.1.1.3">𝜏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.16.m16.2c">\mathcal{H}^{(l)}\mathrm{(}s_{\tau}\mathrm{)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.16.m16.2d">caligraphic_H start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT )</annotation></semantics></math>.
We wish to find an embedding <math alttext="\mathbf{e^{\star}}" class="ltx_Math" display="inline" id="S3.SS2.p1.17.m17.1"><semantics id="S3.SS2.p1.17.m17.1a"><msup id="S3.SS2.p1.17.m17.1.1" xref="S3.SS2.p1.17.m17.1.1.cmml"><mi id="S3.SS2.p1.17.m17.1.1.2" xref="S3.SS2.p1.17.m17.1.1.2.cmml">𝐞</mi><mo id="S3.SS2.p1.17.m17.1.1.3" xref="S3.SS2.p1.17.m17.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.17.m17.1b"><apply id="S3.SS2.p1.17.m17.1.1.cmml" xref="S3.SS2.p1.17.m17.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.17.m17.1.1.1.cmml" xref="S3.SS2.p1.17.m17.1.1">superscript</csymbol><ci id="S3.SS2.p1.17.m17.1.1.2.cmml" xref="S3.SS2.p1.17.m17.1.1.2">𝐞</ci><ci id="S3.SS2.p1.17.m17.1.1.3.cmml" xref="S3.SS2.p1.17.m17.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.17.m17.1c">\mathbf{e^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.17.m17.1d">bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> for a new token <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS2.p1.18.m18.1"><semantics id="S3.SS2.p1.18.m18.1a"><msup id="S3.SS2.p1.18.m18.1.1" xref="S3.SS2.p1.18.m18.1.1.cmml"><mi id="S3.SS2.p1.18.m18.1.1.2" xref="S3.SS2.p1.18.m18.1.1.2.cmml">t</mi><mo id="S3.SS2.p1.18.m18.1.1.3" xref="S3.SS2.p1.18.m18.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.18.m18.1b"><apply id="S3.SS2.p1.18.m18.1.1.cmml" xref="S3.SS2.p1.18.m18.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.18.m18.1.1.1.cmml" xref="S3.SS2.p1.18.m18.1.1">superscript</csymbol><ci id="S3.SS2.p1.18.m18.1.1.2.cmml" xref="S3.SS2.p1.18.m18.1.1.2">𝑡</ci><ci id="S3.SS2.p1.18.m18.1.1.3.cmml" xref="S3.SS2.p1.18.m18.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.18.m18.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.18.m18.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> such that hidden states produced by processing the input sequence <math alttext="s_{\tau^{\star}}" class="ltx_Math" display="inline" id="S3.SS2.p1.19.m19.1"><semantics id="S3.SS2.p1.19.m19.1a"><msub id="S3.SS2.p1.19.m19.1.1" xref="S3.SS2.p1.19.m19.1.1.cmml"><mi id="S3.SS2.p1.19.m19.1.1.2" xref="S3.SS2.p1.19.m19.1.1.2.cmml">s</mi><msup id="S3.SS2.p1.19.m19.1.1.3" xref="S3.SS2.p1.19.m19.1.1.3.cmml"><mi id="S3.SS2.p1.19.m19.1.1.3.2" xref="S3.SS2.p1.19.m19.1.1.3.2.cmml">τ</mi><mo id="S3.SS2.p1.19.m19.1.1.3.3" xref="S3.SS2.p1.19.m19.1.1.3.3.cmml">⋆</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.19.m19.1b"><apply id="S3.SS2.p1.19.m19.1.1.cmml" xref="S3.SS2.p1.19.m19.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.19.m19.1.1.1.cmml" xref="S3.SS2.p1.19.m19.1.1">subscript</csymbol><ci id="S3.SS2.p1.19.m19.1.1.2.cmml" xref="S3.SS2.p1.19.m19.1.1.2">𝑠</ci><apply id="S3.SS2.p1.19.m19.1.1.3.cmml" xref="S3.SS2.p1.19.m19.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.19.m19.1.1.3.1.cmml" xref="S3.SS2.p1.19.m19.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.19.m19.1.1.3.2.cmml" xref="S3.SS2.p1.19.m19.1.1.3.2">𝜏</ci><ci id="S3.SS2.p1.19.m19.1.1.3.3.cmml" xref="S3.SS2.p1.19.m19.1.1.3.3">⋆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.19.m19.1c">s_{\tau^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.19.m19.1d">italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> instead of <math alttext="s_{\tau}" class="ltx_Math" display="inline" id="S3.SS2.p1.20.m20.1"><semantics id="S3.SS2.p1.20.m20.1a"><msub id="S3.SS2.p1.20.m20.1.1" xref="S3.SS2.p1.20.m20.1.1.cmml"><mi id="S3.SS2.p1.20.m20.1.1.2" xref="S3.SS2.p1.20.m20.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.20.m20.1.1.3" xref="S3.SS2.p1.20.m20.1.1.3.cmml">τ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.20.m20.1b"><apply id="S3.SS2.p1.20.m20.1.1.cmml" xref="S3.SS2.p1.20.m20.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.20.m20.1.1.1.cmml" xref="S3.SS2.p1.20.m20.1.1">subscript</csymbol><ci id="S3.SS2.p1.20.m20.1.1.2.cmml" xref="S3.SS2.p1.20.m20.1.1.2">𝑠</ci><ci id="S3.SS2.p1.20.m20.1.1.3.cmml" xref="S3.SS2.p1.20.m20.1.1.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.20.m20.1c">s_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.20.m20.1d">italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT</annotation></semantics></math> are similar to our target <math alttext="\mathcal{H}^{(l)}\mathrm{(}s_{\tau}\mathrm{)}" class="ltx_Math" display="inline" id="S3.SS2.p1.21.m21.2"><semantics id="S3.SS2.p1.21.m21.2a"><mrow id="S3.SS2.p1.21.m21.2.2" xref="S3.SS2.p1.21.m21.2.2.cmml"><msup id="S3.SS2.p1.21.m21.2.2.3" xref="S3.SS2.p1.21.m21.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.21.m21.2.2.3.2" xref="S3.SS2.p1.21.m21.2.2.3.2.cmml">ℋ</mi><mrow id="S3.SS2.p1.21.m21.1.1.1.3" xref="S3.SS2.p1.21.m21.2.2.3.cmml"><mo id="S3.SS2.p1.21.m21.1.1.1.3.1" stretchy="false" xref="S3.SS2.p1.21.m21.2.2.3.cmml">(</mo><mi id="S3.SS2.p1.21.m21.1.1.1.1" xref="S3.SS2.p1.21.m21.1.1.1.1.cmml">l</mi><mo id="S3.SS2.p1.21.m21.1.1.1.3.2" stretchy="false" xref="S3.SS2.p1.21.m21.2.2.3.cmml">)</mo></mrow></msup><mo id="S3.SS2.p1.21.m21.2.2.2" xref="S3.SS2.p1.21.m21.2.2.2.cmml">⁢</mo><mrow id="S3.SS2.p1.21.m21.2.2.1.1" xref="S3.SS2.p1.21.m21.2.2.1.1.1.cmml"><mo id="S3.SS2.p1.21.m21.2.2.1.1.2" stretchy="false" xref="S3.SS2.p1.21.m21.2.2.1.1.1.cmml">(</mo><msub id="S3.SS2.p1.21.m21.2.2.1.1.1" xref="S3.SS2.p1.21.m21.2.2.1.1.1.cmml"><mi id="S3.SS2.p1.21.m21.2.2.1.1.1.2" xref="S3.SS2.p1.21.m21.2.2.1.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.21.m21.2.2.1.1.1.3" xref="S3.SS2.p1.21.m21.2.2.1.1.1.3.cmml">τ</mi></msub><mo id="S3.SS2.p1.21.m21.2.2.1.1.3" stretchy="false" xref="S3.SS2.p1.21.m21.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.21.m21.2b"><apply id="S3.SS2.p1.21.m21.2.2.cmml" xref="S3.SS2.p1.21.m21.2.2"><times id="S3.SS2.p1.21.m21.2.2.2.cmml" xref="S3.SS2.p1.21.m21.2.2.2"></times><apply id="S3.SS2.p1.21.m21.2.2.3.cmml" xref="S3.SS2.p1.21.m21.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.21.m21.2.2.3.1.cmml" xref="S3.SS2.p1.21.m21.2.2.3">superscript</csymbol><ci id="S3.SS2.p1.21.m21.2.2.3.2.cmml" xref="S3.SS2.p1.21.m21.2.2.3.2">ℋ</ci><ci id="S3.SS2.p1.21.m21.1.1.1.1.cmml" xref="S3.SS2.p1.21.m21.1.1.1.1">𝑙</ci></apply><apply id="S3.SS2.p1.21.m21.2.2.1.1.1.cmml" xref="S3.SS2.p1.21.m21.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.21.m21.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.21.m21.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p1.21.m21.2.2.1.1.1.2.cmml" xref="S3.SS2.p1.21.m21.2.2.1.1.1.2">𝑠</ci><ci id="S3.SS2.p1.21.m21.2.2.1.1.1.3.cmml" xref="S3.SS2.p1.21.m21.2.2.1.1.1.3">𝜏</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.21.m21.2c">\mathcal{H}^{(l)}\mathrm{(}s_{\tau}\mathrm{)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.21.m21.2d">caligraphic_H start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT )</annotation></semantics></math>.
We denote these new hidden states using our new embedding <math alttext="\mathbf{e^{\star}}" class="ltx_Math" display="inline" id="S3.SS2.p1.22.m22.1"><semantics id="S3.SS2.p1.22.m22.1a"><msup id="S3.SS2.p1.22.m22.1.1" xref="S3.SS2.p1.22.m22.1.1.cmml"><mi id="S3.SS2.p1.22.m22.1.1.2" xref="S3.SS2.p1.22.m22.1.1.2.cmml">𝐞</mi><mo id="S3.SS2.p1.22.m22.1.1.3" xref="S3.SS2.p1.22.m22.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.22.m22.1b"><apply id="S3.SS2.p1.22.m22.1.1.cmml" xref="S3.SS2.p1.22.m22.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.22.m22.1.1.1.cmml" xref="S3.SS2.p1.22.m22.1.1">superscript</csymbol><ci id="S3.SS2.p1.22.m22.1.1.2.cmml" xref="S3.SS2.p1.22.m22.1.1.2">𝐞</ci><ci id="S3.SS2.p1.22.m22.1.1.3.cmml" xref="S3.SS2.p1.22.m22.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.22.m22.1c">\mathbf{e^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.22.m22.1d">bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math> as <math alttext="\mathcal{H}_{\mathbf{e^{\star}}}^{(l)}\mathrm{(}s_{\tau^{\star}}\mathrm{)}" class="ltx_Math" display="inline" id="S3.SS2.p1.23.m23.2"><semantics id="S3.SS2.p1.23.m23.2a"><mrow id="S3.SS2.p1.23.m23.2.2" xref="S3.SS2.p1.23.m23.2.2.cmml"><msubsup id="S3.SS2.p1.23.m23.2.2.3" xref="S3.SS2.p1.23.m23.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.23.m23.2.2.3.2.2" xref="S3.SS2.p1.23.m23.2.2.3.2.2.cmml">ℋ</mi><msup id="S3.SS2.p1.23.m23.2.2.3.2.3" xref="S3.SS2.p1.23.m23.2.2.3.2.3.cmml"><mi id="S3.SS2.p1.23.m23.2.2.3.2.3.2" xref="S3.SS2.p1.23.m23.2.2.3.2.3.2.cmml">𝐞</mi><mo id="S3.SS2.p1.23.m23.2.2.3.2.3.3" xref="S3.SS2.p1.23.m23.2.2.3.2.3.3.cmml">⋆</mo></msup><mrow id="S3.SS2.p1.23.m23.1.1.1.3" xref="S3.SS2.p1.23.m23.2.2.3.cmml"><mo id="S3.SS2.p1.23.m23.1.1.1.3.1" stretchy="false" xref="S3.SS2.p1.23.m23.2.2.3.cmml">(</mo><mi id="S3.SS2.p1.23.m23.1.1.1.1" xref="S3.SS2.p1.23.m23.1.1.1.1.cmml">l</mi><mo id="S3.SS2.p1.23.m23.1.1.1.3.2" stretchy="false" xref="S3.SS2.p1.23.m23.2.2.3.cmml">)</mo></mrow></msubsup><mo id="S3.SS2.p1.23.m23.2.2.2" xref="S3.SS2.p1.23.m23.2.2.2.cmml">⁢</mo><mrow id="S3.SS2.p1.23.m23.2.2.1.1" xref="S3.SS2.p1.23.m23.2.2.1.1.1.cmml"><mo id="S3.SS2.p1.23.m23.2.2.1.1.2" stretchy="false" xref="S3.SS2.p1.23.m23.2.2.1.1.1.cmml">(</mo><msub id="S3.SS2.p1.23.m23.2.2.1.1.1" xref="S3.SS2.p1.23.m23.2.2.1.1.1.cmml"><mi id="S3.SS2.p1.23.m23.2.2.1.1.1.2" xref="S3.SS2.p1.23.m23.2.2.1.1.1.2.cmml">s</mi><msup id="S3.SS2.p1.23.m23.2.2.1.1.1.3" xref="S3.SS2.p1.23.m23.2.2.1.1.1.3.cmml"><mi id="S3.SS2.p1.23.m23.2.2.1.1.1.3.2" xref="S3.SS2.p1.23.m23.2.2.1.1.1.3.2.cmml">τ</mi><mo id="S3.SS2.p1.23.m23.2.2.1.1.1.3.3" xref="S3.SS2.p1.23.m23.2.2.1.1.1.3.3.cmml">⋆</mo></msup></msub><mo id="S3.SS2.p1.23.m23.2.2.1.1.3" stretchy="false" xref="S3.SS2.p1.23.m23.2.2.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.23.m23.2b"><apply id="S3.SS2.p1.23.m23.2.2.cmml" xref="S3.SS2.p1.23.m23.2.2"><times id="S3.SS2.p1.23.m23.2.2.2.cmml" xref="S3.SS2.p1.23.m23.2.2.2"></times><apply id="S3.SS2.p1.23.m23.2.2.3.cmml" xref="S3.SS2.p1.23.m23.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.23.m23.2.2.3.1.cmml" xref="S3.SS2.p1.23.m23.2.2.3">superscript</csymbol><apply id="S3.SS2.p1.23.m23.2.2.3.2.cmml" xref="S3.SS2.p1.23.m23.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.23.m23.2.2.3.2.1.cmml" xref="S3.SS2.p1.23.m23.2.2.3">subscript</csymbol><ci id="S3.SS2.p1.23.m23.2.2.3.2.2.cmml" xref="S3.SS2.p1.23.m23.2.2.3.2.2">ℋ</ci><apply id="S3.SS2.p1.23.m23.2.2.3.2.3.cmml" xref="S3.SS2.p1.23.m23.2.2.3.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.23.m23.2.2.3.2.3.1.cmml" xref="S3.SS2.p1.23.m23.2.2.3.2.3">superscript</csymbol><ci id="S3.SS2.p1.23.m23.2.2.3.2.3.2.cmml" xref="S3.SS2.p1.23.m23.2.2.3.2.3.2">𝐞</ci><ci id="S3.SS2.p1.23.m23.2.2.3.2.3.3.cmml" xref="S3.SS2.p1.23.m23.2.2.3.2.3.3">⋆</ci></apply></apply><ci id="S3.SS2.p1.23.m23.1.1.1.1.cmml" xref="S3.SS2.p1.23.m23.1.1.1.1">𝑙</ci></apply><apply id="S3.SS2.p1.23.m23.2.2.1.1.1.cmml" xref="S3.SS2.p1.23.m23.2.2.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.23.m23.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.23.m23.2.2.1.1">subscript</csymbol><ci id="S3.SS2.p1.23.m23.2.2.1.1.1.2.cmml" xref="S3.SS2.p1.23.m23.2.2.1.1.1.2">𝑠</ci><apply id="S3.SS2.p1.23.m23.2.2.1.1.1.3.cmml" xref="S3.SS2.p1.23.m23.2.2.1.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.23.m23.2.2.1.1.1.3.1.cmml" xref="S3.SS2.p1.23.m23.2.2.1.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.23.m23.2.2.1.1.1.3.2.cmml" xref="S3.SS2.p1.23.m23.2.2.1.1.1.3.2">𝜏</ci><ci id="S3.SS2.p1.23.m23.2.2.1.1.1.3.3.cmml" xref="S3.SS2.p1.23.m23.2.2.1.1.1.3.3">⋆</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.23.m23.2c">\mathcal{H}_{\mathbf{e^{\star}}}^{(l)}\mathrm{(}s_{\tau^{\star}}\mathrm{)}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.23.m23.2d">caligraphic_H start_POSTSUBSCRIPT bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math>.
As the two input sequence indices are not aligned due to replacing subtokens by a single new token, we define a set of mapped positions <math alttext="(i,j)\in\mathcal{M}({s_{\tau},s_{\tau^{\star}}})" class="ltx_Math" display="inline" id="S3.SS2.p1.24.m24.4"><semantics id="S3.SS2.p1.24.m24.4a"><mrow id="S3.SS2.p1.24.m24.4.4" xref="S3.SS2.p1.24.m24.4.4.cmml"><mrow id="S3.SS2.p1.24.m24.4.4.4.2" xref="S3.SS2.p1.24.m24.4.4.4.1.cmml"><mo id="S3.SS2.p1.24.m24.4.4.4.2.1" stretchy="false" xref="S3.SS2.p1.24.m24.4.4.4.1.cmml">(</mo><mi id="S3.SS2.p1.24.m24.1.1" xref="S3.SS2.p1.24.m24.1.1.cmml">i</mi><mo id="S3.SS2.p1.24.m24.4.4.4.2.2" xref="S3.SS2.p1.24.m24.4.4.4.1.cmml">,</mo><mi id="S3.SS2.p1.24.m24.2.2" xref="S3.SS2.p1.24.m24.2.2.cmml">j</mi><mo id="S3.SS2.p1.24.m24.4.4.4.2.3" stretchy="false" xref="S3.SS2.p1.24.m24.4.4.4.1.cmml">)</mo></mrow><mo id="S3.SS2.p1.24.m24.4.4.3" xref="S3.SS2.p1.24.m24.4.4.3.cmml">∈</mo><mrow id="S3.SS2.p1.24.m24.4.4.2" xref="S3.SS2.p1.24.m24.4.4.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.24.m24.4.4.2.4" xref="S3.SS2.p1.24.m24.4.4.2.4.cmml">ℳ</mi><mo id="S3.SS2.p1.24.m24.4.4.2.3" xref="S3.SS2.p1.24.m24.4.4.2.3.cmml">⁢</mo><mrow id="S3.SS2.p1.24.m24.4.4.2.2.2" xref="S3.SS2.p1.24.m24.4.4.2.2.3.cmml"><mo id="S3.SS2.p1.24.m24.4.4.2.2.2.3" stretchy="false" xref="S3.SS2.p1.24.m24.4.4.2.2.3.cmml">(</mo><msub id="S3.SS2.p1.24.m24.3.3.1.1.1.1" xref="S3.SS2.p1.24.m24.3.3.1.1.1.1.cmml"><mi id="S3.SS2.p1.24.m24.3.3.1.1.1.1.2" xref="S3.SS2.p1.24.m24.3.3.1.1.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.24.m24.3.3.1.1.1.1.3" xref="S3.SS2.p1.24.m24.3.3.1.1.1.1.3.cmml">τ</mi></msub><mo id="S3.SS2.p1.24.m24.4.4.2.2.2.4" xref="S3.SS2.p1.24.m24.4.4.2.2.3.cmml">,</mo><msub id="S3.SS2.p1.24.m24.4.4.2.2.2.2" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.cmml"><mi id="S3.SS2.p1.24.m24.4.4.2.2.2.2.2" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.2.cmml">s</mi><msup id="S3.SS2.p1.24.m24.4.4.2.2.2.2.3" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.cmml"><mi id="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.2" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.2.cmml">τ</mi><mo id="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.3" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.3.cmml">⋆</mo></msup></msub><mo id="S3.SS2.p1.24.m24.4.4.2.2.2.5" stretchy="false" xref="S3.SS2.p1.24.m24.4.4.2.2.3.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.24.m24.4b"><apply id="S3.SS2.p1.24.m24.4.4.cmml" xref="S3.SS2.p1.24.m24.4.4"><in id="S3.SS2.p1.24.m24.4.4.3.cmml" xref="S3.SS2.p1.24.m24.4.4.3"></in><interval closure="open" id="S3.SS2.p1.24.m24.4.4.4.1.cmml" xref="S3.SS2.p1.24.m24.4.4.4.2"><ci id="S3.SS2.p1.24.m24.1.1.cmml" xref="S3.SS2.p1.24.m24.1.1">𝑖</ci><ci id="S3.SS2.p1.24.m24.2.2.cmml" xref="S3.SS2.p1.24.m24.2.2">𝑗</ci></interval><apply id="S3.SS2.p1.24.m24.4.4.2.cmml" xref="S3.SS2.p1.24.m24.4.4.2"><times id="S3.SS2.p1.24.m24.4.4.2.3.cmml" xref="S3.SS2.p1.24.m24.4.4.2.3"></times><ci id="S3.SS2.p1.24.m24.4.4.2.4.cmml" xref="S3.SS2.p1.24.m24.4.4.2.4">ℳ</ci><interval closure="open" id="S3.SS2.p1.24.m24.4.4.2.2.3.cmml" xref="S3.SS2.p1.24.m24.4.4.2.2.2"><apply id="S3.SS2.p1.24.m24.3.3.1.1.1.1.cmml" xref="S3.SS2.p1.24.m24.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.24.m24.3.3.1.1.1.1.1.cmml" xref="S3.SS2.p1.24.m24.3.3.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.24.m24.3.3.1.1.1.1.2.cmml" xref="S3.SS2.p1.24.m24.3.3.1.1.1.1.2">𝑠</ci><ci id="S3.SS2.p1.24.m24.3.3.1.1.1.1.3.cmml" xref="S3.SS2.p1.24.m24.3.3.1.1.1.1.3">𝜏</ci></apply><apply id="S3.SS2.p1.24.m24.4.4.2.2.2.2.cmml" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.24.m24.4.4.2.2.2.2.1.cmml" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.24.m24.4.4.2.2.2.2.2.cmml" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.2">𝑠</ci><apply id="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.cmml" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.1.cmml" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.3">superscript</csymbol><ci id="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.2.cmml" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.2">𝜏</ci><ci id="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.3.cmml" xref="S3.SS2.p1.24.m24.4.4.2.2.2.2.3.3">⋆</ci></apply></apply></interval></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.24.m24.4c">(i,j)\in\mathcal{M}({s_{\tau},s_{\tau^{\star}}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.24.m24.4d">( italic_i , italic_j ) ∈ caligraphic_M ( italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math>, where <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.25.m25.1"><semantics id="S3.SS2.p1.25.m25.1a"><mi id="S3.SS2.p1.25.m25.1.1" xref="S3.SS2.p1.25.m25.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.25.m25.1b"><ci id="S3.SS2.p1.25.m25.1.1.cmml" xref="S3.SS2.p1.25.m25.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.25.m25.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.25.m25.1d">italic_i</annotation></semantics></math> and <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p1.26.m26.1"><semantics id="S3.SS2.p1.26.m26.1a"><mi id="S3.SS2.p1.26.m26.1.1" xref="S3.SS2.p1.26.m26.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.26.m26.1b"><ci id="S3.SS2.p1.26.m26.1.1.cmml" xref="S3.SS2.p1.26.m26.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.26.m26.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.26.m26.1d">italic_j</annotation></semantics></math> are the positions of the same token in <math alttext="s_{\tau^{\star}}" class="ltx_Math" display="inline" id="S3.SS2.p1.27.m27.1"><semantics id="S3.SS2.p1.27.m27.1a"><msub id="S3.SS2.p1.27.m27.1.1" xref="S3.SS2.p1.27.m27.1.1.cmml"><mi id="S3.SS2.p1.27.m27.1.1.2" xref="S3.SS2.p1.27.m27.1.1.2.cmml">s</mi><msup id="S3.SS2.p1.27.m27.1.1.3" xref="S3.SS2.p1.27.m27.1.1.3.cmml"><mi id="S3.SS2.p1.27.m27.1.1.3.2" xref="S3.SS2.p1.27.m27.1.1.3.2.cmml">τ</mi><mo id="S3.SS2.p1.27.m27.1.1.3.3" xref="S3.SS2.p1.27.m27.1.1.3.3.cmml">⋆</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.27.m27.1b"><apply id="S3.SS2.p1.27.m27.1.1.cmml" xref="S3.SS2.p1.27.m27.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.27.m27.1.1.1.cmml" xref="S3.SS2.p1.27.m27.1.1">subscript</csymbol><ci id="S3.SS2.p1.27.m27.1.1.2.cmml" xref="S3.SS2.p1.27.m27.1.1.2">𝑠</ci><apply id="S3.SS2.p1.27.m27.1.1.3.cmml" xref="S3.SS2.p1.27.m27.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.27.m27.1.1.3.1.cmml" xref="S3.SS2.p1.27.m27.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.27.m27.1.1.3.2.cmml" xref="S3.SS2.p1.27.m27.1.1.3.2">𝜏</ci><ci id="S3.SS2.p1.27.m27.1.1.3.3.cmml" xref="S3.SS2.p1.27.m27.1.1.3.3">⋆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.27.m27.1c">s_{\tau^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.27.m27.1d">italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> and <math alttext="s_{\tau}" class="ltx_Math" display="inline" id="S3.SS2.p1.28.m28.1"><semantics id="S3.SS2.p1.28.m28.1a"><msub id="S3.SS2.p1.28.m28.1.1" xref="S3.SS2.p1.28.m28.1.1.cmml"><mi id="S3.SS2.p1.28.m28.1.1.2" xref="S3.SS2.p1.28.m28.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.28.m28.1.1.3" xref="S3.SS2.p1.28.m28.1.1.3.cmml">τ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.28.m28.1b"><apply id="S3.SS2.p1.28.m28.1.1.cmml" xref="S3.SS2.p1.28.m28.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.28.m28.1.1.1.cmml" xref="S3.SS2.p1.28.m28.1.1">subscript</csymbol><ci id="S3.SS2.p1.28.m28.1.1.2.cmml" xref="S3.SS2.p1.28.m28.1.1.2">𝑠</ci><ci id="S3.SS2.p1.28.m28.1.1.3.cmml" xref="S3.SS2.p1.28.m28.1.1.3">𝜏</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.28.m28.1c">s_{\tau}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.28.m28.1d">italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT</annotation></semantics></math>, respectively.
Additionally, <math alttext="\mathcal{M}({s_{\tau},s_{\tau^{\star}}})" class="ltx_Math" display="inline" id="S3.SS2.p1.29.m29.2"><semantics id="S3.SS2.p1.29.m29.2a"><mrow id="S3.SS2.p1.29.m29.2.2" xref="S3.SS2.p1.29.m29.2.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.29.m29.2.2.4" xref="S3.SS2.p1.29.m29.2.2.4.cmml">ℳ</mi><mo id="S3.SS2.p1.29.m29.2.2.3" xref="S3.SS2.p1.29.m29.2.2.3.cmml">⁢</mo><mrow id="S3.SS2.p1.29.m29.2.2.2.2" xref="S3.SS2.p1.29.m29.2.2.2.3.cmml"><mo id="S3.SS2.p1.29.m29.2.2.2.2.3" stretchy="false" xref="S3.SS2.p1.29.m29.2.2.2.3.cmml">(</mo><msub id="S3.SS2.p1.29.m29.1.1.1.1.1" xref="S3.SS2.p1.29.m29.1.1.1.1.1.cmml"><mi id="S3.SS2.p1.29.m29.1.1.1.1.1.2" xref="S3.SS2.p1.29.m29.1.1.1.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.29.m29.1.1.1.1.1.3" xref="S3.SS2.p1.29.m29.1.1.1.1.1.3.cmml">τ</mi></msub><mo id="S3.SS2.p1.29.m29.2.2.2.2.4" xref="S3.SS2.p1.29.m29.2.2.2.3.cmml">,</mo><msub id="S3.SS2.p1.29.m29.2.2.2.2.2" xref="S3.SS2.p1.29.m29.2.2.2.2.2.cmml"><mi id="S3.SS2.p1.29.m29.2.2.2.2.2.2" xref="S3.SS2.p1.29.m29.2.2.2.2.2.2.cmml">s</mi><msup id="S3.SS2.p1.29.m29.2.2.2.2.2.3" xref="S3.SS2.p1.29.m29.2.2.2.2.2.3.cmml"><mi id="S3.SS2.p1.29.m29.2.2.2.2.2.3.2" xref="S3.SS2.p1.29.m29.2.2.2.2.2.3.2.cmml">τ</mi><mo id="S3.SS2.p1.29.m29.2.2.2.2.2.3.3" xref="S3.SS2.p1.29.m29.2.2.2.2.2.3.3.cmml">⋆</mo></msup></msub><mo id="S3.SS2.p1.29.m29.2.2.2.2.5" stretchy="false" xref="S3.SS2.p1.29.m29.2.2.2.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.29.m29.2b"><apply id="S3.SS2.p1.29.m29.2.2.cmml" xref="S3.SS2.p1.29.m29.2.2"><times id="S3.SS2.p1.29.m29.2.2.3.cmml" xref="S3.SS2.p1.29.m29.2.2.3"></times><ci id="S3.SS2.p1.29.m29.2.2.4.cmml" xref="S3.SS2.p1.29.m29.2.2.4">ℳ</ci><interval closure="open" id="S3.SS2.p1.29.m29.2.2.2.3.cmml" xref="S3.SS2.p1.29.m29.2.2.2.2"><apply id="S3.SS2.p1.29.m29.1.1.1.1.1.cmml" xref="S3.SS2.p1.29.m29.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.29.m29.1.1.1.1.1.1.cmml" xref="S3.SS2.p1.29.m29.1.1.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.29.m29.1.1.1.1.1.2.cmml" xref="S3.SS2.p1.29.m29.1.1.1.1.1.2">𝑠</ci><ci id="S3.SS2.p1.29.m29.1.1.1.1.1.3.cmml" xref="S3.SS2.p1.29.m29.1.1.1.1.1.3">𝜏</ci></apply><apply id="S3.SS2.p1.29.m29.2.2.2.2.2.cmml" xref="S3.SS2.p1.29.m29.2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS2.p1.29.m29.2.2.2.2.2.1.cmml" xref="S3.SS2.p1.29.m29.2.2.2.2.2">subscript</csymbol><ci id="S3.SS2.p1.29.m29.2.2.2.2.2.2.cmml" xref="S3.SS2.p1.29.m29.2.2.2.2.2.2">𝑠</ci><apply id="S3.SS2.p1.29.m29.2.2.2.2.2.3.cmml" xref="S3.SS2.p1.29.m29.2.2.2.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.29.m29.2.2.2.2.2.3.1.cmml" xref="S3.SS2.p1.29.m29.2.2.2.2.2.3">superscript</csymbol><ci id="S3.SS2.p1.29.m29.2.2.2.2.2.3.2.cmml" xref="S3.SS2.p1.29.m29.2.2.2.2.2.3.2">𝜏</ci><ci id="S3.SS2.p1.29.m29.2.2.2.2.2.3.3.cmml" xref="S3.SS2.p1.29.m29.2.2.2.2.2.3.3">⋆</ci></apply></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.29.m29.2c">\mathcal{M}({s_{\tau},s_{\tau^{\star}}})</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.29.m29.2d">caligraphic_M ( italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT )</annotation></semantics></math> only includes pairs <math alttext="(i,j)" class="ltx_Math" display="inline" id="S3.SS2.p1.30.m30.2"><semantics id="S3.SS2.p1.30.m30.2a"><mrow id="S3.SS2.p1.30.m30.2.3.2" xref="S3.SS2.p1.30.m30.2.3.1.cmml"><mo id="S3.SS2.p1.30.m30.2.3.2.1" stretchy="false" xref="S3.SS2.p1.30.m30.2.3.1.cmml">(</mo><mi id="S3.SS2.p1.30.m30.1.1" xref="S3.SS2.p1.30.m30.1.1.cmml">i</mi><mo id="S3.SS2.p1.30.m30.2.3.2.2" xref="S3.SS2.p1.30.m30.2.3.1.cmml">,</mo><mi id="S3.SS2.p1.30.m30.2.2" xref="S3.SS2.p1.30.m30.2.2.cmml">j</mi><mo id="S3.SS2.p1.30.m30.2.3.2.3" stretchy="false" xref="S3.SS2.p1.30.m30.2.3.1.cmml">)</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.30.m30.2b"><interval closure="open" id="S3.SS2.p1.30.m30.2.3.1.cmml" xref="S3.SS2.p1.30.m30.2.3.2"><ci id="S3.SS2.p1.30.m30.1.1.cmml" xref="S3.SS2.p1.30.m30.1.1">𝑖</ci><ci id="S3.SS2.p1.30.m30.2.2.cmml" xref="S3.SS2.p1.30.m30.2.2">𝑗</ci></interval></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.30.m30.2c">(i,j)</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.30.m30.2d">( italic_i , italic_j )</annotation></semantics></math> where position <math alttext="i" class="ltx_Math" display="inline" id="S3.SS2.p1.31.m31.1"><semantics id="S3.SS2.p1.31.m31.1a"><mi id="S3.SS2.p1.31.m31.1.1" xref="S3.SS2.p1.31.m31.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.31.m31.1b"><ci id="S3.SS2.p1.31.m31.1.1.cmml" xref="S3.SS2.p1.31.m31.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.31.m31.1c">i</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.31.m31.1d">italic_i</annotation></semantics></math> in <math alttext="s_{\tau^{\star}}" class="ltx_Math" display="inline" id="S3.SS2.p1.32.m32.1"><semantics id="S3.SS2.p1.32.m32.1a"><msub id="S3.SS2.p1.32.m32.1.1" xref="S3.SS2.p1.32.m32.1.1.cmml"><mi id="S3.SS2.p1.32.m32.1.1.2" xref="S3.SS2.p1.32.m32.1.1.2.cmml">s</mi><msup id="S3.SS2.p1.32.m32.1.1.3" xref="S3.SS2.p1.32.m32.1.1.3.cmml"><mi id="S3.SS2.p1.32.m32.1.1.3.2" xref="S3.SS2.p1.32.m32.1.1.3.2.cmml">τ</mi><mo id="S3.SS2.p1.32.m32.1.1.3.3" xref="S3.SS2.p1.32.m32.1.1.3.3.cmml">⋆</mo></msup></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.32.m32.1b"><apply id="S3.SS2.p1.32.m32.1.1.cmml" xref="S3.SS2.p1.32.m32.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.32.m32.1.1.1.cmml" xref="S3.SS2.p1.32.m32.1.1">subscript</csymbol><ci id="S3.SS2.p1.32.m32.1.1.2.cmml" xref="S3.SS2.p1.32.m32.1.1.2">𝑠</ci><apply id="S3.SS2.p1.32.m32.1.1.3.cmml" xref="S3.SS2.p1.32.m32.1.1.3"><csymbol cd="ambiguous" id="S3.SS2.p1.32.m32.1.1.3.1.cmml" xref="S3.SS2.p1.32.m32.1.1.3">superscript</csymbol><ci id="S3.SS2.p1.32.m32.1.1.3.2.cmml" xref="S3.SS2.p1.32.m32.1.1.3.2">𝜏</ci><ci id="S3.SS2.p1.32.m32.1.1.3.3.cmml" xref="S3.SS2.p1.32.m32.1.1.3.3">⋆</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.32.m32.1c">s_{\tau^{\star}}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.32.m32.1d">italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT</annotation></semantics></math> would attend to <math alttext="t^{\star}" class="ltx_Math" display="inline" id="S3.SS2.p1.33.m33.1"><semantics id="S3.SS2.p1.33.m33.1a"><msup id="S3.SS2.p1.33.m33.1.1" xref="S3.SS2.p1.33.m33.1.1.cmml"><mi id="S3.SS2.p1.33.m33.1.1.2" xref="S3.SS2.p1.33.m33.1.1.2.cmml">t</mi><mo id="S3.SS2.p1.33.m33.1.1.3" xref="S3.SS2.p1.33.m33.1.1.3.cmml">⋆</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.33.m33.1b"><apply id="S3.SS2.p1.33.m33.1.1.cmml" xref="S3.SS2.p1.33.m33.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.33.m33.1.1.1.cmml" xref="S3.SS2.p1.33.m33.1.1">superscript</csymbol><ci id="S3.SS2.p1.33.m33.1.1.2.cmml" xref="S3.SS2.p1.33.m33.1.1.2">𝑡</ci><ci id="S3.SS2.p1.33.m33.1.1.3.cmml" xref="S3.SS2.p1.33.m33.1.1.3">⋆</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.33.m33.1c">t^{\star}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.33.m33.1d">italic_t start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT</annotation></semantics></math>.
We use subscripts such as <math alttext="\mathcal{H}^{(l)}\mathrm{(}s_{\tau}\mathrm{)}_{j}" class="ltx_Math" display="inline" id="S3.SS2.p1.34.m34.2"><semantics id="S3.SS2.p1.34.m34.2a"><mrow id="S3.SS2.p1.34.m34.2.2" xref="S3.SS2.p1.34.m34.2.2.cmml"><msup id="S3.SS2.p1.34.m34.2.2.3" xref="S3.SS2.p1.34.m34.2.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.34.m34.2.2.3.2" xref="S3.SS2.p1.34.m34.2.2.3.2.cmml">ℋ</mi><mrow id="S3.SS2.p1.34.m34.1.1.1.3" xref="S3.SS2.p1.34.m34.2.2.3.cmml"><mo id="S3.SS2.p1.34.m34.1.1.1.3.1" stretchy="false" xref="S3.SS2.p1.34.m34.2.2.3.cmml">(</mo><mi id="S3.SS2.p1.34.m34.1.1.1.1" xref="S3.SS2.p1.34.m34.1.1.1.1.cmml">l</mi><mo id="S3.SS2.p1.34.m34.1.1.1.3.2" stretchy="false" xref="S3.SS2.p1.34.m34.2.2.3.cmml">)</mo></mrow></msup><mo id="S3.SS2.p1.34.m34.2.2.2" xref="S3.SS2.p1.34.m34.2.2.2.cmml">⁢</mo><msub id="S3.SS2.p1.34.m34.2.2.1" xref="S3.SS2.p1.34.m34.2.2.1.cmml"><mrow id="S3.SS2.p1.34.m34.2.2.1.1.1" xref="S3.SS2.p1.34.m34.2.2.1.1.1.1.cmml"><mo id="S3.SS2.p1.34.m34.2.2.1.1.1.2" stretchy="false" xref="S3.SS2.p1.34.m34.2.2.1.1.1.1.cmml">(</mo><msub id="S3.SS2.p1.34.m34.2.2.1.1.1.1" xref="S3.SS2.p1.34.m34.2.2.1.1.1.1.cmml"><mi id="S3.SS2.p1.34.m34.2.2.1.1.1.1.2" xref="S3.SS2.p1.34.m34.2.2.1.1.1.1.2.cmml">s</mi><mi id="S3.SS2.p1.34.m34.2.2.1.1.1.1.3" xref="S3.SS2.p1.34.m34.2.2.1.1.1.1.3.cmml">τ</mi></msub><mo id="S3.SS2.p1.34.m34.2.2.1.1.1.3" stretchy="false" xref="S3.SS2.p1.34.m34.2.2.1.1.1.1.cmml">)</mo></mrow><mi id="S3.SS2.p1.34.m34.2.2.1.3" xref="S3.SS2.p1.34.m34.2.2.1.3.cmml">j</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.34.m34.2b"><apply id="S3.SS2.p1.34.m34.2.2.cmml" xref="S3.SS2.p1.34.m34.2.2"><times id="S3.SS2.p1.34.m34.2.2.2.cmml" xref="S3.SS2.p1.34.m34.2.2.2"></times><apply id="S3.SS2.p1.34.m34.2.2.3.cmml" xref="S3.SS2.p1.34.m34.2.2.3"><csymbol cd="ambiguous" id="S3.SS2.p1.34.m34.2.2.3.1.cmml" xref="S3.SS2.p1.34.m34.2.2.3">superscript</csymbol><ci id="S3.SS2.p1.34.m34.2.2.3.2.cmml" xref="S3.SS2.p1.34.m34.2.2.3.2">ℋ</ci><ci id="S3.SS2.p1.34.m34.1.1.1.1.cmml" xref="S3.SS2.p1.34.m34.1.1.1.1">𝑙</ci></apply><apply id="S3.SS2.p1.34.m34.2.2.1.cmml" xref="S3.SS2.p1.34.m34.2.2.1"><csymbol cd="ambiguous" id="S3.SS2.p1.34.m34.2.2.1.2.cmml" xref="S3.SS2.p1.34.m34.2.2.1">subscript</csymbol><apply id="S3.SS2.p1.34.m34.2.2.1.1.1.1.cmml" xref="S3.SS2.p1.34.m34.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.34.m34.2.2.1.1.1.1.1.cmml" xref="S3.SS2.p1.34.m34.2.2.1.1.1">subscript</csymbol><ci id="S3.SS2.p1.34.m34.2.2.1.1.1.1.2.cmml" xref="S3.SS2.p1.34.m34.2.2.1.1.1.1.2">𝑠</ci><ci id="S3.SS2.p1.34.m34.2.2.1.1.1.1.3.cmml" xref="S3.SS2.p1.34.m34.2.2.1.1.1.1.3">𝜏</ci></apply><ci id="S3.SS2.p1.34.m34.2.2.1.3.cmml" xref="S3.SS2.p1.34.m34.2.2.1.3">𝑗</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.34.m34.2c">\mathcal{H}^{(l)}\mathrm{(}s_{\tau}\mathrm{)}_{j}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.34.m34.2d">caligraphic_H start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT</annotation></semantics></math> to signify the hidden state at the <math alttext="j" class="ltx_Math" display="inline" id="S3.SS2.p1.35.m35.1"><semantics id="S3.SS2.p1.35.m35.1a"><mi id="S3.SS2.p1.35.m35.1.1" xref="S3.SS2.p1.35.m35.1.1.cmml">j</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.35.m35.1b"><ci id="S3.SS2.p1.35.m35.1.1.cmml" xref="S3.SS2.p1.35.m35.1.1">𝑗</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.35.m35.1c">j</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.35.m35.1d">italic_j</annotation></semantics></math>-th token position.
Therefore, our objective for a given target layer <math alttext="l" class="ltx_Math" display="inline" id="S3.SS2.p1.36.m36.1"><semantics id="S3.SS2.p1.36.m36.1a"><mi id="S3.SS2.p1.36.m36.1.1" xref="S3.SS2.p1.36.m36.1.1.cmml">l</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.36.m36.1b"><ci id="S3.SS2.p1.36.m36.1.1.cmml" xref="S3.SS2.p1.36.m36.1.1">𝑙</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.36.m36.1c">l</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.36.m36.1d">italic_l</annotation></semantics></math> is defined as:</p>
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{L}_{\textsc{AweDist}{}}=\min_{\mathbf{e^{\star}}\in\mathbb{R}^{d}}%
\mathbb{E}_{s\sim S}\left[\frac{1}{|\mathcal{M}|}\sum_{(i,j)\in\mathcal{M}(s_{%
\tau},s_{\tau^{\star}})}\left|\left|\mathcal{H}_{\mathbf{e^{\star}}}^{(l)}%
\mathrm{(}s_{\tau^{\star}}\mathrm{)}_{i}-\mathcal{H}^{(l)}\mathrm{(}s_{\tau}%
\mathrm{)}_{j}\right|\right|^{2}_{2}\right]." class="ltx_Math" display="block" id="S3.E1.m1.8"><semantics id="S3.E1.m1.8a"><mrow id="S3.E1.m1.8.8.1" xref="S3.E1.m1.8.8.1.1.cmml"><mrow id="S3.E1.m1.8.8.1.1" xref="S3.E1.m1.8.8.1.1.cmml"><msub id="S3.E1.m1.8.8.1.1.3" xref="S3.E1.m1.8.8.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.8.8.1.1.3.2" xref="S3.E1.m1.8.8.1.1.3.2.cmml">ℒ</mi><mtext class="ltx_font_smallcaps" id="S3.E1.m1.8.8.1.1.3.3" xref="S3.E1.m1.8.8.1.1.3.3a.cmml">AweDist</mtext></msub><mo id="S3.E1.m1.8.8.1.1.2" xref="S3.E1.m1.8.8.1.1.2.cmml">=</mo><mrow id="S3.E1.m1.8.8.1.1.1" xref="S3.E1.m1.8.8.1.1.1.cmml"><mrow id="S3.E1.m1.8.8.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.3.cmml"><munder id="S3.E1.m1.8.8.1.1.1.3.1" xref="S3.E1.m1.8.8.1.1.1.3.1.cmml"><mi id="S3.E1.m1.8.8.1.1.1.3.1.2" xref="S3.E1.m1.8.8.1.1.1.3.1.2.cmml">min</mi><mrow id="S3.E1.m1.8.8.1.1.1.3.1.3" xref="S3.E1.m1.8.8.1.1.1.3.1.3.cmml"><msup id="S3.E1.m1.8.8.1.1.1.3.1.3.2" xref="S3.E1.m1.8.8.1.1.1.3.1.3.2.cmml"><mi id="S3.E1.m1.8.8.1.1.1.3.1.3.2.2" xref="S3.E1.m1.8.8.1.1.1.3.1.3.2.2.cmml">𝐞</mi><mo id="S3.E1.m1.8.8.1.1.1.3.1.3.2.3" xref="S3.E1.m1.8.8.1.1.1.3.1.3.2.3.cmml">⋆</mo></msup><mo id="S3.E1.m1.8.8.1.1.1.3.1.3.1" xref="S3.E1.m1.8.8.1.1.1.3.1.3.1.cmml">∈</mo><msup id="S3.E1.m1.8.8.1.1.1.3.1.3.3" xref="S3.E1.m1.8.8.1.1.1.3.1.3.3.cmml"><mi id="S3.E1.m1.8.8.1.1.1.3.1.3.3.2" xref="S3.E1.m1.8.8.1.1.1.3.1.3.3.2.cmml">ℝ</mi><mi id="S3.E1.m1.8.8.1.1.1.3.1.3.3.3" xref="S3.E1.m1.8.8.1.1.1.3.1.3.3.3.cmml">d</mi></msup></mrow></munder><mo id="S3.E1.m1.8.8.1.1.1.3a" lspace="0.167em" xref="S3.E1.m1.8.8.1.1.1.3.cmml">⁡</mo><msub id="S3.E1.m1.8.8.1.1.1.3.2" xref="S3.E1.m1.8.8.1.1.1.3.2.cmml"><mi id="S3.E1.m1.8.8.1.1.1.3.2.2" xref="S3.E1.m1.8.8.1.1.1.3.2.2.cmml">𝔼</mi><mrow id="S3.E1.m1.8.8.1.1.1.3.2.3" xref="S3.E1.m1.8.8.1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.8.8.1.1.1.3.2.3.2" xref="S3.E1.m1.8.8.1.1.1.3.2.3.2.cmml">s</mi><mo id="S3.E1.m1.8.8.1.1.1.3.2.3.1" xref="S3.E1.m1.8.8.1.1.1.3.2.3.1.cmml">∼</mo><mi id="S3.E1.m1.8.8.1.1.1.3.2.3.3" xref="S3.E1.m1.8.8.1.1.1.3.2.3.3.cmml">S</mi></mrow></msub></mrow><mo id="S3.E1.m1.8.8.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.8.8.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.2.cmml"><mo id="S3.E1.m1.8.8.1.1.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.2.1.cmml">[</mo><mrow id="S3.E1.m1.8.8.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.cmml"><mfrac id="S3.E1.m1.1.1" xref="S3.E1.m1.1.1.cmml"><mn id="S3.E1.m1.1.1.3" xref="S3.E1.m1.1.1.3.cmml">1</mn><mrow id="S3.E1.m1.1.1.1.3" xref="S3.E1.m1.1.1.1.2.cmml"><mo id="S3.E1.m1.1.1.1.3.1" stretchy="false" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1.1.1" xref="S3.E1.m1.1.1.1.1.cmml">ℳ</mi><mo id="S3.E1.m1.1.1.1.3.2" stretchy="false" xref="S3.E1.m1.1.1.1.2.1.cmml">|</mo></mrow></mfrac><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.2.cmml">⁢</mo><mrow id="S3.E1.m1.8.8.1.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.cmml"><munder id="S3.E1.m1.8.8.1.1.1.1.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.2.2" movablelimits="false" rspace="0em" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.2.2.cmml">∑</mo><mrow id="S3.E1.m1.5.5.4" xref="S3.E1.m1.5.5.4.cmml"><mrow id="S3.E1.m1.5.5.4.6.2" xref="S3.E1.m1.5.5.4.6.1.cmml"><mo id="S3.E1.m1.5.5.4.6.2.1" stretchy="false" xref="S3.E1.m1.5.5.4.6.1.cmml">(</mo><mi id="S3.E1.m1.2.2.1.1" xref="S3.E1.m1.2.2.1.1.cmml">i</mi><mo id="S3.E1.m1.5.5.4.6.2.2" xref="S3.E1.m1.5.5.4.6.1.cmml">,</mo><mi id="S3.E1.m1.3.3.2.2" xref="S3.E1.m1.3.3.2.2.cmml">j</mi><mo id="S3.E1.m1.5.5.4.6.2.3" stretchy="false" xref="S3.E1.m1.5.5.4.6.1.cmml">)</mo></mrow><mo id="S3.E1.m1.5.5.4.5" xref="S3.E1.m1.5.5.4.5.cmml">∈</mo><mrow id="S3.E1.m1.5.5.4.4" xref="S3.E1.m1.5.5.4.4.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.5.5.4.4.4" xref="S3.E1.m1.5.5.4.4.4.cmml">ℳ</mi><mo id="S3.E1.m1.5.5.4.4.3" xref="S3.E1.m1.5.5.4.4.3.cmml">⁢</mo><mrow id="S3.E1.m1.5.5.4.4.2.2" xref="S3.E1.m1.5.5.4.4.2.3.cmml"><mo id="S3.E1.m1.5.5.4.4.2.2.3" stretchy="false" xref="S3.E1.m1.5.5.4.4.2.3.cmml">(</mo><msub id="S3.E1.m1.4.4.3.3.1.1.1" xref="S3.E1.m1.4.4.3.3.1.1.1.cmml"><mi id="S3.E1.m1.4.4.3.3.1.1.1.2" xref="S3.E1.m1.4.4.3.3.1.1.1.2.cmml">s</mi><mi id="S3.E1.m1.4.4.3.3.1.1.1.3" xref="S3.E1.m1.4.4.3.3.1.1.1.3.cmml">τ</mi></msub><mo id="S3.E1.m1.5.5.4.4.2.2.4" xref="S3.E1.m1.5.5.4.4.2.3.cmml">,</mo><msub id="S3.E1.m1.5.5.4.4.2.2.2" xref="S3.E1.m1.5.5.4.4.2.2.2.cmml"><mi id="S3.E1.m1.5.5.4.4.2.2.2.2" xref="S3.E1.m1.5.5.4.4.2.2.2.2.cmml">s</mi><msup id="S3.E1.m1.5.5.4.4.2.2.2.3" xref="S3.E1.m1.5.5.4.4.2.2.2.3.cmml"><mi id="S3.E1.m1.5.5.4.4.2.2.2.3.2" xref="S3.E1.m1.5.5.4.4.2.2.2.3.2.cmml">τ</mi><mo id="S3.E1.m1.5.5.4.4.2.2.2.3.3" xref="S3.E1.m1.5.5.4.4.2.2.2.3.3.cmml">⋆</mo></msup></msub><mo id="S3.E1.m1.5.5.4.4.2.2.5" stretchy="false" xref="S3.E1.m1.5.5.4.4.2.3.cmml">)</mo></mrow></mrow></mrow></munder><msubsup id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml"><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo><mrow id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><msubsup id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml">ℋ</mi><msup id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml"><mi id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml">𝐞</mi><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml">⋆</mo></msup><mrow id="S3.E1.m1.6.6.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mo id="S3.E1.m1.6.6.1.3.1" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">(</mo><mi id="S3.E1.m1.6.6.1.1" xref="S3.E1.m1.6.6.1.1.cmml">l</mi><mo id="S3.E1.m1.6.6.1.3.2" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">)</mo></mrow></msubsup><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">⁢</mo><msub id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mrow id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">s</mi><msup id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml">τ</mi><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml">⋆</mo></msup></msub><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub></mrow><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">−</mo><mrow id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml"><msup id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">ℋ</mi><mrow id="S3.E1.m1.7.7.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mo id="S3.E1.m1.7.7.1.3.1" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">(</mo><mi id="S3.E1.m1.7.7.1.1" xref="S3.E1.m1.7.7.1.1.cmml">l</mi><mo id="S3.E1.m1.7.7.1.3.2" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml">)</mo></mrow></msup><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml">⁢</mo><msub id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml"><mrow id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml"><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.2" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml">(</mo><msub id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml"><mi id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml">s</mi><mi id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.3.cmml">τ</mi></msub><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.3" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml">)</mo></mrow><mi id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.3.cmml">j</mi></msub></mrow></mrow><mo id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.3" stretchy="false" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.1.cmml">‖</mo></mrow><mn id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.3.cmml">2</mn><mn id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.3.cmml">2</mn></msubsup></mrow></mrow><mo id="S3.E1.m1.8.8.1.1.1.1.1.3" xref="S3.E1.m1.8.8.1.1.1.1.2.1.cmml">]</mo></mrow></mrow></mrow><mo id="S3.E1.m1.8.8.1.2" lspace="0em" xref="S3.E1.m1.8.8.1.1.cmml">.</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.8b"><apply id="S3.E1.m1.8.8.1.1.cmml" xref="S3.E1.m1.8.8.1"><eq id="S3.E1.m1.8.8.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.2"></eq><apply id="S3.E1.m1.8.8.1.1.3.cmml" xref="S3.E1.m1.8.8.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.3.1.cmml" xref="S3.E1.m1.8.8.1.1.3">subscript</csymbol><ci id="S3.E1.m1.8.8.1.1.3.2.cmml" xref="S3.E1.m1.8.8.1.1.3.2">ℒ</ci><ci id="S3.E1.m1.8.8.1.1.3.3a.cmml" xref="S3.E1.m1.8.8.1.1.3.3"><mtext class="ltx_font_smallcaps" id="S3.E1.m1.8.8.1.1.3.3.cmml" mathsize="70%" xref="S3.E1.m1.8.8.1.1.3.3">AweDist</mtext></ci></apply><apply id="S3.E1.m1.8.8.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1"><times id="S3.E1.m1.8.8.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.2"></times><apply id="S3.E1.m1.8.8.1.1.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.3"><apply id="S3.E1.m1.8.8.1.1.1.3.1.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.3.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1">subscript</csymbol><min id="S3.E1.m1.8.8.1.1.1.3.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.2"></min><apply id="S3.E1.m1.8.8.1.1.1.3.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3"><in id="S3.E1.m1.8.8.1.1.1.3.1.3.1.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.1"></in><apply id="S3.E1.m1.8.8.1.1.1.3.1.3.2.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.3.1.3.2.1.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.2">superscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.3.1.3.2.2.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.2.2">𝐞</ci><ci id="S3.E1.m1.8.8.1.1.1.3.1.3.2.3.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.2.3">⋆</ci></apply><apply id="S3.E1.m1.8.8.1.1.1.3.1.3.3.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.3.1.3.3.1.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.3">superscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.3.1.3.3.2.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.3.2">ℝ</ci><ci id="S3.E1.m1.8.8.1.1.1.3.1.3.3.3.cmml" xref="S3.E1.m1.8.8.1.1.1.3.1.3.3.3">𝑑</ci></apply></apply></apply><apply id="S3.E1.m1.8.8.1.1.1.3.2.cmml" xref="S3.E1.m1.8.8.1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.3.2.1.cmml" xref="S3.E1.m1.8.8.1.1.1.3.2">subscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.3.2.2.cmml" xref="S3.E1.m1.8.8.1.1.1.3.2.2">𝔼</ci><apply id="S3.E1.m1.8.8.1.1.1.3.2.3.cmml" xref="S3.E1.m1.8.8.1.1.1.3.2.3"><csymbol cd="latexml" id="S3.E1.m1.8.8.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.8.8.1.1.1.3.2.3.1">similar-to</csymbol><ci id="S3.E1.m1.8.8.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.8.8.1.1.1.3.2.3.2">𝑠</ci><ci id="S3.E1.m1.8.8.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.8.8.1.1.1.3.2.3.3">𝑆</ci></apply></apply></apply><apply id="S3.E1.m1.8.8.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.8.8.1.1.1.1.2.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.2">delimited-[]</csymbol><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1"><times id="S3.E1.m1.8.8.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1"><divide id="S3.E1.m1.1.1.2.cmml" xref="S3.E1.m1.1.1"></divide><cn id="S3.E1.m1.1.1.3.cmml" type="integer" xref="S3.E1.m1.1.1.3">1</cn><apply id="S3.E1.m1.1.1.1.2.cmml" xref="S3.E1.m1.1.1.1.3"><abs id="S3.E1.m1.1.1.1.2.1.cmml" xref="S3.E1.m1.1.1.1.3.1"></abs><ci id="S3.E1.m1.1.1.1.1.cmml" xref="S3.E1.m1.1.1.1.1">ℳ</ci></apply></apply><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1"><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.2">subscript</csymbol><sum id="S3.E1.m1.8.8.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.2.2"></sum><apply id="S3.E1.m1.5.5.4.cmml" xref="S3.E1.m1.5.5.4"><in id="S3.E1.m1.5.5.4.5.cmml" xref="S3.E1.m1.5.5.4.5"></in><interval closure="open" id="S3.E1.m1.5.5.4.6.1.cmml" xref="S3.E1.m1.5.5.4.6.2"><ci id="S3.E1.m1.2.2.1.1.cmml" xref="S3.E1.m1.2.2.1.1">𝑖</ci><ci id="S3.E1.m1.3.3.2.2.cmml" xref="S3.E1.m1.3.3.2.2">𝑗</ci></interval><apply id="S3.E1.m1.5.5.4.4.cmml" xref="S3.E1.m1.5.5.4.4"><times id="S3.E1.m1.5.5.4.4.3.cmml" xref="S3.E1.m1.5.5.4.4.3"></times><ci id="S3.E1.m1.5.5.4.4.4.cmml" xref="S3.E1.m1.5.5.4.4.4">ℳ</ci><interval closure="open" id="S3.E1.m1.5.5.4.4.2.3.cmml" xref="S3.E1.m1.5.5.4.4.2.2"><apply id="S3.E1.m1.4.4.3.3.1.1.1.cmml" xref="S3.E1.m1.4.4.3.3.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.4.4.3.3.1.1.1.1.cmml" xref="S3.E1.m1.4.4.3.3.1.1.1">subscript</csymbol><ci id="S3.E1.m1.4.4.3.3.1.1.1.2.cmml" xref="S3.E1.m1.4.4.3.3.1.1.1.2">𝑠</ci><ci id="S3.E1.m1.4.4.3.3.1.1.1.3.cmml" xref="S3.E1.m1.4.4.3.3.1.1.1.3">𝜏</ci></apply><apply id="S3.E1.m1.5.5.4.4.2.2.2.cmml" xref="S3.E1.m1.5.5.4.4.2.2.2"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.4.4.2.2.2.1.cmml" xref="S3.E1.m1.5.5.4.4.2.2.2">subscript</csymbol><ci id="S3.E1.m1.5.5.4.4.2.2.2.2.cmml" xref="S3.E1.m1.5.5.4.4.2.2.2.2">𝑠</ci><apply id="S3.E1.m1.5.5.4.4.2.2.2.3.cmml" xref="S3.E1.m1.5.5.4.4.2.2.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.5.5.4.4.2.2.2.3.1.cmml" xref="S3.E1.m1.5.5.4.4.2.2.2.3">superscript</csymbol><ci id="S3.E1.m1.5.5.4.4.2.2.2.3.2.cmml" xref="S3.E1.m1.5.5.4.4.2.2.2.3.2">𝜏</ci><ci id="S3.E1.m1.5.5.4.4.2.2.2.3.3.cmml" xref="S3.E1.m1.5.5.4.4.2.2.2.3.3">⋆</ci></apply></apply></interval></apply></apply></apply><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1">superscript</csymbol><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="latexml" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.2">norm</csymbol><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1"><minus id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.3"></minus><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1"><times id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.2"></times><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.2">ℋ</ci><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3">superscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.2">𝐞</ci><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.3.3">⋆</ci></apply></apply><ci id="S3.E1.m1.6.6.1.1.cmml" xref="S3.E1.m1.6.6.1.1">𝑙</ci></apply><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.2">𝑠</ci><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">superscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.2">𝜏</ci><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.3">⋆</ci></apply></apply><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply></apply><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2"><times id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.2"></times><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3">superscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.3.2">ℋ</ci><ci id="S3.E1.m1.7.7.1.1.cmml" xref="S3.E1.m1.7.7.1.1">𝑙</ci></apply><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1">subscript</csymbol><apply id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.1.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1">subscript</csymbol><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.2">𝑠</ci><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.1.1.1.3">𝜏</ci></apply><ci id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.1.1.1.2.1.3">𝑗</ci></apply></apply></apply></apply><cn id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.1.3">2</cn></apply><cn id="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S3.E1.m1.8.8.1.1.1.1.1.1.1.1.3">2</cn></apply></apply></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.8c">\mathcal{L}_{\textsc{AweDist}{}}=\min_{\mathbf{e^{\star}}\in\mathbb{R}^{d}}%
\mathbb{E}_{s\sim S}\left[\frac{1}{|\mathcal{M}|}\sum_{(i,j)\in\mathcal{M}(s_{%
\tau},s_{\tau^{\star}})}\left|\left|\mathcal{H}_{\mathbf{e^{\star}}}^{(l)}%
\mathrm{(}s_{\tau^{\star}}\mathrm{)}_{i}-\mathcal{H}^{(l)}\mathrm{(}s_{\tau}%
\mathrm{)}_{j}\right|\right|^{2}_{2}\right].</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.8d">caligraphic_L start_POSTSUBSCRIPT AweDist end_POSTSUBSCRIPT = roman_min start_POSTSUBSCRIPT bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_d end_POSTSUPERSCRIPT end_POSTSUBSCRIPT blackboard_E start_POSTSUBSCRIPT italic_s ∼ italic_S end_POSTSUBSCRIPT [ divide start_ARG 1 end_ARG start_ARG | caligraphic_M | end_ARG ∑ start_POSTSUBSCRIPT ( italic_i , italic_j ) ∈ caligraphic_M ( italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) end_POSTSUBSCRIPT | | caligraphic_H start_POSTSUBSCRIPT bold_e start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_τ start_POSTSUPERSCRIPT ⋆ end_POSTSUPERSCRIPT end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT - caligraphic_H start_POSTSUPERSCRIPT ( italic_l ) end_POSTSUPERSCRIPT ( italic_s start_POSTSUBSCRIPT italic_τ end_POSTSUBSCRIPT ) start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | | start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ] .</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS2.p1.37">In practice, we simply use the last layer’s hidden state but analyze this choice in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" title="5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.2</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Further Details</h3>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Retrieving relevant contexts for new tokens.</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">Like other methods optimizing embeddings based on contexts, our method also needs input sequences. Firstly, we note that randomly sampling texts from a domain-specific or general corpus is inefficient for our goal of learning newly added embeddings: typically, the new tokens will only make up a small fraction, so most gradient updates will actually not affect the new input embeddings at all and simply learn to minimize their log-probability in the case of output embeddings. As we aim to have a fast method, we need a better approach. We propose two different approaches: (1) Our main approach is to simply retrieve snippets that contain our target tokens from a domain-specific or general corpus. This can be implemented efficiently using the algorithm proposed by <cite class="ltx_cite ltx_citemacro_citet">Aho and Corasick (<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib2" title="">1975</a>)</cite>. Then we can truncate the snippets to a small window around our target token to optimize computational efficiency. (2) For causal language models, we can simply generate snippets by prompting the model with our target token. We provide implementation details in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A2.SS5" title="B.5 Generation of Relevant Contexts ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section B.5</span></a>.
In our main experiments, we focus on the first approach but note the availability of the second approach in case a reference corpus is not available. In most cases of domain and language adaptation, the availability of such a corpus is a reasonable assumption.
Nevertheless, we study an ablation instead using the generative approach in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" title="5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Output embeddings.</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">Since our method backpropagates gradients back from the hidden states, we do not learn output embeddings with our distillation-based objective.
In fact, this is not possible, as our new tokens are not part of the original model that serves as the “teacher”.
In practice, for learning output embeddings, we can simply add a next-token prediction objective just for the output embeddings at a minimal computational overhead or combine our method with any other method for initializing output embeddings.
Recent work suggests that input and output embeddings should in fact be treated differently <cite class="ltx_cite ltx_citemacro_citep">(Nakash et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib38" title="">2025</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib20" title="">2025</a>)</cite>.
Indeed, by no means does the Transformer architecture necessitate a shared vocabulary between the input and output embeddings.
This is merely a convention that makes some use cases easier to implement.
Therefore, we can either only add new tokens to the input embedding matrix or – for compatibility with existing frameworks – set their output embeddings to a vector of zeros.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Hyperparameters.</h5>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.1">We employ a simple setup and use the AdamW <cite class="ltx_cite ltx_citemacro_citep">(Kingma and Ba, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib24" title="">2017</a>; Loshchilov and Hutter, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib32" title="">2019</a>)</cite> optimizer for all trainable parameters. We set the batch size to optimize throughput and run all experiments on Nvidia H100 80GB GPUs. We do not use weight decay and maintain a constant learning rate with a linear warmup. For fair comparison, we sweep for the best learning rate for all methods that require a learning rate.
Since our method is aimed to serve as an “initialization” rather than as full-scale further training, we restrict the number of example sequences to a maximum of 25 per target token and truncate to a context length of 50 tokens. These restrictions ensure that our method is quick to run, initializing 2,500 new tokens on a single GPU in under 10 minutes.
We use the same data for all training-based methods, including baselines and variations of our method. We provide all details in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A2" title="Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Appendix B</span></a>.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Experimental Setup</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Evaluation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">A common use case for adding new tokens to a pretrained model’s tokenizer is domain adaptation <cite class="ltx_cite ltx_citemacro_citep">(Gururangan et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib14" title="">2020</a>)</cite>, especially for the biomedical domain <cite class="ltx_cite ltx_citemacro_citep">(Poerner et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib44" title="">2020</a>; Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib12" title="">2023</a>; Hasan et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib16" title="">2024</a>; Singh et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib52" title="">2024</a>)</cite>. This is because it is a particularly challenging domain with highly complex domain-specific terminology, which can serve as a benchmark to stress test embedding initialization methods.
Therefore, we evaluate our method on a collection of standard benchmarks in the biomedical domain <cite class="ltx_cite ltx_citemacro_citep">(Pal et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib42" title="">2024</a>)</cite> and add frequently occurring words as new tokens.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">Additionally, for a more in-depth evaluation of the quality of new representations provided by our methods, we prompt the adapted models to generate definitions for new tokens and evaluate their quality with an LLM-as-a-Judge <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib29" title="">2023</a>)</cite>.
Furthermore, to push the limits of embedding initialization methods, we move beyond domain adaptation and apply our method to multi-word tokens, which have recently gained further interest <cite class="ltx_cite ltx_citemacro_citep">(Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib12" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib30" title="">2025</a>; Huang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib20" title="">2025</a>)</cite>.
We follow the zero-shot tokenizer transfer setting in our evaluations <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite>, i.e., we evaluate initialization methods without further training on a corpus, as we want to directly judge the quality of the resulting representations.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">We select target tokens that actually occur frequently in the chosen benchmarks so that we can effectively judge the quality of newly generated embeddings. We select words that occur more frequently than a threshold and additionally ensure that all individual benchmarks are represented. We report the full methodology in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A2.SS1" title="B.1 Token Selection for Domain Adaptation ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section B.1</span></a>.
For our multi-word token experiments, we prompt <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.1">gpt-o4-mini-high</span> to generate suitable candidates (full details in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A2.SS2" title="B.2 Multi-Word Token Generation ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section B.2</span></a>).
For our experiments in the biomedical domain, we retrieve contexts from <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.2">ncbi/pubmed</span> as a reference corpus. For our multi-word token experiments, we prompt the original models to generate sequences containing the new tokens.
For the LLM-as-a-Judge evaluations, we use <span class="ltx_text ltx_font_typewriter" id="S4.SS1.p3.1.3">Llama-3.3-70B-Instruct</span> as the judge model. We evaluate the general <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.4">correctness</span> of the generated definitions to test the quality and completeness of the resulting representations as well as <span class="ltx_text ltx_font_italic" id="S4.SS1.p3.1.5">semantic similarity</span> with the target model’s output <cite class="ltx_cite ltx_citemacro_citep">(Villegas et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib56" title="">2025</a>)</cite>, as inducing as little behavior change as possible can be an important desideratum.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>Modeling</h3>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Considered base models.</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px1.p1.1">We conduct experiments using a wide range of open-weight model checkpoints to ensure our results are not merely a function of peculiarities in any specific model’s embedding space. In particular, we choose the following models: <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.1">Mistral-7B-v0.1</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.2">OLMo-2-7B-1124-Instruct</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.3">Llama-3-8B</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.4">Llama-3-8B-Instruct</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.5">Llama-3.1-8B</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.6">Llama-3.1-8B-Instruct</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.7">Llama-3.2-3B</span>, and <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.8">Llama-3.2-3B-Instruct</span>. In the remainder of the paper, we denote <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.9">-Instruct</span> variants with a simple <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.10">-i</span>. Through this extensive evaluation, we study different model families, model sizes (<span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.11">3B</span>, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.12">7B</span>, and <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px1.p1.1.13">8B</span>), instruction-tuned and base models, as well as models with separate input/output embeddings and tied (shared) embeddings between input and output.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Baselines.</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p1.1">For all results, we report the performance of the unmodified base model using the original tokenization, which serves as our “Target”.
To establish a lower bound, we also report results for initializing the new token embeddings randomly from a normal distribution with a per-channel mean and standard deviation of the original embeddings <cite class="ltx_cite ltx_citemacro_citep">(Hewitt, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib18" title="">2021</a>)</cite>. Furthermore, we report results for the commonly used method of taking the subtoken mean <cite class="ltx_cite ltx_citemacro_citep">(Sachidananda et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib48" title="">2021</a>; Koto et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib25" title="">2021</a>; Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>)</cite> as initialization for a new token, which has been shown to perform similarly to more sophisticated initialization methods <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite> that also use a weighted average of existing embeddings.</p>
</div>
<div class="ltx_para" id="S4.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S4.SS2.SSS0.Px2.p2.1">Since our proposed method conducts a short optimization on a few reference sequences per new token, we compare against the common approach of training embeddings using causal language modeling using the same data. Specifically, we report results for “classic” embedding tuning using the next-token prediction objective (NTP) as well as masking updates to the original embeddings such that only new embeddings are optimized (NTP++), which corresponds to the method used by <cite class="ltx_cite ltx_citemacro_citet">Lampinen and McClelland (<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib28" title="">2018</a>)</cite>. These methods use the subtoken mean as their starting point.
A strong alternative method for obtaining embeddings for new tokens are hyper-networks specifically pretrained for this task. We report results for ZeTT <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite> using their provided hyper-network checkpoints.
We run five random seeds for all methods that include randomness and report the mean and standard deviation.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Our method.</h5>
<div class="ltx_para" id="S4.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS2.SSS0.Px3.p1.3">As the initial embedding input into our method, we also use the subtoken mean. We investigate alternatives to this choice in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" title="5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.2</span></a>.
Our proposed objective can also be combined with the NTP-based objectives.
We report results for combining <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px3.p1.3.1">AweDist</span> and NTP++.
Since the NTP++ and <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px3.p1.3.2">AweDist</span> objectives have different scales, a simple addition is suboptimal. In fact, NTP++ is usually of larger magnitude, which leads it to overpower <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px3.p1.3.3">AweDist</span>.
Therefore, we also consider an “autoscaled” variant <span class="ltx_text ltx_font_smallcaps" id="S4.SS2.SSS0.Px3.p1.3.4">AweDist</span>-<math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S4.SS2.SSS0.Px3.p1.1.m1.1a"><mi id="S4.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.1.m1.1b"><ci id="S4.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.1.m1.1d">italic_α</annotation></semantics></math>NTP++ where the NTP loss is scaled by <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="S4.SS2.SSS0.Px3.p1.2.m2.1a"><mi id="S4.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S4.SS2.SSS0.Px3.p1.2.m2.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.2.m2.1b"><ci id="S4.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.2.m2.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.2.m2.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.2.m2.1d">italic_α</annotation></semantics></math> = <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px3.p1.3.5">stop_gradient</span>(<span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px3.p1.3.6">loss_awedist</span> / <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px3.p1.3.7">loss_ntp</span>) before summing.
Here, <span class="ltx_text ltx_font_typewriter" id="S4.SS2.SSS0.Px3.p1.3.8">stop_gradient</span> prevents gradient flow through the scaling factor <math alttext="\alpha" class="ltx_Math" display="inline" id="S4.SS2.SSS0.Px3.p1.3.m3.1"><semantics id="S4.SS2.SSS0.Px3.p1.3.m3.1a"><mi id="S4.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S4.SS2.SSS0.Px3.p1.3.m3.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S4.SS2.SSS0.Px3.p1.3.m3.1b"><ci id="S4.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S4.SS2.SSS0.Px3.p1.3.m3.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS2.SSS0.Px3.p1.3.m3.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S4.SS2.SSS0.Px3.p1.3.m3.1d">italic_α</annotation></semantics></math>.<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span>Multi-objective optimization is a rich field with extensive literature, we leave a thorough investigation of alternatives such as GradNorm <cite class="ltx_cite ltx_citemacro_citep">(Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib6" title="">2018</a>)</cite> for future work.</span></span></span></p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Results</h2>
<figure class="ltx_table" id="S5.T1">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T1.4" style="width:433.6pt;height:121.5pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-174.7pt,48.7pt) scale(0.55382,0.55382) ;">
<p class="ltx_p" id="S5.T1.4.4"><span class="ltx_text" id="S5.T1.4.4.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4" style="width:783.0pt;height:219.3pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4"><span class="ltx_text" id="S5.T1.4.4.4.4.4.4">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T1.4.4.4.4.4.4.4">
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T1.4.4.4.4.4.4.4.5.1">
<span class="ltx_td ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.1"></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4.4.4.4.5.1.2.1" style="width:38.1pt;height:38.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.9pt;transform:translate(-4.41pt,-15.56pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4.4.4.5.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.4.4.4.4.4.4.4.5.1.2.1.1.1">Mistral-7B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.3">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4.4.4.4.5.1.3.1" style="width:41.1pt;height:41.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.1pt;transform:translate(-5.03pt,-17.05pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4.4.4.5.1.3.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.4.4.4.4.4.4.4.5.1.3.1.1.1">Llama-3-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4.4.4.4.5.1.4.1" style="width:45.4pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:57.2pt;transform:translate(-5.92pt,-19.21pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4.4.4.5.1.4.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.4.4.4.4.4.4.4.5.1.4.1.1.1">Llama-3-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.5">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4.4.4.4.5.1.5.1" style="width:46.6pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.9pt;transform:translate(-6.17pt,-19.8pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4.4.4.5.1.5.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.4.4.4.4.4.4.4.5.1.5.1.1.1">Llama-3.1-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.6">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4.4.4.4.5.1.6.1" style="width:50.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.0pt;transform:translate(-7.06pt,-21.96pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4.4.4.5.1.6.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.4.4.4.4.4.4.4.5.1.6.1.1.1">Llama-3.1-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.7">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4.4.4.4.5.1.7.1" style="width:46.6pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.9pt;transform:translate(-6.17pt,-19.8pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4.4.4.5.1.7.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.4.4.4.4.4.4.4.5.1.7.1.1.1">Llama-3.2-3B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.8">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4.4.4.4.5.1.8.1" style="width:50.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.0pt;transform:translate(-7.06pt,-21.96pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4.4.4.5.1.8.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.4.4.4.4.4.4.4.5.1.8.1.1.1">Llama-3.2-3B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.9">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T1.4.4.4.4.4.4.4.5.1.9.1" style="width:45.9pt;height:45.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.1pt;transform:translate(-6.09pt,-19.53pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T1.4.4.4.4.4.4.4.5.1.9.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T1.4.4.4.4.4.4.4.5.1.9.1.1.1">OLMo-2-7B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T1.4.4.4.4.4.4.4.5.1.10">Avg.</span></span>
<span class="ltx_tr" id="S5.T1.4.4.4.4.4.4.4.6.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.1">Target</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.2">64.5</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.3">69.8</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.4">70.6</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.5">69.2</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.6">72.3</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.7">60.1</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.8">64.4</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.9">61.3</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.6.2.10">66.5</span></span>
<span class="ltx_tr" id="S5.T1.4.4.4.4.4.4.4.7.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.1">Random <cite class="ltx_cite ltx_citemacro_citep">(Hewitt, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib18" title="">2021</a>)</cite></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.2">57.0<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.7.3.2.1" style="font-size:90%;">±0.6</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.3">58.8<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.7.3.3.1" style="font-size:90%;">±0.5</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.4">60.6<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.7.3.4.1" style="font-size:90%;">±0.3</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.5">59.3<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.7.3.5.1" style="font-size:90%;">±0.5</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.6">62.6<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.7.3.6.1" style="font-size:90%;">±0.6</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.7">51.6<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.7.3.7.1" style="font-size:90%;">±0.4</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.8">55.8<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.7.3.8.1" style="font-size:90%;">±0.2</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.9">53.9<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.7.3.9.1" style="font-size:90%;">±0.6</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T1.4.4.4.4.4.4.4.7.3.10">57.5</span></span>
<span class="ltx_tr" id="S5.T1.4.4.4.4.4.4.4.8.4">
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.1">NTP</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.2">55.7<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.8.4.2.1" style="font-size:90%;">±0.5</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.3">63.8<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.8.4.3.1" style="font-size:90%;">±0.1</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.4">63.8<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.8.4.4.1" style="font-size:90%;">±0.2</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.5">62.9<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.8.4.5.1" style="font-size:90%;">±0.4</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.6">66.8<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.8.4.6.1" style="font-size:90%;">±0.4</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.7">51.5<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.8.4.7.1" style="font-size:90%;">±0.6</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.8">55.5<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.8.4.8.1" style="font-size:90%;">±0.5</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.9">58.0<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.8.4.9.1" style="font-size:90%;">±0.3</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.8.4.10">59.8</span></span>
<span class="ltx_tr" id="S5.T1.4.4.4.4.4.4.4.9.5">
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.1">Mean <cite class="ltx_cite ltx_citemacro_citep">(Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>)</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.2">58.3</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.3">63.8</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.4">63.4</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.5">63.7</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.6">66.9</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.7">54.2</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.8">58.4</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.9">58.0</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.9.5.10">60.8</span></span>
<span class="ltx_tr" id="S5.T1.4.4.4.4.4.4.4.10.6">
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.1">NTP++ <cite class="ltx_cite ltx_citemacro_citep">(Lampinen and McClelland, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib28" title="">2018</a>)</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.2">61.2<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.10.6.2.1" style="font-size:90%;">±0.4</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.3">65.6<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.10.6.3.1" style="font-size:90%;">±0.3</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.4">65.3<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.10.6.4.1" style="font-size:90%;">±0.6</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.5">66.0<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.10.6.5.1" style="font-size:90%;">±0.5</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.6">68.9<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.10.6.6.1" style="font-size:90%;">±0.2</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.7">56.6<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.10.6.7.1" style="font-size:90%;">±0.6</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.8">61.3<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.10.6.8.1" style="font-size:90%;">±0.5</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.9">58.7<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.10.6.9.1" style="font-size:90%;">±0.6</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.10.6.10">63.0</span></span>
<span class="ltx_tr" id="S5.T1.4.4.4.4.4.4.4.11.7">
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.1">ZeTT <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.4.4.4.4.4.4.11.7.2.1">62.7</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.3">66.1</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.4">66.3</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.5">–</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.6">–</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.7">–</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.8">–</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.9">–</span>
<span class="ltx_td ltx_align_left" id="S5.T1.4.4.4.4.4.4.4.11.7.10">–</span></span>
<span class="ltx_tr" id="S5.T1.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T1.1.1.1.1.1.1.1.1.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T1.1.1.1.1.1.1.1.1.1.2">AweDist</span>-NTP++</span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T1.1.1.1.1.1.1.1.1.2.1">63.0<span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.2.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.1.1.1.1.1.1.1.1.3.1">67.2<span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.3.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.1.1.1.1.1.1.1.1.4.1">66.7<span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.4.1.1" style="font-size:90%;">±1.6</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.1.1.1.1.1.1.1.1.5.1">67.2<span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.5.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.1.1.1.1.1.1.1.1.6.1">70.6<span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.6.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.1.1.1.1.1.1.1.1.7.1">57.6<span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.7.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.8">62.2<span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.8.1" style="font-size:90%;">±0.3</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.9">59.8<span class="ltx_text" id="S5.T1.1.1.1.1.1.1.1.1.9.1" style="font-size:90%;">±0.5</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.1.1.1.1.1.1.1.1.10">64.3</span></span>
<span class="ltx_tr" id="S5.T1.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1.1" xref="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T1.2.2.2.2.2.2.2.2.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T1.2.2.2.2.2.2.2.2.1.2">AweDist</span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.2.2.2.2.2.2.2.2.2.1">62.8<span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.2.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.2.2.2.2.2.2.2.2.3.1">67.3<span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.3.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.2.2.2.2.2.2.2.2.4.1">67.6<span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.4.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.2.2.2.2.2.2.2.2.5.1">67.3<span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.5.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S5.T1.2.2.2.2.2.2.2.2.6.1">71.0<span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.6.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.2.2.2.2.2.2.2.2.7.1">56.2<span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.7.1.1" style="font-size:90%;">±1.9</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.8"><span class="ltx_text ltx_font_bold" id="S5.T1.2.2.2.2.2.2.2.2.8.1">63.1<span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.8.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.2.2.2.2.2.2.2.2.9.1">61.2<span class="ltx_text" id="S5.T1.2.2.2.2.2.2.2.2.9.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T1.2.2.2.2.2.2.2.2.10">64.6</span></span>
<span class="ltx_tr" id="S5.T1.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.2"><span class="ltx_text" id="S5.T1.3.3.3.3.3.3.3.3.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1"><semantics id="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1a"><mo id="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1.1" xref="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1b"><ci id="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml" xref="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T1.3.3.3.3.3.3.3.3.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T1.4.4.4.4.4.4.4.4.2.2">AweDist</span>-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T1.4.4.4.4.4.4.4.4.2.m1.1"><semantics id="S5.T1.4.4.4.4.4.4.4.4.2.m1.1a"><mi id="S5.T1.4.4.4.4.4.4.4.4.2.m1.1.1" xref="S5.T1.4.4.4.4.4.4.4.4.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T1.4.4.4.4.4.4.4.4.2.m1.1b"><ci id="S5.T1.4.4.4.4.4.4.4.4.2.m1.1.1.cmml" xref="S5.T1.4.4.4.4.4.4.4.4.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.4.4.4.4.4.4.4.4.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T1.4.4.4.4.4.4.4.4.2.m1.1d">italic_α</annotation></semantics></math>NTP++</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.4.4.4.4.4.4.4.3.1">62.8<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.4.3.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.4.4.4.4.4.4.1">67.6<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.4.4.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.4.4.4.4.4.5.1">67.8<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.4.5.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.4.4.4.4.4.6.1">67.4<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.4.6.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.4.4.4.4.4.4.4.7.1">70.9<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.4.7.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.8"><span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.4.4.4.4.4.8.1">57.9<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.4.8.1.1" style="font-size:90%;">±0.1</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.4.4.4.4.4.4.4.4.9.1">62.5<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.4.9.1.1" style="font-size:90%;">±0.6</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.10"><span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.4.4.4.4.4.10.1">61.2<span class="ltx_text" id="S5.T1.4.4.4.4.4.4.4.4.10.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T1.4.4.4.4.4.4.4.4.11"><span class="ltx_text ltx_font_bold" id="S5.T1.4.4.4.4.4.4.4.4.11.1">64.7</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Benchmark results on domain adaptation for different initialization methods. We report a macro-average ± standard deviation of the tasks in the Open Medical-LLM leaderboard (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S4.SS1" title="4.1 Evaluation ‣ 4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 4.1</span></a>). The best initialization result for each model is given in <span class="ltx_text ltx_font_bold" id="S5.T1.14.2">boldface</span>, while
all results that are not significantly worse (one-sided Welch’s <math alttext="t" class="ltx_Math" display="inline" id="S5.T1.8.m1.1"><semantics id="S5.T1.8.m1.1b"><mi id="S5.T1.8.m1.1.1" xref="S5.T1.8.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.T1.8.m1.1c"><ci id="S5.T1.8.m1.1.1.cmml" xref="S5.T1.8.m1.1.1">𝑡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.8.m1.1d">t</annotation><annotation encoding="application/x-llamapun" id="S5.T1.8.m1.1e">italic_t</annotation></semantics></math>-test with Bonferroni correction, <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S5.T1.9.m2.1"><semantics id="S5.T1.9.m2.1b"><mrow id="S5.T1.9.m2.1.1" xref="S5.T1.9.m2.1.1.cmml"><mi id="S5.T1.9.m2.1.1.2" xref="S5.T1.9.m2.1.1.2.cmml">p</mi><mo id="S5.T1.9.m2.1.1.1" xref="S5.T1.9.m2.1.1.1.cmml">&lt;</mo><mn id="S5.T1.9.m2.1.1.3" xref="S5.T1.9.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T1.9.m2.1c"><apply id="S5.T1.9.m2.1.1.cmml" xref="S5.T1.9.m2.1.1"><lt id="S5.T1.9.m2.1.1.1.cmml" xref="S5.T1.9.m2.1.1.1"></lt><ci id="S5.T1.9.m2.1.1.2.cmml" xref="S5.T1.9.m2.1.1.2">𝑝</ci><cn id="S5.T1.9.m2.1.1.3.cmml" type="float" xref="S5.T1.9.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.9.m2.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.T1.9.m2.1e">italic_p &lt; 0.05</annotation></semantics></math>) are <span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T1.15.3">underlined</span>.
Methods without <span class="ltx_text" id="S5.T1.16.4" style="font-size:90%;">±x.x</span> are deterministic. –: We only report results for ZeTT where pretrained hyper-networks are available. <span class="ltx_text" id="S5.T1.10.1" style="position:relative; bottom:0.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T1.10.1.m1.1"><semantics id="S5.T1.10.1.m1.1b"><mo id="S5.T1.10.1.m1.1.1" xref="S5.T1.10.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T1.10.1.m1.1c"><ci id="S5.T1.10.1.m1.1.1.cmml" xref="S5.T1.10.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T1.10.1.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T1.10.1.m1.1e">⋆</annotation></semantics></math></span>: Our method(s).
</figcaption>
</figure>
<figure class="ltx_table" id="S5.T2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T2.4" style="width:433.6pt;height:98.2pt;vertical-align:-0.5pt;"><span class="ltx_transformed_inner" style="transform:translate(-222.7pt,50.2pt) scale(0.4933,0.4933) ;">
<p class="ltx_p" id="S5.T2.4.4"><span class="ltx_text" id="S5.T2.4.4.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T2.4.4.4.4" style="width:879.0pt;height:199pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T2.4.4.4.4.4"><span class="ltx_text" id="S5.T2.4.4.4.4.4.4">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T2.4.4.4.4.4.4.4">
<span class="ltx_thead">
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.5.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S5.T2.4.4.4.4.4.4.4.5.1.1">Method</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.2"><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.4.4.4.4.4.5.1.2.1">Mistral-7B</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.3"><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.4.4.4.4.4.5.1.3.1">Llama-3-8B</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.4"><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.4.4.4.4.4.5.1.4.1">Llama-3-8B-i</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.5"><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.4.4.4.4.4.5.1.5.1">Llama-3.1-8B</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.6"><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.4.4.4.4.4.5.1.6.1">Llama-3.1-8B-i</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.7"><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.4.4.4.4.4.5.1.7.1">Llama-3.2-3B</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.8"><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.4.4.4.4.4.5.1.8.1">Llama-3.2-3B-i</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.9"><span class="ltx_text ltx_font_typewriter" id="S5.T2.4.4.4.4.4.4.4.5.1.9.1">OLMo-7B-i</span></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt ltx_colspan ltx_colspan_2" id="S5.T2.4.4.4.4.4.4.4.5.1.10"><span class="ltx_text" id="S5.T2.4.4.4.4.4.4.4.5.1.10.1">Avg.</span></span></span>
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.6.2">
<span class="ltx_td ltx_th ltx_th_row" id="S5.T2.4.4.4.4.4.4.4.6.2.1"></span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.2">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.3">Corr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.4">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.5">Corr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.6">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.7">Corr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.8">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.9">Corr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.10">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.11">Corr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.12">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.13">Corr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.14">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.15">Corr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.16">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.17">Corr</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.18">Sim</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.6.2.19">Corr</span></span>
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.7.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.1">Target</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.2">99.8</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.3">96.5</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.4">100.0</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.5">94.3</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.6">100.0</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.7">98.4</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.8">100.0</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.9">94.7</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.10">100.0</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.11">98.4</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.12">100.0</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.13">93.2</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.14">100.0</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.15">93.8</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.16">99.4</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.17">96.9</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.18">99.9</span>
<span class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.7.3.19">95.8</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.8.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.1">Random <cite class="ltx_cite ltx_citemacro_citep">(Hewitt, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib18" title="">2021</a>)</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.2">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.3">0.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.4">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.5">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.6">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.7">0.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.8">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.9">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.10">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.11">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.12">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.13">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.14">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.15">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.16">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.17">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.18">0.0</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T2.4.4.4.4.4.4.4.8.1.19">0.1</span></span>
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.9.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.4.4.4.4.4.4.9.2.1">Mean <cite class="ltx_cite ltx_citemacro_citep">(Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.2">2.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.3">4.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.4">16.8</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.5">16.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.6">17.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.7">20.7</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.8">25.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.9">25.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.10">23.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.11">28.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.12">15.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.13">15.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.14">12.1</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.15">14.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.16">20.1</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.17">24.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.18">16.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.9.2.19">18.6</span></span>
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.10.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.4.4.4.4.4.4.10.3.1">NTP</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.2">26.8</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.3">30.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.4">33.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.5">39.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.6">34.4</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.7">43.4</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.8">28.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.9">35.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.10">41.4</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.11">50.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.12">19.7</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.13">20.7</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.14">18.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.15">21.1</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.16">22.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.17">26.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.18">28.1</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.10.3.19">33.3</span></span>
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.11.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.4.4.4.4.4.4.11.4.1">NTP++ <cite class="ltx_cite ltx_citemacro_citep">(Lampinen and McClelland, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib28" title="">2018</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.2">49.8</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.3">58.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.4">45.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.5">49.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.6">48.4</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.7">58.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.8">58.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.9">65.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.10">60.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.11">69.9</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.12">51.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.13">58.4</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.14">50.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.15">58.8</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.16">52.1</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.17">57.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.18">52.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.11.4.19">59.4</span></span>
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.12.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.4.4.4.4.4.4.4.12.5.1">ZeTT <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.2">63.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.3">68.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.4">69.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.5">73.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.6">70.7</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.7">80.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.8">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.9">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.10">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.11">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.12">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.13">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.14">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.15">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.16">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.17">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.18">–</span>
<span class="ltx_td ltx_align_center" id="S5.T2.4.4.4.4.4.4.4.12.5.19">–</span></span>
<span class="ltx_tr" id="S5.T2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S5.T2.1.1.1.1.1.1.1.1.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T2.1.1.1.1.1.1.1.1.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T2.1.1.1.1.1.1.1.1.1.2">AweDist</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.2.1">79.7</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.3"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.3.1">85.2</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.4.1">72.7</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.5"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.5.1">76.2</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.6">79.7</span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.7">91.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.8">75.8</span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.9">80.1</span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.10"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.10.1">81.6</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.11"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.11.1">89.8</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.12">0.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.13">0.0</span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.14"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.14.1">75.0</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.15"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.15.1">83.0</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.16"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.16.1">83.8</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.17"><span class="ltx_text ltx_font_bold" id="S5.T2.1.1.1.1.1.1.1.1.17.1">89.5</span></span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.18">68.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.1.1.1.1.1.1.1.1.19">74.4</span></span>
<span class="ltx_tr" id="S5.T2.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T2.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="S5.T2.2.2.2.2.2.2.2.2.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1.1" xref="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T2.2.2.2.2.2.2.2.2.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T2.2.2.2.2.2.2.2.2.1.2">AweDist</span>-NTP++</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.2">77.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.3">82.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.4">68.4</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.5">73.8</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.6">75.8</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.7">86.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.8">72.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.9">79.9</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.10">77.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.11">86.1</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.12">62.3</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.13">69.1</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.14">65.2</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.15">75.8</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.16">65.4</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.17">74.6</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.18">70.5</span>
<span class="ltx_td ltx_align_center" id="S5.T2.2.2.2.2.2.2.2.2.19">78.5</span></span>
<span class="ltx_tr" id="S5.T2.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.2"><span class="ltx_text" id="S5.T2.3.3.3.3.3.3.3.3.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1"><semantics id="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1a"><mo id="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1.1" xref="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1b"><ci id="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml" xref="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T2.3.3.3.3.3.3.3.3.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T2.4.4.4.4.4.4.4.4.2.2">AweDist</span>-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T2.4.4.4.4.4.4.4.4.2.m1.1"><semantics id="S5.T2.4.4.4.4.4.4.4.4.2.m1.1a"><mi id="S5.T2.4.4.4.4.4.4.4.4.2.m1.1.1" xref="S5.T2.4.4.4.4.4.4.4.4.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T2.4.4.4.4.4.4.4.4.2.m1.1b"><ci id="S5.T2.4.4.4.4.4.4.4.4.2.m1.1.1.cmml" xref="S5.T2.4.4.4.4.4.4.4.4.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.4.4.4.4.4.4.4.4.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T2.4.4.4.4.4.4.4.4.2.m1.1d">italic_α</annotation></semantics></math>NTP++</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.3">76.8</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.4">82.0</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.5">71.7</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.6">75.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.7"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.4.4.4.4.7.1">80.9</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.8"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.4.4.4.4.8.1">91.6</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.9"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.4.4.4.4.9.1">77.5</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.10"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.4.4.4.4.10.1">81.2</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.11">81.1</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.12">89.5</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.13"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.4.4.4.4.13.1">72.7</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.14"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.4.4.4.4.14.1">79.5</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.15">71.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.16">80.5</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.17">81.2</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.18">86.9</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.19"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.4.4.4.4.19.1">76.7</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T2.4.4.4.4.4.4.4.4.20"><span class="ltx_text ltx_font_bold" id="S5.T2.4.4.4.4.4.4.4.4.20.1">83.3</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Results for prompting models to generate definitions for newly initialized tokens. We report similarity with the original target model’s definition (Sim) and correctness (Corr) both judged by <span class="ltx_text ltx_font_typewriter" id="S5.T2.9.1">Llama-3.3-70B-Instruct</span>. We <span class="ltx_text ltx_font_bold" id="S5.T2.10.2">bold</span> the best (non-target) result in each column. <math alttext="\star" class="ltx_Math" display="inline" id="S5.T2.6.m1.1"><semantics id="S5.T2.6.m1.1b"><mo id="S5.T2.6.m1.1.1" xref="S5.T2.6.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T2.6.m1.1c"><ci id="S5.T2.6.m1.1.1.cmml" xref="S5.T2.6.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T2.6.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T2.6.m1.1e">⋆</annotation></semantics></math>: Our method(s). For full judge prompting details, see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A2.SS4" title="B.4 LLM-as-a-Judge Experiments ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section B.4</span></a>.</figcaption>
</figure>
<section class="ltx_subsection" id="S5.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.1 </span>Main Experiments</h3>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Main results.</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p1.1">We provide our main results on benchmarks in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T1" title="Table 1 ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 1</span></a> (reporting a macro-average over benchmark tasks) and our definition generation results in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T2" title="Table 2 ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 2</span></a> (reporting similarity to the original target model’s generation and correctness as judged by a larger LLM). On average, our method outperforms all other baseline embedding initialization methods. In particular, <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p1.1.1">AweDist</span> shows superior results on benchmarks and in its ability to generate correct definitions, which is indicative of higher-quality representations. Additionally, <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p1.1.2">AweDist</span> also exhibits greater similarity to the original tokenization behavior (“Target”) than all other methods, which substantiates our claim that a distillation-based objective is able to better approximate complex multi-subtoken interactions into a single token embedding.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p2.1">Note that even with its very competitive performance, <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p2.1.1">AweDist</span> does not fully attain the level of the original tokenization, which benefits from the massive pretraining of the original model. This is expected in our zero-shot tokenizer transfer setting without any further training <cite class="ltx_cite ltx_citemacro_citep">(see Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite>. We seek to directly investigate the embedding initialization quality and the advantages of our distillation-based objective compared to next-token prediction rather than the effects of further training. However, results can naturally be improved via further training; we argue that <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p2.1.2">AweDist</span> will provide the best starting point for such further processing.
In the following, we analyze various aspects of the results in further detail.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="S5.SS1.SSS0.Px1.p3.1">Note that in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T2" title="Table 2 ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 2</span></a>, only for <span class="ltx_text ltx_font_typewriter" id="S5.SS1.SSS0.Px1.p3.1.1">Llama-3.2-3B</span>, <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p3.1.2">AweDist</span> has performance on par with random initialization. <span class="ltx_text ltx_font_typewriter" id="S5.SS1.SSS0.Px1.p3.1.3">Llama-3.2-3B(-i)</span> are the only models in our lineup with tied embedding weights.
Our objective does not explicitly enforce a bound on the norm of the new embedding, which in this case led to the failure mode of always generating a specific new embedding with very large norm.
Note that this does not always happen for checkpoints with tied weights, as evidenced by the results for <span class="ltx_text ltx_font_typewriter" id="S5.SS1.SSS0.Px1.p3.1.4">Llama-3.2-3B-i</span>.
However, we additionally propose a simple modification to alleviate the occurrence of this issue by combining <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p3.1.5">AweDist</span> with the next-token prediction objective, which (implicitly) acts as a regularizer on the output embedding norm.
In support of our argument that a subtoken attention distillation objective is superior to next-token prediction for learning new token embeddings, the combination via a sum of the two objectives (<span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p3.1.6">AweDist</span>-NTP++) generally yields worse results than <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p3.1.7">AweDist</span> alone.
However, we can add a dynamic downweighting factor (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S4.SS2" title="4.2 Modeling ‣ 4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 4.2</span></a>) to the next-token prediction objective (<span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px1.p3.1.8">AweDist</span>-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.SS1.SSS0.Px1.p3.1.m1.1"><semantics id="S5.SS1.SSS0.Px1.p3.1.m1.1a"><mi id="S5.SS1.SSS0.Px1.p3.1.m1.1.1" xref="S5.SS1.SSS0.Px1.p3.1.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.SS1.SSS0.Px1.p3.1.m1.1b"><ci id="S5.SS1.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S5.SS1.SSS0.Px1.p3.1.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.SS1.SSS0.Px1.p3.1.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.SS1.SSS0.Px1.p3.1.m1.1d">italic_α</annotation></semantics></math>NTP++), which mostly alleviates the negative interference while keeping the regularizing effect.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Freezing original embeddings.</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p1.1">Vanilla NTP-tuning of the embeddings yields disappointing results, even underperforming the subtoken mean initialization. We investigate this and observe a degradation of the original token embeddings. Note that for <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px2.p1.1.1">AweDist</span>, we only optimize new token embeddings. Therefore, we also compare against an improved NTP baseline (NTP++) where we similarly optimize only the new token embeddings.
NTP++ outperforms NTP, supporting our analysis. However, even with this improvement, <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px2.p1.1.2">AweDist</span> in turn still further surpasses NTP++.</p>
</div>
<div class="ltx_para" id="S5.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S5.SS1.SSS0.Px2.p2.1">We note that masking gradient updates to the original embeddings during further training is not commonly done when adding new token embeddings, even during initial “embedding tuning” phases where all weights but the embedding layers are frozen. In these phases, the goal is to improve the initialized embeddings for new tokens or “adapt” the new embedding matrix to the existing Transformer layers <cite class="ltx_cite ltx_citemacro_citep">(de Vries and Nissim, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib7" title="">2021</a>)</cite>.
When normally sampling from a large-scale training corpus, the degradation of original embeddings will be less dramatic, as the number of new tokens per batch will be much lower. However, this is much less computationally efficient and – at least initially – still potentially harmful. We leave a further investigation of this as a practically useful direction for future work.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS1.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Hypernetworks.</h5>
<div class="ltx_para" id="S5.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS1.SSS0.Px3.p1.1">ZeTT, which uses a pretrained hypernetwork, is the strongest baseline we compare against. We only compare against ZeTT when a pretrained hyper-network by <cite class="ltx_cite ltx_citemacro_citet">Minixhofer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite> is available<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span><cite class="ltx_cite ltx_citemacro_citet">Minixhofer et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite> do not provide a checkpoint trained specifically for the <span class="ltx_text ltx_font_typewriter" id="footnote5.1">-instruct</span> variant of <span class="ltx_text ltx_font_typewriter" id="footnote5.2">Llama-3-8B</span> but show that a transfer from the base version is possible.</span></span></span>. We first note that ZeTT yields quite impressive results, outperforming even our optimized NTP++ baseline. Evidently, the pretraining of ZeTT’s hypernetwork has internalized a better embedding prediction than the iterative gradient descent optimization on selected samples we employ for NTP++. Nevertheless, our proposed method <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px3.p1.1.1">AweDist</span> outperforms even ZeTT – without needing any hyper-network pretraining. It is important to note that at <span class="ltx_text ltx_font_italic" id="S5.SS1.SSS0.Px3.p1.1.2">inference time</span>, hyper-network based methods such as ZeTT are actually faster than <span class="ltx_text ltx_font_smallcaps" id="S5.SS1.SSS0.Px3.p1.1.3">AweDist</span> (and NTP-based baselines), since only a single forward pass through the hyper-network per token is required. However, they require expensive pretraining for each new model and – in some cases – target domain.
Also, we can actually use ZeTT-generated embeddings as a starting point and further tune them using our method for even better results. We analyze this possibility in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" title="5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.2</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S5.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">5.2 </span>In-Depth Analysis</h3>
<figure class="ltx_table" id="S5.T3">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T3.4" style="width:433.6pt;height:111.5pt;vertical-align:-0.6pt;"><span class="ltx_transformed_inner" style="transform:translate(-174.7pt,44.7pt) scale(0.55382,0.55382) ;">
<p class="ltx_p" id="S5.T3.4.4"><span class="ltx_text" id="S5.T3.4.4.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4" style="width:783.0pt;height:201.3pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4"><span class="ltx_text" id="S5.T3.4.4.4.4.4.4">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T3.4.4.4.4.4.4.4">
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T3.4.4.4.4.4.4.4.5.1">
<span class="ltx_td ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.1"></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4.4.4.4.5.1.2.1" style="width:38.1pt;height:38.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.9pt;transform:translate(-4.41pt,-15.56pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4.4.4.5.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T3.4.4.4.4.4.4.4.5.1.2.1.1.1">Mistral-7B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.3">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4.4.4.4.5.1.3.1" style="width:41.1pt;height:41.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.1pt;transform:translate(-5.03pt,-17.05pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4.4.4.5.1.3.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T3.4.4.4.4.4.4.4.5.1.3.1.1.1">Llama-3-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4.4.4.4.5.1.4.1" style="width:45.4pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:57.2pt;transform:translate(-5.92pt,-19.21pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4.4.4.5.1.4.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T3.4.4.4.4.4.4.4.5.1.4.1.1.1">Llama-3-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.5">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4.4.4.4.5.1.5.1" style="width:46.6pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.9pt;transform:translate(-6.17pt,-19.8pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4.4.4.5.1.5.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T3.4.4.4.4.4.4.4.5.1.5.1.1.1">Llama-3.1-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.6">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4.4.4.4.5.1.6.1" style="width:50.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.0pt;transform:translate(-7.06pt,-21.96pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4.4.4.5.1.6.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T3.4.4.4.4.4.4.4.5.1.6.1.1.1">Llama-3.1-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.7">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4.4.4.4.5.1.7.1" style="width:46.6pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.9pt;transform:translate(-6.17pt,-19.8pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4.4.4.5.1.7.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T3.4.4.4.4.4.4.4.5.1.7.1.1.1">Llama-3.2-3B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.8">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4.4.4.4.5.1.8.1" style="width:50.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.0pt;transform:translate(-7.06pt,-21.96pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4.4.4.5.1.8.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T3.4.4.4.4.4.4.4.5.1.8.1.1.1">Llama-3.2-3B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.9">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T3.4.4.4.4.4.4.4.5.1.9.1" style="width:45.9pt;height:45.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.1pt;transform:translate(-6.09pt,-19.53pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T3.4.4.4.4.4.4.4.5.1.9.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T3.4.4.4.4.4.4.4.5.1.9.1.1.1">OLMo-2-7B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T3.4.4.4.4.4.4.4.5.1.10"><span class="ltx_text" id="S5.T3.4.4.4.4.4.4.4.5.1.10.1">Avg.</span></span></span>
<span class="ltx_tr" id="S5.T3.4.4.4.4.4.4.4.6.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.1"><span class="ltx_text" id="S5.T3.4.4.4.4.4.4.4.6.2.1.1">Target</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.2">93.4</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.3">88.1</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.4">99.2</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.5">91.1</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.6">97.8</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.7">65.2</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.8">90.9</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.9">94.0</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.6.2.10">90.0</span></span>
<span class="ltx_tr" id="S5.T3.4.4.4.4.4.4.4.7.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.1">Random <cite class="ltx_cite ltx_citemacro_citep">(Hewitt, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib18" title="">2021</a>)</cite></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.2">0.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.3">0.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.4">0.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.5">0.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.6">0.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.7">0.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.8">0.2</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.9">0.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T3.4.4.4.4.4.4.4.7.3.10">0.0</span></span>
<span class="ltx_tr" id="S5.T3.4.4.4.4.4.4.4.8.4">
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.1">Mean <cite class="ltx_cite ltx_citemacro_citep">(Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>)</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.2">2.2</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.3">4.2</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.4">9.1</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.5">8.0</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.6">11.3</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.7">4.2</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.8">7.4</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.9">7.4</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.8.4.10">6.7</span></span>
<span class="ltx_tr" id="S5.T3.4.4.4.4.4.4.4.9.5">
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.1">NTP++ <cite class="ltx_cite ltx_citemacro_citep">(Lampinen and McClelland, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib28" title="">2018</a>)</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.2">34.0</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.3">54.3</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.4">70.6</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.5">49.7</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.6">71.6</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.7">29.0</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.8">53.9</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.9">70.0</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.9.5.10">54.1</span></span>
<span class="ltx_tr" id="S5.T3.4.4.4.4.4.4.4.10.6">
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.1">ZeTT <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite></span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.2">15.3</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.3">25.4</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.4">28.4</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.5">–</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.6">–</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.7">–</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.8">–</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.9">–</span>
<span class="ltx_td ltx_align_left" id="S5.T3.4.4.4.4.4.4.4.10.6.10">–</span></span>
<span class="ltx_tr" id="S5.T3.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S5.T3.1.1.1.1.1.1.1.1.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.1.1.1.1.1.1.1.1.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T3.1.1.1.1.1.1.1.1.1.2">AweDist</span></span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.2">46.3</span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.3">68.0</span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1.1.1.4.1">86.3</span></span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.5">61.8</span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.6"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1.1.1.6.1">81.3</span></span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.7">31.6</span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.8">57.5</span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.9"><span class="ltx_text ltx_font_bold" id="S5.T3.1.1.1.1.1.1.1.1.9.1">84.3</span></span>
<span class="ltx_td ltx_align_left" id="S5.T3.1.1.1.1.1.1.1.1.10">64.6</span></span>
<span class="ltx_tr" id="S5.T3.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="S5.T3.2.2.2.2.2.2.2.2.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1.1" xref="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.2.2.2.2.2.2.2.2.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T3.2.2.2.2.2.2.2.2.1.2">AweDist</span>-NTP++</span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T3.2.2.2.2.2.2.2.2.2.1">48.3</span></span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.3">68.0</span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.4">85.3</span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.5">60.2</span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.6">79.7</span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.7"><span class="ltx_text ltx_font_bold" id="S5.T3.2.2.2.2.2.2.2.2.7.1">38.2</span></span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.8">63.4</span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.9">80.9</span>
<span class="ltx_td ltx_align_left" id="S5.T3.2.2.2.2.2.2.2.2.10">65.5</span></span>
<span class="ltx_tr" id="S5.T3.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.2"><span class="ltx_text" id="S5.T3.3.3.3.3.3.3.3.3.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1"><semantics id="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1a"><mo id="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1.1" xref="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1b"><ci id="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml" xref="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.3.3.3.3.3.3.3.3.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T3.4.4.4.4.4.4.4.4.2.2">AweDist</span>-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T3.4.4.4.4.4.4.4.4.2.m1.1"><semantics id="S5.T3.4.4.4.4.4.4.4.4.2.m1.1a"><mi id="S5.T3.4.4.4.4.4.4.4.4.2.m1.1.1" xref="S5.T3.4.4.4.4.4.4.4.4.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T3.4.4.4.4.4.4.4.4.2.m1.1b"><ci id="S5.T3.4.4.4.4.4.4.4.4.2.m1.1.1.cmml" xref="S5.T3.4.4.4.4.4.4.4.4.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.4.4.4.4.4.4.4.4.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T3.4.4.4.4.4.4.4.4.2.m1.1d">italic_α</annotation></semantics></math>NTP++</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.3">46.9</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.4.4.4.4.4.4.4.1">71.0</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.5">85.7</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.4.4.4.4.4.4.6.1">62.0</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.7">80.1</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.8">37.2</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.9"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.4.4.4.4.4.4.9.1">66.4</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.10">83.7</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T3.4.4.4.4.4.4.4.4.11"><span class="ltx_text ltx_font_bold" id="S5.T3.4.4.4.4.4.4.4.4.11.1">66.6</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Correctness of generated definitions for newly initialized multi-word tokens (famous people, places, entities, sayings and concepts) as judged by <span class="ltx_text ltx_font_typewriter" id="S5.T3.9.2">Llama-3.3-70B-Instruct</span>. The best non-target model result in each column is given in <span class="ltx_text ltx_font_bold" id="S5.T3.10.3">boldface</span>.
<span class="ltx_text" id="S5.T3.6.1" style="position:relative; bottom:0.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T3.6.1.m1.1"><semantics id="S5.T3.6.1.m1.1b"><mo id="S5.T3.6.1.m1.1.1" xref="S5.T3.6.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T3.6.1.m1.1c"><ci id="S5.T3.6.1.m1.1.1.cmml" xref="S5.T3.6.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T3.6.1.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T3.6.1.m1.1e">⋆</annotation></semantics></math></span>: Our method(s). </figcaption>
</figure>
<figure class="ltx_table" id="S5.T4">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T4.2" style="width:433.6pt;height:64.6pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-96.1pt,14.2pt) scale(0.69299,0.69299) ;">
<p class="ltx_p" id="S5.T4.2.2"><span class="ltx_text" id="S5.T4.2.2.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2" style="width:625.7pt;height:93.3pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2"><span class="ltx_text" id="S5.T4.2.2.2.2.2.2">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T4.2.2.2.2.2.2.2">
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T4.2.2.2.2.2.2.2.3.1">
<span class="ltx_td ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.1"></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2.2.2.2.3.1.2.1" style="width:38.1pt;height:38.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.9pt;transform:translate(-4.41pt,-15.56pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2.2.2.3.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T4.2.2.2.2.2.2.2.3.1.2.1.1.1">Mistral-7B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.3">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2.2.2.2.3.1.3.1" style="width:41.1pt;height:41.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.1pt;transform:translate(-5.03pt,-17.05pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2.2.2.3.1.3.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T4.2.2.2.2.2.2.2.3.1.3.1.1.1">Llama-3-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2.2.2.2.3.1.4.1" style="width:45.4pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:57.2pt;transform:translate(-5.92pt,-19.21pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2.2.2.3.1.4.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T4.2.2.2.2.2.2.2.3.1.4.1.1.1">Llama-3-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.5">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2.2.2.2.3.1.5.1" style="width:46.6pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.9pt;transform:translate(-6.17pt,-19.8pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2.2.2.3.1.5.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T4.2.2.2.2.2.2.2.3.1.5.1.1.1">Llama-3.1-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.6">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2.2.2.2.3.1.6.1" style="width:50.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.0pt;transform:translate(-7.06pt,-21.96pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2.2.2.3.1.6.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T4.2.2.2.2.2.2.2.3.1.6.1.1.1">Llama-3.1-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.7">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2.2.2.2.3.1.7.1" style="width:46.6pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.9pt;transform:translate(-6.17pt,-19.8pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2.2.2.3.1.7.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T4.2.2.2.2.2.2.2.3.1.7.1.1.1">Llama-3.2-3B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.8">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2.2.2.2.3.1.8.1" style="width:50.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.0pt;transform:translate(-7.06pt,-21.96pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2.2.2.3.1.8.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T4.2.2.2.2.2.2.2.3.1.8.1.1.1">Llama-3.2-3B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.9">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T4.2.2.2.2.2.2.2.3.1.9.1" style="width:45.9pt;height:45.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.1pt;transform:translate(-6.09pt,-19.53pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T4.2.2.2.2.2.2.2.3.1.9.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T4.2.2.2.2.2.2.2.3.1.9.1.1.1">OLMo-2-7B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T4.2.2.2.2.2.2.2.3.1.10">Avg.</span></span>
<span class="ltx_tr" id="S5.T4.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S5.T4.1.1.1.1.1.1.1.1.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T4.1.1.1.1.1.1.1.1.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T4.1.1.1.1.1.1.1.1.1.2">AweDist</span> (generated data)</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.2">62.4</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.3">67.1</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.1.1.4.1">67.6</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.5">67.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.6">70.1</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.7"><span class="ltx_text ltx_font_bold" id="S5.T4.1.1.1.1.1.1.1.1.7.1">57.1</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.8">62.3</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.9">60.9</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T4.1.1.1.1.1.1.1.1.10">64.3</span></span>
<span class="ltx_tr" id="S5.T4.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1.1" xref="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T4.2.2.2.2.2.2.2.2.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T4.2.2.2.2.2.2.2.2.1.2">AweDist</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.2.2.2.2.2.1">62.8<span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.2.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.3"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.2.2.2.2.3.1">67.3<span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.3.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.4"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.2.2.2.2.4.1">67.6<span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.4.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.5"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.2.2.2.2.5.1">67.3<span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.5.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.6"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.2.2.2.2.6.1">71.0<span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.6.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.7">56.2<span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.7.1" style="font-size:90%;">±1.9</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.8"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.2.2.2.2.8.1">63.1<span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.8.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.9"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.2.2.2.2.9.1">61.2<span class="ltx_text" id="S5.T4.2.2.2.2.2.2.2.2.9.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T4.2.2.2.2.2.2.2.2.10"><span class="ltx_text ltx_font_bold" id="S5.T4.2.2.2.2.2.2.2.2.10.1">64.6</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Comparison of using data generated from the model instead of retrieving from a corpus. We only run a single seed with generated data and report average performance on biomedical benchmarks.
</figcaption>
</figure>
<figure class="ltx_table" id="S5.T5">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S5.T5.9" style="width:433.6pt;height:129.5pt;vertical-align:-0.7pt;"><span class="ltx_transformed_inner" style="transform:translate(-90.1pt,26.7pt) scale(0.7065,0.7065) ;">
<p class="ltx_p" id="S5.T5.9.9"><span class="ltx_text" id="S5.T5.9.9.9">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9" style="width:613.8pt;height:183.3pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9"><span class="ltx_text" id="S5.T5.9.9.9.9.9.9">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T5.9.9.9.9.9.9.9">
<span class="ltx_thead">
<span class="ltx_tr" id="S5.T5.9.9.9.9.9.9.9.10.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.1"></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9.9.9.9.10.1.2.1" style="width:38.1pt;height:38.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.9pt;transform:translate(-4.41pt,-15.56pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9.9.9.10.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.9.9.9.9.9.9.9.10.1.2.1.1.1">Mistral-7B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.3">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9.9.9.9.10.1.3.1" style="width:41.1pt;height:41.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.1pt;transform:translate(-5.03pt,-17.05pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9.9.9.10.1.3.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.9.9.9.9.9.9.9.10.1.3.1.1.1">Llama-3-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9.9.9.9.10.1.4.1" style="width:45.4pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:57.2pt;transform:translate(-5.92pt,-19.21pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9.9.9.10.1.4.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.9.9.9.9.9.9.9.10.1.4.1.1.1">Llama-3-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.5">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9.9.9.9.10.1.5.1" style="width:46.6pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.9pt;transform:translate(-6.17pt,-19.8pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9.9.9.10.1.5.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.9.9.9.9.9.9.9.10.1.5.1.1.1">Llama-3.1-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.6">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9.9.9.9.10.1.6.1" style="width:50.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.0pt;transform:translate(-7.06pt,-21.96pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9.9.9.10.1.6.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.9.9.9.9.9.9.9.10.1.6.1.1.1">Llama-3.1-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.7">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9.9.9.9.10.1.7.1" style="width:46.6pt;height:46.6pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.9pt;transform:translate(-6.17pt,-19.8pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9.9.9.10.1.7.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.9.9.9.9.9.9.9.10.1.7.1.1.1">Llama-3.2-3B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.8">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9.9.9.9.10.1.8.1" style="width:50.9pt;height:50.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:65.0pt;transform:translate(-7.06pt,-21.96pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9.9.9.10.1.8.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.9.9.9.9.9.9.9.10.1.8.1.1.1">Llama-3.2-3B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.9">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T5.9.9.9.9.9.9.9.10.1.9.1" style="width:45.9pt;height:45.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.1pt;transform:translate(-6.09pt,-19.53pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T5.9.9.9.9.9.9.9.10.1.9.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T5.9.9.9.9.9.9.9.10.1.9.1.1.1">OLMo-2-7B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S5.T5.9.9.9.9.9.9.9.10.1.10">Avg.</span></span>
<span class="ltx_tr" id="S5.T5.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T5.1.1.1.1.1.1.1.1.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T5.1.1.1.1.1.1.1.1.1.2">AweDist</span>-Logits</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.2"><span class="ltx_text ltx_font_bold" id="S5.T5.1.1.1.1.1.1.1.1.2.1">63.0<span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.2.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.1.1.1.1.1.1.1.3.1">67.2<span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.3.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.1.1.1.1.1.1.1.4.1">67.5<span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.4.1.1" style="font-size:90%;">±0.8</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.1.1.1.1.1.1.1.5.1">67.3<span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.5.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.1.1.1.1.1.1.1.6.1">70.7<span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.6.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.1.1.1.1.1.1.1.7.1">57.4<span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.7.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.1.1.1.1.1.1.1.1.8.1">62.8<span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.8.1.1" style="font-size:90%;">±0.7</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.9">60.2<span class="ltx_text" id="S5.T5.1.1.1.1.1.1.1.1.9.1" style="font-size:90%;">±0.2</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="S5.T5.1.1.1.1.1.1.1.1.10">64.5</span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T5.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1.1" xref="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T5.2.2.2.2.2.2.2.2.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T5.2.2.2.2.2.2.2.2.1.2">AweDist</span>-NTP++</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.2.2.2.2.2.2.2.2.2.1">63.0<span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.2.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.2.2.2.2.2.2.2.2.3.1">67.2<span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.3.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.2.2.2.2.2.2.2.2.4.1">66.7<span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.4.1.1" style="font-size:90%;">±1.6</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.2.2.2.2.2.2.2.2.5.1">67.2<span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.5.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.2.2.2.2.2.2.2.2.6.1">70.6<span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.6.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.2.2.2.2.2.2.2.2.7.1">57.6<span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.7.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.8">62.2<span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.8.1" style="font-size:90%;">±0.3</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.9">59.8<span class="ltx_text" id="S5.T5.2.2.2.2.2.2.2.2.9.1" style="font-size:90%;">±0.5</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.2.2.2.2.2.2.2.2.10">64.3</span></span>
<span class="ltx_tr" id="S5.T5.4.4.4.4.4.4.4.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.4.4.4.4.4.4.4.4.2"><span class="ltx_text" id="S5.T5.3.3.3.3.3.3.3.3.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1"><semantics id="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1a"><mo id="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1.1" xref="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1b"><ci id="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml" xref="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T5.3.3.3.3.3.3.3.3.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T5.4.4.4.4.4.4.4.4.2.2">AweDist</span>-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T5.4.4.4.4.4.4.4.4.2.m1.1"><semantics id="S5.T5.4.4.4.4.4.4.4.4.2.m1.1a"><mi id="S5.T5.4.4.4.4.4.4.4.4.2.m1.1.1" xref="S5.T5.4.4.4.4.4.4.4.4.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T5.4.4.4.4.4.4.4.4.2.m1.1b"><ci id="S5.T5.4.4.4.4.4.4.4.4.2.m1.1.1.cmml" xref="S5.T5.4.4.4.4.4.4.4.4.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.4.4.4.4.4.4.4.4.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T5.4.4.4.4.4.4.4.4.2.m1.1d">italic_α</annotation></semantics></math>NTP++</span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.4.4.4.4.4.4.4.4.3.1">62.8<span class="ltx_text" id="S5.T5.4.4.4.4.4.4.4.4.3.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.4"><span class="ltx_text ltx_font_bold" id="S5.T5.4.4.4.4.4.4.4.4.4.1">67.6<span class="ltx_text" id="S5.T5.4.4.4.4.4.4.4.4.4.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.5"><span class="ltx_text ltx_font_bold" id="S5.T5.4.4.4.4.4.4.4.4.5.1">67.8<span class="ltx_text" id="S5.T5.4.4.4.4.4.4.4.4.5.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.6"><span class="ltx_text ltx_font_bold" id="S5.T5.4.4.4.4.4.4.4.4.6.1">67.4<span class="ltx_text" id="S5.T5.4.4.4.4.4.4.4.4.6.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.4.4.4.4.4.4.4.4.7.1">70.9<span class="ltx_text" id="S5.T5.4.4.4.4.4.4.4.4.7.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.8"><span class="ltx_text ltx_font_bold" id="S5.T5.4.4.4.4.4.4.4.4.8.1">57.9<span class="ltx_text" id="S5.T5.4.4.4.4.4.4.4.4.8.1.1" style="font-size:90%;">±0.1</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.4.4.4.4.4.4.4.4.9.1">62.5<span class="ltx_text" id="S5.T5.4.4.4.4.4.4.4.4.9.1.1" style="font-size:90%;">±0.6</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.10"><span class="ltx_text ltx_font_bold" id="S5.T5.4.4.4.4.4.4.4.4.10.1">61.2<span class="ltx_text" id="S5.T5.4.4.4.4.4.4.4.4.10.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.4.4.4.4.4.4.4.4.11"><span class="ltx_text ltx_font_bold" id="S5.T5.4.4.4.4.4.4.4.4.11.1">64.7</span></span></span>
<span class="ltx_tr" id="S5.T5.5.5.5.5.5.5.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.5.5.5.5.5.5.5.5.1"><span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1"><semantics id="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1a"><mo id="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1.1" xref="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1b"><ci id="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1.1.cmml" xref="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T5.5.5.5.5.5.5.5.5.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T5.5.5.5.5.5.5.5.5.1.2">AweDist</span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.5.5.5.5.5.5.5.5.2.1">62.8<span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.2.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.5.5.5.5.5.5.5.5.3.1">67.3<span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.3.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.5.5.5.5.5.5.5.5.4.1">67.6<span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.4.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.5.5.5.5.5.5.5.5.5.1">67.3<span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.5.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.6"><span class="ltx_text ltx_font_bold" id="S5.T5.5.5.5.5.5.5.5.5.6.1">71.0<span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.6.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.5.5.5.5.5.5.5.5.7.1">56.2<span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.7.1.1" style="font-size:90%;">±1.9</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.8"><span class="ltx_text ltx_font_bold" id="S5.T5.5.5.5.5.5.5.5.5.8.1">63.1<span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.8.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.5.5.5.5.5.5.5.5.9.1">61.2<span class="ltx_text" id="S5.T5.5.5.5.5.5.5.5.5.9.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.5.5.5.5.5.5.5.5.10">64.6</span></span>
<span class="ltx_tr" id="S5.T5.6.6.6.6.6.6.6.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.1"><span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1"><semantics id="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1a"><mo id="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1.1" xref="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1b"><ci id="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1.1.cmml" xref="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T5.6.6.6.6.6.6.6.6.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T5.6.6.6.6.6.6.6.6.1.2">AweDist</span>-KL-NTP++</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.6.6.6.6.6.6.6.6.2.1">62.3<span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.2.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.3">66.7<span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.3.1" style="font-size:90%;">±0.2</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.4">66.4<span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.4.1" style="font-size:90%;">±0.3</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.6.6.6.6.6.6.6.6.5.1">66.7<span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.5.1.1" style="font-size:90%;">±0.7</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.6">70.1<span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.6.1" style="font-size:90%;">±0.4</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.7">57.3<span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.7.1" style="font-size:90%;">±0.2</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.6.6.6.6.6.6.6.6.8.1">62.1<span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.8.1.1" style="font-size:90%;">±0.6</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.9">59.7<span class="ltx_text" id="S5.T5.6.6.6.6.6.6.6.6.9.1" style="font-size:90%;">±0.3</span></span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S5.T5.6.6.6.6.6.6.6.6.10">63.9</span></span>
<span class="ltx_tr" id="S5.T5.8.8.8.8.8.8.8.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T5.8.8.8.8.8.8.8.8.2"><span class="ltx_text" id="S5.T5.7.7.7.7.7.7.7.7.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1"><semantics id="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1a"><mo id="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1.1" xref="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1b"><ci id="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1.1.cmml" xref="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T5.7.7.7.7.7.7.7.7.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T5.8.8.8.8.8.8.8.8.2.2">AweDist</span>-KL-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T5.8.8.8.8.8.8.8.8.2.m1.1"><semantics id="S5.T5.8.8.8.8.8.8.8.8.2.m1.1a"><mi id="S5.T5.8.8.8.8.8.8.8.8.2.m1.1.1" xref="S5.T5.8.8.8.8.8.8.8.8.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T5.8.8.8.8.8.8.8.8.2.m1.1b"><ci id="S5.T5.8.8.8.8.8.8.8.8.2.m1.1.1.cmml" xref="S5.T5.8.8.8.8.8.8.8.8.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.8.8.8.8.8.8.8.8.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T5.8.8.8.8.8.8.8.8.2.m1.1d">italic_α</annotation></semantics></math>NTP++</span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.8.8.8.8.8.8.8.8.3.1">62.8<span class="ltx_text" id="S5.T5.8.8.8.8.8.8.8.8.3.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.8.8.8.8.8.8.8.8.4.1">67.1<span class="ltx_text" id="S5.T5.8.8.8.8.8.8.8.8.4.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.8.8.8.8.8.8.8.8.5.1">66.3<span class="ltx_text" id="S5.T5.8.8.8.8.8.8.8.8.5.1.1" style="font-size:90%;">±1.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.8.8.8.8.8.8.8.8.6.1">67.1<span class="ltx_text" id="S5.T5.8.8.8.8.8.8.8.8.6.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.8.8.8.8.8.8.8.8.7.1">70.5<span class="ltx_text" id="S5.T5.8.8.8.8.8.8.8.8.7.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.8.8.8.8.8.8.8.8.8.1">57.5<span class="ltx_text" id="S5.T5.8.8.8.8.8.8.8.8.8.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.9">62.5<span class="ltx_text" id="S5.T5.8.8.8.8.8.8.8.8.9.1" style="font-size:90%;">±0.2</span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.10"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.8.8.8.8.8.8.8.8.10.1">60.5<span class="ltx_text" id="S5.T5.8.8.8.8.8.8.8.8.10.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left" id="S5.T5.8.8.8.8.8.8.8.8.11">64.3</span></span>
<span class="ltx_tr" id="S5.T5.9.9.9.9.9.9.9.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.1"><span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1"><semantics id="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1a"><mo id="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1.1" xref="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1b"><ci id="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1.1.cmml" xref="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T5.9.9.9.9.9.9.9.9.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="S5.T5.9.9.9.9.9.9.9.9.1.2">AweDist</span>-KL</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.2"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.9.9.9.9.9.9.9.9.2.1">63.0<span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.2.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.3"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.9.9.9.9.9.9.9.9.3.1">67.3<span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.3.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.4"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.9.9.9.9.9.9.9.9.4.1">67.2<span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.4.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.5"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.9.9.9.9.9.9.9.9.5.1">67.1<span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.5.1.1" style="font-size:90%;">±0.2</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.6"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.9.9.9.9.9.9.9.9.6.1">70.8<span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.6.1.1" style="font-size:90%;">±0.5</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.7"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.9.9.9.9.9.9.9.9.7.1">57.5<span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.7.1.1" style="font-size:90%;">±0.4</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.8"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.9.9.9.9.9.9.9.9.8.1">62.7<span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.8.1.1" style="font-size:90%;">±0.3</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.9"><span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.9.9.9.9.9.9.9.9.9.1">60.5<span class="ltx_text" id="S5.T5.9.9.9.9.9.9.9.9.9.1.1" style="font-size:90%;">±0.6</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S5.T5.9.9.9.9.9.9.9.9.10">64.5</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Comparison of different variations of our distillation objective with results on biomedical benchmarks. We <span class="ltx_text ltx_font_bold" id="S5.T5.16.2">bold</span> the best result in each column and
<span class="ltx_text ltx_framed ltx_framed_underline" id="S5.T5.17.3">underline</span> all results that are not significantly worse (one-sided Welch’s t-test with Bonferroni correction, <math alttext="p&lt;0.05" class="ltx_Math" display="inline" id="S5.T5.12.m1.1"><semantics id="S5.T5.12.m1.1b"><mrow id="S5.T5.12.m1.1.1" xref="S5.T5.12.m1.1.1.cmml"><mi id="S5.T5.12.m1.1.1.2" xref="S5.T5.12.m1.1.1.2.cmml">p</mi><mo id="S5.T5.12.m1.1.1.1" xref="S5.T5.12.m1.1.1.1.cmml">&lt;</mo><mn id="S5.T5.12.m1.1.1.3" xref="S5.T5.12.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.T5.12.m1.1c"><apply id="S5.T5.12.m1.1.1.cmml" xref="S5.T5.12.m1.1.1"><lt id="S5.T5.12.m1.1.1.1.cmml" xref="S5.T5.12.m1.1.1.1"></lt><ci id="S5.T5.12.m1.1.1.2.cmml" xref="S5.T5.12.m1.1.1.2">𝑝</ci><cn id="S5.T5.12.m1.1.1.3.cmml" type="float" xref="S5.T5.12.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.12.m1.1d">p&lt;0.05</annotation><annotation encoding="application/x-llamapun" id="S5.T5.12.m1.1e">italic_p &lt; 0.05</annotation></semantics></math>).
<span class="ltx_text" id="S5.T5.13.1" style="position:relative; bottom:0.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T5.13.1.m1.1"><semantics id="S5.T5.13.1.m1.1b"><mo id="S5.T5.13.1.m1.1.1" xref="S5.T5.13.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T5.13.1.m1.1c"><ci id="S5.T5.13.1.m1.1.1.cmml" xref="S5.T5.13.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T5.13.1.m1.1d">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T5.13.1.m1.1e">⋆</annotation></semantics></math></span>: Our method(s). </figcaption>
</figure>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px1">
<h5 class="ltx_title ltx_title_paragraph">Multi-word tokens.</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px1.p1.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T3" title="Table 3 ‣ 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 3</span></a>, we report LLM-as-a-Judge results on the correctness of generated definitions. This tests the limits of our method, as the new tokens are now more complicated, such as <span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS0.Px1.p1.1.1">"Software Development"</span> or even common phrases such as <span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS0.Px1.p1.1.2">"spill the beans"</span>. In this setting, even the original target model sometimes is not able to generate correct definitions. In turn, the gap of <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.SSS0.Px1.p1.1.3">AweDist</span> to the target model is also larger in this experiment. However, <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.SSS0.Px1.p1.1.4">AweDist</span> still outperforms all baselines by a large margin.
ZeTT <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite> in particular struggles in the multi-word token setup, likely because it is out-of-distribution from the hyper-network pretraining.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px2">
<h5 class="ltx_title ltx_title_paragraph">Retrieved vs. generated data.</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px2.p1.1">In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S3" title="3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 3</span></a>, we discuss generating contexts containing our new tokens by prompting the model to generate some text containing the new token. In our main experiments, we instead retrieve relevant contexts from a corpus. We compare the two approaches in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T4" title="Table 4 ‣ 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 4</span></a>. In general, data generation performs slightly worse than retrieving contexts but is still competitive, outperforming next-token prediction objective on retrieved contexts and even pretrained hyper-works from ZeTT. This highlights the applicability of our method even in scenarios where no corpus containing relevant contexts might be available. For example, we use context generation instead of retrieval in our multi-word token experiments in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T3" title="Table 3 ‣ 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 3</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px3">
<h5 class="ltx_title ltx_title_paragraph">Choice of initial embedding for <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.SSS0.Px3.1.1">AweDist</span>.</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px3.p1.1">We find that the subtoken mean initialization as starting point yields better results than random initialization (see “Random + <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.SSS0.Px3.p1.1.1">AweDist</span>” in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T6" title="Table 6 ‣ Different distillation objectives. ‣ 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 6</span></a>).
If a pretrained embedding prediction hyper-network is available, we can instead use these predicted embeddings as the starting point.
We run this experiment using ZeTT; this combination further improves performance compared to using the subtoken mean (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T6" title="Table 6 ‣ Different distillation objectives. ‣ 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 6</span></a>). We also compare refining the hypernetwork embeddings with NTP++. This greatly improves over NTP++ but yields only minimal gains over using the hypernetwork embeddings themselves without further NTP++.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px4">
<h5 class="ltx_title ltx_title_paragraph">Different distillation objectives.</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px4.p1.1">Our main proposed method <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.SSS0.Px4.p1.1.1">AweDist</span> computes the distillation objective based on a mean-squared error (MSE) of last layer hidden states. A natural question is whether we can also use the “traditional” knowledge distillation objective <cite class="ltx_cite ltx_citemacro_citep">(Hinton et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib19" title="">2015</a>)</cite> based on the Kullback-Leibler divergence (KL-Div) <cite class="ltx_cite ltx_citemacro_citep">(Kullback and Leibler, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib26" title="">1951</a>)</cite>. We analyze this and report the results in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T5" title="Table 5 ‣ 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 5</span></a>. We additionally include computing the MSE between logits instead of hidden states. For the KL-Div based results, we include different variations of combining the objective with a NTP-based objective. In general, the choice of distillation objective does not matter much. This is hardly surprising, as the logits are merely a linear projection (potentially with an additional normalization layer) of the hidden state; the log-probabilities used for KL-Div are then in turn merely an additional log-normalization via log-softmax. Combining the KL-based results with NTP-based objectives follows similar trends as their combinations with hidden state MSE-based objectives. We opt for the MSE on hidden states for two reasons: (1) In terms of implementation convenience, the objectives with a projection to logits over the vocabulary require a masking step for the “student” model, as the original “teacher” model does not have the new tokens in its vocabulary. (2) Choosing hidden states allows us to explore other layers than just the last one with potentially large computational speedups – we explore this next.</p>
</div>
<figure class="ltx_table" id="S5.T6">
<div class="ltx_flex_figure ltx_flex_table">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_pruned_first ltx_align_center ltx_align_top" id="S5.T6.4" style="width:208.1pt;">
<div class="ltx_para ltx_noindent" id="S5.T6.2.p2">
<div class="ltx_inline-block ltx_transformed_outer" id="S5.T6.2.p2.7" style="width:433.6pt;height:225.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-23.0pt,11.9pt) scale(0.90403,0.90403) ;">
<p class="ltx_p" id="S5.T6.2.p2.7.7"><span class="ltx_text" id="S5.T6.2.p2.7.7.7">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T6.2.p2.7.7.7.7" style="width:479.7pt;height:249.8pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="S5.T6.2.p2.7.7.7.7.7"><span class="ltx_text" id="S5.T6.2.p2.7.7.7.7.7.7">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S5.T6.2.p2.7.7.7.7.7.7.7">
<span class="ltx_tbody">
<span class="ltx_tr" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1">
<span class="ltx_td ltx_th ltx_th_row ltx_border_tt" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.1"></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.2">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.2.1" style="width:38.1pt;height:38.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.9pt;transform:translate(-4.41pt,-15.56pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.2.1.1.1">Mistral-7B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.3">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.3.1" style="width:41.1pt;height:41.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.1pt;transform:translate(-5.03pt,-17.05pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.3.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.3.1.1.1">Llama-3-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.4">
<span class="ltx_inline-block ltx_transformed_outer" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.4.1" style="width:45.4pt;height:45.4pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:57.2pt;transform:translate(-5.92pt,-19.21pt) rotate(-45deg) ;">
<span class="ltx_p" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.4.1.1"><span class="ltx_text ltx_font_typewriter" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.4.1.1.1">Llama-3-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_center ltx_border_tt" id="S5.T6.2.p2.7.7.7.7.7.7.7.8.1.5">Avg.</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.7.7.7.7.7.7.7.9.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.9.2.1">Target</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.9.2.2">64.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.9.2.3">69.8</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.9.2.4">70.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.9.2.5">68.3</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.7.7.7.7.7.7.7.10.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.10.3.1">Random <cite class="ltx_cite ltx_citemacro_citep">(Hewitt, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib18" title="">2021</a>)</cite></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.10.3.2">56.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.10.3.3">58.7</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.10.3.4">60.6</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.10.3.5">58.6</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.7.7.7.7.7.7.7.11.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.2.p2.7.7.7.7.7.7.7.11.4.1">Mean <cite class="ltx_cite ltx_citemacro_citep">(Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.11.4.2">58.3</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.11.4.3">63.8</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.11.4.4">63.4</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.11.4.5">61.8</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.7.7.7.7.7.7.7.12.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.2.p2.7.7.7.7.7.7.7.12.5.1">ZeTT <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite></span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.12.5.2">62.7</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.12.5.3">66.1</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.12.5.4">66.3</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.12.5.5">65.0</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.7.7.7.7.7.7.7.13.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.13.6.1">(Mean +) NTP++</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.13.6.2">61.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.13.6.3">65.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.13.6.4">65.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.7.7.7.7.7.7.7.13.6.5">64.3</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.7.7.7.7.7.7.7.14.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.2.p2.7.7.7.7.7.7.7.14.7.1">ZeTT + NTP++</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.14.7.2">62.8</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.14.7.3">66.1</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.14.7.4">66.8</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.7.7.7.7.7.7.7.14.7.5">65.3</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.1.m1.1d">⋆</annotation></semantics></math></span> Random + <span class="ltx_text ltx_font_smallcaps" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.1.2">AweDist</span></span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.2">62.5</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.3">65.3</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.4">65.9</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.1.1.1.1.1.1.1.1.5">64.6</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1" xref="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.1.m1.1d">⋆</annotation></semantics></math></span> (Mean +) <span class="ltx_text ltx_font_smallcaps" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.1.2">AweDist</span></span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.2">63.5</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.3">67.3</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.4">68.0</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.2.2.2.2.2.2.2.2.5">66.3</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.3.3.3.3.3.3.3.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1"><span class="ltx_text" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1"><semantics id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1a"><mo id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1" xref="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1b"><ci id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1.cmml" xref="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.1.m1.1d">⋆</annotation></semantics></math></span> ZeTT + <span class="ltx_text ltx_font_smallcaps" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.1.2">AweDist</span></span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.2"><span class="ltx_text ltx_font_bold" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.2.1">64.2</span></span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.3"><span class="ltx_text ltx_font_bold" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.3.1">67.9</span></span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.4">68.4</span>
<span class="ltx_td ltx_align_center" id="S5.T6.2.p2.3.3.3.3.3.3.3.3.5">66.8</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.5.5.5.5.5.5.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2"><span class="ltx_text" id="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1"><semantics id="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1a"><mo id="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1" xref="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1b"><ci id="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1.cmml" xref="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.p2.4.4.4.4.4.4.4.4.1.1.m1.1d">⋆</annotation></semantics></math></span> (Mean +) <span class="ltx_text ltx_font_smallcaps" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.2">AweDist</span>-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1"><semantics id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1a"><mi id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1.1" xref="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1b"><ci id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1.1.cmml" xref="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.2.m1.1d">italic_α</annotation></semantics></math>NTP++</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.3">63.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.4">67.2</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.5">68.4</span>
<span class="ltx_td ltx_align_center ltx_border_t" id="S5.T6.2.p2.5.5.5.5.5.5.5.5.6">66.3</span></span>
<span class="ltx_tr" id="S5.T6.2.p2.7.7.7.7.7.7.7.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2"><span class="ltx_text" id="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1"><semantics id="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1a"><mo id="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1.1" xref="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1b"><ci id="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1.1.cmml" xref="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.p2.6.6.6.6.6.6.6.6.1.1.m1.1d">⋆</annotation></semantics></math></span> ZeTT + <span class="ltx_text ltx_font_smallcaps" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.2">AweDist</span>-<math alttext="\alpha" class="ltx_Math" display="inline" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1"><semantics id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1a"><mi id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1.1" xref="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1b"><ci id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1.1.cmml" xref="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.2.m1.1d">italic_α</annotation></semantics></math>NTP++</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.3">64.1</span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.4"><span class="ltx_text ltx_font_bold" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.4.1">67.9</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.5"><span class="ltx_text ltx_font_bold" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.5.1">68.7</span></span>
<span class="ltx_td ltx_align_center ltx_border_bb" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.6"><span class="ltx_text ltx_font_bold" id="S5.T6.2.p2.7.7.7.7.7.7.7.7.6.1">66.9</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_block">Table 6: </span>Biomedical benchmark results with different starting points for <span class="ltx_text ltx_font_smallcaps" id="S5.T6.4.4.2">AweDist</span>.
We report results for models where a pretrained hypernetwork from ZeTT is available and report the same single seed for all methods.
<span class="ltx_text" id="S5.T6.4.2.1" style="position:relative; bottom:0.0pt;"><math alttext="\star" class="ltx_Math" display="inline" id="S5.T6.4.2.1.m1.1"><semantics id="S5.T6.4.2.1.m1.1a"><mo id="S5.T6.4.2.1.m1.1.1" xref="S5.T6.4.2.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="S5.T6.4.2.1.m1.1b"><ci id="S5.T6.4.2.1.m1.1.1.cmml" xref="S5.T6.4.2.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.T6.4.2.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="S5.T6.4.2.1.m1.1d">⋆</annotation></semantics></math></span>: Our method(s).
</figcaption>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<figure class="ltx_figure ltx_figure_panel ltx_align_center ltx_align_top" id="S5.F2" style="width:208.1pt;"><img alt="Refer to caption" class="ltx_graphics ltx_figure_panel ltx_img_landscape" height="399" id="S5.T6.5.g1" src="extracted/6480660/assets/performance_vs_layer_all_lrs_meta_llama_8b_ins.png" width="598"/>
<br class="ltx_break ltx_break"/>
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Average performance on biomedical benchmarks while varying the target layer for the <span class="ltx_text ltx_font_smallcaps" id="S5.F2.3.1">AweDist</span> objective. We show results for <span class="ltx_text ltx_font_typewriter" id="S5.F2.4.2">Llama-3-8B-Instruct</span>.</figcaption>
</figure>
</div>
</div>
</figure>
</section>
<section class="ltx_paragraph" id="S5.SS2.SSS0.Px5">
<h5 class="ltx_title ltx_title_paragraph">Target layer.</h5>
<div class="ltx_para" id="S5.SS2.SSS0.Px5.p1">
<p class="ltx_p" id="S5.SS2.SSS0.Px5.p1.1">In our main experiments, we apply the <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.SSS0.Px5.p1.1.1">AweDist</span> objective on the hidden states after the last layer. However, the last hidden state might not be the most optimal, as it might be overspecialized for next-token prediction <cite class="ltx_cite ltx_citemacro_citep">(Rogers et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib46" title="">2020</a>)</cite>. In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.F2" title="Figure 2 ‣ Table 6 ‣ Different distillation objectives. ‣ 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>, we plot the average performance on our biomedical benchmark datasets using <span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS0.Px5.p1.1.2">Llama-3-8B-Instruct</span> while varying the target layer for extracting hidden states for our <span class="ltx_text ltx_font_smallcaps" id="S5.SS2.SSS0.Px5.p1.1.3">AweDist</span> objective. Indeed we do see a slight downward trend as we approach the last layer – choosing a different layer than the last one can further boost our already strong results. We repeat the analysis with <span class="ltx_text ltx_font_typewriter" id="S5.SS2.SSS0.Px5.p1.1.4">Llama-3.1-8B-Instruct</span> in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A3" title="Appendix C Additional Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Appendix C</span></a> with matching results.</p>
</div>
<div class="ltx_para" id="S5.SS2.SSS0.Px5.p2">
<p class="ltx_p" id="S5.SS2.SSS0.Px5.p2.1">Interestingly, good results can already be achieved with very early layers, e.g., after layer four. This suggests that much of the subtoken-specific contextual information is already added to the residual stream in early layers, which echoes findings of previous work <cite class="ltx_cite ltx_citemacro_citep">(Vulić et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib57" title="">2020</a>; Elhage et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib10" title="">2022</a>; Lad et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib27" title="">2024</a>; Nakash et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib38" title="">2025</a>)</cite>.
This opens up an avenue to further speed up our method by terminating the forward pass after our target layer (and also saving that compute on the backward pass), which can be much faster if we select early target layers. We leave further exploration of this as an exciting direction for future work.
For our main experiments, we choose to keep the last layer because this choice does not necessitate a model-specific sweep over target layers. Also, the last layer is a principled choice, as it guarantees that no subtoken interactions that are only modeled in later layers are excluded from the objective.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Conclusion</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">In this work, we have described a fundamental limitation of many existing embedding initialization methods, which only exploit knowledge stored in a model’s embedding matrices, whereas much of the knowledge about token compositions actually resides in the Transformer layers <cite class="ltx_cite ltx_citemacro_citep">(Elhage et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib10" title="">2022</a>; Lad et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib27" title="">2024</a>)</cite>.
To address this limitation, we have proposed <span class="ltx_text ltx_font_smallcaps" id="S6.p1.1.1">AweDist</span>, which systematically incorporates this knowledge via distilling contextual information from hidden states.
Experimental results confirm that our method not only clearly outperforms “simple” aggregation of embedding matrix rows but also yields better performance than training embeddings using traditional causal language modeling as well as hyper-networks extensively pretrained for new token embedding prediction <cite class="ltx_cite ltx_citemacro_citep">(Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>)</cite>.
In future work, we believe research into embedding initialization should move beyond an aggregation of information stored in the embedding tables.
In particular, a more localized identification of subtoken contextualization (e.g., specific attention heads that aggregate subtokens) is a promising direction for a more targeted distillation objective.</p>
</div>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">We thank the German Federal Ministry for Education and Research (BMBF) for their compute grant through the project &lt;&lt;KI-Servicezentrum Berlin Brandenburg&gt;&gt; (01IS22092). Konstantin Dobler further thanks the European Laboratory for Learning and Intelligent Systems (ELLIS) PhD program for support.
The research was supported by a research grant (VIL53122) from VILLUM FONDEN, and by the European Union’s Horizon 2020 research and innovation program under grant agreement No. 101135671 (TrustLLM).</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ahia et al. [2023]</span>
<span class="ltx_bibblock">
Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, and Yulia Tsvetkov.

</span>
<span class="ltx_bibblock">Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models, May 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2305.13707" title="">http://arxiv.org/abs/2305.13707</a>.

</span>
<span class="ltx_bibblock">arXiv:2305.13707 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aho and Corasick [1975]</span>
<span class="ltx_bibblock">
Alfred V. Aho and Margaret J. Corasick.

</span>
<span class="ltx_bibblock">Efficient string matching: An aid to bibliographic search.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Commun. ACM</em>, 18(6):333–340, June 1975.

</span>
<span class="ltx_bibblock">ISSN 0001-0782.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1145/360825.360855</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://doi.acm.org/10.1145/360825.360855" title="">http://doi.acm.org/10.1145/360825.360855</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ali et al. [2024]</span>
<span class="ltx_bibblock">
Mehdi Ali, Michael Fromm, Klaudia Thellmann, Richard Rutmann, Max Lübbering, Johannes Leveling, Katrin Klug, Jan Ebert, Niclas Doll, Jasper Buschhoff, Charvi Jain, Alexander Weber, Lena Jurkschat, Hammam Abdelwahab, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff, Samuel Weinbach, Rafet Sifa, Stefan Kesselheim, and Nicolas Flores-Herr.

</span>
<span class="ltx_bibblock">Tokenizer Choice For LLM Training: Negligible or Crucial?

</span>
<span class="ltx_bibblock">In Kevin Duh, Helena Gomez, and Steven Bethard, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Findings of the Association for Computational Linguistics: NAACL 2024</em>, pages 3907–3924, Mexico City, Mexico, June 2024. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2024.findings-naacl.247</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.findings-naacl.247/" title="">https://aclanthology.org/2024.findings-naacl.247/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Artetxe et al. [2020]</span>
<span class="ltx_bibblock">
Mikel Artetxe, Sebastian Ruder, and Dani Yogatama.

</span>
<span class="ltx_bibblock">On the Cross-lingual Transferability of Monolingual Representations.

</span>
<span class="ltx_bibblock">In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 4623–4637, Online, July 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.acl-main.421</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.421/" title="">https://aclanthology.org/2020.acl-main.421/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Biderman et al. [2024]</span>
<span class="ltx_bibblock">
Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, François Yvon, and Andy Zou.

</span>
<span class="ltx_bibblock">Lessons from the Trenches on Reproducible Evaluation of Language Models, May 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.14782" title="">http://arxiv.org/abs/2405.14782</a>.

</span>
<span class="ltx_bibblock">arXiv:2405.14782 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. [2018]</span>
<span class="ltx_bibblock">
Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich.

</span>
<span class="ltx_bibblock">GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks, June 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1711.02257" title="">http://arxiv.org/abs/1711.02257</a>.

</span>
<span class="ltx_bibblock">arXiv:1711.02257 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Vries and Nissim [2021]</span>
<span class="ltx_bibblock">
Wietse de Vries and Malvina Nissim.

</span>
<span class="ltx_bibblock">As Good as New. How to Successfully Recycle English GPT-2 to Make Models for Other Languages.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021</em>, pages 836–846, Online, August 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.findings-acl.74</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.findings-acl.74/" title="">https://aclanthology.org/2021.findings-acl.74/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dobler and de Melo [2023]</span>
<span class="ltx_bibblock">
Konstantin Dobler and Gerard de Melo.

</span>
<span class="ltx_bibblock">FOCUS: Effective Embedding Initialization for Monolingual Specialization of Multilingual Models.

</span>
<span class="ltx_bibblock">In Houda Bouamor, Juan Pino, and Kalika Bali, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, pages 13440–13454, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.emnlp-main.829</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-main.829/" title="">https://aclanthology.org/2023.emnlp-main.829/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Downey et al. [2023]</span>
<span class="ltx_bibblock">
C. M. Downey, Terra Blevins, Nora Goldfine, and Shane Steinert-Threlkeld.

</span>
<span class="ltx_bibblock">Embedding structure matters: Comparing methods to adapt multilingual vocabularies to new languages, October 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2309.04679" title="">http://arxiv.org/abs/2309.04679</a>.

</span>
<span class="ltx_bibblock">arXiv:2309.04679 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elhage et al. [2022]</span>
<span class="ltx_bibblock">
Nelson Elhage, Tristan Hume, Catherine Olsson, Neel Nanda, Tom Henighan, Scott Johnston, Sheer ElShowk, Nicholas Joseph, Nova DasSarma, Ben Mann, Danny Hernandez, Amanda Askell, Kamal Ndousse, Andy Jones, Dawn Drain, Anna Chen, Yuntao Bai, Deep Ganguli, Liane Lovitt, Zac Hatfield-Dodds, Jackson Kernion, Tom Conerly, Shauna Kravec, Stanislav Fort, Saurav Kadavath, Josh Jacobson, Eli Tran-Johnson, Jared Kaplan, Jack Clark, Tom Brown, Sam McCandlish, Dario Amodei, and Christopher Olah.

</span>
<span class="ltx_bibblock">Softmax linear units.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Transformer Circuits Thread</em>, 2022.

</span>
<span class="ltx_bibblock">https://transformer-circuits.pub/2022/solu/index.html.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gee et al. [2022]</span>
<span class="ltx_bibblock">
Leonidas Gee, Andrea Zugarini, Leonardo Rigutini, and Paolo Torroni.

</span>
<span class="ltx_bibblock">Fast Vocabulary Transfer for Language Model Compression.

</span>
<span class="ltx_bibblock">In Yunyao Li and Angeliki Lazaridou, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: Industry Track</em>, pages 409–416, Abu Dhabi, UAE, December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.emnlp-industry.41</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.emnlp-industry.41/" title="">https://aclanthology.org/2022.emnlp-industry.41/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gee et al. [2023]</span>
<span class="ltx_bibblock">
Leonidas Gee, Leonardo Rigutini, Marco Ernandes, and Andrea Zugarini.

</span>
<span class="ltx_bibblock">Multi-word Tokenization for Sequence Compression.

</span>
<span class="ltx_bibblock">In Mingxuan Wang and Imed Zitouni, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Industry Track</em>, pages 612–621, Singapore, December 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.emnlp-industry.58</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.emnlp-industry.58/" title="">https://aclanthology.org/2023.emnlp-industry.58/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Grattafiori et al. [2024]</span>
<span class="ltx_bibblock">
Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, Arun Rao, Aston Zhang, Aurelien Rodriguez, Austen Gregerson, Ava Spataru, Baptiste Roziere, Bethany Biron, Binh Tang, Bobbie Chern, Charlotte Caucheteux, Chaya Nayak, Chloe Bi, Chris Marra, Chris McConnell, Christian Keller, Christophe Touret, Chunyang Wu, Corinne Wong, Cristian Canton Ferrer, Cyrus Nikolaidis, Damien Allonsius, Daniel Song, Danielle Pintz, Danny Livshits, Danny Wyatt, David Esiobu, Dhruv Choudhary, Dhruv Mahajan, Diego Garcia-Olano, Diego Perino, Dieuwke Hupkes, Egor Lakomkin, Ehab AlBadawy, Elina Lobanova, Emily Dinan, Eric Michael Smith, Filip Radenovic, Francisco Guzmán, Frank Zhang, Gabriel Synnaeve, Gabrielle Lee, Georgia Lewis Anderson, Govind Thattai, Graeme Nail, Gregoire Mialon, Guan Pang,
Guillem Cucurell, Hailey Nguyen, Hannah Korevaar, Hu Xu, Hugo Touvron, Iliyan Zarov, Imanol Arrieta Ibarra, Isabel Kloumann, Ishan Misra, Ivan Evtimov, Jack Zhang, Jade Copet, Jaewon Lee, Jan Geffert, Jana Vranes, Jason Park, Jay Mahadeokar, Jeet Shah, Jelmer van der Linde, Jennifer Billock, Jenny Hong, Jenya Lee, Jeremy Fu, Jianfeng Chi, Jianyu Huang, Jiawen Liu, Jie Wang, Jiecao Yu, Joanna Bitton, Joe Spisak, Jongsoo Park, Joseph Rocca, Joshua Johnstun, Joshua Saxe, Junteng Jia, Kalyan Vasuden Alwala, Karthik Prasad, Kartikeya Upasani, Kate Plawiak, Ke Li, Kenneth Heafield, Kevin Stone, Khalid El-Arini, Krithika Iyer, Kshitiz Malik, Kuenley Chiu, Kunal Bhalla, Kushal Lakhotia, Lauren Rantala-Yeary, Laurens van der Maaten, Lawrence Chen, Liang Tan, Liz Jenkins, Louis Martin, Lovish Madaan, Lubo Malo, Lukas Blecher, Lukas Landzaat, Luke de Oliveira, Madeline Muzzi, Mahesh Pasupuleti, Mannat Singh, Manohar Paluri, Marcin Kardas, Maria Tsimpoukelli, Mathew Oldham, Mathieu Rita, Maya Pavlova, Melanie Kambadur,
Mike Lewis, Min Si, Mitesh Kumar Singh, Mona Hassan, Naman Goyal, Narjes Torabi, Nikolay Bashlykov, Nikolay Bogoychev, Niladri Chatterji, Ning Zhang, Olivier Duchenne, Onur Çelebi, Patrick Alrassy, Pengchuan Zhang, Pengwei Li, Petar Vasic, Peter Weng, Prajjwal Bhargava, Pratik Dubal, Praveen Krishnan, Punit Singh Koura, Puxin Xu, Qing He, Qingxiao Dong, Ragavan Srinivasan, Raj Ganapathy, Ramon Calderer, Ricardo Silveira Cabral, Robert Stojnic, Roberta Raileanu, Rohan Maheswari, Rohit Girdhar, Rohit Patel, Romain Sauvestre, Ronnie Polidoro, Roshan Sumbaly, Ross Taylor, Ruan Silva, Rui Hou, Rui Wang, Saghar Hosseini, Sahana Chennabasappa, Sanjay Singh, Sean Bell, Seohyun Sonia Kim, Sergey Edunov, Shaoliang Nie, Sharan Narang, Sharath Raparthy, Sheng Shen, Shengye Wan, Shruti Bhosale, Shun Zhang, Simon Vandenhende, Soumya Batra, Spencer Whitman, Sten Sootla, Stephane Collot, Suchin Gururangan, Sydney Borodinsky, Tamar Herman, Tara Fowler, Tarek Sheasha, Thomas Georgiou, Thomas Scialom, Tobias Speckbacher,
Todor Mihaylov, Tong Xiao, Ujjwal Karn, Vedanuj Goswami, Vibhor Gupta, Vignesh Ramanathan, Viktor Kerkez, Vincent Gonguet, Virginie Do, Vish Vogeti, Vítor Albiero, Vladan Petrovic, Weiwei Chu, Wenhan Xiong, Wenyin Fu, Whitney Meers, Xavier Martinet, Xiaodong Wang, Xiaofang Wang, Xiaoqing Ellen Tan, Xide Xia, Xinfeng Xie, Xuchao Jia, Xuewei Wang, Yaelle Goldschlag, Yashesh Gaur, Yasmine Babaei, Yi Wen, Yiwen Song, Yuchen Zhang, Yue Li, Yuning Mao, Zacharie Delpierre Coudert, Zheng Yan, Zhengxing Chen, Zoe Papakipos, Aaditya Singh, Aayushi Srivastava, Abha Jain, Adam Kelsey, Adam Shajnfeld, Adithya Gangidi, Adolfo Victoria, Ahuva Goldstand, Ajay Menon, Ajay Sharma, Alex Boesenberg, Alexei Baevski, Allie Feinstein, Amanda Kallet, Amit Sangani, Amos Teo, Anam Yunus, Andrei Lupu, Andres Alvarado, Andrew Caples, Andrew Gu, Andrew Ho, Andrew Poulton, Andrew Ryan, Ankit Ramchandani, Annie Dong, Annie Franco, Anuj Goyal, Aparajita Saraf, Arkabandhu Chowdhury, Ashley Gabriel, Ashwin Bharambe, Assaf Eisenman, Azadeh
Yazdan, Beau James, Ben Maurer, Benjamin Leonhardi, Bernie Huang, Beth Loyd, Beto De Paola, Bhargavi Paranjape, Bing Liu, Bo Wu, Boyu Ni, Braden Hancock, Bram Wasti, Brandon Spence, Brani Stojkovic, Brian Gamido, Britt Montalvo, Carl Parker, Carly Burton, Catalina Mejia, Ce Liu, Changhan Wang, Changkyu Kim, Chao Zhou, Chester Hu, Ching-Hsiang Chu, Chris Cai, Chris Tindal, Christoph Feichtenhofer, Cynthia Gao, Damon Civin, Dana Beaty, Daniel Kreymer, Daniel Li, David Adkins, David Xu, Davide Testuggine, Delia David, Devi Parikh, Diana Liskovich, Didem Foss, Dingkang Wang, Duc Le, Dustin Holland, Edward Dowling, Eissa Jamil, Elaine Montgomery, Eleonora Presani, Emily Hahn, Emily Wood, Eric-Tuan Le, Erik Brinkman, Esteban Arcaute, Evan Dunbar, Evan Smothers, Fei Sun, Felix Kreuk, Feng Tian, Filippos Kokkinos, Firat Ozgenel, Francesco Caggioni, Frank Kanayet, Frank Seide, Gabriela Medina Florez, Gabriella Schwarz, Gada Badeer, Georgia Swee, Gil Halpern, Grant Herman, Grigory Sizov, Guangyi, Zhang, Guna
Lakshminarayanan, Hakan Inan, Hamid Shojanazeri, Han Zou, Hannah Wang, Hanwen Zha, Haroun Habeeb, Harrison Rudolph, Helen Suk, Henry Aspegren, Hunter Goldman, Hongyuan Zhan, Ibrahim Damlaj, Igor Molybog, Igor Tufanov, Ilias Leontiadis, Irina-Elena Veliche, Itai Gat, Jake Weissman, James Geboski, James Kohli, Janice Lam, Japhet Asher, Jean-Baptiste Gaya, Jeff Marcus, Jeff Tang, Jennifer Chan, Jenny Zhen, Jeremy Reizenstein, Jeremy Teboul, Jessica Zhong, Jian Jin, Jingyi Yang, Joe Cummings, Jon Carvill, Jon Shepard, Jonathan McPhie, Jonathan Torres, Josh Ginsburg, Junjie Wang, Kai Wu, Kam Hou U, Karan Saxena, Kartikay Khandelwal, Katayoun Zand, Kathy Matosich, Kaushik Veeraraghavan, Kelly Michelena, Keqian Li, Kiran Jagadeesh, Kun Huang, Kunal Chawla, Kyle Huang, Lailin Chen, Lakshya Garg, Lavender A, Leandro Silva, Lee Bell, Lei Zhang, Liangpeng Guo, Licheng Yu, Liron Moshkovich, Luca Wehrstedt, Madian Khabsa, Manav Avalani, Manish Bhatt, Martynas Mankus, Matan Hasson, Matthew Lennie, Matthias Reso, Maxim
Groshev, Maxim Naumov, Maya Lathi, Meghan Keneally, Miao Liu, Michael L. Seltzer, Michal Valko, Michelle Restrepo, Mihir Patel, Mik Vyatskov, Mikayel Samvelyan, Mike Clark, Mike Macey, Mike Wang, Miquel Jubert Hermoso, Mo Metanat, Mohammad Rastegari, Munish Bansal, Nandhini Santhanam, Natascha Parks, Natasha White, Navyata Bawa, Nayan Singhal, Nick Egebo, Nicolas Usunier, Nikhil Mehta, Nikolay Pavlovich Laptev, Ning Dong, Norman Cheng, Oleg Chernoguz, Olivia Hart, Omkar Salpekar, Ozlem Kalinli, Parkin Kent, Parth Parekh, Paul Saab, Pavan Balaji, Pedro Rittner, Philip Bontrager, Pierre Roux, Piotr Dollar, Polina Zvyagina, Prashant Ratanchandani, Pritish Yuvraj, Qian Liang, Rachad Alao, Rachel Rodriguez, Rafi Ayub, Raghotham Murthy, Raghu Nayani, Rahul Mitra, Rangaprabhu Parthasarathy, Raymond Li, Rebekkah Hogan, Robin Battey, Rocky Wang, Russ Howes, Ruty Rinott, Sachin Mehta, Sachin Siby, Sai Jayesh Bondu, Samyak Datta, Sara Chugh, Sara Hunt, Sargun Dhillon, Sasha Sidorov, Satadru Pan, Saurabh Mahajan,
Saurabh Verma, Seiji Yamamoto, Sharadh Ramaswamy, Shaun Lindsay, Shaun Lindsay, Sheng Feng, Shenghao Lin, Shengxin Cindy Zha, Shishir Patil, Shiva Shankar, Shuqiang Zhang, Shuqiang Zhang, Sinong Wang, Sneha Agarwal, Soji Sajuyigbe, Soumith Chintala, Stephanie Max, Stephen Chen, Steve Kehoe, Steve Satterfield, Sudarshan Govindaprasad, Sumit Gupta, Summer Deng, Sungmin Cho, Sunny Virk, Suraj Subramanian, Sy Choudhury, Sydney Goldman, Tal Remez, Tamar Glaser, Tamara Best, Thilo Koehler, Thomas Robinson, Tianhe Li, Tianjun Zhang, Tim Matthews, Timothy Chou, Tzook Shaked, Varun Vontimitta, Victoria Ajayi, Victoria Montanez, Vijai Mohan, Vinay Satish Kumar, Vishal Mangla, Vlad Ionescu, Vlad Poenaru, Vlad Tiberiu Mihailescu, Vladimir Ivanov, Wei Li, Wenchen Wang, Wenwen Jiang, Wes Bouaziz, Will Constable, Xiaocheng Tang, Xiaojian Wu, Xiaolan Wang, Xilun Wu, Xinbo Gao, Yaniv Kleinman, Yanjun Chen, Ye Hu, Ye Jia, Ye Qi, Yenda Li, Yilin Zhang, Ying Zhang, Yossi Adi, Youngjin Nam, Yu, Wang, Yu Zhao, Yuchen Hao,
Yundi Qian, Yunlu Li, Yuzi He, Zach Rait, Zachary DeVito, Zef Rosnbrick, Zhaoduo Wen, Zhenyu Yang, Zhiwei Zhao, and Zhiyu Ma.

</span>
<span class="ltx_bibblock">The Llama 3 Herd of Models, November 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2407.21783" title="">http://arxiv.org/abs/2407.21783</a>.

</span>
<span class="ltx_bibblock">arXiv:2407.21783 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gururangan et al. [2020]</span>
<span class="ltx_bibblock">
Suchin Gururangan, Ana Marasović, Swabha Swayamdipta, Kyle Lo, Iz Beltagy, Doug Downey, and Noah A. Smith.

</span>
<span class="ltx_bibblock">Don’t Stop Pretraining: Adapt Language Models to Domains and Tasks, May 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2004.10964" title="">http://arxiv.org/abs/2004.10964</a>.

</span>
<span class="ltx_bibblock">arXiv:2004.10964 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ha et al. [2017]</span>
<span class="ltx_bibblock">
David Ha, Andrew M. Dai, and Quoc V. Le.

</span>
<span class="ltx_bibblock">HyperNetworks.

</span>
<span class="ltx_bibblock">February 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=rkpACe1lx" title="">https://openreview.net/forum?id=rkpACe1lx</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hasan et al. [2024]</span>
<span class="ltx_bibblock">
Abul Hasan, Jinge Wu, Quang Ngoc Nguyen, Salomé Andres, Imane Guellil, Huayu Zhang, Arlene Casey, Beatrice Alex, Bruce Guthrie, and Honghan Wu.

</span>
<span class="ltx_bibblock">Infusing clinical knowledge into tokenisers for language models.

</span>
<span class="ltx_bibblock">2024.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.48550/ARXIV.2406.14312</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2406.14312" title="">https://arxiv.org/abs/2406.14312</a>.

</span>
<span class="ltx_bibblock">Publisher: arXiv Version Number: 1.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hendrycks et al. [2021]</span>
<span class="ltx_bibblock">
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt.

</span>
<span class="ltx_bibblock">Measuring Massive Multitask Language Understanding, January 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2009.03300" title="">http://arxiv.org/abs/2009.03300</a>.

</span>
<span class="ltx_bibblock">arXiv:2009.03300 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hewitt [2021]</span>
<span class="ltx_bibblock">
John Hewitt.

</span>
<span class="ltx_bibblock">Initializing New Word Embeddings for Pretrained Language Models, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://nlp.stanford.edu//~johnhew//vocab-expansion.html" title="">https://nlp.stanford.edu//~johnhew//vocab-expansion.html</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hinton et al. [2015]</span>
<span class="ltx_bibblock">
Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.

</span>
<span class="ltx_bibblock">Distilling the Knowledge in a Neural Network, March 2015.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1503.02531" title="">http://arxiv.org/abs/1503.02531</a>.

</span>
<span class="ltx_bibblock">arXiv:1503.02531 [stat].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huang et al. [2025]</span>
<span class="ltx_bibblock">
Hongzhi Huang, Defa Zhu, Banggu Wu, Yutao Zeng, Ya Wang, Qiyang Min, and Xun Zhou.

</span>
<span class="ltx_bibblock">Over-Tokenized Transformer: Vocabulary is Generally Worth Scaling, January 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2501.16975" title="">http://arxiv.org/abs/2501.16975</a>.

</span>
<span class="ltx_bibblock">arXiv:2501.16975 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. [2023]</span>
<span class="ltx_bibblock">
Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed.

</span>
<span class="ltx_bibblock">Mistral 7B, October 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2310.06825" title="">http://arxiv.org/abs/2310.06825</a>.

</span>
<span class="ltx_bibblock">arXiv:2310.06825 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2020]</span>
<span class="ltx_bibblock">
Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits.

</span>
<span class="ltx_bibblock">What Disease does this Patient Have? A Large-scale Open Domain Question Answering Dataset from Medical Exams, September 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2009.13081" title="">http://arxiv.org/abs/2009.13081</a>.

</span>
<span class="ltx_bibblock">arXiv:2009.13081 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jin et al. [2019]</span>
<span class="ltx_bibblock">
Qiao Jin, Bhuwan Dhingra, Zhengping Liu, William W. Cohen, and Xinghua Lu.

</span>
<span class="ltx_bibblock">PubMedQA: A Dataset for Biomedical Research Question Answering, September 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1909.06146" title="">http://arxiv.org/abs/1909.06146</a>.

</span>
<span class="ltx_bibblock">arXiv:1909.06146 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kingma and Ba [2017]</span>
<span class="ltx_bibblock">
Diederik P. Kingma and Jimmy Ba.

</span>
<span class="ltx_bibblock">Adam: A Method for Stochastic Optimization, January 2017.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1412.6980" title="">http://arxiv.org/abs/1412.6980</a>.

</span>
<span class="ltx_bibblock">arXiv:1412.6980 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Koto et al. [2021]</span>
<span class="ltx_bibblock">
Fajri Koto, Jey Han Lau, and Timothy Baldwin.

</span>
<span class="ltx_bibblock">IndoBERTweet: A Pretrained Language Model for Indonesian Twitter with Effective Domain-Specific Vocabulary Initialization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pages 10660–10668, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.833</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.833" title="">https://aclanthology.org/2021.emnlp-main.833</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kullback and Leibler [1951]</span>
<span class="ltx_bibblock">
S. Kullback and R. A. Leibler.

</span>
<span class="ltx_bibblock">On Information and Sufficiency.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">The Annals of Mathematical Statistics</em>, 22(1):79–86, 1951.

</span>
<span class="ltx_bibblock">ISSN 0003-4851.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.jstor.org/stable/2236703" title="">https://www.jstor.org/stable/2236703</a>.

</span>
<span class="ltx_bibblock">Publisher: Institute of Mathematical Statistics.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lad et al. [2024]</span>
<span class="ltx_bibblock">
Vedang Lad, Wes Gurnee, and Max Tegmark.

</span>
<span class="ltx_bibblock">The Remarkable Robustness of LLMs: Stages of Inference?, June 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2406.19384" title="">http://arxiv.org/abs/2406.19384</a>.

</span>
<span class="ltx_bibblock">arXiv:2406.19384 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lampinen and McClelland [2018]</span>
<span class="ltx_bibblock">
Andrew K. Lampinen and James L. McClelland.

</span>
<span class="ltx_bibblock">One-shot and few-shot learning of word embeddings, January 2018.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1710.10280" title="">http://arxiv.org/abs/1710.10280</a>.

</span>
<span class="ltx_bibblock">arXiv:1710.10280 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. [2023]</span>
<span class="ltx_bibblock">
Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto.

</span>
<span class="ltx_bibblock">AlpacaEval: An Automatic Evaluator of Instruction-following Models, May 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/tatsu-lab/alpaca_eval" title="">https://github.com/tatsu-lab/alpaca_eval</a>.

</span>
<span class="ltx_bibblock">original-date: 2023-05-25T09:35:28Z.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2025]</span>
<span class="ltx_bibblock">
Alisa Liu, Jonathan Hayase, Valentin Hofmann, Sewoong Oh, Noah A. Smith, and Yejin Choi.

</span>
<span class="ltx_bibblock">SuperBPE: Space Travel for Language Models, April 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2503.13423" title="">http://arxiv.org/abs/2503.13423</a>.

</span>
<span class="ltx_bibblock">arXiv:2503.13423 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. [2024]</span>
<span class="ltx_bibblock">
Yihong Liu, Peiqin Lin, Mingyang Wang, and Hinrich Schütze.

</span>
<span class="ltx_bibblock">OFA: A Framework of Initializing Unseen Subword Embeddings for Efficient Large-scale Multilingual Continued Pretraining, March 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2311.08849" title="">http://arxiv.org/abs/2311.08849</a>.

</span>
<span class="ltx_bibblock">arXiv:2311.08849 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loshchilov and Hutter [2019]</span>
<span class="ltx_bibblock">
Ilya Loshchilov and Frank Hutter.

</span>
<span class="ltx_bibblock">Decoupled Weight Decay Regularization, January 2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1711.05101" title="">http://arxiv.org/abs/1711.05101</a>.

</span>
<span class="ltx_bibblock">arXiv:1711.05101 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Marchisio et al. [2023]</span>
<span class="ltx_bibblock">
Kelly Marchisio, Patrick Lewis, Yihong Chen, and Mikel Artetxe.

</span>
<span class="ltx_bibblock">Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training.

</span>
<span class="ltx_bibblock">In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Findings of the Association for Computational Linguistics: ACL 2023</em>, pages 5474–5490, Toronto, Canada, July 2023. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2023.findings-acl.338</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2023.findings-acl.338/" title="">https://aclanthology.org/2023.findings-acl.338/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minixhofer et al. [2022]</span>
<span class="ltx_bibblock">
Benjamin Minixhofer, Fabian Paischer, and Navid Rekabsaz.

</span>
<span class="ltx_bibblock">WECHSEL: Effective initialization of subword embeddings for cross-lingual transfer of monolingual language models.

</span>
<span class="ltx_bibblock">In Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pages 3992–4006, Seattle, United States, July 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.naacl-main.293</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.naacl-main.293/" title="">https://aclanthology.org/2022.naacl-main.293/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Minixhofer et al. [2024]</span>
<span class="ltx_bibblock">
Benjamin Minixhofer, Edoardo Maria Ponti, and Ivan Vulić.

</span>
<span class="ltx_bibblock">Zero-Shot Tokenizer Transfer, May 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2405.07883" title="">http://arxiv.org/abs/2405.07883</a>.

</span>
<span class="ltx_bibblock">arXiv:2405.07883 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mosin et al. [2023]</span>
<span class="ltx_bibblock">
Vladislav Mosin, Igor Samenko, Alexey Tikhonov, Borislav Kozlovskii, and Ivan P. Yamshchikov.

</span>
<span class="ltx_bibblock">Fine-Tuning Transformers: Vocabulary Transfer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Artificial Intelligence</em>, 317:103860, April 2023.

</span>
<span class="ltx_bibblock">ISSN 00043702.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1016/j.artint.2023.103860</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2112.14569" title="">http://arxiv.org/abs/2112.14569</a>.

</span>
<span class="ltx_bibblock">arXiv:2112.14569 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mundra et al. [2024]</span>
<span class="ltx_bibblock">
Nandini Mundra, Aditya Nanda Kishore, Raj Dabre, Ratish Puduppully, Anoop Kunchukuttan, and Mitesh M. Khapra.

</span>
<span class="ltx_bibblock">An Empirical Comparison of Vocabulary Expansion and Initialization Approaches for Language Models, October 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2407.05841" title="">http://arxiv.org/abs/2407.05841</a>.

</span>
<span class="ltx_bibblock">arXiv:2407.05841 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakash et al. [2025]</span>
<span class="ltx_bibblock">
Itay Nakash, Nitay Calderon, Eyal Ben David, Elad Hoffer, and Roi Reichart.

</span>
<span class="ltx_bibblock">AdaptiVocab: Enhancing LLM Efficiency in Focused Domains through Lightweight Vocabulary Adaptation, March 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2503.19693" title="">http://arxiv.org/abs/2503.19693</a>.

</span>
<span class="ltx_bibblock">arXiv:2503.19693 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OLMo et al. [2025]</span>
<span class="ltx_bibblock">
Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, Nathan Lambert, Dustin Schwenk, Oyvind Tafjord, Taira Anderson, David Atkinson, Faeze Brahman, Christopher Clark, Pradeep Dasigi, Nouha Dziri, Michal Guerquin, Hamish Ivison, Pang Wei Koh, Jiacheng Liu, Saumya Malik, William Merrill, Lester James V. Miranda, Jacob Morrison, Tyler Murray, Crystal Nam, Valentina Pyatkin, Aman Rangapur, Michael Schmitz, Sam Skjonsberg, David Wadden, Christopher Wilhelm, Michael Wilson, Luke Zettlemoyer, Ali Farhadi, Noah A. Smith, and Hannaneh Hajishirzi.

</span>
<span class="ltx_bibblock">2 OLMo 2 Furious, January 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2501.00656" title="">http://arxiv.org/abs/2501.00656</a>.

</span>
<span class="ltx_bibblock">arXiv:2501.00656 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ostendorff and Rehm [2023]</span>
<span class="ltx_bibblock">
Malte Ostendorff and Georg Rehm.

</span>
<span class="ltx_bibblock">Efficient Language Model Training through Cross-Lingual and Progressive Transfer Learning, January 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2301.09626" title="">http://arxiv.org/abs/2301.09626</a>.

</span>
<span class="ltx_bibblock">arXiv:2301.09626 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal et al. [2022]</span>
<span class="ltx_bibblock">
Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu.

</span>
<span class="ltx_bibblock">MedMCQA : A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain Question Answering, March 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2203.14371" title="">http://arxiv.org/abs/2203.14371</a>.

</span>
<span class="ltx_bibblock">arXiv:2203.14371 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pal et al. [2024]</span>
<span class="ltx_bibblock">
Ankit Pal, Pasquale Minervini, Andreas Geert Motzfeldt, and Beatrice Alex.

</span>
<span class="ltx_bibblock">openlifescienceai/open_medical_llm_leaderboard.

</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard" title="">https://huggingface.co/spaces/openlifescienceai/open_medical_llm_leaderboard</a>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Pinter et al. [2017]</span>
<span class="ltx_bibblock">
Yuval Pinter, Robert Guthrie, and Jacob Eisenstein.

</span>
<span class="ltx_bibblock">Mimicking Word Embeddings using Subword RNNs.

</span>
<span class="ltx_bibblock">In Martha Palmer, Rebecca Hwa, and Sebastian Riedel, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</em>, pages 102–112, Copenhagen, Denmark, September 2017. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D17-1010</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D17-1010/" title="">https://aclanthology.org/D17-1010/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Poerner et al. [2020]</span>
<span class="ltx_bibblock">
Nina Poerner, Ulli Waltinger, and Hinrich Schütze.

</span>
<span class="ltx_bibblock">Inexpensive Domain Adaptation of Pretrained Language Models: Case Studies on Biomedical NER and Covid-19 QA.

</span>
<span class="ltx_bibblock">In Trevor Cohn, Yulan He, and Yang Liu, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</em>, pages 1482–1490, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.findings-emnlp.134</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.findings-emnlp.134/" title="">https://aclanthology.org/2020.findings-emnlp.134/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Remy et al. [2024]</span>
<span class="ltx_bibblock">
François Remy, Pieter Delobelle, Hayastan Avetisyan, Alfiya Khabibullina, Miryam de Lhoneux, and Thomas Demeester.

</span>
<span class="ltx_bibblock">Trans-Tokenization and Cross-lingual Vocabulary Transfers: Language Adaptation of LLMs for Low-Resource NLP, August 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2408.04303" title="">http://arxiv.org/abs/2408.04303</a>.

</span>
<span class="ltx_bibblock">arXiv:2408.04303 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rogers et al. [2020]</span>
<span class="ltx_bibblock">
Anna Rogers, Olga Kovaleva, and Anna Rumshisky.

</span>
<span class="ltx_bibblock">A Primer in BERTology: What we know about how BERT works, November 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2002.12327" title="">http://arxiv.org/abs/2002.12327</a>.

</span>
<span class="ltx_bibblock">arXiv:2002.12327 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rust et al. [2021]</span>
<span class="ltx_bibblock">
Phillip Rust, Jonas Pfeiffer, Ivan Vulić, Sebastian Ruder, and Iryna Gurevych.

</span>
<span class="ltx_bibblock">How Good is Your Tokenizer? On the Monolingual Performance of Multilingual Language Models.

</span>
<span class="ltx_bibblock">In Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)</em>, pages 3118–3135, Online, August 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.acl-long.243</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.acl-long.243/" title="">https://aclanthology.org/2021.acl-long.243/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sachidananda et al. [2021]</span>
<span class="ltx_bibblock">
Vin Sachidananda, Jason Kessler, and Yi-An Lai.

</span>
<span class="ltx_bibblock">Efficient Domain Adaptation of Language Models via Adaptive Tokenization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the Second Workshop on Simple and Efficient Natural Language Processing</em>, pages 155–165, Virtual, 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.sustainlp-1.16</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.sustainlp-1.16" title="">https://aclanthology.org/2021.sustainlp-1.16</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick and Schütze [2019]</span>
<span class="ltx_bibblock">
Timo Schick and Hinrich Schütze.

</span>
<span class="ltx_bibblock">Attentive Mimicking: Better Word Embeddings by Attending to Informative Contexts.

</span>
<span class="ltx_bibblock">In Jill Burstein, Christy Doran, and Thamar Solorio, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pages 489–494, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1048</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1048/" title="">https://aclanthology.org/N19-1048/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick and Schütze [2020]</span>
<span class="ltx_bibblock">
Timo Schick and Hinrich Schütze.

</span>
<span class="ltx_bibblock">BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance.

</span>
<span class="ltx_bibblock">In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pages 3996–4007, Online, July 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.acl-main.368</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.368/" title="">https://aclanthology.org/2020.acl-main.368/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sennrich et al. [2016]</span>
<span class="ltx_bibblock">
Rico Sennrich, Barry Haddow, and Alexandra Birch.

</span>
<span class="ltx_bibblock">Neural Machine Translation of Rare Words with Subword Units, June 2016.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/1508.07909" title="">http://arxiv.org/abs/1508.07909</a>.

</span>
<span class="ltx_bibblock">arXiv:1508.07909 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Singh et al. [2024]</span>
<span class="ltx_bibblock">
Priyanka Singh, Vladislav D. Mosin, and Ivan P. Yamshchikov.

</span>
<span class="ltx_bibblock">Vocabulary Transfer for Biomedical Texts: Add Tokens if You Can Not Add Data, November 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2208.02554" title="">http://arxiv.org/abs/2208.02554</a>.

</span>
<span class="ltx_bibblock">arXiv:2208.02554 [cs] version: 3.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snell et al. [2022]</span>
<span class="ltx_bibblock">
Charlie Snell, Dan Klein, and Ruiqi Zhong.

</span>
<span class="ltx_bibblock">Learning by Distilling Context, September 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2209.15189" title="">http://arxiv.org/abs/2209.15189</a>.

</span>
<span class="ltx_bibblock">arXiv:2209.15189 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Teehan et al. [2024]</span>
<span class="ltx_bibblock">
Ryan Teehan, Brenden Lake, and Mengye Ren.

</span>
<span class="ltx_bibblock">CoLLEGe: Concept Embedding Generation for Large Language Models, October 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2403.15362" title="">http://arxiv.org/abs/2403.15362</a>.

</span>
<span class="ltx_bibblock">arXiv:2403.15362 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tran [2020]</span>
<span class="ltx_bibblock">
Ke Tran.

</span>
<span class="ltx_bibblock">From English To Foreign Languages: Transferring Pre-trained Language Models, April 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2002.07306" title="">http://arxiv.org/abs/2002.07306</a>.

</span>
<span class="ltx_bibblock">arXiv:2002.07306 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Villegas et al. [2025]</span>
<span class="ltx_bibblock">
Danae Sánchez Villegas, Ingo Ziegler, and Desmond Elliott.

</span>
<span class="ltx_bibblock">ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models, February 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2502.19409" title="">http://arxiv.org/abs/2502.19409</a>.

</span>
<span class="ltx_bibblock">arXiv:2502.19409 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vulić et al. [2020]</span>
<span class="ltx_bibblock">
Ivan Vulić, Edoardo Maria Ponti, Robert Litschko, Goran Glavaš, and Anna Korhonen.

</span>
<span class="ltx_bibblock">Probing Pretrained Language Models for Lexical Semantics.

</span>
<span class="ltx_bibblock">In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, pages 7222–7240, Online, November 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.emnlp-main.586</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.emnlp-main.586/" title="">https://aclanthology.org/2020.emnlp-main.586/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. [2019]</span>
<span class="ltx_bibblock">
Hai Wang, Dian Yu, Kai Sun, Jianshu Chen, and Dong Yu.

</span>
<span class="ltx_bibblock">Improving Pre-Trained Multilingual Model with Vocabulary Expansion.

</span>
<span class="ltx_bibblock">In Mohit Bansal and Aline Villavicencio, editors, <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL)</em>, pages 316–327, Hong Kong, China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/K19-1030</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/K19-1030/" title="">https://aclanthology.org/K19-1030/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. [2020]</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-Art Natural Language Processing, October 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.aclweb.org/anthology/2020.emnlp-demos.6" title="">https://www.aclweb.org/anthology/2020.emnlp-demos.6</a>.

</span>
<span class="ltx_bibblock">Pages: 38–45 original-date: 2018-10-29T13:56:00Z.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamaguchi et al. [2024a]</span>
<span class="ltx_bibblock">
Atsuki Yamaguchi, Aline Villavicencio, and Nikolaos Aletras.

</span>
<span class="ltx_bibblock">An Empirical Study on Cross-lingual Vocabulary Adaptation for Efficient Language Model Inference, September 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2402.10712" title="">http://arxiv.org/abs/2402.10712</a>.

</span>
<span class="ltx_bibblock">arXiv:2402.10712 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamaguchi et al. [2024b]</span>
<span class="ltx_bibblock">
Atsuki Yamaguchi, Aline Villavicencio, and Nikolaos Aletras.

</span>
<span class="ltx_bibblock">How Can We Effectively Expand the Vocabulary of LLMs with 0.01GB of Target Language Text?, September 2024b.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2406.11477" title="">http://arxiv.org/abs/2406.11477</a>.

</span>
<span class="ltx_bibblock">arXiv:2406.11477 [cs].

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zeng et al. [2023]</span>
<span class="ltx_bibblock">
Qingcheng Zeng, Lucas Garay, Peilin Zhou, Dading Chong, Yining Hua, Jiageng Wu, Yikang Pan, Han Zhou, Rob Voigt, and Jie Yang.

</span>
<span class="ltx_bibblock">GreenPLM: Cross-Lingual Transfer of Monolingual Pre-Trained Language Models at Almost No Cost, May 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="http://arxiv.org/abs/2211.06993" title="">http://arxiv.org/abs/2211.06993</a>.

</span>
<span class="ltx_bibblock">arXiv:2211.06993 [cs].

</span>
</li>
</ul>
</section>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Limitations</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.1">We have already highlighted and analyzed various choices and potential weaknesses of our method in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS1" title="5.1 Main Experiments ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.1</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" title="5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.2</span></a>. Nevertheless, we find that our method outperforms even very strong baselines. However, we now want to explicitly summarize and expand on the limitations of our method.</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1"><span class="ltx_text ltx_font_bold" id="A1.p2.1.1">Unknown tokens.</span> Since the motivation for the distillation-based objective is to match the original model’s behavior, our method is generally not applicable if the original model cannot meaningfully process a new token. Note that this specifically limits the applicability of our method for adapting a model’s vocabulary to a previously unseen language.</p>
</div>
<div class="ltx_para" id="A1.p3">
<p class="ltx_p" id="A1.p3.1"><span class="ltx_text ltx_font_bold" id="A1.p3.1.1">Learning rate.</span> Since our method involves gradient descent, we require setting a learning rate. However, as can be seen in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.F2" title="Figure 2 ‣ Table 6 ‣ Different distillation objectives. ‣ 5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>, our method seems to afford a wide range of viable learning rates that achieve good results, rendering the tuning of this hyperparameter less critical.</p>
</div>
<div class="ltx_para" id="A1.p4">
<p class="ltx_p" id="A1.p4.1"><span class="ltx_text ltx_font_bold" id="A1.p4.1.1">Only input embeddings.</span> As discussed in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S3" title="3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 3</span></a>, our method only addresses the task of inducing <span class="ltx_text ltx_font_italic" id="A1.p4.1.2">input embeddings</span>, since our distillation objective is not applicable to output embeddings. Our method can be freely combined with any other method for obtaining valid output embeddings. Alternatively, new tokens can be added only in the input embeddings, since the Transformer architecture does not require the same input and output vocabulary (although this assumption is made in some popular implementations).</p>
</div>
<div class="ltx_para" id="A1.p5">
<p class="ltx_p" id="A1.p5.1"><span class="ltx_text ltx_font_bold" id="A1.p5.1.1">Short subword tokens.</span> We find that our method does not work well for polysemic tokens, i.e., tokens with many different meanings in different contexts. These are often short subwords like <span class="ltx_text ltx_font_typewriter" id="A1.p5.1.2">&lt;ed&gt;</span> or <span class="ltx_text ltx_font_typewriter" id="A1.p5.1.3">&lt;fer&gt;</span> that occur in many words. This also means that our method is not ideal to initialize embeddings for an entirely new vocabulary that contains such new subwords. However, these tokens could be initialized using other methods <cite class="ltx_cite ltx_citemacro_citep">[Minixhofer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a>, Gee et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib11" title="">2022</a>]</cite>, while our method is applied to the rest.</p>
</div>
<div class="ltx_para" id="A1.p6">
<p class="ltx_p" id="A1.p6.1"><span class="ltx_text ltx_font_bold" id="A1.p6.1.1">Impact of chosen reference snippets.</span> The reference corpus for retrieving contexts for training with the distillation objective can bias the resulting learned embeddings. Consider the word “tender”, which in general means gentle or soft, but in the financial domain describes a specific type of buy offer. Note however that this can also be beneficial, as the resulting representations are now specialized to the target domain. Additionally, if such an effect is undesired, we can use our proposed data generation scheme via prompting the model to generate contexts containing the new word. This can be interpreted as distillation training on samples from the original model’s learned data distribution of samples containing the new word.</p>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Implementation Details</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Token Selection for Domain Adaptation</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">When using benchmarks to evaluate the quality of new token embeddings, it is crucial to ensure that the new tokens actually occur frequently enough to affect the performance if the new embeddings are poor. See, e.g., the “Random” baseline for randomly initialized embeddings, which still performs better than random chance on the benchmarks in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T1" title="Table 1 ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 1</span></a>.</p>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">Therefore, we select all whole words from the chosen benchmarks that are not already existing tokens in the original vocabulary. We exclude tokens that include digits or any of the characters in the following string in order to reduce noise:
<span class="ltx_ERROR undefined" id="A2.SS1.p2.1.2">{minted}</span>[
framesep=2mm,
baselinestretch=1.2,
fontsize=<span class="ltx_text" id="A2.SS1.p2.1.1" style="font-size:80%;">,
linenos
]Python
"[]()&lt;&gt;.,;:!?@#<math alttext="Additionally,toexcludeveryrarewordsthatwouldnotbecommonlyusedasnewtokens,%
weapplythefollowingtwofilters:(1)%
Weexcludewordsthatappearfewerthanfivetimesacrossallbenchmarks,and(2)%
weexcludewordsthatoccurfewerthan25timesinasampleof768,000documentsfromadomain-%
specificreferencecorpus(weuse\texttt{ncbi/pubmed}forbiomedicalbenchmarks).%
ThefulllistofnewwordsisavailableinourGitHubrepository.\par" class="ltx_math_unparsed" display="inline" id="A2.SS1.p2.1.1.m1.2"><semantics id="A2.SS1.p2.1.1.m1.2a"><mrow id="A2.SS1.p2.1.1.m1.2b"><mi id="A2.SS1.p2.1.1.m1.2.3">A</mi><mi id="A2.SS1.p2.1.1.m1.2.4">d</mi><mi id="A2.SS1.p2.1.1.m1.2.5">d</mi><mi id="A2.SS1.p2.1.1.m1.2.6">i</mi><mi id="A2.SS1.p2.1.1.m1.2.7">t</mi><mi id="A2.SS1.p2.1.1.m1.2.8">i</mi><mi id="A2.SS1.p2.1.1.m1.2.9">o</mi><mi id="A2.SS1.p2.1.1.m1.2.10">n</mi><mi id="A2.SS1.p2.1.1.m1.2.11">a</mi><mi id="A2.SS1.p2.1.1.m1.2.12">l</mi><mi id="A2.SS1.p2.1.1.m1.2.13">l</mi><mi id="A2.SS1.p2.1.1.m1.2.14">y</mi><mo id="A2.SS1.p2.1.1.m1.2.15">,</mo><mi id="A2.SS1.p2.1.1.m1.2.16">t</mi><mi id="A2.SS1.p2.1.1.m1.2.17">o</mi><mi id="A2.SS1.p2.1.1.m1.2.18">e</mi><mi id="A2.SS1.p2.1.1.m1.2.19">x</mi><mi id="A2.SS1.p2.1.1.m1.2.20">c</mi><mi id="A2.SS1.p2.1.1.m1.2.21">l</mi><mi id="A2.SS1.p2.1.1.m1.2.22">u</mi><mi id="A2.SS1.p2.1.1.m1.2.23">d</mi><mi id="A2.SS1.p2.1.1.m1.2.24">e</mi><mi id="A2.SS1.p2.1.1.m1.2.25">v</mi><mi id="A2.SS1.p2.1.1.m1.2.26">e</mi><mi id="A2.SS1.p2.1.1.m1.2.27">r</mi><mi id="A2.SS1.p2.1.1.m1.2.28">y</mi><mi id="A2.SS1.p2.1.1.m1.2.29">r</mi><mi id="A2.SS1.p2.1.1.m1.2.30">a</mi><mi id="A2.SS1.p2.1.1.m1.2.31">r</mi><mi id="A2.SS1.p2.1.1.m1.2.32">e</mi><mi id="A2.SS1.p2.1.1.m1.2.33">w</mi><mi id="A2.SS1.p2.1.1.m1.2.34">o</mi><mi id="A2.SS1.p2.1.1.m1.2.35">r</mi><mi id="A2.SS1.p2.1.1.m1.2.36">d</mi><mi id="A2.SS1.p2.1.1.m1.2.37">s</mi><mi id="A2.SS1.p2.1.1.m1.2.38">t</mi><mi id="A2.SS1.p2.1.1.m1.2.39">h</mi><mi id="A2.SS1.p2.1.1.m1.2.40">a</mi><mi id="A2.SS1.p2.1.1.m1.2.41">t</mi><mi id="A2.SS1.p2.1.1.m1.2.42">w</mi><mi id="A2.SS1.p2.1.1.m1.2.43">o</mi><mi id="A2.SS1.p2.1.1.m1.2.44">u</mi><mi id="A2.SS1.p2.1.1.m1.2.45">l</mi><mi id="A2.SS1.p2.1.1.m1.2.46">d</mi><mi id="A2.SS1.p2.1.1.m1.2.47">n</mi><mi id="A2.SS1.p2.1.1.m1.2.48">o</mi><mi id="A2.SS1.p2.1.1.m1.2.49">t</mi><mi id="A2.SS1.p2.1.1.m1.2.50">b</mi><mi id="A2.SS1.p2.1.1.m1.2.51">e</mi><mi id="A2.SS1.p2.1.1.m1.2.52">c</mi><mi id="A2.SS1.p2.1.1.m1.2.53">o</mi><mi id="A2.SS1.p2.1.1.m1.2.54">m</mi><mi id="A2.SS1.p2.1.1.m1.2.55">m</mi><mi id="A2.SS1.p2.1.1.m1.2.56">o</mi><mi id="A2.SS1.p2.1.1.m1.2.57">n</mi><mi id="A2.SS1.p2.1.1.m1.2.58">l</mi><mi id="A2.SS1.p2.1.1.m1.2.59">y</mi><mi id="A2.SS1.p2.1.1.m1.2.60">u</mi><mi id="A2.SS1.p2.1.1.m1.2.61">s</mi><mi id="A2.SS1.p2.1.1.m1.2.62">e</mi><mi id="A2.SS1.p2.1.1.m1.2.63">d</mi><mi id="A2.SS1.p2.1.1.m1.2.64">a</mi><mi id="A2.SS1.p2.1.1.m1.2.65">s</mi><mi id="A2.SS1.p2.1.1.m1.2.66">n</mi><mi id="A2.SS1.p2.1.1.m1.2.67">e</mi><mi id="A2.SS1.p2.1.1.m1.2.68">w</mi><mi id="A2.SS1.p2.1.1.m1.2.69">t</mi><mi id="A2.SS1.p2.1.1.m1.2.70">o</mi><mi id="A2.SS1.p2.1.1.m1.2.71">k</mi><mi id="A2.SS1.p2.1.1.m1.2.72">e</mi><mi id="A2.SS1.p2.1.1.m1.2.73">n</mi><mi id="A2.SS1.p2.1.1.m1.2.74">s</mi><mo id="A2.SS1.p2.1.1.m1.2.75">,</mo><mi id="A2.SS1.p2.1.1.m1.2.76">w</mi><mi id="A2.SS1.p2.1.1.m1.2.77">e</mi><mi id="A2.SS1.p2.1.1.m1.2.78">a</mi><mi id="A2.SS1.p2.1.1.m1.2.79">p</mi><mi id="A2.SS1.p2.1.1.m1.2.80">p</mi><mi id="A2.SS1.p2.1.1.m1.2.81">l</mi><mi id="A2.SS1.p2.1.1.m1.2.82">y</mi><mi id="A2.SS1.p2.1.1.m1.2.83">t</mi><mi id="A2.SS1.p2.1.1.m1.2.84">h</mi><mi id="A2.SS1.p2.1.1.m1.2.85">e</mi><mi id="A2.SS1.p2.1.1.m1.2.86">f</mi><mi id="A2.SS1.p2.1.1.m1.2.87">o</mi><mi id="A2.SS1.p2.1.1.m1.2.88">l</mi><mi id="A2.SS1.p2.1.1.m1.2.89">l</mi><mi id="A2.SS1.p2.1.1.m1.2.90">o</mi><mi id="A2.SS1.p2.1.1.m1.2.91">w</mi><mi id="A2.SS1.p2.1.1.m1.2.92">i</mi><mi id="A2.SS1.p2.1.1.m1.2.93">n</mi><mi id="A2.SS1.p2.1.1.m1.2.94">g</mi><mi id="A2.SS1.p2.1.1.m1.2.95">t</mi><mi id="A2.SS1.p2.1.1.m1.2.96">w</mi><mi id="A2.SS1.p2.1.1.m1.2.97">o</mi><mi id="A2.SS1.p2.1.1.m1.2.98">f</mi><mi id="A2.SS1.p2.1.1.m1.2.99">i</mi><mi id="A2.SS1.p2.1.1.m1.2.100">l</mi><mi id="A2.SS1.p2.1.1.m1.2.101">t</mi><mi id="A2.SS1.p2.1.1.m1.2.102">e</mi><mi id="A2.SS1.p2.1.1.m1.2.103">r</mi><mi id="A2.SS1.p2.1.1.m1.2.104">s</mi><mo id="A2.SS1.p2.1.1.m1.2.105" lspace="0.278em" rspace="0.278em">:</mo><mrow id="A2.SS1.p2.1.1.m1.2.106"><mo id="A2.SS1.p2.1.1.m1.2.106.1" stretchy="false">(</mo><mn id="A2.SS1.p2.1.1.m1.1.1">1</mn><mo id="A2.SS1.p2.1.1.m1.2.106.2" stretchy="false">)</mo></mrow><mi id="A2.SS1.p2.1.1.m1.2.107">W</mi><mi id="A2.SS1.p2.1.1.m1.2.108">e</mi><mi id="A2.SS1.p2.1.1.m1.2.109">e</mi><mi id="A2.SS1.p2.1.1.m1.2.110">x</mi><mi id="A2.SS1.p2.1.1.m1.2.111">c</mi><mi id="A2.SS1.p2.1.1.m1.2.112">l</mi><mi id="A2.SS1.p2.1.1.m1.2.113">u</mi><mi id="A2.SS1.p2.1.1.m1.2.114">d</mi><mi id="A2.SS1.p2.1.1.m1.2.115">e</mi><mi id="A2.SS1.p2.1.1.m1.2.116">w</mi><mi id="A2.SS1.p2.1.1.m1.2.117">o</mi><mi id="A2.SS1.p2.1.1.m1.2.118">r</mi><mi id="A2.SS1.p2.1.1.m1.2.119">d</mi><mi id="A2.SS1.p2.1.1.m1.2.120">s</mi><mi id="A2.SS1.p2.1.1.m1.2.121">t</mi><mi id="A2.SS1.p2.1.1.m1.2.122">h</mi><mi id="A2.SS1.p2.1.1.m1.2.123">a</mi><mi id="A2.SS1.p2.1.1.m1.2.124">t</mi><mi id="A2.SS1.p2.1.1.m1.2.125">a</mi><mi id="A2.SS1.p2.1.1.m1.2.126">p</mi><mi id="A2.SS1.p2.1.1.m1.2.127">p</mi><mi id="A2.SS1.p2.1.1.m1.2.128">e</mi><mi id="A2.SS1.p2.1.1.m1.2.129">a</mi><mi id="A2.SS1.p2.1.1.m1.2.130">r</mi><mi id="A2.SS1.p2.1.1.m1.2.131">f</mi><mi id="A2.SS1.p2.1.1.m1.2.132">e</mi><mi id="A2.SS1.p2.1.1.m1.2.133">w</mi><mi id="A2.SS1.p2.1.1.m1.2.134">e</mi><mi id="A2.SS1.p2.1.1.m1.2.135">r</mi><mi id="A2.SS1.p2.1.1.m1.2.136">t</mi><mi id="A2.SS1.p2.1.1.m1.2.137">h</mi><mi id="A2.SS1.p2.1.1.m1.2.138">a</mi><mi id="A2.SS1.p2.1.1.m1.2.139">n</mi><mi id="A2.SS1.p2.1.1.m1.2.140">f</mi><mi id="A2.SS1.p2.1.1.m1.2.141">i</mi><mi id="A2.SS1.p2.1.1.m1.2.142">v</mi><mi id="A2.SS1.p2.1.1.m1.2.143">e</mi><mi id="A2.SS1.p2.1.1.m1.2.144">t</mi><mi id="A2.SS1.p2.1.1.m1.2.145">i</mi><mi id="A2.SS1.p2.1.1.m1.2.146">m</mi><mi id="A2.SS1.p2.1.1.m1.2.147">e</mi><mi id="A2.SS1.p2.1.1.m1.2.148">s</mi><mi id="A2.SS1.p2.1.1.m1.2.149">a</mi><mi id="A2.SS1.p2.1.1.m1.2.150">c</mi><mi id="A2.SS1.p2.1.1.m1.2.151">r</mi><mi id="A2.SS1.p2.1.1.m1.2.152">o</mi><mi id="A2.SS1.p2.1.1.m1.2.153">s</mi><mi id="A2.SS1.p2.1.1.m1.2.154">s</mi><mi id="A2.SS1.p2.1.1.m1.2.155">a</mi><mi id="A2.SS1.p2.1.1.m1.2.156">l</mi><mi id="A2.SS1.p2.1.1.m1.2.157">l</mi><mi id="A2.SS1.p2.1.1.m1.2.158">b</mi><mi id="A2.SS1.p2.1.1.m1.2.159">e</mi><mi id="A2.SS1.p2.1.1.m1.2.160">n</mi><mi id="A2.SS1.p2.1.1.m1.2.161">c</mi><mi id="A2.SS1.p2.1.1.m1.2.162">h</mi><mi id="A2.SS1.p2.1.1.m1.2.163">m</mi><mi id="A2.SS1.p2.1.1.m1.2.164">a</mi><mi id="A2.SS1.p2.1.1.m1.2.165">r</mi><mi id="A2.SS1.p2.1.1.m1.2.166">k</mi><mi id="A2.SS1.p2.1.1.m1.2.167">s</mi><mo id="A2.SS1.p2.1.1.m1.2.168">,</mo><mi id="A2.SS1.p2.1.1.m1.2.169">a</mi><mi id="A2.SS1.p2.1.1.m1.2.170">n</mi><mi id="A2.SS1.p2.1.1.m1.2.171">d</mi><mrow id="A2.SS1.p2.1.1.m1.2.172"><mo id="A2.SS1.p2.1.1.m1.2.172.1" stretchy="false">(</mo><mn id="A2.SS1.p2.1.1.m1.2.2">2</mn><mo id="A2.SS1.p2.1.1.m1.2.172.2" stretchy="false">)</mo></mrow><mi id="A2.SS1.p2.1.1.m1.2.173">w</mi><mi id="A2.SS1.p2.1.1.m1.2.174">e</mi><mi id="A2.SS1.p2.1.1.m1.2.175">e</mi><mi id="A2.SS1.p2.1.1.m1.2.176">x</mi><mi id="A2.SS1.p2.1.1.m1.2.177">c</mi><mi id="A2.SS1.p2.1.1.m1.2.178">l</mi><mi id="A2.SS1.p2.1.1.m1.2.179">u</mi><mi id="A2.SS1.p2.1.1.m1.2.180">d</mi><mi id="A2.SS1.p2.1.1.m1.2.181">e</mi><mi id="A2.SS1.p2.1.1.m1.2.182">w</mi><mi id="A2.SS1.p2.1.1.m1.2.183">o</mi><mi id="A2.SS1.p2.1.1.m1.2.184">r</mi><mi id="A2.SS1.p2.1.1.m1.2.185">d</mi><mi id="A2.SS1.p2.1.1.m1.2.186">s</mi><mi id="A2.SS1.p2.1.1.m1.2.187">t</mi><mi id="A2.SS1.p2.1.1.m1.2.188">h</mi><mi id="A2.SS1.p2.1.1.m1.2.189">a</mi><mi id="A2.SS1.p2.1.1.m1.2.190">t</mi><mi id="A2.SS1.p2.1.1.m1.2.191">o</mi><mi id="A2.SS1.p2.1.1.m1.2.192">c</mi><mi id="A2.SS1.p2.1.1.m1.2.193">c</mi><mi id="A2.SS1.p2.1.1.m1.2.194">u</mi><mi id="A2.SS1.p2.1.1.m1.2.195">r</mi><mi id="A2.SS1.p2.1.1.m1.2.196">f</mi><mi id="A2.SS1.p2.1.1.m1.2.197">e</mi><mi id="A2.SS1.p2.1.1.m1.2.198">w</mi><mi id="A2.SS1.p2.1.1.m1.2.199">e</mi><mi id="A2.SS1.p2.1.1.m1.2.200">r</mi><mi id="A2.SS1.p2.1.1.m1.2.201">t</mi><mi id="A2.SS1.p2.1.1.m1.2.202">h</mi><mi id="A2.SS1.p2.1.1.m1.2.203">a</mi><mi id="A2.SS1.p2.1.1.m1.2.204">n</mi><mn id="A2.SS1.p2.1.1.m1.2.205">25</mn><mi id="A2.SS1.p2.1.1.m1.2.206">t</mi><mi id="A2.SS1.p2.1.1.m1.2.207">i</mi><mi id="A2.SS1.p2.1.1.m1.2.208">m</mi><mi id="A2.SS1.p2.1.1.m1.2.209">e</mi><mi id="A2.SS1.p2.1.1.m1.2.210">s</mi><mi id="A2.SS1.p2.1.1.m1.2.211">i</mi><mi id="A2.SS1.p2.1.1.m1.2.212">n</mi><mi id="A2.SS1.p2.1.1.m1.2.213">a</mi><mi id="A2.SS1.p2.1.1.m1.2.214">s</mi><mi id="A2.SS1.p2.1.1.m1.2.215">a</mi><mi id="A2.SS1.p2.1.1.m1.2.216">m</mi><mi id="A2.SS1.p2.1.1.m1.2.217">p</mi><mi id="A2.SS1.p2.1.1.m1.2.218">l</mi><mi id="A2.SS1.p2.1.1.m1.2.219">e</mi><mi id="A2.SS1.p2.1.1.m1.2.220">o</mi><mi id="A2.SS1.p2.1.1.m1.2.221">f</mi><mn id="A2.SS1.p2.1.1.m1.2.222">768</mn><mo id="A2.SS1.p2.1.1.m1.2.223">,</mo><mn id="A2.SS1.p2.1.1.m1.2.224">000</mn><mi id="A2.SS1.p2.1.1.m1.2.225">d</mi><mi id="A2.SS1.p2.1.1.m1.2.226">o</mi><mi id="A2.SS1.p2.1.1.m1.2.227">c</mi><mi id="A2.SS1.p2.1.1.m1.2.228">u</mi><mi id="A2.SS1.p2.1.1.m1.2.229">m</mi><mi id="A2.SS1.p2.1.1.m1.2.230">e</mi><mi id="A2.SS1.p2.1.1.m1.2.231">n</mi><mi id="A2.SS1.p2.1.1.m1.2.232">t</mi><mi id="A2.SS1.p2.1.1.m1.2.233">s</mi><mi id="A2.SS1.p2.1.1.m1.2.234">f</mi><mi id="A2.SS1.p2.1.1.m1.2.235">r</mi><mi id="A2.SS1.p2.1.1.m1.2.236">o</mi><mi id="A2.SS1.p2.1.1.m1.2.237">m</mi><mi id="A2.SS1.p2.1.1.m1.2.238">a</mi><mi id="A2.SS1.p2.1.1.m1.2.239">d</mi><mi id="A2.SS1.p2.1.1.m1.2.240">o</mi><mi id="A2.SS1.p2.1.1.m1.2.241">m</mi><mi id="A2.SS1.p2.1.1.m1.2.242">a</mi><mi id="A2.SS1.p2.1.1.m1.2.243">i</mi><mi id="A2.SS1.p2.1.1.m1.2.244">n</mi><mo id="A2.SS1.p2.1.1.m1.2.245">−</mo><mi id="A2.SS1.p2.1.1.m1.2.246">s</mi><mi id="A2.SS1.p2.1.1.m1.2.247">p</mi><mi id="A2.SS1.p2.1.1.m1.2.248">e</mi><mi id="A2.SS1.p2.1.1.m1.2.249">c</mi><mi id="A2.SS1.p2.1.1.m1.2.250">i</mi><mi id="A2.SS1.p2.1.1.m1.2.251">f</mi><mi id="A2.SS1.p2.1.1.m1.2.252">i</mi><mi id="A2.SS1.p2.1.1.m1.2.253">c</mi><mi id="A2.SS1.p2.1.1.m1.2.254">r</mi><mi id="A2.SS1.p2.1.1.m1.2.255">e</mi><mi id="A2.SS1.p2.1.1.m1.2.256">f</mi><mi id="A2.SS1.p2.1.1.m1.2.257">e</mi><mi id="A2.SS1.p2.1.1.m1.2.258">r</mi><mi id="A2.SS1.p2.1.1.m1.2.259">e</mi><mi id="A2.SS1.p2.1.1.m1.2.260">n</mi><mi id="A2.SS1.p2.1.1.m1.2.261">c</mi><mi id="A2.SS1.p2.1.1.m1.2.262">e</mi><mi id="A2.SS1.p2.1.1.m1.2.263">c</mi><mi id="A2.SS1.p2.1.1.m1.2.264">o</mi><mi id="A2.SS1.p2.1.1.m1.2.265">r</mi><mi id="A2.SS1.p2.1.1.m1.2.266">p</mi><mi id="A2.SS1.p2.1.1.m1.2.267">u</mi><mi id="A2.SS1.p2.1.1.m1.2.268">s</mi><mrow id="A2.SS1.p2.1.1.m1.2.269"><mo id="A2.SS1.p2.1.1.m1.2.269.1" stretchy="false">(</mo><mi id="A2.SS1.p2.1.1.m1.2.269.2">w</mi><mi id="A2.SS1.p2.1.1.m1.2.269.3">e</mi><mi id="A2.SS1.p2.1.1.m1.2.269.4">u</mi><mi id="A2.SS1.p2.1.1.m1.2.269.5">s</mi><mi id="A2.SS1.p2.1.1.m1.2.269.6">e</mi><mtext class="ltx_mathvariant_monospace" id="A2.SS1.p2.1.1.m1.2.269.7">ncbi/pubmed</mtext><mi id="A2.SS1.p2.1.1.m1.2.269.8">f</mi><mi id="A2.SS1.p2.1.1.m1.2.269.9">o</mi><mi id="A2.SS1.p2.1.1.m1.2.269.10">r</mi><mi id="A2.SS1.p2.1.1.m1.2.269.11">b</mi><mi id="A2.SS1.p2.1.1.m1.2.269.12">i</mi><mi id="A2.SS1.p2.1.1.m1.2.269.13">o</mi><mi id="A2.SS1.p2.1.1.m1.2.269.14">m</mi><mi id="A2.SS1.p2.1.1.m1.2.269.15">e</mi><mi id="A2.SS1.p2.1.1.m1.2.269.16">d</mi><mi id="A2.SS1.p2.1.1.m1.2.269.17">i</mi><mi id="A2.SS1.p2.1.1.m1.2.269.18">c</mi><mi id="A2.SS1.p2.1.1.m1.2.269.19">a</mi><mi id="A2.SS1.p2.1.1.m1.2.269.20">l</mi><mi id="A2.SS1.p2.1.1.m1.2.269.21">b</mi><mi id="A2.SS1.p2.1.1.m1.2.269.22">e</mi><mi id="A2.SS1.p2.1.1.m1.2.269.23">n</mi><mi id="A2.SS1.p2.1.1.m1.2.269.24">c</mi><mi id="A2.SS1.p2.1.1.m1.2.269.25">h</mi><mi id="A2.SS1.p2.1.1.m1.2.269.26">m</mi><mi id="A2.SS1.p2.1.1.m1.2.269.27">a</mi><mi id="A2.SS1.p2.1.1.m1.2.269.28">r</mi><mi id="A2.SS1.p2.1.1.m1.2.269.29">k</mi><mi id="A2.SS1.p2.1.1.m1.2.269.30">s</mi><mo id="A2.SS1.p2.1.1.m1.2.269.31" stretchy="false">)</mo></mrow><mo id="A2.SS1.p2.1.1.m1.2.270" lspace="0em" rspace="0.167em">.</mo><mi id="A2.SS1.p2.1.1.m1.2.271">T</mi><mi id="A2.SS1.p2.1.1.m1.2.272">h</mi><mi id="A2.SS1.p2.1.1.m1.2.273">e</mi><mi id="A2.SS1.p2.1.1.m1.2.274">f</mi><mi id="A2.SS1.p2.1.1.m1.2.275">u</mi><mi id="A2.SS1.p2.1.1.m1.2.276">l</mi><mi id="A2.SS1.p2.1.1.m1.2.277">l</mi><mi id="A2.SS1.p2.1.1.m1.2.278">l</mi><mi id="A2.SS1.p2.1.1.m1.2.279">i</mi><mi id="A2.SS1.p2.1.1.m1.2.280">s</mi><mi id="A2.SS1.p2.1.1.m1.2.281">t</mi><mi id="A2.SS1.p2.1.1.m1.2.282">o</mi><mi id="A2.SS1.p2.1.1.m1.2.283">f</mi><mi id="A2.SS1.p2.1.1.m1.2.284">n</mi><mi id="A2.SS1.p2.1.1.m1.2.285">e</mi><mi id="A2.SS1.p2.1.1.m1.2.286">w</mi><mi id="A2.SS1.p2.1.1.m1.2.287">w</mi><mi id="A2.SS1.p2.1.1.m1.2.288">o</mi><mi id="A2.SS1.p2.1.1.m1.2.289">r</mi><mi id="A2.SS1.p2.1.1.m1.2.290">d</mi><mi id="A2.SS1.p2.1.1.m1.2.291">s</mi><mi id="A2.SS1.p2.1.1.m1.2.292">i</mi><mi id="A2.SS1.p2.1.1.m1.2.293">s</mi><mi id="A2.SS1.p2.1.1.m1.2.294">a</mi><mi id="A2.SS1.p2.1.1.m1.2.295">v</mi><mi id="A2.SS1.p2.1.1.m1.2.296">a</mi><mi id="A2.SS1.p2.1.1.m1.2.297">i</mi><mi id="A2.SS1.p2.1.1.m1.2.298">l</mi><mi id="A2.SS1.p2.1.1.m1.2.299">a</mi><mi id="A2.SS1.p2.1.1.m1.2.300">b</mi><mi id="A2.SS1.p2.1.1.m1.2.301">l</mi><mi id="A2.SS1.p2.1.1.m1.2.302">e</mi><mi id="A2.SS1.p2.1.1.m1.2.303">i</mi><mi id="A2.SS1.p2.1.1.m1.2.304">n</mi><mi id="A2.SS1.p2.1.1.m1.2.305">o</mi><mi id="A2.SS1.p2.1.1.m1.2.306">u</mi><mi id="A2.SS1.p2.1.1.m1.2.307">r</mi><mi id="A2.SS1.p2.1.1.m1.2.308">G</mi><mi id="A2.SS1.p2.1.1.m1.2.309">i</mi><mi id="A2.SS1.p2.1.1.m1.2.310">t</mi><mi id="A2.SS1.p2.1.1.m1.2.311">H</mi><mi id="A2.SS1.p2.1.1.m1.2.312">u</mi><mi id="A2.SS1.p2.1.1.m1.2.313">b</mi><mi id="A2.SS1.p2.1.1.m1.2.314">r</mi><mi id="A2.SS1.p2.1.1.m1.2.315">e</mi><mi id="A2.SS1.p2.1.1.m1.2.316">p</mi><mi id="A2.SS1.p2.1.1.m1.2.317">o</mi><mi id="A2.SS1.p2.1.1.m1.2.318">s</mi><mi id="A2.SS1.p2.1.1.m1.2.319">i</mi><mi id="A2.SS1.p2.1.1.m1.2.320">t</mi><mi id="A2.SS1.p2.1.1.m1.2.321">o</mi><mi id="A2.SS1.p2.1.1.m1.2.322">r</mi><mi id="A2.SS1.p2.1.1.m1.2.323">y</mi><mo id="A2.SS1.p2.1.1.m1.2.324" lspace="0em">.</mo></mrow><annotation encoding="application/x-tex" id="A2.SS1.p2.1.1.m1.2c">Additionally,toexcludeveryrarewordsthatwouldnotbecommonlyusedasnewtokens,%
weapplythefollowingtwofilters:(1)%
Weexcludewordsthatappearfewerthanfivetimesacrossallbenchmarks,and(2)%
weexcludewordsthatoccurfewerthan25timesinasampleof768,000documentsfromadomain-%
specificreferencecorpus(weuse\texttt{ncbi/pubmed}forbiomedicalbenchmarks).%
ThefulllistofnewwordsisavailableinourGitHubrepository.\par</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.1.1.m1.2d">italic_A italic_d italic_d italic_i italic_t italic_i italic_o italic_n italic_a italic_l italic_l italic_y , italic_t italic_o italic_e italic_x italic_c italic_l italic_u italic_d italic_e italic_v italic_e italic_r italic_y italic_r italic_a italic_r italic_e italic_w italic_o italic_r italic_d italic_s italic_t italic_h italic_a italic_t italic_w italic_o italic_u italic_l italic_d italic_n italic_o italic_t italic_b italic_e italic_c italic_o italic_m italic_m italic_o italic_n italic_l italic_y italic_u italic_s italic_e italic_d italic_a italic_s italic_n italic_e italic_w italic_t italic_o italic_k italic_e italic_n italic_s , italic_w italic_e italic_a italic_p italic_p italic_l italic_y italic_t italic_h italic_e italic_f italic_o italic_l italic_l italic_o italic_w italic_i italic_n italic_g italic_t italic_w italic_o italic_f italic_i italic_l italic_t italic_e italic_r italic_s : ( 1 ) italic_W italic_e italic_e italic_x italic_c italic_l italic_u italic_d italic_e italic_w italic_o italic_r italic_d italic_s italic_t italic_h italic_a italic_t italic_a italic_p italic_p italic_e italic_a italic_r italic_f italic_e italic_w italic_e italic_r italic_t italic_h italic_a italic_n italic_f italic_i italic_v italic_e italic_t italic_i italic_m italic_e italic_s italic_a italic_c italic_r italic_o italic_s italic_s italic_a italic_l italic_l italic_b italic_e italic_n italic_c italic_h italic_m italic_a italic_r italic_k italic_s , italic_a italic_n italic_d ( 2 ) italic_w italic_e italic_e italic_x italic_c italic_l italic_u italic_d italic_e italic_w italic_o italic_r italic_d italic_s italic_t italic_h italic_a italic_t italic_o italic_c italic_c italic_u italic_r italic_f italic_e italic_w italic_e italic_r italic_t italic_h italic_a italic_n 25 italic_t italic_i italic_m italic_e italic_s italic_i italic_n italic_a italic_s italic_a italic_m italic_p italic_l italic_e italic_o italic_f 768 , 000 italic_d italic_o italic_c italic_u italic_m italic_e italic_n italic_t italic_s italic_f italic_r italic_o italic_m italic_a italic_d italic_o italic_m italic_a italic_i italic_n - italic_s italic_p italic_e italic_c italic_i italic_f italic_i italic_c italic_r italic_e italic_f italic_e italic_r italic_e italic_n italic_c italic_e italic_c italic_o italic_r italic_p italic_u italic_s ( italic_w italic_e italic_u italic_s italic_e ncbi/pubmed italic_f italic_o italic_r italic_b italic_i italic_o italic_m italic_e italic_d italic_i italic_c italic_a italic_l italic_b italic_e italic_n italic_c italic_h italic_m italic_a italic_r italic_k italic_s ) . italic_T italic_h italic_e italic_f italic_u italic_l italic_l italic_l italic_i italic_s italic_t italic_o italic_f italic_n italic_e italic_w italic_w italic_o italic_r italic_d italic_s italic_i italic_s italic_a italic_v italic_a italic_i italic_l italic_a italic_b italic_l italic_e italic_i italic_n italic_o italic_u italic_r italic_G italic_i italic_t italic_H italic_u italic_b italic_r italic_e italic_p italic_o italic_s italic_i italic_t italic_o italic_r italic_y .</annotation></semantics></math></span></p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Multi-Word Token Generation</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1"><span class="ltx_text" id="A2.SS2.p1.1.1" style="font-size:80%;">We prompted </span><span class="ltx_text ltx_font_typewriter" id="A2.SS2.p1.1.2" style="font-size:80%;">gpt-o4-mini-high</span><span class="ltx_text" id="A2.SS2.p1.1.3" style="font-size:80%;"> (as of April 7, 2025) with the following prompt:

</span><span class="ltx_ERROR undefined" id="A2.SS2.p1.1.4">{minted}</span><span class="ltx_text" id="A2.SS2.p1.1.5" style="font-size:80%;">[fontsize=</span><span class="ltx_text" id="A2.SS2.p1.1.6" style="font-size:90%;">, breaklines=true, linenos]python
f"""Provide a list of entities, places, people, concepts, phrases or phrasings, idioms, or other terms that span multiple words, at least two and maximum five. Provide them in JSON format grouped by category and at least 100 per category. Ensure that you do not shorten coherent names just to fit them into the five word limit."""
</span><span class="ltx_text" id="A2.SS2.p1.1.7" style="font-size:80%;">
We refined this prompt to exclude common failure modes through trial-and-error. We include the list of resulting multi-word tokens in our GitHub repository.</span></p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:80%;">
<span class="ltx_tag ltx_tag_subsection">B.3 </span>Open Medical-LLM Leaderboard Experiments</h3>
<div class="ltx_para" id="A2.SS3.p1">
<p class="ltx_p" id="A2.SS3.p1.1"><span class="ltx_text" id="A2.SS3.p1.1.1" style="font-size:80%;">For evaluation, we use the </span><span class="ltx_text ltx_font_typewriter" id="A2.SS3.p1.1.2" style="font-size:80%;">lm-evaluation-harness</span><span class="ltx_text" id="A2.SS3.p1.1.3" style="font-size:80%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="A2.SS3.p1.1.4.1" style="font-size:80%;">[</span>Biderman et al.<span class="ltx_text" id="A2.SS3.p1.1.5.2.1.1" style="font-size:80%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib5" title="">2024</a><span class="ltx_text" id="A2.SS3.p1.1.6.3" style="font-size:80%;">]</span></cite><span class="ltx_text" id="A2.SS3.p1.1.7" style="font-size:80%;"> library at the version </span><span class="ltx_text ltx_font_typewriter" id="A2.SS3.p1.1.8" style="font-size:80%;">0.4.7</span><span class="ltx_text" id="A2.SS3.p1.1.9" style="font-size:80%;">. We use 5-shot evaluation on a single Nvidia H100 GPU using </span><span class="ltx_text ltx_font_typewriter" id="A2.SS3.p1.1.10" style="font-size:80%;">bfloat16</span><span class="ltx_text" id="A2.SS3.p1.1.11" style="font-size:80%;"> precision. Specifically, we use the following command:</span></p>
</div>
<div class="ltx_para" id="A2.SS3.p2">
<span class="ltx_ERROR undefined" id="A2.SS3.p2.2">{minted}</span>
<p class="ltx_p" id="A2.SS3.p2.1"><span class="ltx_text" id="A2.SS3.p2.1.2" style="font-size:80%;">[fontsize=</span><span class="ltx_text" id="A2.SS3.p2.1.1" style="font-size:90%;">, breaklines=true, linenos]sh
lm_eval –model hf  –model_args pretrained=<math alttext='MODEL_{P}ATH,dtype="bfloat16"\ --tasksmedmcqa,medqa_{4}options,mmlu_{a}natomy,%
mmlu_{c}linical_{k}nowledge,mmlu_{c}ollege_{b}iology,mmlu_{c}ollege_{m}edicine%
,mmlu_{m}edical_{g}enetics,mmlu_{p}rofessional_{m}edicine,pubmedqa\ --num_{f}%
ewshot5\ --devicecuda:0\ --batch_{s}ize8ThisevaluatesthetaskoftheOpenMedical-%
LLMLeaderboard\cite[citep]{[\@@bibref{AuthorsPhrase1Year}{openlifescienceai_%
open_medical_llm_leaderboard, jin_pubmedqa_2019, jin_what_2020, hendrycks_%
measuring_2021, pal_medmcqa_2022}{\@@citephrase{, }}{}]}.Allcodeandalock-%
filewithspecificversionsisavailableinourGitHubrepository.\par' class="ltx_math_unparsed" display="inline" id="A2.SS3.p2.1.1.m1.1"><semantics id="A2.SS3.p2.1.1.m1.1a"><mrow id="A2.SS3.p2.1.1.m1.1b"><mi id="A2.SS3.p2.1.1.m1.1.1">M</mi><mi id="A2.SS3.p2.1.1.m1.1.2">O</mi><mi id="A2.SS3.p2.1.1.m1.1.3">D</mi><mi id="A2.SS3.p2.1.1.m1.1.4">E</mi><msub id="A2.SS3.p2.1.1.m1.1.5"><mi id="A2.SS3.p2.1.1.m1.1.5.2">L</mi><mi id="A2.SS3.p2.1.1.m1.1.5.3">P</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.6">A</mi><mi id="A2.SS3.p2.1.1.m1.1.7">T</mi><mi id="A2.SS3.p2.1.1.m1.1.8">H</mi><mo id="A2.SS3.p2.1.1.m1.1.9">,</mo><mi id="A2.SS3.p2.1.1.m1.1.10">d</mi><mi id="A2.SS3.p2.1.1.m1.1.11">t</mi><mi id="A2.SS3.p2.1.1.m1.1.12">y</mi><mi id="A2.SS3.p2.1.1.m1.1.13">p</mi><mi id="A2.SS3.p2.1.1.m1.1.14">e</mi><mo id="A2.SS3.p2.1.1.m1.1.15">=</mo><mi id="A2.SS3.p2.1.1.m1.1.16" mathvariant="normal">"</mi><mi id="A2.SS3.p2.1.1.m1.1.17">b</mi><mi id="A2.SS3.p2.1.1.m1.1.18">f</mi><mi id="A2.SS3.p2.1.1.m1.1.19">l</mi><mi id="A2.SS3.p2.1.1.m1.1.20">o</mi><mi id="A2.SS3.p2.1.1.m1.1.21">a</mi><mi id="A2.SS3.p2.1.1.m1.1.22">t</mi><mn id="A2.SS3.p2.1.1.m1.1.23">16</mn><mi id="A2.SS3.p2.1.1.m1.1.24" mathvariant="normal">"</mi><mo id="A2.SS3.p2.1.1.m1.1.25" lspace="0.672em" rspace="0em">−</mo><mo id="A2.SS3.p2.1.1.m1.1.26" lspace="0em">−</mo><mi id="A2.SS3.p2.1.1.m1.1.27">t</mi><mi id="A2.SS3.p2.1.1.m1.1.28">a</mi><mi id="A2.SS3.p2.1.1.m1.1.29">s</mi><mi id="A2.SS3.p2.1.1.m1.1.30">k</mi><mi id="A2.SS3.p2.1.1.m1.1.31">s</mi><mi id="A2.SS3.p2.1.1.m1.1.32">m</mi><mi id="A2.SS3.p2.1.1.m1.1.33">e</mi><mi id="A2.SS3.p2.1.1.m1.1.34">d</mi><mi id="A2.SS3.p2.1.1.m1.1.35">m</mi><mi id="A2.SS3.p2.1.1.m1.1.36">c</mi><mi id="A2.SS3.p2.1.1.m1.1.37">q</mi><mi id="A2.SS3.p2.1.1.m1.1.38">a</mi><mo id="A2.SS3.p2.1.1.m1.1.39">,</mo><mi id="A2.SS3.p2.1.1.m1.1.40">m</mi><mi id="A2.SS3.p2.1.1.m1.1.41">e</mi><mi id="A2.SS3.p2.1.1.m1.1.42">d</mi><mi id="A2.SS3.p2.1.1.m1.1.43">q</mi><msub id="A2.SS3.p2.1.1.m1.1.44"><mi id="A2.SS3.p2.1.1.m1.1.44.2">a</mi><mn id="A2.SS3.p2.1.1.m1.1.44.3">4</mn></msub><mi id="A2.SS3.p2.1.1.m1.1.45">o</mi><mi id="A2.SS3.p2.1.1.m1.1.46">p</mi><mi id="A2.SS3.p2.1.1.m1.1.47">t</mi><mi id="A2.SS3.p2.1.1.m1.1.48">i</mi><mi id="A2.SS3.p2.1.1.m1.1.49">o</mi><mi id="A2.SS3.p2.1.1.m1.1.50">n</mi><mi id="A2.SS3.p2.1.1.m1.1.51">s</mi><mo id="A2.SS3.p2.1.1.m1.1.52">,</mo><mi id="A2.SS3.p2.1.1.m1.1.53">m</mi><mi id="A2.SS3.p2.1.1.m1.1.54">m</mi><mi id="A2.SS3.p2.1.1.m1.1.55">l</mi><msub id="A2.SS3.p2.1.1.m1.1.56"><mi id="A2.SS3.p2.1.1.m1.1.56.2">u</mi><mi id="A2.SS3.p2.1.1.m1.1.56.3">a</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.57">n</mi><mi id="A2.SS3.p2.1.1.m1.1.58">a</mi><mi id="A2.SS3.p2.1.1.m1.1.59">t</mi><mi id="A2.SS3.p2.1.1.m1.1.60">o</mi><mi id="A2.SS3.p2.1.1.m1.1.61">m</mi><mi id="A2.SS3.p2.1.1.m1.1.62">y</mi><mo id="A2.SS3.p2.1.1.m1.1.63">,</mo><mi id="A2.SS3.p2.1.1.m1.1.64">m</mi><mi id="A2.SS3.p2.1.1.m1.1.65">m</mi><mi id="A2.SS3.p2.1.1.m1.1.66">l</mi><msub id="A2.SS3.p2.1.1.m1.1.67"><mi id="A2.SS3.p2.1.1.m1.1.67.2">u</mi><mi id="A2.SS3.p2.1.1.m1.1.67.3">c</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.68">l</mi><mi id="A2.SS3.p2.1.1.m1.1.69">i</mi><mi id="A2.SS3.p2.1.1.m1.1.70">n</mi><mi id="A2.SS3.p2.1.1.m1.1.71">i</mi><mi id="A2.SS3.p2.1.1.m1.1.72">c</mi><mi id="A2.SS3.p2.1.1.m1.1.73">a</mi><msub id="A2.SS3.p2.1.1.m1.1.74"><mi id="A2.SS3.p2.1.1.m1.1.74.2">l</mi><mi id="A2.SS3.p2.1.1.m1.1.74.3">k</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.75">n</mi><mi id="A2.SS3.p2.1.1.m1.1.76">o</mi><mi id="A2.SS3.p2.1.1.m1.1.77">w</mi><mi id="A2.SS3.p2.1.1.m1.1.78">l</mi><mi id="A2.SS3.p2.1.1.m1.1.79">e</mi><mi id="A2.SS3.p2.1.1.m1.1.80">d</mi><mi id="A2.SS3.p2.1.1.m1.1.81">g</mi><mi id="A2.SS3.p2.1.1.m1.1.82">e</mi><mo id="A2.SS3.p2.1.1.m1.1.83">,</mo><mi id="A2.SS3.p2.1.1.m1.1.84">m</mi><mi id="A2.SS3.p2.1.1.m1.1.85">m</mi><mi id="A2.SS3.p2.1.1.m1.1.86">l</mi><msub id="A2.SS3.p2.1.1.m1.1.87"><mi id="A2.SS3.p2.1.1.m1.1.87.2">u</mi><mi id="A2.SS3.p2.1.1.m1.1.87.3">c</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.88">o</mi><mi id="A2.SS3.p2.1.1.m1.1.89">l</mi><mi id="A2.SS3.p2.1.1.m1.1.90">l</mi><mi id="A2.SS3.p2.1.1.m1.1.91">e</mi><mi id="A2.SS3.p2.1.1.m1.1.92">g</mi><msub id="A2.SS3.p2.1.1.m1.1.93"><mi id="A2.SS3.p2.1.1.m1.1.93.2">e</mi><mi id="A2.SS3.p2.1.1.m1.1.93.3">b</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.94">i</mi><mi id="A2.SS3.p2.1.1.m1.1.95">o</mi><mi id="A2.SS3.p2.1.1.m1.1.96">l</mi><mi id="A2.SS3.p2.1.1.m1.1.97">o</mi><mi id="A2.SS3.p2.1.1.m1.1.98">g</mi><mi id="A2.SS3.p2.1.1.m1.1.99">y</mi><mo id="A2.SS3.p2.1.1.m1.1.100">,</mo><mi id="A2.SS3.p2.1.1.m1.1.101">m</mi><mi id="A2.SS3.p2.1.1.m1.1.102">m</mi><mi id="A2.SS3.p2.1.1.m1.1.103">l</mi><msub id="A2.SS3.p2.1.1.m1.1.104"><mi id="A2.SS3.p2.1.1.m1.1.104.2">u</mi><mi id="A2.SS3.p2.1.1.m1.1.104.3">c</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.105">o</mi><mi id="A2.SS3.p2.1.1.m1.1.106">l</mi><mi id="A2.SS3.p2.1.1.m1.1.107">l</mi><mi id="A2.SS3.p2.1.1.m1.1.108">e</mi><mi id="A2.SS3.p2.1.1.m1.1.109">g</mi><msub id="A2.SS3.p2.1.1.m1.1.110"><mi id="A2.SS3.p2.1.1.m1.1.110.2">e</mi><mi id="A2.SS3.p2.1.1.m1.1.110.3">m</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.111">e</mi><mi id="A2.SS3.p2.1.1.m1.1.112">d</mi><mi id="A2.SS3.p2.1.1.m1.1.113">i</mi><mi id="A2.SS3.p2.1.1.m1.1.114">c</mi><mi id="A2.SS3.p2.1.1.m1.1.115">i</mi><mi id="A2.SS3.p2.1.1.m1.1.116">n</mi><mi id="A2.SS3.p2.1.1.m1.1.117">e</mi><mo id="A2.SS3.p2.1.1.m1.1.118">,</mo><mi id="A2.SS3.p2.1.1.m1.1.119">m</mi><mi id="A2.SS3.p2.1.1.m1.1.120">m</mi><mi id="A2.SS3.p2.1.1.m1.1.121">l</mi><msub id="A2.SS3.p2.1.1.m1.1.122"><mi id="A2.SS3.p2.1.1.m1.1.122.2">u</mi><mi id="A2.SS3.p2.1.1.m1.1.122.3">m</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.123">e</mi><mi id="A2.SS3.p2.1.1.m1.1.124">d</mi><mi id="A2.SS3.p2.1.1.m1.1.125">i</mi><mi id="A2.SS3.p2.1.1.m1.1.126">c</mi><mi id="A2.SS3.p2.1.1.m1.1.127">a</mi><msub id="A2.SS3.p2.1.1.m1.1.128"><mi id="A2.SS3.p2.1.1.m1.1.128.2">l</mi><mi id="A2.SS3.p2.1.1.m1.1.128.3">g</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.129">e</mi><mi id="A2.SS3.p2.1.1.m1.1.130">n</mi><mi id="A2.SS3.p2.1.1.m1.1.131">e</mi><mi id="A2.SS3.p2.1.1.m1.1.132">t</mi><mi id="A2.SS3.p2.1.1.m1.1.133">i</mi><mi id="A2.SS3.p2.1.1.m1.1.134">c</mi><mi id="A2.SS3.p2.1.1.m1.1.135">s</mi><mo id="A2.SS3.p2.1.1.m1.1.136">,</mo><mi id="A2.SS3.p2.1.1.m1.1.137">m</mi><mi id="A2.SS3.p2.1.1.m1.1.138">m</mi><mi id="A2.SS3.p2.1.1.m1.1.139">l</mi><msub id="A2.SS3.p2.1.1.m1.1.140"><mi id="A2.SS3.p2.1.1.m1.1.140.2">u</mi><mi id="A2.SS3.p2.1.1.m1.1.140.3">p</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.141">r</mi><mi id="A2.SS3.p2.1.1.m1.1.142">o</mi><mi id="A2.SS3.p2.1.1.m1.1.143">f</mi><mi id="A2.SS3.p2.1.1.m1.1.144">e</mi><mi id="A2.SS3.p2.1.1.m1.1.145">s</mi><mi id="A2.SS3.p2.1.1.m1.1.146">s</mi><mi id="A2.SS3.p2.1.1.m1.1.147">i</mi><mi id="A2.SS3.p2.1.1.m1.1.148">o</mi><mi id="A2.SS3.p2.1.1.m1.1.149">n</mi><mi id="A2.SS3.p2.1.1.m1.1.150">a</mi><msub id="A2.SS3.p2.1.1.m1.1.151"><mi id="A2.SS3.p2.1.1.m1.1.151.2">l</mi><mi id="A2.SS3.p2.1.1.m1.1.151.3">m</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.152">e</mi><mi id="A2.SS3.p2.1.1.m1.1.153">d</mi><mi id="A2.SS3.p2.1.1.m1.1.154">i</mi><mi id="A2.SS3.p2.1.1.m1.1.155">c</mi><mi id="A2.SS3.p2.1.1.m1.1.156">i</mi><mi id="A2.SS3.p2.1.1.m1.1.157">n</mi><mi id="A2.SS3.p2.1.1.m1.1.158">e</mi><mo id="A2.SS3.p2.1.1.m1.1.159">,</mo><mi id="A2.SS3.p2.1.1.m1.1.160">p</mi><mi id="A2.SS3.p2.1.1.m1.1.161">u</mi><mi id="A2.SS3.p2.1.1.m1.1.162">b</mi><mi id="A2.SS3.p2.1.1.m1.1.163">m</mi><mi id="A2.SS3.p2.1.1.m1.1.164">e</mi><mi id="A2.SS3.p2.1.1.m1.1.165">d</mi><mi id="A2.SS3.p2.1.1.m1.1.166">q</mi><mi id="A2.SS3.p2.1.1.m1.1.167">a</mi><mo id="A2.SS3.p2.1.1.m1.1.168" lspace="0.672em" rspace="0em">−</mo><mo id="A2.SS3.p2.1.1.m1.1.169" lspace="0em">−</mo><mi id="A2.SS3.p2.1.1.m1.1.170">n</mi><mi id="A2.SS3.p2.1.1.m1.1.171">u</mi><msub id="A2.SS3.p2.1.1.m1.1.172"><mi id="A2.SS3.p2.1.1.m1.1.172.2">m</mi><mi id="A2.SS3.p2.1.1.m1.1.172.3">f</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.173">e</mi><mi id="A2.SS3.p2.1.1.m1.1.174">w</mi><mi id="A2.SS3.p2.1.1.m1.1.175">s</mi><mi id="A2.SS3.p2.1.1.m1.1.176">h</mi><mi id="A2.SS3.p2.1.1.m1.1.177">o</mi><mi id="A2.SS3.p2.1.1.m1.1.178">t</mi><mn id="A2.SS3.p2.1.1.m1.1.179">5</mn><mo id="A2.SS3.p2.1.1.m1.1.180" lspace="0.672em" rspace="0em">−</mo><mo id="A2.SS3.p2.1.1.m1.1.181" lspace="0em">−</mo><mi id="A2.SS3.p2.1.1.m1.1.182">d</mi><mi id="A2.SS3.p2.1.1.m1.1.183">e</mi><mi id="A2.SS3.p2.1.1.m1.1.184">v</mi><mi id="A2.SS3.p2.1.1.m1.1.185">i</mi><mi id="A2.SS3.p2.1.1.m1.1.186">c</mi><mi id="A2.SS3.p2.1.1.m1.1.187">e</mi><mi id="A2.SS3.p2.1.1.m1.1.188">c</mi><mi id="A2.SS3.p2.1.1.m1.1.189">u</mi><mi id="A2.SS3.p2.1.1.m1.1.190">d</mi><mi id="A2.SS3.p2.1.1.m1.1.191">a</mi><mo id="A2.SS3.p2.1.1.m1.1.192" lspace="0.278em" rspace="0.278em">:</mo><mn id="A2.SS3.p2.1.1.m1.1.193">0</mn><mo id="A2.SS3.p2.1.1.m1.1.194" lspace="0.672em" rspace="0em">−</mo><mo id="A2.SS3.p2.1.1.m1.1.195" lspace="0em">−</mo><mi id="A2.SS3.p2.1.1.m1.1.196">b</mi><mi id="A2.SS3.p2.1.1.m1.1.197">a</mi><mi id="A2.SS3.p2.1.1.m1.1.198">t</mi><mi id="A2.SS3.p2.1.1.m1.1.199">c</mi><msub id="A2.SS3.p2.1.1.m1.1.200"><mi id="A2.SS3.p2.1.1.m1.1.200.2">h</mi><mi id="A2.SS3.p2.1.1.m1.1.200.3">s</mi></msub><mi id="A2.SS3.p2.1.1.m1.1.201">i</mi><mi id="A2.SS3.p2.1.1.m1.1.202">z</mi><mi id="A2.SS3.p2.1.1.m1.1.203">e</mi><mn id="A2.SS3.p2.1.1.m1.1.204">8</mn><mi id="A2.SS3.p2.1.1.m1.1.205">T</mi><mi id="A2.SS3.p2.1.1.m1.1.206">h</mi><mi id="A2.SS3.p2.1.1.m1.1.207">i</mi><mi id="A2.SS3.p2.1.1.m1.1.208">s</mi><mi id="A2.SS3.p2.1.1.m1.1.209">e</mi><mi id="A2.SS3.p2.1.1.m1.1.210">v</mi><mi id="A2.SS3.p2.1.1.m1.1.211">a</mi><mi id="A2.SS3.p2.1.1.m1.1.212">l</mi><mi id="A2.SS3.p2.1.1.m1.1.213">u</mi><mi id="A2.SS3.p2.1.1.m1.1.214">a</mi><mi id="A2.SS3.p2.1.1.m1.1.215">t</mi><mi id="A2.SS3.p2.1.1.m1.1.216">e</mi><mi id="A2.SS3.p2.1.1.m1.1.217">s</mi><mi id="A2.SS3.p2.1.1.m1.1.218">t</mi><mi id="A2.SS3.p2.1.1.m1.1.219">h</mi><mi id="A2.SS3.p2.1.1.m1.1.220">e</mi><mi id="A2.SS3.p2.1.1.m1.1.221">t</mi><mi id="A2.SS3.p2.1.1.m1.1.222">a</mi><mi id="A2.SS3.p2.1.1.m1.1.223">s</mi><mi id="A2.SS3.p2.1.1.m1.1.224">k</mi><mi id="A2.SS3.p2.1.1.m1.1.225">o</mi><mi id="A2.SS3.p2.1.1.m1.1.226">f</mi><mi id="A2.SS3.p2.1.1.m1.1.227">t</mi><mi id="A2.SS3.p2.1.1.m1.1.228">h</mi><mi id="A2.SS3.p2.1.1.m1.1.229">e</mi><mi id="A2.SS3.p2.1.1.m1.1.230">O</mi><mi id="A2.SS3.p2.1.1.m1.1.231">p</mi><mi id="A2.SS3.p2.1.1.m1.1.232">e</mi><mi id="A2.SS3.p2.1.1.m1.1.233">n</mi><mi id="A2.SS3.p2.1.1.m1.1.234">M</mi><mi id="A2.SS3.p2.1.1.m1.1.235">e</mi><mi id="A2.SS3.p2.1.1.m1.1.236">d</mi><mi id="A2.SS3.p2.1.1.m1.1.237">i</mi><mi id="A2.SS3.p2.1.1.m1.1.238">c</mi><mi id="A2.SS3.p2.1.1.m1.1.239">a</mi><mi id="A2.SS3.p2.1.1.m1.1.240">l</mi><mo id="A2.SS3.p2.1.1.m1.1.241">−</mo><mi id="A2.SS3.p2.1.1.m1.1.242">L</mi><mi id="A2.SS3.p2.1.1.m1.1.243">L</mi><mi id="A2.SS3.p2.1.1.m1.1.244">M</mi><mi id="A2.SS3.p2.1.1.m1.1.245">L</mi><mi id="A2.SS3.p2.1.1.m1.1.246">e</mi><mi id="A2.SS3.p2.1.1.m1.1.247">a</mi><mi id="A2.SS3.p2.1.1.m1.1.248">d</mi><mi id="A2.SS3.p2.1.1.m1.1.249">e</mi><mi id="A2.SS3.p2.1.1.m1.1.250">r</mi><mi id="A2.SS3.p2.1.1.m1.1.251">b</mi><mi id="A2.SS3.p2.1.1.m1.1.252">o</mi><mi id="A2.SS3.p2.1.1.m1.1.253">a</mi><mi id="A2.SS3.p2.1.1.m1.1.254">r</mi><mi id="A2.SS3.p2.1.1.m1.1.255">d</mi><mtext class="ltx_citemacro_citep" id="A2.SS3.p2.1.1.m1.1.256"><cite class="ltx_cite ltx_citemacro_citep">[Pal et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib42" title="">2024</a>, Jin et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib23" title="">2019</a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib22" title="">2020</a>, Hendrycks et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib17" title="">2021</a>, Pal et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib41" title="">2022</a>]</cite></mtext><mo id="A2.SS3.p2.1.1.m1.1.257" lspace="0em" rspace="0.167em">.</mo><mi id="A2.SS3.p2.1.1.m1.1.258">A</mi><mi id="A2.SS3.p2.1.1.m1.1.259">l</mi><mi id="A2.SS3.p2.1.1.m1.1.260">l</mi><mi id="A2.SS3.p2.1.1.m1.1.261">c</mi><mi id="A2.SS3.p2.1.1.m1.1.262">o</mi><mi id="A2.SS3.p2.1.1.m1.1.263">d</mi><mi id="A2.SS3.p2.1.1.m1.1.264">e</mi><mi id="A2.SS3.p2.1.1.m1.1.265">a</mi><mi id="A2.SS3.p2.1.1.m1.1.266">n</mi><mi id="A2.SS3.p2.1.1.m1.1.267">d</mi><mi id="A2.SS3.p2.1.1.m1.1.268">a</mi><mi id="A2.SS3.p2.1.1.m1.1.269">l</mi><mi id="A2.SS3.p2.1.1.m1.1.270">o</mi><mi id="A2.SS3.p2.1.1.m1.1.271">c</mi><mi id="A2.SS3.p2.1.1.m1.1.272">k</mi><mo id="A2.SS3.p2.1.1.m1.1.273">−</mo><mi id="A2.SS3.p2.1.1.m1.1.274">f</mi><mi id="A2.SS3.p2.1.1.m1.1.275">i</mi><mi id="A2.SS3.p2.1.1.m1.1.276">l</mi><mi id="A2.SS3.p2.1.1.m1.1.277">e</mi><mi id="A2.SS3.p2.1.1.m1.1.278">w</mi><mi id="A2.SS3.p2.1.1.m1.1.279">i</mi><mi id="A2.SS3.p2.1.1.m1.1.280">t</mi><mi id="A2.SS3.p2.1.1.m1.1.281">h</mi><mi id="A2.SS3.p2.1.1.m1.1.282">s</mi><mi id="A2.SS3.p2.1.1.m1.1.283">p</mi><mi id="A2.SS3.p2.1.1.m1.1.284">e</mi><mi id="A2.SS3.p2.1.1.m1.1.285">c</mi><mi id="A2.SS3.p2.1.1.m1.1.286">i</mi><mi id="A2.SS3.p2.1.1.m1.1.287">f</mi><mi id="A2.SS3.p2.1.1.m1.1.288">i</mi><mi id="A2.SS3.p2.1.1.m1.1.289">c</mi><mi id="A2.SS3.p2.1.1.m1.1.290">v</mi><mi id="A2.SS3.p2.1.1.m1.1.291">e</mi><mi id="A2.SS3.p2.1.1.m1.1.292">r</mi><mi id="A2.SS3.p2.1.1.m1.1.293">s</mi><mi id="A2.SS3.p2.1.1.m1.1.294">i</mi><mi id="A2.SS3.p2.1.1.m1.1.295">o</mi><mi id="A2.SS3.p2.1.1.m1.1.296">n</mi><mi id="A2.SS3.p2.1.1.m1.1.297">s</mi><mi id="A2.SS3.p2.1.1.m1.1.298">i</mi><mi id="A2.SS3.p2.1.1.m1.1.299">s</mi><mi id="A2.SS3.p2.1.1.m1.1.300">a</mi><mi id="A2.SS3.p2.1.1.m1.1.301">v</mi><mi id="A2.SS3.p2.1.1.m1.1.302">a</mi><mi id="A2.SS3.p2.1.1.m1.1.303">i</mi><mi id="A2.SS3.p2.1.1.m1.1.304">l</mi><mi id="A2.SS3.p2.1.1.m1.1.305">a</mi><mi id="A2.SS3.p2.1.1.m1.1.306">b</mi><mi id="A2.SS3.p2.1.1.m1.1.307">l</mi><mi id="A2.SS3.p2.1.1.m1.1.308">e</mi><mi id="A2.SS3.p2.1.1.m1.1.309">i</mi><mi id="A2.SS3.p2.1.1.m1.1.310">n</mi><mi id="A2.SS3.p2.1.1.m1.1.311">o</mi><mi id="A2.SS3.p2.1.1.m1.1.312">u</mi><mi id="A2.SS3.p2.1.1.m1.1.313">r</mi><mi id="A2.SS3.p2.1.1.m1.1.314">G</mi><mi id="A2.SS3.p2.1.1.m1.1.315">i</mi><mi id="A2.SS3.p2.1.1.m1.1.316">t</mi><mi id="A2.SS3.p2.1.1.m1.1.317">H</mi><mi id="A2.SS3.p2.1.1.m1.1.318">u</mi><mi id="A2.SS3.p2.1.1.m1.1.319">b</mi><mi id="A2.SS3.p2.1.1.m1.1.320">r</mi><mi id="A2.SS3.p2.1.1.m1.1.321">e</mi><mi id="A2.SS3.p2.1.1.m1.1.322">p</mi><mi id="A2.SS3.p2.1.1.m1.1.323">o</mi><mi id="A2.SS3.p2.1.1.m1.1.324">s</mi><mi id="A2.SS3.p2.1.1.m1.1.325">i</mi><mi id="A2.SS3.p2.1.1.m1.1.326">t</mi><mi id="A2.SS3.p2.1.1.m1.1.327">o</mi><mi id="A2.SS3.p2.1.1.m1.1.328">r</mi><mi id="A2.SS3.p2.1.1.m1.1.329">y</mi><mo id="A2.SS3.p2.1.1.m1.1.330" lspace="0em">.</mo></mrow><annotation encoding="application/x-tex" id="A2.SS3.p2.1.1.m1.1c">MODEL_{P}ATH,dtype="bfloat16"\ --tasksmedmcqa,medqa_{4}options,mmlu_{a}natomy,%
mmlu_{c}linical_{k}nowledge,mmlu_{c}ollege_{b}iology,mmlu_{c}ollege_{m}edicine%
,mmlu_{m}edical_{g}enetics,mmlu_{p}rofessional_{m}edicine,pubmedqa\ --num_{f}%
ewshot5\ --devicecuda:0\ --batch_{s}ize8ThisevaluatesthetaskoftheOpenMedical-%
LLMLeaderboard\cite[citep]{[\@@bibref{AuthorsPhrase1Year}{openlifescienceai_%
open_medical_llm_leaderboard, jin_pubmedqa_2019, jin_what_2020, hendrycks_%
measuring_2021, pal_medmcqa_2022}{\@@citephrase{, }}{}]}.Allcodeandalock-%
filewithspecificversionsisavailableinourGitHubrepository.\par</annotation><annotation encoding="application/x-llamapun" id="A2.SS3.p2.1.1.m1.1d">italic_M italic_O italic_D italic_E italic_L start_POSTSUBSCRIPT italic_P end_POSTSUBSCRIPT italic_A italic_T italic_H , italic_d italic_t italic_y italic_p italic_e = " italic_b italic_f italic_l italic_o italic_a italic_t 16 " - - italic_t italic_a italic_s italic_k italic_s italic_m italic_e italic_d italic_m italic_c italic_q italic_a , italic_m italic_e italic_d italic_q italic_a start_POSTSUBSCRIPT 4 end_POSTSUBSCRIPT italic_o italic_p italic_t italic_i italic_o italic_n italic_s , italic_m italic_m italic_l italic_u start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT italic_n italic_a italic_t italic_o italic_m italic_y , italic_m italic_m italic_l italic_u start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT italic_l italic_i italic_n italic_i italic_c italic_a italic_l start_POSTSUBSCRIPT italic_k end_POSTSUBSCRIPT italic_n italic_o italic_w italic_l italic_e italic_d italic_g italic_e , italic_m italic_m italic_l italic_u start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT italic_o italic_l italic_l italic_e italic_g italic_e start_POSTSUBSCRIPT italic_b end_POSTSUBSCRIPT italic_i italic_o italic_l italic_o italic_g italic_y , italic_m italic_m italic_l italic_u start_POSTSUBSCRIPT italic_c end_POSTSUBSCRIPT italic_o italic_l italic_l italic_e italic_g italic_e start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT italic_e italic_d italic_i italic_c italic_i italic_n italic_e , italic_m italic_m italic_l italic_u start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT italic_e italic_d italic_i italic_c italic_a italic_l start_POSTSUBSCRIPT italic_g end_POSTSUBSCRIPT italic_e italic_n italic_e italic_t italic_i italic_c italic_s , italic_m italic_m italic_l italic_u start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT italic_r italic_o italic_f italic_e italic_s italic_s italic_i italic_o italic_n italic_a italic_l start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT italic_e italic_d italic_i italic_c italic_i italic_n italic_e , italic_p italic_u italic_b italic_m italic_e italic_d italic_q italic_a - - italic_n italic_u italic_m start_POSTSUBSCRIPT italic_f end_POSTSUBSCRIPT italic_e italic_w italic_s italic_h italic_o italic_t 5 - - italic_d italic_e italic_v italic_i italic_c italic_e italic_c italic_u italic_d italic_a : 0 - - italic_b italic_a italic_t italic_c italic_h start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT italic_i italic_z italic_e 8 italic_T italic_h italic_i italic_s italic_e italic_v italic_a italic_l italic_u italic_a italic_t italic_e italic_s italic_t italic_h italic_e italic_t italic_a italic_s italic_k italic_o italic_f italic_t italic_h italic_e italic_O italic_p italic_e italic_n italic_M italic_e italic_d italic_i italic_c italic_a italic_l - italic_L italic_L italic_M italic_L italic_e italic_a italic_d italic_e italic_r italic_b italic_o italic_a italic_r italic_d . italic_A italic_l italic_l italic_c italic_o italic_d italic_e italic_a italic_n italic_d italic_a italic_l italic_o italic_c italic_k - italic_f italic_i italic_l italic_e italic_w italic_i italic_t italic_h italic_s italic_p italic_e italic_c italic_i italic_f italic_i italic_c italic_v italic_e italic_r italic_s italic_i italic_o italic_n italic_s italic_i italic_s italic_a italic_v italic_a italic_i italic_l italic_a italic_b italic_l italic_e italic_i italic_n italic_o italic_u italic_r italic_G italic_i italic_t italic_H italic_u italic_b italic_r italic_e italic_p italic_o italic_s italic_i italic_t italic_o italic_r italic_y .</annotation></semantics></math></span></p>
</div>
<section class="ltx_subsubsection" id="A2.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsubsection">B.3.1 </span>Compute Budget &amp; Runtime</h4>
<div class="ltx_para" id="A2.SS3.SSS1.p1">
<p class="ltx_p" id="A2.SS3.SSS1.p1.1"><span class="ltx_text" id="A2.SS3.SSS1.p1.1.1" style="font-size:90%;">We report the average training time for </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.p1.1.2" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.p1.1.3" style="font-size:90%;"> as well as NTP++ for initializing the new biomedical domain tokens per model in </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A2.T7" style="font-size:90%;" title="Table 7 ‣ Additional computational budget. ‣ B.3.1 Compute Budget &amp; Runtime ‣ B.3 Open Medical-LLM Leaderboard Experiments ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 7</span></a><span class="ltx_text" id="A2.SS3.SSS1.p1.1.4" style="font-size:90%;">.
We also report the number of new tokens per model. These differ slightly, as we build an initial fixed candidate list, which is then filtered against the existing vocabularies of each model.</span></p>
</div>
<section class="ltx_paragraph" id="A2.SS3.SSS1.Px1">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:90%;">Additional computational budget.</h5>
<div class="ltx_para" id="A2.SS3.SSS1.Px1.p1">
<p class="ltx_p" id="A2.SS3.SSS1.Px1.p1.1"><span class="ltx_text" id="A2.SS3.SSS1.Px1.p1.1.1" style="font-size:90%;">Since </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p1.1.2" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p1.1.3" style="font-size:90%;"> includes an additional forward pass through the model for generating “teacher” hidden states, we do see this overhead reflected in the run times. A forward pass can be approximated as taking one third of the computational cost of a complete forward-backward pass – which roughly matches our measured run times.
Note however that – as demonstrated in </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" style="font-size:90%;" title="5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.2</span></a><span class="ltx_text" id="A2.SS3.SSS1.Px1.p1.1.4" style="font-size:90%;"> – </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p1.1.5" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p1.1.6" style="font-size:90%;"> can be significantly sped up while potentially even improving results by choosing an earlier target layer than the last one, yielding even faster run times than NTP++.</span></p>
</div>
<div class="ltx_para" id="A2.SS3.SSS1.Px1.p2">
<p class="ltx_p" id="A2.SS3.SSS1.Px1.p2.1"><span class="ltx_text" id="A2.SS3.SSS1.Px1.p2.1.1" style="font-size:90%;">However, our main version of </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p2.1.2" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p2.1.3" style="font-size:90%;"> is slightly more expensive than NTP++ – even though both have the same data budget. Thus, we also run the baseline NTP++ with an </span><span class="ltx_text ltx_font_italic" id="A2.SS3.SSS1.Px1.p2.1.4" style="font-size:90%;">additional epoch</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p2.1.5" style="font-size:90%;"> to compensate for the additional teacher forward pass that </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p2.1.6" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p2.1.7" style="font-size:90%;"> uses. Note that this in fact significantly overcompensates for the additional budget </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p2.1.8" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p2.1.9" style="font-size:90%;"> uses. We report the results in </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A2.T8" style="font-size:90%;" title="Table 8 ‣ Additional computational budget. ‣ B.3.1 Compute Budget &amp; Runtime ‣ B.3 Open Medical-LLM Leaderboard Experiments ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 8</span></a><span class="ltx_text" id="A2.SS3.SSS1.Px1.p2.1.10" style="font-size:90%;">. NTP++ is </span><span class="ltx_text ltx_font_italic" id="A2.SS3.SSS1.Px1.p2.1.11" style="font-size:90%;">not able</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p2.1.12" style="font-size:90%;"> to take any major advantage of an additional epoch, yielding similar results to using a single epoch. We investigate the loss curves and find that the training converges towards the beginning of the second epoch.</span><span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Note that we do not anneal the learning rate to zero, so this is not an artifact of the learning rate schedule.</span></span></span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p2.1.13" style="font-size:90%;"></span></p>
</div>
<div class="ltx_para" id="A2.SS3.SSS1.Px1.p3">
<p class="ltx_p" id="A2.SS3.SSS1.Px1.p3.1"><span class="ltx_text" id="A2.SS3.SSS1.Px1.p3.1.1" style="font-size:90%;">We believe that this is at least partly due to the fundamental limitation of next-token prediction for our task of learning a single new embedding for an already pretrained model.
Instead of matching model behavior between seeing the single new and the multiple original subtokens, NTP simply adjusts weights to maximize the likelihood of the given sequences. Consider “</span><span class="ltx_text ltx_font_typewriter" id="A2.SS3.SSS1.Px1.p3.1.2" style="font-size:90%;">&lt;new_token&gt; is a football player</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p3.1.3" style="font-size:90%;">”, where we want </span><span class="ltx_text ltx_font_typewriter" id="A2.SS3.SSS1.Px1.p3.1.4" style="font-size:90%;">&lt;new_token&gt;</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p3.1.5" style="font-size:90%;"> to represent </span><span class="ltx_text ltx_font_typewriter" id="A2.SS3.SSS1.Px1.p3.1.6" style="font-size:90%;">Messi</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p3.1.7" style="font-size:90%;">: NTP will only be able to learn a generic embedding from this sequence that will capture semantics from many different (American football </span><span class="ltx_text ltx_font_italic" id="A2.SS3.SSS1.Px1.p3.1.8" style="font-size:90%;">and</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p3.1.9" style="font-size:90%;"> soccer) players. Our proposed distillation objective </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p3.1.10" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p3.1.11" style="font-size:90%;"> however is able to learn a more specific representation, as it has access to hidden states from the sequence “</span><span class="ltx_text ltx_font_typewriter" id="A2.SS3.SSS1.Px1.p3.1.12" style="font-size:90%;">Messi is a football player</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p3.1.13" style="font-size:90%;">”, which capture more specific details necessary for a good representation.</span></p>
</div>
<div class="ltx_para" id="A2.SS3.SSS1.Px1.p4">
<p class="ltx_p" id="A2.SS3.SSS1.Px1.p4.1"><span class="ltx_text" id="A2.SS3.SSS1.Px1.p4.1.1" style="font-size:90%;">To demonstrate, we also run </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p4.1.2" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p4.1.3" style="font-size:90%;"> for an additional epoch. </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p4.1.4" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p4.1.5" style="font-size:90%;"> is in fact able to take advantage of a second pass over the data, in some case with major improvements in the benchmark results. This illustrates the richer training signal that our distillation-based objective provides. Remarkably, for </span><span class="ltx_text ltx_font_typewriter" id="A2.SS3.SSS1.Px1.p4.1.6" style="font-size:90%;">OLMo-2-7B-i</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p4.1.7" style="font-size:90%;">, </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px1.p4.1.8" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px1.p4.1.9" style="font-size:90%;"> now is actually able to match the original model with the original tokenization while instead using the new tokenization.</span></p>
</div>
<figure class="ltx_table" id="A2.T7">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T7.1" style="width:433.6pt;height:93.8pt;vertical-align:-0.9pt;"><span class="ltx_transformed_inner" style="transform:translate(-28.6pt,6.1pt) scale(0.88359,0.88359) ;">
<p class="ltx_p" id="A2.T7.1.1"><span class="ltx_text" id="A2.T7.1.1.1" style="font-size:90%;">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1" style="width:490.7pt;height:106.2pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1"><span class="ltx_text" id="A2.T7.1.1.1.1.1.1">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T7.1.1.1.1.1.1.1">
<span class="ltx_tbody">
<span class="ltx_tr" id="A2.T7.1.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.1">Init Method</span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.2">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1.1.1.1.1.1.2.1" style="width:41.9pt;height:41.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:53.0pt;transform:translate(-5.55pt,-17.82pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1.1.1.1.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T7.1.1.1.1.1.1.1.1.1.2.1.1.1">Llama-3.1-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.3">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1.1.1.1.1.1.3.1" style="width:45.8pt;height:45.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.5pt;transform:translate(-6.36pt,-19.77pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1.1.1.1.1.3.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T7.1.1.1.1.1.1.1.1.1.3.1.1.1">Llama-3.1-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.4">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1.1.1.1.1.1.4.1" style="width:41.9pt;height:41.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:53.0pt;transform:translate(-5.55pt,-17.82pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1.1.1.1.1.4.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T7.1.1.1.1.1.1.1.1.1.4.1.1.1">Llama-3.2-3B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.5">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1.1.1.1.1.1.5.1" style="width:45.8pt;height:45.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.5pt;transform:translate(-6.36pt,-19.77pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1.1.1.1.1.5.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T7.1.1.1.1.1.1.1.1.1.5.1.1.1">Llama-3.2-3B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.6">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1.1.1.1.1.1.6.1" style="width:36.9pt;height:36.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.0pt;transform:translate(-4.53pt,-15.35pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1.1.1.1.1.6.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T7.1.1.1.1.1.1.1.1.1.6.1.1.1">Llama-3-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.7">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1.1.1.1.1.1.7.1" style="width:40.8pt;height:40.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.5pt;transform:translate(-5.33pt,-17.29pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1.1.1.1.1.7.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T7.1.1.1.1.1.1.1.1.1.7.1.1.1">Llama-3-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.8">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1.1.1.1.1.1.8.1" style="width:34.3pt;height:34.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.2pt;transform:translate(-3.97pt,-14pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1.1.1.1.1.8.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T7.1.1.1.1.1.1.1.1.1.8.1.1.1">Mistral-7B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_border_tt" id="A2.T7.1.1.1.1.1.1.1.1.1.9">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T7.1.1.1.1.1.1.1.1.1.9.1" style="width:41.3pt;height:41.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:52.3pt;transform:translate(-5.48pt,-17.57pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T7.1.1.1.1.1.1.1.1.1.9.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T7.1.1.1.1.1.1.1.1.1.9.1.1.1">OLMo-2-7B-i</span></span>
</span></span></span></span>
<span class="ltx_tr" id="A2.T7.1.1.1.1.1.1.1.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.1">NTP++</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.2">08:44</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.3">08:49</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.4">05:52</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.5">05:18</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.6">08:43</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.7">08:46</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.8">06:01</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.2.2.9">07:40</span></span>
<span class="ltx_tr" id="A2.T7.1.1.1.1.1.1.1.3.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="A2.T7.1.1.1.1.1.1.1.3.3.1"><span class="ltx_text ltx_font_smallcaps" id="A2.T7.1.1.1.1.1.1.1.3.3.1.1">AweDist</span></span>
<span class="ltx_td ltx_align_left" id="A2.T7.1.1.1.1.1.1.1.3.3.2">11:31</span>
<span class="ltx_td ltx_align_left" id="A2.T7.1.1.1.1.1.1.1.3.3.3">11:33</span>
<span class="ltx_td ltx_align_left" id="A2.T7.1.1.1.1.1.1.1.3.3.4">07:38</span>
<span class="ltx_td ltx_align_left" id="A2.T7.1.1.1.1.1.1.1.3.3.5">08:07</span>
<span class="ltx_td ltx_align_left" id="A2.T7.1.1.1.1.1.1.1.3.3.6">12:00</span>
<span class="ltx_td ltx_align_left" id="A2.T7.1.1.1.1.1.1.1.3.3.7">11:29</span>
<span class="ltx_td ltx_align_left" id="A2.T7.1.1.1.1.1.1.1.3.3.8">09:09</span>
<span class="ltx_td ltx_align_left" id="A2.T7.1.1.1.1.1.1.1.3.3.9">10:22</span></span>
<span class="ltx_tr" id="A2.T7.1.1.1.1.1.1.1.4.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.1"># new tokens</span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.2">2589</span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.3">2589</span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.4">2589</span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.5">2589</span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.6">2589</span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.7">2589</span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.8">2637</span>
<span class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A2.T7.1.1.1.1.1.1.1.4.4.9">2591</span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 7: </span>Minimum training times (mm:ss) on the domain adaptation token initialization experiments for each model and initialization method measured over three runs. We report the minimum, as we run experiments on a shared cluster. We use a single H100 80GB GPU. We also include the number of new tokens for each model type.</figcaption>
</figure>
<figure class="ltx_table" id="A2.T8">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="A2.T8.2" style="width:433.6pt;height:112.7pt;vertical-align:-0.8pt;"><span class="ltx_transformed_inner" style="transform:translate(-56.7pt,14.6pt) scale(0.79256,0.79256) ;">
<p class="ltx_p" id="A2.T8.2.2"><span class="ltx_text" id="A2.T8.2.2.2" style="font-size:90%;">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2" style="width:547.1pt;height:142.2pt;vertical-align:-1.0pt;"><span class="ltx_transformed_inner" style="transform:translate(0.0pt,0.0pt) scale(1,1) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2"><span class="ltx_text" id="A2.T8.2.2.2.2.2.2">
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="A2.T8.2.2.2.2.2.2.2">
<span class="ltx_tbody">
<span class="ltx_tr" id="A2.T8.2.2.2.2.2.2.2.3.1">
<span class="ltx_td ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.1"></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.2">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2.2.2.2.3.1.2.1" style="width:34.3pt;height:34.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:42.2pt;transform:translate(-3.97pt,-14pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2.2.2.3.1.2.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T8.2.2.2.2.2.2.2.3.1.2.1.1.1">Mistral-7B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.3">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2.2.2.2.3.1.3.1" style="width:36.9pt;height:36.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:46.0pt;transform:translate(-4.53pt,-15.35pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2.2.2.3.1.3.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T8.2.2.2.2.2.2.2.3.1.3.1.1.1">Llama-3-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.4">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2.2.2.2.3.1.4.1" style="width:40.8pt;height:40.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:51.5pt;transform:translate(-5.33pt,-17.29pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2.2.2.3.1.4.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T8.2.2.2.2.2.2.2.3.1.4.1.1.1">Llama-3-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.5">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2.2.2.2.3.1.5.1" style="width:41.9pt;height:41.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:53.0pt;transform:translate(-5.55pt,-17.82pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2.2.2.3.1.5.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T8.2.2.2.2.2.2.2.3.1.5.1.1.1">Llama-3.1-8B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.6">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2.2.2.2.3.1.6.1" style="width:45.8pt;height:45.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.5pt;transform:translate(-6.36pt,-19.77pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2.2.2.3.1.6.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T8.2.2.2.2.2.2.2.3.1.6.1.1.1">Llama-3.1-8B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.7">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2.2.2.2.3.1.7.1" style="width:41.9pt;height:41.9pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:53.0pt;transform:translate(-5.55pt,-17.82pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2.2.2.3.1.7.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T8.2.2.2.2.2.2.2.3.1.7.1.1.1">Llama-3.2-3B</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.8">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2.2.2.2.3.1.8.1" style="width:45.8pt;height:45.8pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:58.5pt;transform:translate(-6.36pt,-19.77pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2.2.2.3.1.8.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T8.2.2.2.2.2.2.2.3.1.8.1.1.1">Llama-3.2-3B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.9">
<span class="ltx_inline-block ltx_transformed_outer" id="A2.T8.2.2.2.2.2.2.2.3.1.9.1" style="width:41.3pt;height:41.3pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:52.3pt;transform:translate(-5.48pt,-17.57pt) rotate(-45deg) ;">
<span class="ltx_p" id="A2.T8.2.2.2.2.2.2.2.3.1.9.1.1"><span class="ltx_text ltx_font_typewriter" id="A2.T8.2.2.2.2.2.2.2.3.1.9.1.1.1">OLMo-2-7B-i</span></span>
</span></span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A2.T8.2.2.2.2.2.2.2.3.1.10">Avg.</span></span>
<span class="ltx_tr" id="A2.T8.2.2.2.2.2.2.2.4.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.1">Target</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.2">64.5</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.3">69.8</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.4">70.6</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.5">69.2</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.6">72.3</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.7">60.1</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.8">64.4</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.9">61.3</span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.4.2.10">66.5</span></span>
<span class="ltx_tr" id="A2.T8.2.2.2.2.2.2.2.5.3">
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.1">NTP++ (1 epoch)</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.2">61.2±0.4</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.3">65.6±0.3</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.4">65.3±0.6</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.5">66.0±0.5</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.6">68.9±0.2</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.7">56.6±0.6</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.8">61.3±0.5</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.9">58.7±0.6</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.5.3.10">63.0</span></span>
<span class="ltx_tr" id="A2.T8.1.1.1.1.1.1.1.1">
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.1"><span class="ltx_text" id="A2.T8.1.1.1.1.1.1.1.1.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1"><semantics id="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1a"><mo id="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1.1" xref="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1b"><ci id="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1.1.cmml" xref="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="A2.T8.1.1.1.1.1.1.1.1.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="A2.T8.1.1.1.1.1.1.1.1.1.2">AweDist</span> (1 epoch)</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.2">62.8±0.5</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.3">67.3±0.2</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.4">67.6±0.3</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.5">67.3±0.5</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.6">71.0±0.2</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.7">56.2±1.9</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.8">63.1±0.2</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.9">61.2±0.3</span>
<span class="ltx_td ltx_align_left" id="A2.T8.1.1.1.1.1.1.1.1.10">64.6</span></span>
<span class="ltx_tr" id="A2.T8.2.2.2.2.2.2.2.6.4">
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.1">NTP++ (2 epoch)</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.2">61.0</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.3">66.1</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.4">65.4</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.5">65.9</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.6">68.7</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.7">56.7</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.8">61.5</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.9">58.9</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="A2.T8.2.2.2.2.2.2.2.6.4.10">63.0</span></span>
<span class="ltx_tr" id="A2.T8.2.2.2.2.2.2.2.2">
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.1"><span class="ltx_text" id="A2.T8.2.2.2.2.2.2.2.2.1.1" style="position:relative; bottom:0.9pt;"><math alttext="\star" class="ltx_Math" display="inline" id="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1"><semantics id="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1a"><mo id="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1.1" xref="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml">⋆</mo><annotation-xml encoding="MathML-Content" id="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1b"><ci id="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1.1.cmml" xref="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1.1">⋆</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1c">\star</annotation><annotation encoding="application/x-llamapun" id="A2.T8.2.2.2.2.2.2.2.2.1.1.m1.1d">⋆</annotation></semantics></math></span> <span class="ltx_text ltx_font_smallcaps" id="A2.T8.2.2.2.2.2.2.2.2.1.2">AweDist</span> (2 epoch)</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.2"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.2.1">63.2</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.3"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.3.1">68.5</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.4"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.4.1">68.9</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.5"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.5.1">67.7</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.6"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.6.1">71.8</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.7"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.7.1">58.4</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.8"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.8.1">63.6</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.9"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.9.1">61.3</span></span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="A2.T8.2.2.2.2.2.2.2.2.10"><span class="ltx_text ltx_font_bold" id="A2.T8.2.2.2.2.2.2.2.2.10.1">65.4</span></span></span>
</span>
</span></span></span>
</span></span></span></p>
</span></div>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparison of different initialization methods for domain adaptation. We report a macro-average of the tasks in the Open Medical-LLM leaderboard (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S4.SS1" title="4.1 Evaluation ‣ 4 Experimental Setup ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 4.1</span></a>). The best non-target result for each model is given in <span class="ltx_text ltx_font_bold" id="A2.T8.8.1">boldface</span>.
We only run a single seed for the two epoch variants.
</figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="A2.SS3.SSS1.Px2">
<h5 class="ltx_title ltx_title_paragraph" style="font-size:90%;">
<span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px2.1.1">AweDist</span> versus hyper-networks.</h5>
<div class="ltx_para" id="A2.SS3.SSS1.Px2.p1">
<p class="ltx_p" id="A2.SS3.SSS1.Px2.p1.1"><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.1" style="font-size:90%;">Comparing the computational budget with ZeTT </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.2.1" style="font-size:90%;">[</span>Minixhofer et al.<span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib35" title="">2024</a><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.4.3" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.5" style="font-size:90%;">, ZeTT sees 3.2 </span><span class="ltx_text ltx_font_italic" id="A2.SS3.SSS1.Px2.p1.1.6" style="font-size:90%;">billion</span><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.7" style="font-size:90%;"> tokens during hyper-network pretraining, while </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px2.p1.1.8" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.9" style="font-size:90%;"> sees only 3.2 </span><span class="ltx_text ltx_font_italic" id="A2.SS3.SSS1.Px2.p1.1.10" style="font-size:90%;">million</span><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.11" style="font-size:90%;"> tokens (for initializing 2,600 new tokens), less than 1% of ZeTT’s budget – still </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px2.p1.1.12" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.13" style="font-size:90%;"> outperforms ZeTT. In return, ZeTT is faster at inference time (in our experiments on a single H100 80GB GPU, ZeTT took less than a minute).
In future work, we believe that investigating an </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS3.SSS1.Px2.p1.1.14" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS3.SSS1.Px2.p1.1.15" style="font-size:90%;">-style objective for hyper-network pretraining would be beneficial, as our experiments show that this significantly outperforms next-token prediction for learning new embeddings (ZeTT uses next-token prediction).</span></p>
</div>
</section>
</section>
</section>
<section class="ltx_subsection" id="A2.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">B.4 </span>LLM-as-a-Judge Experiments</h3>
<div class="ltx_para" id="A2.SS4.p1">
<p class="ltx_p" id="A2.SS4.p1.1"><span class="ltx_text" id="A2.SS4.p1.1.1" style="font-size:90%;">For generating definitions for a particular new token, we use the prompt “</span><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.1.2" style="font-size:90%;">The word &lt;new_token&gt; is defined as</span><span class="ltx_text" id="A2.SS4.p1.1.3" style="font-size:90%;">” following </span><cite class="ltx_cite ltx_citemacro_citet">Teehan et al. <span class="ltx_text" id="A2.SS4.p1.1.4.1.1.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib54" title="">2024</a><span class="ltx_text" id="A2.SS4.p1.1.5.2.2.1" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS4.p1.1.6" style="font-size:90%;">, where </span><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p1.1.7" style="font-size:90%;">&lt;new_token&gt;</span><span class="ltx_text" id="A2.SS4.p1.1.8" style="font-size:90%;"> is replaced by the string representation of the new token (with whitespace stripped from the left side). We use greedy decoding. Our evaluation considers the model checkpoint corresponding to the best learning rate from the experiments described in </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A2.SS3" style="font-size:90%;" title="B.3 Open Medical-LLM Leaderboard Experiments ‣ Appendix B Implementation Details ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section B.3</span></a><span class="ltx_text" id="A2.SS4.p1.1.9" style="font-size:90%;"> for each method.</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p2">
<p class="ltx_p" id="A2.SS4.p2.1"><span class="ltx_text" id="A2.SS4.p2.1.1" style="font-size:90%;">LLM-as-a-Judge evaluations are not perfect – see for example in </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.T2" style="font-size:90%;" title="Table 2 ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Table 2</span></a><span class="ltx_text" id="A2.SS4.p2.1.2" style="font-size:90%;"> the similarity scores (Sim) of the target model (measuring similarity </span><span class="ltx_text ltx_font_italic" id="A2.SS4.p2.1.3" style="font-size:90%;">with the target model</span><span class="ltx_text" id="A2.SS4.p2.1.4" style="font-size:90%;">), which should obviously always be 100%, but turn out to be 99.4% / 99.8% in the case of </span><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p2.1.5" style="font-size:90%;">OLMo-2-7B-i</span><span class="ltx_text" id="A2.SS4.p2.1.6" style="font-size:90%;"> and </span><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p2.1.7" style="font-size:90%;">Mistral-7B</span><span class="ltx_text" id="A2.SS4.p2.1.8" style="font-size:90%;">, respectively. Nevertheless, these results (which we report as a sanity check on the judge quality) are very close to correct, which is encouraging.</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p3">
<p class="ltx_p" id="A2.SS4.p3.1"><span class="ltx_text" id="A2.SS4.p3.1.1" style="font-size:90%;">We use </span><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p3.1.2" style="font-size:90%;">meta-llama/Llama-3.3-70B-Instruct</span><span class="ltx_text" id="A2.SS4.p3.1.3" style="font-size:90%;"> as the judge model and run it using </span><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p3.1.4" style="font-size:90%;">bfloat16</span><span class="ltx_text" id="A2.SS4.p3.1.5" style="font-size:90%;"> layer-parallel inference over two H100 GPUs using HuggingFace </span><span class="ltx_text ltx_font_typewriter" id="A2.SS4.p3.1.6" style="font-size:90%;">transformers</span><span class="ltx_text" id="A2.SS4.p3.1.7" style="font-size:90%;">.
We use prompts adapted from </span><cite class="ltx_cite ltx_citemacro_citet">Li et al. <span class="ltx_text" id="A2.SS4.p3.1.8.1.1.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib29" title="">2023</a><span class="ltx_text" id="A2.SS4.p3.1.9.2.2.1" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS4.p3.1.10" style="font-size:90%;"> and </span><cite class="ltx_cite ltx_citemacro_citet">Villegas et al. <span class="ltx_text" id="A2.SS4.p3.1.11.1.1.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib56" title="">2025</a><span class="ltx_text" id="A2.SS4.p3.1.12.2.2.1" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS4.p3.1.13" style="font-size:90%;"> for evaluating the general correctness of generated definitions as well as the similarity with the original model.</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p4">
<p class="ltx_p" id="A2.SS4.p4.1"><span class="ltx_text" id="A2.SS4.p4.1.1" style="font-size:90%;">For correctness evaluation, we use the following prompt:</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p5">
<span class="ltx_ERROR undefined" id="A2.SS4.p5.1">{minted}</span>
<p class="ltx_p" id="A2.SS4.p5.2"><span class="ltx_text" id="A2.SS4.p5.2.1" style="font-size:90%;">[fontsize=, breaklines=true, linenos]python
f"""&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p6">
<p class="ltx_p" id="A2.SS4.p6.1"><span class="ltx_text" id="A2.SS4.p6.1.1" style="font-size:90%;">You are a pattern-following assistant that can only answer with "Yes" or "No". Your goal is to determine whether a provided definition for a given word is correct. The definition should be on topic and specific but does not need to be exhaustive.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p7">
<p class="ltx_p" id="A2.SS4.p7.1"><span class="ltx_text" id="A2.SS4.p7.1.1" style="font-size:90%;">Remember to answer with one word either "Yes" or "No".</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p8">
<p class="ltx_p" id="A2.SS4.p8.1"><span class="ltx_text" id="A2.SS4.p8.1.1" style="font-size:90%;">### Instruction:
Determine if the provided definition for the word "token.lstrip()" is correct.</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p9">
<p class="ltx_p" id="A2.SS4.p9.1"><span class="ltx_text" id="A2.SS4.p9.1.1" style="font-size:90%;">### Definition token.lstrip():
line["model_definition"].strip()</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p10">
<p class="ltx_p" id="A2.SS4.p10.1"><span class="ltx_text" id="A2.SS4.p10.1.1" style="font-size:90%;">### Is the provided definition correct, specific, and on topic (Yes or No)?:&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span><span class="ltx_ERROR undefined" id="A2.SS4.p10.1.2">\n</span><span class="ltx_ERROR undefined" id="A2.SS4.p10.1.3">\n</span><span class="ltx_text" id="A2.SS4.p10.1.4" style="font-size:90%;">"""</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p11">
<p class="ltx_p" id="A2.SS4.p11.1"><span class="ltx_text" id="A2.SS4.p11.1.1" style="font-size:90%;">For similarity with the original target model’s definition, we use this prompt:</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p12">
<span class="ltx_ERROR undefined" id="A2.SS4.p12.1">{minted}</span>
<p class="ltx_p" id="A2.SS4.p12.2"><span class="ltx_text" id="A2.SS4.p12.2.1" style="font-size:90%;">[fontsize=, breaklines=true, linenos]python
f"""&lt;|start_header_id|&gt;system&lt;|end_header_id|&gt;</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p13">
<p class="ltx_p" id="A2.SS4.p13.1"><span class="ltx_text" id="A2.SS4.p13.1.1" style="font-size:90%;">You are a pattern-following assistant that can only answer with "Yes" or "No". Your goal is to determine whether a predicted definition conveys a similar enough meaning to the ground truth definition provided for a given word.&lt;|eot_id|&gt;&lt;|start_header_id|&gt;user&lt;|end_header_id|&gt;</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p14">
<p class="ltx_p" id="A2.SS4.p14.1"><span class="ltx_text" id="A2.SS4.p14.1.1" style="font-size:90%;">Remember to answer with one word either "Yes" or "No".</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p15">
<p class="ltx_p" id="A2.SS4.p15.1"><span class="ltx_text" id="A2.SS4.p15.1.1" style="font-size:90%;">### Instruction:
Determine if the predicted definition conveys a similar meaning to the ground truth definition. The word is "token.lstrip()".</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p16">
<p class="ltx_p" id="A2.SS4.p16.1"><span class="ltx_text" id="A2.SS4.p16.1.1" style="font-size:90%;">### Ground truth definition:
line["target_definition"].strip()</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p17">
<p class="ltx_p" id="A2.SS4.p17.1"><span class="ltx_text" id="A2.SS4.p17.1.1" style="font-size:90%;">### Predicted definition:
line["model_definition"].strip()</span></p>
</div>
<div class="ltx_para" id="A2.SS4.p18">
<p class="ltx_p" id="A2.SS4.p18.1"><span class="ltx_text" id="A2.SS4.p18.1.1" style="font-size:90%;">### Does the predicted definition convey a similar meaning to the ground truth definition (Yes or No)?:&lt;|eot_id|&gt;&lt;|start_header_id|&gt;assistant&lt;|end_header_id|&gt;</span><span class="ltx_ERROR undefined" id="A2.SS4.p18.1.2">\n</span><span class="ltx_ERROR undefined" id="A2.SS4.p18.1.3">\n</span><span class="ltx_text" id="A2.SS4.p18.1.4" style="font-size:90%;">"""</span></p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS5">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">B.5 </span>Generation of Relevant Contexts</h3>
<div class="ltx_para" id="A2.SS5.p1">
<p class="ltx_p" id="A2.SS5.p1.6"><span class="ltx_text" id="A2.SS5.p1.6.1" style="font-size:90%;">When generating contexts that contain a new token, we prompt the model with the beginning-of-sequence (BOS) token followed by the textual representation of the new token. Using the new token “</span><span class="ltx_text ltx_font_typewriter" id="A2.SS5.p1.6.2" style="font-size:90%;">palatable</span><span class="ltx_text" id="A2.SS5.p1.6.3" style="font-size:90%;">” as an example, our prompt is then: “</span><span class="ltx_text ltx_font_typewriter" id="A2.SS5.p1.6.4" style="font-size:90%;">&lt;s&gt; palatable</span><span class="ltx_text" id="A2.SS5.p1.6.5" style="font-size:90%;">”. Note that we add a prefix space after the BOS token </span><span class="ltx_text ltx_font_typewriter" id="A2.SS5.p1.6.6" style="font-size:90%;">&lt;s&gt;</span><span class="ltx_text" id="A2.SS5.p1.6.7" style="font-size:90%;">. We then sample </span><math alttext="N" class="ltx_Math" display="inline" id="A2.SS5.p1.1.m1.1"><semantics id="A2.SS5.p1.1.m1.1a"><mi id="A2.SS5.p1.1.m1.1.1" mathsize="90%" xref="A2.SS5.p1.1.m1.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.SS5.p1.1.m1.1b"><ci id="A2.SS5.p1.1.m1.1.1.cmml" xref="A2.SS5.p1.1.m1.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS5.p1.1.m1.1c">N</annotation><annotation encoding="application/x-llamapun" id="A2.SS5.p1.1.m1.1d">italic_N</annotation></semantics></math><span class="ltx_text" id="A2.SS5.p1.6.8" style="font-size:90%;"> new sequences of length </span><math alttext="L" class="ltx_Math" display="inline" id="A2.SS5.p1.2.m2.1"><semantics id="A2.SS5.p1.2.m2.1a"><mi id="A2.SS5.p1.2.m2.1.1" mathsize="90%" xref="A2.SS5.p1.2.m2.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="A2.SS5.p1.2.m2.1b"><ci id="A2.SS5.p1.2.m2.1.1.cmml" xref="A2.SS5.p1.2.m2.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS5.p1.2.m2.1c">L</annotation><annotation encoding="application/x-llamapun" id="A2.SS5.p1.2.m2.1d">italic_L</annotation></semantics></math><span class="ltx_text" id="A2.SS5.p1.6.9" style="font-size:90%;">, where </span><math alttext="N" class="ltx_Math" display="inline" id="A2.SS5.p1.3.m3.1"><semantics id="A2.SS5.p1.3.m3.1a"><mi id="A2.SS5.p1.3.m3.1.1" mathsize="90%" xref="A2.SS5.p1.3.m3.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="A2.SS5.p1.3.m3.1b"><ci id="A2.SS5.p1.3.m3.1.1.cmml" xref="A2.SS5.p1.3.m3.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS5.p1.3.m3.1c">N</annotation><annotation encoding="application/x-llamapun" id="A2.SS5.p1.3.m3.1d">italic_N</annotation></semantics></math><span class="ltx_text" id="A2.SS5.p1.6.10" style="font-size:90%;"> and </span><math alttext="L" class="ltx_Math" display="inline" id="A2.SS5.p1.4.m4.1"><semantics id="A2.SS5.p1.4.m4.1a"><mi id="A2.SS5.p1.4.m4.1.1" mathsize="90%" xref="A2.SS5.p1.4.m4.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="A2.SS5.p1.4.m4.1b"><ci id="A2.SS5.p1.4.m4.1.1.cmml" xref="A2.SS5.p1.4.m4.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="A2.SS5.p1.4.m4.1c">L</annotation><annotation encoding="application/x-llamapun" id="A2.SS5.p1.4.m4.1d">italic_L</annotation></semantics></math><span class="ltx_text" id="A2.SS5.p1.6.11" style="font-size:90%;"> are the same as in the settings we use for context retrieval from an existing corpus (in this work we use </span><math alttext="N=25" class="ltx_Math" display="inline" id="A2.SS5.p1.5.m5.1"><semantics id="A2.SS5.p1.5.m5.1a"><mrow id="A2.SS5.p1.5.m5.1.1" xref="A2.SS5.p1.5.m5.1.1.cmml"><mi id="A2.SS5.p1.5.m5.1.1.2" mathsize="90%" xref="A2.SS5.p1.5.m5.1.1.2.cmml">N</mi><mo id="A2.SS5.p1.5.m5.1.1.1" mathsize="90%" xref="A2.SS5.p1.5.m5.1.1.1.cmml">=</mo><mn id="A2.SS5.p1.5.m5.1.1.3" mathsize="90%" xref="A2.SS5.p1.5.m5.1.1.3.cmml">25</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS5.p1.5.m5.1b"><apply id="A2.SS5.p1.5.m5.1.1.cmml" xref="A2.SS5.p1.5.m5.1.1"><eq id="A2.SS5.p1.5.m5.1.1.1.cmml" xref="A2.SS5.p1.5.m5.1.1.1"></eq><ci id="A2.SS5.p1.5.m5.1.1.2.cmml" xref="A2.SS5.p1.5.m5.1.1.2">𝑁</ci><cn id="A2.SS5.p1.5.m5.1.1.3.cmml" type="integer" xref="A2.SS5.p1.5.m5.1.1.3">25</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS5.p1.5.m5.1c">N=25</annotation><annotation encoding="application/x-llamapun" id="A2.SS5.p1.5.m5.1d">italic_N = 25</annotation></semantics></math><span class="ltx_text" id="A2.SS5.p1.6.12" style="font-size:90%;"> and </span><math alttext="L=50" class="ltx_Math" display="inline" id="A2.SS5.p1.6.m6.1"><semantics id="A2.SS5.p1.6.m6.1a"><mrow id="A2.SS5.p1.6.m6.1.1" xref="A2.SS5.p1.6.m6.1.1.cmml"><mi id="A2.SS5.p1.6.m6.1.1.2" mathsize="90%" xref="A2.SS5.p1.6.m6.1.1.2.cmml">L</mi><mo id="A2.SS5.p1.6.m6.1.1.1" mathsize="90%" xref="A2.SS5.p1.6.m6.1.1.1.cmml">=</mo><mn id="A2.SS5.p1.6.m6.1.1.3" mathsize="90%" xref="A2.SS5.p1.6.m6.1.1.3.cmml">50</mn></mrow><annotation-xml encoding="MathML-Content" id="A2.SS5.p1.6.m6.1b"><apply id="A2.SS5.p1.6.m6.1.1.cmml" xref="A2.SS5.p1.6.m6.1.1"><eq id="A2.SS5.p1.6.m6.1.1.1.cmml" xref="A2.SS5.p1.6.m6.1.1.1"></eq><ci id="A2.SS5.p1.6.m6.1.1.2.cmml" xref="A2.SS5.p1.6.m6.1.1.2">𝐿</ci><cn id="A2.SS5.p1.6.m6.1.1.3.cmml" type="integer" xref="A2.SS5.p1.6.m6.1.1.3">50</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS5.p1.6.m6.1c">L=50</annotation><annotation encoding="application/x-llamapun" id="A2.SS5.p1.6.m6.1d">italic_L = 50</annotation></semantics></math><span class="ltx_text" id="A2.SS5.p1.6.13" style="font-size:90%;">). We sample using the standard sampling settings of each model from the </span><span class="ltx_text ltx_font_typewriter" id="A2.SS5.p1.6.14" style="font-size:90%;">transformers</span><span class="ltx_text" id="A2.SS5.p1.6.15" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="A2.SS5.p1.6.16.1" style="font-size:90%;">[</span>Wolf et al.<span class="ltx_text" id="A2.SS5.p1.6.17.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib59" title="">2020</a><span class="ltx_text" id="A2.SS5.p1.6.18.3" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS5.p1.6.19" style="font-size:90%;"> library.</span></p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS6">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">B.6 </span>Isolated Per-Token Optimization vs. Joint Optimization</h3>
<div class="ltx_para" id="A2.SS6.p1">
<p class="ltx_p" id="A2.SS6.p1.1"><span class="ltx_text" id="A2.SS6.p1.1.1" style="font-size:90%;">When implementing </span><span class="ltx_text ltx_font_smallcaps" id="A2.SS6.p1.1.2" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A2.SS6.p1.1.3" style="font-size:90%;">, we have the choice of performing the optimization process for each new input embedding separately or simply allowing updates to all new input embeddings at the same time.</span></p>
</div>
<div class="ltx_para" id="A2.SS6.p2">
<p class="ltx_p" id="A2.SS6.p2.1"><span class="ltx_text" id="A2.SS6.p2.1.1" style="font-size:90%;">For the experiments in this paper, we utilize the joint optimization approach. However, we also implemented the isolated optimization of each new embedding for initial experiments. In this setting, gradients are only propagated to a single input embedding (corresponding to the target token of the retrieved snippet), even if other new target tokens also occur in that snippet.</span></p>
</div>
<div class="ltx_para" id="A2.SS6.p3">
<p class="ltx_p" id="A2.SS6.p3.1"><span class="ltx_text" id="A2.SS6.p3.1.1" style="font-size:90%;">In our exploratory experiments, we found that the joint approach yields slightly better results and conjecture that this is because we are able to have more total gradient updates per target token due to co-occurrence in a particular snippet. As an added benefit, the joint approach has less implementation complexity.</span></p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS7">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">B.7 </span>Hyperparameters</h3>
<div class="ltx_para" id="A2.SS7.p1">
<p class="ltx_p" id="A2.SS7.p1.1"><span class="ltx_text" id="A2.SS7.p1.1.1" style="font-size:90%;">We use AdamW </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="A2.SS7.p1.1.2.1" style="font-size:90%;">[</span>Kingma and Ba<span class="ltx_text" id="A2.SS7.p1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib24" title="">2017</a>, Loshchilov and Hutter<span class="ltx_text" id="A2.SS7.p1.1.3.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib32" title="">2019</a><span class="ltx_text" id="A2.SS7.p1.1.4.3" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS7.p1.1.5" style="font-size:90%;"> optimization for all trainable parameters (the new token embeddings) with a batch size of 16, which maximizes throughput on our used Nvidia H100 80GB GPUs.
We do not incorporate any weight decay and maintain a constant learning rate schedule with a linear warmup for the first half of the training steps. For fair comparison, we sweep for the best learning rate for each method that requires a learning rate. We use the same data for all training-based methods, including baselines and variations of our method.</span></p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS8">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">B.8 </span>Models</h3>
<div class="ltx_para" id="A2.SS8.p1">
<p class="ltx_p" id="A2.SS8.p1.1"><span class="ltx_text" id="A2.SS8.p1.1.1" style="font-size:90%;">We use the models </span><span class="ltx_text ltx_font_typewriter" id="A2.SS8.p1.1.2" style="font-size:90%;">Mistral-7B-v0.1</span><span class="ltx_text" id="A2.SS8.p1.1.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="A2.SS8.p1.1.4.1" style="font-size:90%;">[</span>Jiang et al.<span class="ltx_text" id="A2.SS8.p1.1.5.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib21" title="">2023</a><span class="ltx_text" id="A2.SS8.p1.1.6.3" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS8.p1.1.7" style="font-size:90%;">, </span><span class="ltx_text ltx_font_typewriter" id="A2.SS8.p1.1.8" style="font-size:90%;">OLMo-7B-1124-Instruct</span><span class="ltx_text" id="A2.SS8.p1.1.9" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="A2.SS8.p1.1.10.1" style="font-size:90%;">[</span>OLMo et al.<span class="ltx_text" id="A2.SS8.p1.1.11.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib39" title="">2025</a><span class="ltx_text" id="A2.SS8.p1.1.12.3" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS8.p1.1.13" style="font-size:90%;">, </span><span class="ltx_text ltx_font_typewriter" id="A2.SS8.p1.1.14" style="font-size:90%;">Llama-3-8B</span><span class="ltx_text" id="A2.SS8.p1.1.15" style="font-size:90%;">, </span><span class="ltx_text ltx_font_typewriter" id="A2.SS8.p1.1.16" style="font-size:90%;">Llama-3-8B-Instruct</span><span class="ltx_text" id="A2.SS8.p1.1.17" style="font-size:90%;">, </span><span class="ltx_text ltx_font_typewriter" id="A2.SS8.p1.1.18" style="font-size:90%;">Llama-3.1-8B</span><span class="ltx_text" id="A2.SS8.p1.1.19" style="font-size:90%;">, </span><span class="ltx_text ltx_font_typewriter" id="A2.SS8.p1.1.20" style="font-size:90%;">Llama-3.1-8B-Instruct</span><span class="ltx_text" id="A2.SS8.p1.1.21" style="font-size:90%;">, </span><span class="ltx_text ltx_font_typewriter" id="A2.SS8.p1.1.22" style="font-size:90%;">Llama-3.2-3B</span><span class="ltx_text" id="A2.SS8.p1.1.23" style="font-size:90%;">, and </span><span class="ltx_text ltx_font_typewriter" id="A2.SS8.p1.1.24" style="font-size:90%;">Llama-3.2-3B-Instruct</span><span class="ltx_text" id="A2.SS8.p1.1.25" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_citep"><span class="ltx_text" id="A2.SS8.p1.1.26.1" style="font-size:90%;">[</span>Grattafiori et al.<span class="ltx_text" id="A2.SS8.p1.1.27.2.1.1" style="font-size:90%;">, </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20133v1#bib.bib13" title="">2024</a><span class="ltx_text" id="A2.SS8.p1.1.28.3" style="font-size:90%;">]</span></cite><span class="ltx_text" id="A2.SS8.p1.1.29" style="font-size:90%;">.</span></p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Additional Results</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1"><span class="ltx_text" id="A3.p1.1.1" style="font-size:90%;">We repeat the analysis of the best target layer for </span><span class="ltx_text ltx_font_smallcaps" id="A3.p1.1.2" style="font-size:90%;">AweDist</span><span class="ltx_text" id="A3.p1.1.3" style="font-size:90%;"> from </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" style="font-size:90%;" title="5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.2</span></a><span class="ltx_text" id="A3.p1.1.4" style="font-size:90%;"> in </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#A3.F3" style="font-size:90%;" title="Figure 3 ‣ Appendix C Additional Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Figure 3</span></a><span class="ltx_text" id="A3.p1.1.5" style="font-size:90%;"> for </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.6" style="font-size:90%;">Llama-3.1-8B-Instruct</span><span class="ltx_text" id="A3.p1.1.7" style="font-size:90%;"> instead of </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.8" style="font-size:90%;">Llama-3-8B-Instruct</span><span class="ltx_text" id="A3.p1.1.9" style="font-size:90%;">. Our analysis for </span><span class="ltx_text ltx_font_typewriter" id="A3.p1.1.10" style="font-size:90%;">Llama-3-8B-Instruct</span><span class="ltx_text" id="A3.p1.1.11" style="font-size:90%;"> in </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S5.SS2" style="font-size:90%;" title="5.2 In-Depth Analysis ‣ 5 Results ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 5.2</span></a><span class="ltx_text" id="A3.p1.1.12" style="font-size:90%;"> applies.</span></p>
</div>
<figure class="ltx_figure" id="A3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="279" id="A3.F3.g1" src="extracted/6480660/assets/performance_vs_layer_all_lrs_llama3-8b-instruct.png" width="419"/>
<figcaption class="ltx_caption ltx_centering" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Analysis of varying the target layer for the <span class="ltx_text ltx_font_smallcaps" id="A3.F3.7.1">AweDist</span> objective. We report the average performance on the biomedical benchmarks for <span class="ltx_text ltx_font_typewriter" id="A3.F3.8.2">Llama-3.1-8B-Instruct</span>.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix" style="font-size:90%;">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Use of LLMs</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1"><span class="ltx_text" id="A4.p1.1.1" style="font-size:90%;">LLMs were used for LLM-as-a-Judge style evaluation, as discussed in the paper.
LLMs were also used for formatting of </span><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20133v1#S3" style="font-size:90%;" title="3 Method: AweDist – Attention-aware Embedding Distillation ‣ AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings"><span class="ltx_text ltx_ref_tag">Section 3</span></a><span class="ltx_text" id="A4.p1.1.2" style="font-size:90%;"> (</span><span class="ltx_text ltx_font_typewriter" id="A4.p1.1.3" style="font-size:90%;">gpt-o4-mini-high</span><span class="ltx_text" id="A4.p1.1.4" style="font-size:90%;">), intermittently while writing the code for experimentation (VSCode Copilot autocomplete), as well as for the creation of scripts that aggregate the final results into tables and figures. All LLM-generated code has been checked for correctness.</span></p>
</div>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 26 15:30:42 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)</title>
<!--Generated on Fri May 23 18:20:32 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2505.20029v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S1" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S2" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S3" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset and Curation</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S3.SS0.SSS0.Px1" title="In 3 Dataset and Curation ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title">Brain dataset.</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S4" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S5" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.SS1" title="In 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>MLLM representations align well to human brain activity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.SS2" title="In 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Encoding performances of task-specific instructions from MLLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.SS3" title="In 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>Partitioning explained variance between task-specific instructions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S7" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion and Conclusion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S8" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Ethics Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A1" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Overview of Appendix Sections</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A2" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Visual functional localizers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A3" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Cross-subject brain predictivity</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A4" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Details of MLLMs with training details and their parameters</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Model generated outputs across instructions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A6" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Normalized brain alignment: Visual Functional Localizers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A7" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Brain Maps for Task-specific Instructions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A8" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Brain Maps for different MLLM Layers</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A9" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>ROI Analysis: Shared and Unique variance across task-specific instructions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A10" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">J </span>Normalized Brain Alignment across different image categories</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A11" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">K </span>Comparison of Instruction-tuned MLLMs, Non-Instruction-tuned MLLMs and text-based LLMs</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">L </span>Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A13" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">M </span>Image only / Instruction only input to the instruction-tuned MLLM</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A14" title="In Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">N </span>Limitations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Subba Reddy Oota<sup class="ltx_sup" id="id17.17.id1"><span class="ltx_text ltx_font_italic" id="id17.17.id1.1">1∗</span></sup>, Akshett Jindal<sup class="ltx_sup" id="id18.18.id2"><span class="ltx_text ltx_font_italic" id="id18.18.id2.1">2∗</span></sup>, Ishani Mondal<sup class="ltx_sup" id="id19.19.id3"><span class="ltx_text ltx_font_italic" id="id19.19.id3.1">3</span></sup>, Khushbu Pahwa<sup class="ltx_sup" id="id20.20.id4"><span class="ltx_text ltx_font_italic" id="id20.20.id4.1">4</span></sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id10.10.6">Satya Sai Srinath Namburi<sup class="ltx_sup" id="id10.10.6.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.6.1.1">5</span></sup>, Manish Shrivastava<sup class="ltx_sup" id="id10.10.6.2"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.6.2.1">2</span></sup>, Maneesh Singh<sup class="ltx_sup" id="id10.10.6.3"><span class="ltx_text ltx_font_medium" id="id10.10.6.3.1">6</span></sup>
<br class="ltx_break"/>Bapi S. Raju<sup class="ltx_sup" id="id10.10.6.4"><span class="ltx_text ltx_font_medium" id="id10.10.6.4.1">2</span></sup>, Manish Gupta<sup class="ltx_sup" id="id10.10.6.5"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id10.10.6.5.1">7</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id10.10.6.6"><span class="ltx_text ltx_font_medium" id="id10.10.6.6.1">1</span></sup></span>Technische Universität
Berlin, Germany, <sup class="ltx_sup" id="id21.21.id5">2</sup>IIIT Hyderabad, India, <sup class="ltx_sup" id="id22.22.id6">3</sup>Univ of Maryland, USA 
<br class="ltx_break"/><sup class="ltx_sup" id="id23.23.id7">4</sup>Rice Univ, USA, <sup class="ltx_sup" id="id24.24.id8">5</sup>Univ of Wisconsin - Madison, USA, <sup class="ltx_sup" id="id25.25.id9">6</sup>Spector Inc, USA 
<br class="ltx_break"/><sup class="ltx_sup" id="id26.26.id10">7</sup>Microsoft, India
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id27.27.id11">subba.reddy.oota@tu-berlin.de, gmanish@microsoft.com</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id28.id1">Transformer-based language models, though not explicitly trained to mimic brain recordings, have demonstrated surprising alignment with brain activity. Progress in these models—through increased size, instruction-tuning, and multimodality—has led to better representational alignment with neural data. Recently, a new class of instruction-tuned multimodal LLMs (MLLMs) have emerged, showing remarkable zero-shot capabilities in open-ended multimodal vision tasks.
However, it is unknown whether MLLMs, when prompted with natural instructions, lead to better brain alignment and effectively capture instruction-specific representations.
To address this, we first investigate brain alignment, i.e., measuring the degree of predictivity of neural visual activity using text output response embeddings from MLLMs as participants engage in watching natural scenes.
Experiments with 10 different instructions (like image captioning, visual question answering, etc.) show that MLLMs exhibit significantly better brain alignment than vision-only models and perform comparably to non-instruction-tuned multimodal models like CLIP.
We also find that while these MLLMs are effective at generating high-quality responses suitable to the task-specific instructions,
not all instructions are relevant for brain alignment.
Further, by varying instructions, we make the MLLMs encode instruction-specific visual concepts related to the input image. This analysis shows that MLLMs effectively capture count-related and recognition-related concepts, demonstrating strong alignment with brain activity.
Notably, the majority of the explained variance of the brain encoding models is shared between MLLM embeddings of image captioning and other instructions.
These results suggest that enhancing MLLMs’ ability to capture task-specific information could lead to better differentiation between various types of instructions, and thereby improving their precision in predicting brain responses. We make the code publicly available<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/subbareddy248/mllm_instruction_brain" title="">https://github.com/subbareddy248/mllm_instruction_brain</a></span></span></span>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Brain encoding aims at constructing neural brain activity recordings given an input stimulus. Prior studies in brain encoding have demonstrated that representations from multimodal models, which align multiple modalities (e.g., vision and language), achieve a higher degree of brain alignment for both image-based and text-based representations compared to vision-only models, particularly when using naturalistic image stimuli <cite class="ltx_cite ltx_citemacro_citep">(Doerig et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib10" title="">2022</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib46" title="">2022</a>; Oota et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib26" title="">2022b</a>; Popham et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib31" title="">2021</a>; Oota et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib29" title="">2024b</a>)</cite>. Specifically, these studies have shown that representations from multimodal models are better at predicting neural responses in the high-level visual cortex as compared to previous vision-only models like convolutional neural networks (CNNs) <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib46" title="">2022</a>; Popham et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib31" title="">2021</a>)</cite>. However, prior research investigating the effectiveness of multimodal models for brain alignment has primarily relied on image-caption pairs, leaving the full potential of multimodal models, especially when enhanced with large language models (LLM) and task-specific instructions, underexplored.
In this paper, we explore how effectively multimodal representations, obtained by prompting MLLMs with various natural instructions, align with visual processing brain regions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Several previous <em class="ltx_emph ltx_font_italic" id="S1.p2.1.1">unimodal</em> studies have found that Transformer models finetuned for specific tasks more closely align with brain processes during language comprehension <cite class="ltx_cite ltx_citemacro_citep">(Oota et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib25" title="">2022a</a>; Aw &amp; Toneva, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib3" title="">2023</a>; Sun &amp; Moens, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib37" title="">2023</a>; Oota et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib30" title="">2024c</a>)</cite>, speech processing <cite class="ltx_cite ltx_citemacro_citep">(Oota et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib27" title="">2023</a>; Tuckute et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib43" title="">2023</a>; Oota et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib28" title="">2024a</a>)</cite>, and visual processing <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib45" title="">2019</a>; Conwell et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib6" title="">2022</a>)</cite>, yielding better brain predictivity than pretrained models. However, these studies rely on separate task-specific models, which limits generalization, as each task-specific representations are aligned with the same human brain recordings. Furthermore, finetuning data needs to be obtained for each task, and a new model needs to be trained separately. Recently, instruction-tuning has become a widely adopted method for fine-tuning the same baseline large language model across multiple different natural language processing (NLP) tasks. This approach has been shown to outperform task-specific models <cite class="ltx_cite ltx_citemacro_citep">(Taori et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib40" title="">2023</a>; Touvron et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib42" title="">2023</a>; Jiang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib16" title="">2023</a>; Abdin et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib1" title="">2024</a>; Dubey et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib12" title="">2024</a>)</cite>, and demonstrates improved brain alignment compared to smaller language models <cite class="ltx_cite ltx_citemacro_citep">(Sun et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib38" title="">2023</a>; Sun &amp; Moens, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib37" title="">2023</a>; Loong Aw et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib23" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Building on these advances, researchers have extended instruction-tuning to multimodal LLMs (MLLMs) <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib48" title="">2023</a>; Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib7" title="">2023</a>; Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib21" title="">2024</a>)</cite>, enabling impressive unimodal and multimodal capabilities. These progressive improvements motivate us to explore the effectiveness of instruction-tuned MLLMs for brain encoding. By leveraging task-specific representations from a single MLLM, we aim to capture multiple aspects of an image beyond simple captioning, such as people, foreground and background elements, interactions between objects, environments, colors, food items, animals, and outdoor scenes. This leads to a critical question related to understanding of human-alignment of AI: <span class="ltx_text ltx_font_italic" id="S1.p3.1.1">Do these multimodal instruction-tuned models prompted using natural language instructions lead to better brain alignment and differentiate instruction-specific representations?</span> To address this, we investigate different ways of prompting MLLMs with various task-specific instructions.
Overall, this research utilizes various task-specific MLLM representations to develop encoding models based on fMRI responses within a multimodal model framework (see Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">1</span></a> for workflow).</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="160" id="S1.F1.g1" src="x1.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>Leveraging instruction-tuned multimodal LLMs for brain encoding with a diverse set of instructions. For the given image, we could obtain different multimodal representations using instructions that ask the model to (i) generate the caption of the image, (ii) identify whether people are present, or (iii) determine the primary colors dominant in the image. Using instruction-specific representations, we estimate the alignment using a simple linear function <math alttext="f" class="ltx_Math" display="inline" id="S1.F1.2.m1.1"><semantics id="S1.F1.2.m1.1b"><mi id="S1.F1.2.m1.1.1" xref="S1.F1.2.m1.1.1.cmml">f</mi><annotation-xml encoding="MathML-Content" id="S1.F1.2.m1.1c"><ci id="S1.F1.2.m1.1.1.cmml" xref="S1.F1.2.m1.1.1">𝑓</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.2.m1.1d">f</annotation><annotation encoding="application/x-llamapun" id="S1.F1.2.m1.1e">italic_f</annotation></semantics></math> (ridge regression) which map MLLM representations to brain recordings.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Using brain recordings of participants watching natural scenes images from the NSD dataset <cite class="ltx_cite ltx_citemacro_citep">(Allen et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib2" title="">2022</a>)</cite>, we investigate
several research questions. First, we explore the effectiveness of MLLM representations and compare their brain alignment with unimodal and multimodal models. For the purposes of this work, we focus on three MLLMs (InstructBLIP <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib7" title="">2023</a>)</cite>, mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib49" title="">2023</a>)</cite> and IDEFICS <cite class="ltx_cite ltx_citemacro_citep">(Laurençon et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib17" title="">2023</a>)</cite>), one image-based (ViT-H) and one multimodal model (CLIP). We probe these MLLMs using ten different instructions across six visual tasks. Specifically, we investigate which of these task-specific instructions result in better brain alignment. Second, do the instruction-specific representations from MLLMs differentiate the visual regions that process this information, thereby aligning with the mechanisms of human visual cognition? Third, do task-specific instructions from MLLMs account for visual concepts understanding, and which brain regions are responsible for processing different visual concepts?
Fourth, we use a variance partitioning approach to assess the unique and shared variance of each task-specific instruction to brain responses. This analysis provides insights into how different visual tasks complement or overlap in explaining brain activity, thereby enhancing our understanding of the functional organization of visual processing in the brain.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our analysis of instruction-tuned MLLMs and brain alignment reveals several key conclusions: (i) MLLMs demonstrate significantly better brain alignment than vision-only models and perform comparably to multimodal models such as CLIP. (ii) While all three MLLMs generate task-specific output tokens based on instructions, not all instructions contribute to higher brain alignment. Specifically, the image captioning instruction leads to stronger brain alignment in regions like the EBA (extrastriate body area), PPA (parahippocampal place area), and FFA (fusiform face area), whereas instructions related to image understanding result in higher alignment in early visual regions. (iii) Furthermore, while MLLMs capture several visual concepts, such as counts and recognition, for other concepts like color, positional understanding, and general scene understanding, MLLMs exhibit similar brain alignment patterns.
(iv) By employing a variance partitioning approach, we find that most of the variance is shared across instructions, with a high overlap between Image Captioning (IC) and other prompts but lower overlap with image understanding and scene recognition. This suggests that MLLMs could improve in differentiating between various types of instructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">Overall, our work is the first to propose the use of instruction-tuned MLLMs and to demonstrate the differences in task-specific representations within MLLMs, along with the reasons behind these differences in relation to brain alignment. We make the code publicly available<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#footnote1" title="footnote 1 ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Brain encoding using multimodal models.</span>
The human brain perceives the environment using information from multiple modalities <cite class="ltx_cite ltx_citemacro_citep">(Gauthier et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib13" title="">2003</a>)</cite>. Therefore, examining the alignment between language and visual representations in the brain by training encoding models on fMRI responses, while extracting joint representations from multimodal models, can offer insights into how our brain processes multimodal information.
For instance, it has been shown in <cite class="ltx_cite ltx_citemacro_citet">Doerig et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib10" title="">2022</a>); Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib46" title="">2022</a>); Oota et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib26" title="">2022b</a>); Popham et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib31" title="">2021</a>)</cite> that multimodal models like CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib33" title="">2021b</a>)</cite> better predict neural responses in the high-level visual cortex as compared to previous vision-only models.
Additionally, <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib39" title="">2024</a>)</cite> demonstrate the use of multimodal models in a cross-modal experiment to assess how well the language encoding models can predict movie-fMRI responses and how well the vision encoding models can predict narrative story-fMRI. <cite class="ltx_cite ltx_citemacro_citet">Nakagi et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib24" title="">2024</a>)</cite> analyzed fMRI related to video content viewing and found distinct brain regions associated with different semantic levels, highlighting the significance of modeling various levels of semantic content simultaneously.
However, these studies have primarily focused on multimodal models aligned in embedding space when text captions were provided as input, leaving the new class of instruction-tuned MLLMs-which utilize task-specific natural language instructions—still unexplored. Unlike previous work, we are the first to study multimodal instruction-tuned models with language-guided instructions and to perform comprehensive brain alignment analysis while subjects are engaged in viewing passive images.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Task-based brain alignment.</span>
Our work is also closely related to that of <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib45" title="">2019</a>); Oota et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib25" title="">2022a</a>); Aw &amp; Toneva (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib3" title="">2023</a>); Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib38" title="">2023</a>)</cite> and <cite class="ltx_cite ltx_citemacro_citet">Aw et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib4" title="">2023</a>)</cite>, who propose using task-specific model representations to study the contribution of individual tasks to brain alignment. <cite class="ltx_cite ltx_citemacro_citet">Wang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib45" title="">2019</a>)</cite> investigated 21 computer vision tasks to explore which vision tasks are more aligned with the brain while subjects engaged in viewing passive images. Similarly, <cite class="ltx_cite ltx_citemacro_citet">Oota et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib25" title="">2022a</a>)</cite> and  <cite class="ltx_cite ltx_citemacro_citet">Sun et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib38" title="">2023</a>)</cite> explored 10 GLUE NLP tasks to study which NLP tasks are more brain-aligned during reading and listening to stories. <cite class="ltx_cite ltx_citemacro_citet">Aw &amp; Toneva (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib3" title="">2023</a>)</cite> further extended the comparison by evaluating pretrained models that were trained either on web data or BookSum stories to determine whether BookSum models provide better character-specific information in brain alignment tasks.
More recent work by <cite class="ltx_cite ltx_citemacro_citet">Aw et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib4" title="">2023</a>)</cite> uses instruction-tuned language models to investigate the effect of natural language instruction model representations on brain alignment across layers for language comprehension. We complement these works by examining the impact of a wide range of multimodal instruction-tuned models on brain alignment and by studying the effect of task-specific, language-guided instructions from MLLMs on their alignment with brain activity.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset and Curation</h2>
<section class="ltx_paragraph" id="S3.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Brain dataset.</h4>
<div class="ltx_para ltx_noindent" id="S3.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p1.5">We use the Natural Scenes Dataset (NSD) introduced by <cite class="ltx_cite ltx_citemacro_citet">Allen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib2" title="">2022</a>)</cite>, which contains high-quality fMRI readings of 8 participants watching images from the COCO dataset <cite class="ltx_cite ltx_citemacro_citep">(Lin et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib20" title="">2014</a>)</cite>. We analyzed brain data for 4 participants (who completed all the sessions) where each participant was presented with <math alttext="10,000" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.1.m1.2"><semantics id="S3.SS0.SSS0.Px1.p1.1.m1.2a"><mrow id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.1.cmml"><mn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">10</mn><mo id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2.1" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px1.p1.1.m1.2.2" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.1.m1.2b"><list id="S3.SS0.SSS0.Px1.p1.1.m1.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.3.2"><cn id="S3.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.1.m1.1.1">10</cn><cn id="S3.SS0.SSS0.Px1.p1.1.m1.2.2.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.1.m1.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.1.m1.2c">10,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p1.1.m1.2d">10 , 000</annotation></semantics></math> images in total (<math alttext="1,000" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.2.m2.2"><semantics id="S3.SS0.SSS0.Px1.p1.2.m2.2a"><mrow id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1.cmml"><mn id="S3.SS0.SSS0.Px1.p1.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml">1</mn><mo id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2.1" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px1.p1.2.m2.2.2" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.2.m2.2b"><list id="S3.SS0.SSS0.Px1.p1.2.m2.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.3.2"><cn id="S3.SS0.SSS0.Px1.p1.2.m2.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.2.m2.1.1">1</cn><cn id="S3.SS0.SSS0.Px1.p1.2.m2.2.2.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.2.m2.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.2.m2.2c">1,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p1.2.m2.2d">1 , 000</annotation></semantics></math> common for all participants and <math alttext="9,000" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.3.m3.2"><semantics id="S3.SS0.SSS0.Px1.p1.3.m3.2a"><mrow id="S3.SS0.SSS0.Px1.p1.3.m3.2.3.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.3.1.cmml"><mn id="S3.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">9</mn><mo id="S3.SS0.SSS0.Px1.p1.3.m3.2.3.2.1" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px1.p1.3.m3.2.2" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.3.m3.2b"><list id="S3.SS0.SSS0.Px1.p1.3.m3.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.3.2"><cn id="S3.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.3.m3.1.1">9</cn><cn id="S3.SS0.SSS0.Px1.p1.3.m3.2.2.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.3.m3.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.3.m3.2c">9,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p1.3.m3.2d">9 , 000</annotation></semantics></math> unique for a participant). Each of the <math alttext="10,000" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.4.m4.2"><semantics id="S3.SS0.SSS0.Px1.p1.4.m4.2a"><mrow id="S3.SS0.SSS0.Px1.p1.4.m4.2.3.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml"><mn id="S3.SS0.SSS0.Px1.p1.4.m4.1.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml">10</mn><mo id="S3.SS0.SSS0.Px1.p1.4.m4.2.3.2.1" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px1.p1.4.m4.2.2" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.4.m4.2b"><list id="S3.SS0.SSS0.Px1.p1.4.m4.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.3.2"><cn id="S3.SS0.SSS0.Px1.p1.4.m4.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.4.m4.1.1">10</cn><cn id="S3.SS0.SSS0.Px1.p1.4.m4.2.2.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.4.m4.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.4.m4.2c">10,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p1.4.m4.2d">10 , 000</annotation></semantics></math> images were repeated three times in random order, giving a total of <math alttext="30,000" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p1.5.m5.2"><semantics id="S3.SS0.SSS0.Px1.p1.5.m5.2a"><mrow id="S3.SS0.SSS0.Px1.p1.5.m5.2.3.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.3.1.cmml"><mn id="S3.SS0.SSS0.Px1.p1.5.m5.1.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1.cmml">30</mn><mo id="S3.SS0.SSS0.Px1.p1.5.m5.2.3.2.1" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.3.1.cmml">,</mo><mn id="S3.SS0.SSS0.Px1.p1.5.m5.2.2" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2.cmml">000</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p1.5.m5.2b"><list id="S3.SS0.SSS0.Px1.p1.5.m5.2.3.1.cmml" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.3.2"><cn id="S3.SS0.SSS0.Px1.p1.5.m5.1.1.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.5.m5.1.1">30</cn><cn id="S3.SS0.SSS0.Px1.p1.5.m5.2.2.cmml" type="integer" xref="S3.SS0.SSS0.Px1.p1.5.m5.2.2">000</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p1.5.m5.2c">30,000</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p1.5.m5.2d">30 , 000</annotation></semantics></math> readings per participant. Similar to prior studies <cite class="ltx_cite ltx_citemacro_citep">(Scotti et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib35" title="">2024a</a>)</cite>,
we compute the mean of the fMRI readings for all the three occurrences and obtain a one-to-one mapping between the image and the corresponding fMRI reading. The images belong to 12 categories: animals, accessories, appliances, electronics, food, furniture, indoor, kitchen, outdoor, person, sports, and vehicles.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS0.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p2.1">The dataset is already pre-processed and a brain mask for each subject is also provided to obtain the activation voxels. Similar to prior studies <cite class="ltx_cite ltx_citemacro_citep">(Scotti et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib35" title="">2024a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib36" title="">b</a>)</cite>, we use preprocessed flattened fMRI voxels in 1.8-mm native volume space corresponding to the “nsdgeneral” brain region, defined by the NSD authors as the subset of voxels in posterior cortex most responsive to the visual stimuli presented (between 13,000 to 16,000 voxels per participant).
We perform the ROI (region of interest) analysis for the NSD dataset considering the following five visual processing regions:
body-selective regions (floc-bodies), face-selective regions (floc-faces) and scene-selective regions (floc-places), word-selective regions (floc-words), and pRF-Visual ROIs
(also called Retinotopic Early Visual Cortex) <cite class="ltx_cite ltx_citemacro_citep">(Allen et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib2" title="">2022</a>)</cite>. Note that floc-bodies, floc-faces, floc-places and floc-words are high-level visual areas while pRF-Visual ROIs are early visual areas.
We list the detailed sub-ROIs of these ROIs in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A2" title="Appendix B Visual functional localizers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.SS0.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS0.SSS0.Px1.p3.3"><span class="ltx_text ltx_font_bold" id="S3.SS0.SSS0.Px1.p3.3.1">Estimating dataset cross-subject prediction accuracy.</span>
To account for the intrinsic noise in biological measurements, we adapt <cite class="ltx_cite ltx_citemacro_citet">Schrimpf et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib34" title="">2021</a>)</cite>’s method to estimate the cross-subject prediction accuracy for a model’s performance. By subsampling fMRI datasets from 4 participants, we generate all possible combinations of <math alttext="s" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p3.1.m1.1"><semantics id="S3.SS0.SSS0.Px1.p3.1.m1.1a"><mi id="S3.SS0.SSS0.Px1.p3.1.m1.1.1" xref="S3.SS0.SSS0.Px1.p3.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.1.m1.1b"><ci id="S3.SS0.SSS0.Px1.p3.1.m1.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p3.1.m1.1d">italic_s</annotation></semantics></math> participants (<math alttext="s" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p3.2.m2.1"><semantics id="S3.SS0.SSS0.Px1.p3.2.m2.1a"><mi id="S3.SS0.SSS0.Px1.p3.2.m2.1.1" xref="S3.SS0.SSS0.Px1.p3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.2.m2.1b"><ci id="S3.SS0.SSS0.Px1.p3.2.m2.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p3.2.m2.1d">italic_s</annotation></semantics></math> <math alttext="\in" class="ltx_Math" display="inline" id="S3.SS0.SSS0.Px1.p3.3.m3.1"><semantics id="S3.SS0.SSS0.Px1.p3.3.m3.1a"><mo id="S3.SS0.SSS0.Px1.p3.3.m3.1.1" xref="S3.SS0.SSS0.Px1.p3.3.m3.1.1.cmml">∈</mo><annotation-xml encoding="MathML-Content" id="S3.SS0.SSS0.Px1.p3.3.m3.1b"><in id="S3.SS0.SSS0.Px1.p3.3.m3.1.1.cmml" xref="S3.SS0.SSS0.Px1.p3.3.m3.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S3.SS0.SSS0.Px1.p3.3.m3.1c">\in</annotation><annotation encoding="application/x-llamapun" id="S3.SS0.SSS0.Px1.p3.3.m3.1d">∈</annotation></semantics></math> [2,4]) watching natural scenes, and use a voxel-wise encoding model (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S5" title="5 Experimental setup ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">5</span></a>) to predict one participant’s response from others.
Note that the estimated cross-subject prediction accuracy is based on the assumption of a perfect model, which might differ from real-world scenarios, yet offers valuable insights into model’s performance.
We present the average cross-subject prediction accuracy across voxels for the <em class="ltx_emph ltx_font_italic" id="S3.SS0.SSS0.Px1.p3.3.2">NSD</em> dataset in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A3" title="Appendix C Cross-subject brain predictivity ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">C</span></a> Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A3.F8" title="Figure 8 ‣ Appendix C Cross-subject brain predictivity ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">8</span></a>. The figure suggests that the cross-subject prediction accuracy is consistent across subjects, indicating that all subjects share a similar amount of explainable variance. Cross-subject prediction accuracy for each participant brainmaps are reported in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A3" title="Appendix C Cross-subject brain predictivity ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">C</span></a> in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A3.F9" title="Figure 9 ‣ Appendix C Cross-subject brain predictivity ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">9</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold" id="S4.p1.1.1">Instruction-tuned Multimodal large language models.</span>
To investigate whether multimodal instruction-tuned models prompted using natural language-guided instructions perfectly align with the way humans process visual information in the brain, we consider three popular modern instruction-tuned multimodal models publicly available on Huggingface <cite class="ltx_cite ltx_citemacro_citep">(Wolf et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib47" title="">2020</a>)</cite>: InstructBLIP <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib7" title="">2023</a>)</cite>, mPLUG-Owl <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib49" title="">2023</a>)</cite>
and IDEFICS <cite class="ltx_cite ltx_citemacro_citep">(Laurençon et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib17" title="">2023</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">InstructBLIP</span> <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib7" title="">2023</a>)</cite> is a vision-language instruction-tuned model built upon the pretrained BLIP-2 model <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib19" title="">2023</a>)</cite>.
<span class="ltx_text ltx_font_bold" id="S4.p2.1.2">mPLUG-Owl</span> <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib49" title="">2023</a>)</cite> is an MLLM designed to perceive and integrate multiple modalities (visual and language) while considering visual context and information to generate corresponding outputs.
<span class="ltx_text ltx_font_bold" id="S4.p2.1.3">IDEFICS</span> <cite class="ltx_cite ltx_citemacro_citep">(Laurençon et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib17" title="">2023</a>)</cite> is an MLLM based on Flamingo <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib50" title="">2024</a>)</cite>, which accepts arbitrary sequences of image and text inputs and generates text tokens.
All MLLMs consist of 32 layers and produce 4096-dimensional representations at each layer.
We provide more details, including model-parameters and training dataset details in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A4.T2" title="Table 2 ‣ Appendix D Details of MLLMs with training details and their parameters ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">2</span></a> in
Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A4" title="Appendix D Details of MLLMs with training details and their parameters ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Natural instructions.</span>
To ensure the diversity of task-specific instructions while considering image as input, we consider 10 instructions, and extract the language-guided representations from multimodal instruction-tuned models. As shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S4.T1" title="Table 1 ‣ 4 Methodology ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">1</span></a>, the 10 natural instructions cover 6 task categories, including image captioning, visual question answering, visual relationships, commonsense reasoning, image understanding and scene recognition. These set of 10 instructions are inspired from the list of <math alttext="62" class="ltx_Math" display="inline" id="S4.p3.1.m1.1"><semantics id="S4.p3.1.m1.1a"><mn id="S4.p3.1.m1.1.1" xref="S4.p3.1.m1.1.1.cmml">62</mn><annotation-xml encoding="MathML-Content" id="S4.p3.1.m1.1b"><cn id="S4.p3.1.m1.1.1.cmml" type="integer" xref="S4.p3.1.m1.1.1">62</cn></annotation-xml><annotation encoding="application/x-tex" id="S4.p3.1.m1.1c">62</annotation><annotation encoding="application/x-llamapun" id="S4.p3.1.m1.1d">62</annotation></semantics></math> multimodal tasks defined in MultiInstruct <cite class="ltx_cite ltx_citemacro_citep">(Xu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib48" title="">2023</a>)</cite>. We borrowed those tasks which are generally applicable to any image regardless of the contents in the image.
We provide a sample of generated outputs for the three MLLMs across 10 instructions in Tables <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T3" title="Table 3 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T4" title="Table 4 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">4</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T5" title="Table 5 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">5</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5" title="Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 1: </span>Instructions for various multimodal tasks (ordered by complexity, from least to most complex.)</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.1.1">Task</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="S4.T1.1.1.1.2.1">Description</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.2.1.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.2.1.1.1">Image Understanding</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">IU1: Describe the most dominant color in the image</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">IU2: List any food items visible.</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.4.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">IU3: How many animals are there in the image?</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.5.4">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.5.4.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="S4.T1.1.5.4.1.1">Visual Question Answering</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.5.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">VQ1: What is in this image?</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.6.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">VQ2: Are there any people in this image? If yes, describe them.</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">VQ3: What is the foreground of the image? What is in the background?</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">Image Captioning</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">IC: Generate some text to describe the image</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.9.8.1" style="padding-left:2.0pt;padding-right:2.0pt;">Scene Recognition</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.9.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">SR: Highlight the area that shows a natural outdoor scene.</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.10.9">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.10.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">Commonsense Reasoning</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.1.10.9.2" style="padding-left:2.0pt;padding-right:2.0pt;">CR: What type of environment is shown in the image?</td>
</tr>
<tr class="ltx_tr" id="S4.T1.1.11.10">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.1.11.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">Visual Relationship</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="S4.T1.1.11.10.2" style="padding-left:2.0pt;padding-right:2.0pt;">VR: What kind of interaction is happening between the animate and inanimate objects here?</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Extraction of features from instruction-tuned multimodal models.</span>
To extract instruction-specific representations from multimodal instruction-tuned models for the brain encoding task, we input an image and task instruction to obtain the embeddings for the language-guided instruction. We perform zero-shot inference on these models.
We use the <span class="ltx_text ltx_font_italic" id="S4.p4.1.2">model.generate</span> option because the hidden states are influenced by both the input tokens and the generated tokens, making them dependent on the generation context. The hidden states are dynamic and continuously evolve until the model generates the final token, which involves multiple forward passes in the process.
For all multimodal instruction-tuned models, we use the pretrained Transformer weights, which generate hidden state representations at each layer. We then average these hidden state representations of the output generated tokens to obtain the final embedding for each image with respect to the task instruction. For uniformity of feature dimensions, we applied PCA to reduce the dimensions to 1024.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold" id="S4.p5.1.1">Unimodal and multimodal models.</span>
As a baseline for comparison, we also included a unimodal model ViT-H <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib11" title="">2020</a>)</cite> and the multimodal model CLIP <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib32" title="">2021a</a>)</cite>, which is an image-text alignment model, in our experiments. The pretrained ViT-H model outputs image representations from different layers, providing a 1024-dimensional feature vector across 32 encoder layers. For the CLIP model, we input both image and ground truth caption pairs and extracted 1024-dimensional representations from the CLIP-Text model.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental setup</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.6"><span class="ltx_text ltx_font_bold" id="S5.p1.6.1">Voxelwise encoding model.</span>

We estimate the brain alignment of multimodal and unimodal models of a image stimulus via training standard voxel-wise encoding models <cite class="ltx_cite ltx_citemacro_citep">(Deniz et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib9" title="">2019</a>; Toneva &amp; Wehbe, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib41" title="">2019</a>; Schrimpf et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib34" title="">2021</a>)</cite>.
We train bootstrap ridge regression based voxel-wise encoding models <cite class="ltx_cite ltx_citemacro_citep">(Deniz et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib9" title="">2019</a>)</cite> to predict the fMRI brain activity associated with the stimulus representations obtained from the multimodal instruction-tuned models as well as other models. Formally, for each subject, we encode the stimuli as <math alttext="X_{i}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mrow id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml"><msub id="S5.p1.1.m1.1.1.2" xref="S5.p1.1.m1.1.1.2.cmml"><mi id="S5.p1.1.m1.1.1.2.2" xref="S5.p1.1.m1.1.1.2.2.cmml">X</mi><mi id="S5.p1.1.m1.1.1.2.3" xref="S5.p1.1.m1.1.1.2.3.cmml">i</mi></msub><mo id="S5.p1.1.m1.1.1.1" xref="S5.p1.1.m1.1.1.1.cmml">∈</mo><msup id="S5.p1.1.m1.1.1.3" xref="S5.p1.1.m1.1.1.3.cmml"><mi id="S5.p1.1.m1.1.1.3.2" xref="S5.p1.1.m1.1.1.3.2.cmml">ℝ</mi><mi id="S5.p1.1.m1.1.1.3.3" xref="S5.p1.1.m1.1.1.3.3.cmml">D</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><apply id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1"><in id="S5.p1.1.m1.1.1.1.cmml" xref="S5.p1.1.m1.1.1.1"></in><apply id="S5.p1.1.m1.1.1.2.cmml" xref="S5.p1.1.m1.1.1.2"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.2.1.cmml" xref="S5.p1.1.m1.1.1.2">subscript</csymbol><ci id="S5.p1.1.m1.1.1.2.2.cmml" xref="S5.p1.1.m1.1.1.2.2">𝑋</ci><ci id="S5.p1.1.m1.1.1.2.3.cmml" xref="S5.p1.1.m1.1.1.2.3">𝑖</ci></apply><apply id="S5.p1.1.m1.1.1.3.cmml" xref="S5.p1.1.m1.1.1.3"><csymbol cd="ambiguous" id="S5.p1.1.m1.1.1.3.1.cmml" xref="S5.p1.1.m1.1.1.3">superscript</csymbol><ci id="S5.p1.1.m1.1.1.3.2.cmml" xref="S5.p1.1.m1.1.1.3.2">ℝ</ci><ci id="S5.p1.1.m1.1.1.3.3.cmml" xref="S5.p1.1.m1.1.1.3.3">𝐷</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">X_{i}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">italic_X start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> and brain region voxels <math alttext="Y_{i}\in\mathbb{R}^{V}" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mrow id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml"><msub id="S5.p1.2.m2.1.1.2" xref="S5.p1.2.m2.1.1.2.cmml"><mi id="S5.p1.2.m2.1.1.2.2" xref="S5.p1.2.m2.1.1.2.2.cmml">Y</mi><mi id="S5.p1.2.m2.1.1.2.3" xref="S5.p1.2.m2.1.1.2.3.cmml">i</mi></msub><mo id="S5.p1.2.m2.1.1.1" xref="S5.p1.2.m2.1.1.1.cmml">∈</mo><msup id="S5.p1.2.m2.1.1.3" xref="S5.p1.2.m2.1.1.3.cmml"><mi id="S5.p1.2.m2.1.1.3.2" xref="S5.p1.2.m2.1.1.3.2.cmml">ℝ</mi><mi id="S5.p1.2.m2.1.1.3.3" xref="S5.p1.2.m2.1.1.3.3.cmml">V</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><apply id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1"><in id="S5.p1.2.m2.1.1.1.cmml" xref="S5.p1.2.m2.1.1.1"></in><apply id="S5.p1.2.m2.1.1.2.cmml" xref="S5.p1.2.m2.1.1.2"><csymbol cd="ambiguous" id="S5.p1.2.m2.1.1.2.1.cmml" xref="S5.p1.2.m2.1.1.2">subscript</csymbol><ci id="S5.p1.2.m2.1.1.2.2.cmml" xref="S5.p1.2.m2.1.1.2.2">𝑌</ci><ci id="S5.p1.2.m2.1.1.2.3.cmml" xref="S5.p1.2.m2.1.1.2.3">𝑖</ci></apply><apply id="S5.p1.2.m2.1.1.3.cmml" xref="S5.p1.2.m2.1.1.3"><csymbol cd="ambiguous" id="S5.p1.2.m2.1.1.3.1.cmml" xref="S5.p1.2.m2.1.1.3">superscript</csymbol><ci id="S5.p1.2.m2.1.1.3.2.cmml" xref="S5.p1.2.m2.1.1.3.2">ℝ</ci><ci id="S5.p1.2.m2.1.1.3.3.cmml" xref="S5.p1.2.m2.1.1.3.3">𝑉</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">Y_{i}\in\mathbb{R}^{V}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ∈ blackboard_R start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="D" class="ltx_Math" display="inline" id="S5.p1.3.m3.1"><semantics id="S5.p1.3.m3.1a"><mi id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><ci id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">D</annotation><annotation encoding="application/x-llamapun" id="S5.p1.3.m3.1d">italic_D</annotation></semantics></math> denotes the dimension of the image representations, and <math alttext="V" class="ltx_Math" display="inline" id="S5.p1.4.m4.1"><semantics id="S5.p1.4.m4.1a"><mi id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><ci id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1">𝑉</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">V</annotation><annotation encoding="application/x-llamapun" id="S5.p1.4.m4.1d">italic_V</annotation></semantics></math> denotes the number of voxels. Overall, with <math alttext="N" class="ltx_Math" display="inline" id="S5.p1.5.m5.1"><semantics id="S5.p1.5.m5.1a"><mi id="S5.p1.5.m5.1.1" xref="S5.p1.5.m5.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p1.5.m5.1b"><ci id="S5.p1.5.m5.1.1.cmml" xref="S5.p1.5.m5.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.5.m5.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.p1.5.m5.1d">italic_N</annotation></semantics></math> such training images, we obtain <math alttext="N" class="ltx_Math" display="inline" id="S5.p1.6.m6.1"><semantics id="S5.p1.6.m6.1a"><mi id="S5.p1.6.m6.1.1" xref="S5.p1.6.m6.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p1.6.m6.1b"><ci id="S5.p1.6.m6.1.1.cmml" xref="S5.p1.6.m6.1.1">𝑁</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.6.m6.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.p1.6.m6.1d">italic_N</annotation></semantics></math> training examples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Train-Test dataset split.</span>
We built an encoding model for each subject as follows: We used all data samples from 9,000 natural images (unique to each subject) for training and tested generalization on samples from the test set of 1,000 images (common across all subjects). Overall, we created per-subject, per-voxel encoding models. Model details and hyper-parameter settings are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A4" title="Appendix D Details of MLLMs with training details and their parameters ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">D</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.7"><span class="ltx_text ltx_font_bold" id="S5.p3.7.1">Evaluation metrics.</span>
We evaluate our models using Pearson Correlation (PC) which is a standard metric for evaluating brain alignment <cite class="ltx_cite ltx_citemacro_citep">(Jain &amp; Huth, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib15" title="">2018</a>; Schrimpf et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib34" title="">2021</a>; Goldstein et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib14" title="">2022</a>)</cite>. Let N<sub class="ltx_sub" id="S5.p3.7.2"><span class="ltx_text ltx_font_italic" id="S5.p3.7.2.1">te</span></sub> be the number of images in the test set. Let <math alttext="Y=\{Y_{i}\}_{i=1}^{N_{te}}" class="ltx_Math" display="inline" id="S5.p3.2.m2.1"><semantics id="S5.p3.2.m2.1a"><mrow id="S5.p3.2.m2.1.1" xref="S5.p3.2.m2.1.1.cmml"><mi id="S5.p3.2.m2.1.1.3" xref="S5.p3.2.m2.1.1.3.cmml">Y</mi><mo id="S5.p3.2.m2.1.1.2" xref="S5.p3.2.m2.1.1.2.cmml">=</mo><msubsup id="S5.p3.2.m2.1.1.1" xref="S5.p3.2.m2.1.1.1.cmml"><mrow id="S5.p3.2.m2.1.1.1.1.1.1" xref="S5.p3.2.m2.1.1.1.1.1.2.cmml"><mo id="S5.p3.2.m2.1.1.1.1.1.1.2" stretchy="false" xref="S5.p3.2.m2.1.1.1.1.1.2.cmml">{</mo><msub id="S5.p3.2.m2.1.1.1.1.1.1.1" xref="S5.p3.2.m2.1.1.1.1.1.1.1.cmml"><mi id="S5.p3.2.m2.1.1.1.1.1.1.1.2" xref="S5.p3.2.m2.1.1.1.1.1.1.1.2.cmml">Y</mi><mi id="S5.p3.2.m2.1.1.1.1.1.1.1.3" xref="S5.p3.2.m2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.p3.2.m2.1.1.1.1.1.1.3" stretchy="false" xref="S5.p3.2.m2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S5.p3.2.m2.1.1.1.1.3" xref="S5.p3.2.m2.1.1.1.1.3.cmml"><mi id="S5.p3.2.m2.1.1.1.1.3.2" xref="S5.p3.2.m2.1.1.1.1.3.2.cmml">i</mi><mo id="S5.p3.2.m2.1.1.1.1.3.1" xref="S5.p3.2.m2.1.1.1.1.3.1.cmml">=</mo><mn id="S5.p3.2.m2.1.1.1.1.3.3" xref="S5.p3.2.m2.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S5.p3.2.m2.1.1.1.3" xref="S5.p3.2.m2.1.1.1.3.cmml"><mi id="S5.p3.2.m2.1.1.1.3.2" xref="S5.p3.2.m2.1.1.1.3.2.cmml">N</mi><mrow id="S5.p3.2.m2.1.1.1.3.3" xref="S5.p3.2.m2.1.1.1.3.3.cmml"><mi id="S5.p3.2.m2.1.1.1.3.3.2" xref="S5.p3.2.m2.1.1.1.3.3.2.cmml">t</mi><mo id="S5.p3.2.m2.1.1.1.3.3.1" xref="S5.p3.2.m2.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.p3.2.m2.1.1.1.3.3.3" xref="S5.p3.2.m2.1.1.1.3.3.3.cmml">e</mi></mrow></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.2.m2.1b"><apply id="S5.p3.2.m2.1.1.cmml" xref="S5.p3.2.m2.1.1"><eq id="S5.p3.2.m2.1.1.2.cmml" xref="S5.p3.2.m2.1.1.2"></eq><ci id="S5.p3.2.m2.1.1.3.cmml" xref="S5.p3.2.m2.1.1.3">𝑌</ci><apply id="S5.p3.2.m2.1.1.1.cmml" xref="S5.p3.2.m2.1.1.1"><csymbol cd="ambiguous" id="S5.p3.2.m2.1.1.1.2.cmml" xref="S5.p3.2.m2.1.1.1">superscript</csymbol><apply id="S5.p3.2.m2.1.1.1.1.cmml" xref="S5.p3.2.m2.1.1.1"><csymbol cd="ambiguous" id="S5.p3.2.m2.1.1.1.1.2.cmml" xref="S5.p3.2.m2.1.1.1">subscript</csymbol><set id="S5.p3.2.m2.1.1.1.1.1.2.cmml" xref="S5.p3.2.m2.1.1.1.1.1.1"><apply id="S5.p3.2.m2.1.1.1.1.1.1.1.cmml" xref="S5.p3.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p3.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S5.p3.2.m2.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.p3.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S5.p3.2.m2.1.1.1.1.1.1.1.2">𝑌</ci><ci id="S5.p3.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S5.p3.2.m2.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S5.p3.2.m2.1.1.1.1.3.cmml" xref="S5.p3.2.m2.1.1.1.1.3"><eq id="S5.p3.2.m2.1.1.1.1.3.1.cmml" xref="S5.p3.2.m2.1.1.1.1.3.1"></eq><ci id="S5.p3.2.m2.1.1.1.1.3.2.cmml" xref="S5.p3.2.m2.1.1.1.1.3.2">𝑖</ci><cn id="S5.p3.2.m2.1.1.1.1.3.3.cmml" type="integer" xref="S5.p3.2.m2.1.1.1.1.3.3">1</cn></apply></apply><apply id="S5.p3.2.m2.1.1.1.3.cmml" xref="S5.p3.2.m2.1.1.1.3"><csymbol cd="ambiguous" id="S5.p3.2.m2.1.1.1.3.1.cmml" xref="S5.p3.2.m2.1.1.1.3">subscript</csymbol><ci id="S5.p3.2.m2.1.1.1.3.2.cmml" xref="S5.p3.2.m2.1.1.1.3.2">𝑁</ci><apply id="S5.p3.2.m2.1.1.1.3.3.cmml" xref="S5.p3.2.m2.1.1.1.3.3"><times id="S5.p3.2.m2.1.1.1.3.3.1.cmml" xref="S5.p3.2.m2.1.1.1.3.3.1"></times><ci id="S5.p3.2.m2.1.1.1.3.3.2.cmml" xref="S5.p3.2.m2.1.1.1.3.3.2">𝑡</ci><ci id="S5.p3.2.m2.1.1.1.3.3.3.cmml" xref="S5.p3.2.m2.1.1.1.3.3.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.2.m2.1c">Y=\{Y_{i}\}_{i=1}^{N_{te}}</annotation><annotation encoding="application/x-llamapun" id="S5.p3.2.m2.1d">italic_Y = { italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\hat{Y}=\{\hat{Y}_{i}\}_{i=1}^{N_{te}}" class="ltx_Math" display="inline" id="S5.p3.3.m3.1"><semantics id="S5.p3.3.m3.1a"><mrow id="S5.p3.3.m3.1.1" xref="S5.p3.3.m3.1.1.cmml"><mover accent="true" id="S5.p3.3.m3.1.1.3" xref="S5.p3.3.m3.1.1.3.cmml"><mi id="S5.p3.3.m3.1.1.3.2" xref="S5.p3.3.m3.1.1.3.2.cmml">Y</mi><mo id="S5.p3.3.m3.1.1.3.1" xref="S5.p3.3.m3.1.1.3.1.cmml">^</mo></mover><mo id="S5.p3.3.m3.1.1.2" xref="S5.p3.3.m3.1.1.2.cmml">=</mo><msubsup id="S5.p3.3.m3.1.1.1" xref="S5.p3.3.m3.1.1.1.cmml"><mrow id="S5.p3.3.m3.1.1.1.1.1.1" xref="S5.p3.3.m3.1.1.1.1.1.2.cmml"><mo id="S5.p3.3.m3.1.1.1.1.1.1.2" stretchy="false" xref="S5.p3.3.m3.1.1.1.1.1.2.cmml">{</mo><msub id="S5.p3.3.m3.1.1.1.1.1.1.1" xref="S5.p3.3.m3.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S5.p3.3.m3.1.1.1.1.1.1.1.2" xref="S5.p3.3.m3.1.1.1.1.1.1.1.2.cmml"><mi id="S5.p3.3.m3.1.1.1.1.1.1.1.2.2" xref="S5.p3.3.m3.1.1.1.1.1.1.1.2.2.cmml">Y</mi><mo id="S5.p3.3.m3.1.1.1.1.1.1.1.2.1" xref="S5.p3.3.m3.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S5.p3.3.m3.1.1.1.1.1.1.1.3" xref="S5.p3.3.m3.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.p3.3.m3.1.1.1.1.1.1.3" stretchy="false" xref="S5.p3.3.m3.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S5.p3.3.m3.1.1.1.1.3" xref="S5.p3.3.m3.1.1.1.1.3.cmml"><mi id="S5.p3.3.m3.1.1.1.1.3.2" xref="S5.p3.3.m3.1.1.1.1.3.2.cmml">i</mi><mo id="S5.p3.3.m3.1.1.1.1.3.1" xref="S5.p3.3.m3.1.1.1.1.3.1.cmml">=</mo><mn id="S5.p3.3.m3.1.1.1.1.3.3" xref="S5.p3.3.m3.1.1.1.1.3.3.cmml">1</mn></mrow><msub id="S5.p3.3.m3.1.1.1.3" xref="S5.p3.3.m3.1.1.1.3.cmml"><mi id="S5.p3.3.m3.1.1.1.3.2" xref="S5.p3.3.m3.1.1.1.3.2.cmml">N</mi><mrow id="S5.p3.3.m3.1.1.1.3.3" xref="S5.p3.3.m3.1.1.1.3.3.cmml"><mi id="S5.p3.3.m3.1.1.1.3.3.2" xref="S5.p3.3.m3.1.1.1.3.3.2.cmml">t</mi><mo id="S5.p3.3.m3.1.1.1.3.3.1" xref="S5.p3.3.m3.1.1.1.3.3.1.cmml">⁢</mo><mi id="S5.p3.3.m3.1.1.1.3.3.3" xref="S5.p3.3.m3.1.1.1.3.3.3.cmml">e</mi></mrow></msub></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.3.m3.1b"><apply id="S5.p3.3.m3.1.1.cmml" xref="S5.p3.3.m3.1.1"><eq id="S5.p3.3.m3.1.1.2.cmml" xref="S5.p3.3.m3.1.1.2"></eq><apply id="S5.p3.3.m3.1.1.3.cmml" xref="S5.p3.3.m3.1.1.3"><ci id="S5.p3.3.m3.1.1.3.1.cmml" xref="S5.p3.3.m3.1.1.3.1">^</ci><ci id="S5.p3.3.m3.1.1.3.2.cmml" xref="S5.p3.3.m3.1.1.3.2">𝑌</ci></apply><apply id="S5.p3.3.m3.1.1.1.cmml" xref="S5.p3.3.m3.1.1.1"><csymbol cd="ambiguous" id="S5.p3.3.m3.1.1.1.2.cmml" xref="S5.p3.3.m3.1.1.1">superscript</csymbol><apply id="S5.p3.3.m3.1.1.1.1.cmml" xref="S5.p3.3.m3.1.1.1"><csymbol cd="ambiguous" id="S5.p3.3.m3.1.1.1.1.2.cmml" xref="S5.p3.3.m3.1.1.1">subscript</csymbol><set id="S5.p3.3.m3.1.1.1.1.1.2.cmml" xref="S5.p3.3.m3.1.1.1.1.1.1"><apply id="S5.p3.3.m3.1.1.1.1.1.1.1.cmml" xref="S5.p3.3.m3.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p3.3.m3.1.1.1.1.1.1.1.1.cmml" xref="S5.p3.3.m3.1.1.1.1.1.1.1">subscript</csymbol><apply id="S5.p3.3.m3.1.1.1.1.1.1.1.2.cmml" xref="S5.p3.3.m3.1.1.1.1.1.1.1.2"><ci id="S5.p3.3.m3.1.1.1.1.1.1.1.2.1.cmml" xref="S5.p3.3.m3.1.1.1.1.1.1.1.2.1">^</ci><ci id="S5.p3.3.m3.1.1.1.1.1.1.1.2.2.cmml" xref="S5.p3.3.m3.1.1.1.1.1.1.1.2.2">𝑌</ci></apply><ci id="S5.p3.3.m3.1.1.1.1.1.1.1.3.cmml" xref="S5.p3.3.m3.1.1.1.1.1.1.1.3">𝑖</ci></apply></set><apply id="S5.p3.3.m3.1.1.1.1.3.cmml" xref="S5.p3.3.m3.1.1.1.1.3"><eq id="S5.p3.3.m3.1.1.1.1.3.1.cmml" xref="S5.p3.3.m3.1.1.1.1.3.1"></eq><ci id="S5.p3.3.m3.1.1.1.1.3.2.cmml" xref="S5.p3.3.m3.1.1.1.1.3.2">𝑖</ci><cn id="S5.p3.3.m3.1.1.1.1.3.3.cmml" type="integer" xref="S5.p3.3.m3.1.1.1.1.3.3">1</cn></apply></apply><apply id="S5.p3.3.m3.1.1.1.3.cmml" xref="S5.p3.3.m3.1.1.1.3"><csymbol cd="ambiguous" id="S5.p3.3.m3.1.1.1.3.1.cmml" xref="S5.p3.3.m3.1.1.1.3">subscript</csymbol><ci id="S5.p3.3.m3.1.1.1.3.2.cmml" xref="S5.p3.3.m3.1.1.1.3.2">𝑁</ci><apply id="S5.p3.3.m3.1.1.1.3.3.cmml" xref="S5.p3.3.m3.1.1.1.3.3"><times id="S5.p3.3.m3.1.1.1.3.3.1.cmml" xref="S5.p3.3.m3.1.1.1.3.3.1"></times><ci id="S5.p3.3.m3.1.1.1.3.3.2.cmml" xref="S5.p3.3.m3.1.1.1.3.3.2">𝑡</ci><ci id="S5.p3.3.m3.1.1.1.3.3.3.cmml" xref="S5.p3.3.m3.1.1.1.3.3.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.3.m3.1c">\hat{Y}=\{\hat{Y}_{i}\}_{i=1}^{N_{te}}</annotation><annotation encoding="application/x-llamapun" id="S5.p3.3.m3.1d">over^ start_ARG italic_Y end_ARG = { over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math> denote the actual and predicted value vectors for a single voxel. Thus, <math alttext="Y" class="ltx_Math" display="inline" id="S5.p3.4.m4.1"><semantics id="S5.p3.4.m4.1a"><mi id="S5.p3.4.m4.1.1" xref="S5.p3.4.m4.1.1.cmml">Y</mi><annotation-xml encoding="MathML-Content" id="S5.p3.4.m4.1b"><ci id="S5.p3.4.m4.1.1.cmml" xref="S5.p3.4.m4.1.1">𝑌</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.4.m4.1c">Y</annotation><annotation encoding="application/x-llamapun" id="S5.p3.4.m4.1d">italic_Y</annotation></semantics></math> and <math alttext="\hat{Y}\in\mathbb{R}^{N_{te}}" class="ltx_Math" display="inline" id="S5.p3.5.m5.1"><semantics id="S5.p3.5.m5.1a"><mrow id="S5.p3.5.m5.1.1" xref="S5.p3.5.m5.1.1.cmml"><mover accent="true" id="S5.p3.5.m5.1.1.2" xref="S5.p3.5.m5.1.1.2.cmml"><mi id="S5.p3.5.m5.1.1.2.2" xref="S5.p3.5.m5.1.1.2.2.cmml">Y</mi><mo id="S5.p3.5.m5.1.1.2.1" xref="S5.p3.5.m5.1.1.2.1.cmml">^</mo></mover><mo id="S5.p3.5.m5.1.1.1" xref="S5.p3.5.m5.1.1.1.cmml">∈</mo><msup id="S5.p3.5.m5.1.1.3" xref="S5.p3.5.m5.1.1.3.cmml"><mi id="S5.p3.5.m5.1.1.3.2" xref="S5.p3.5.m5.1.1.3.2.cmml">ℝ</mi><msub id="S5.p3.5.m5.1.1.3.3" xref="S5.p3.5.m5.1.1.3.3.cmml"><mi id="S5.p3.5.m5.1.1.3.3.2" xref="S5.p3.5.m5.1.1.3.3.2.cmml">N</mi><mrow id="S5.p3.5.m5.1.1.3.3.3" xref="S5.p3.5.m5.1.1.3.3.3.cmml"><mi id="S5.p3.5.m5.1.1.3.3.3.2" xref="S5.p3.5.m5.1.1.3.3.3.2.cmml">t</mi><mo id="S5.p3.5.m5.1.1.3.3.3.1" xref="S5.p3.5.m5.1.1.3.3.3.1.cmml">⁢</mo><mi id="S5.p3.5.m5.1.1.3.3.3.3" xref="S5.p3.5.m5.1.1.3.3.3.3.cmml">e</mi></mrow></msub></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.5.m5.1b"><apply id="S5.p3.5.m5.1.1.cmml" xref="S5.p3.5.m5.1.1"><in id="S5.p3.5.m5.1.1.1.cmml" xref="S5.p3.5.m5.1.1.1"></in><apply id="S5.p3.5.m5.1.1.2.cmml" xref="S5.p3.5.m5.1.1.2"><ci id="S5.p3.5.m5.1.1.2.1.cmml" xref="S5.p3.5.m5.1.1.2.1">^</ci><ci id="S5.p3.5.m5.1.1.2.2.cmml" xref="S5.p3.5.m5.1.1.2.2">𝑌</ci></apply><apply id="S5.p3.5.m5.1.1.3.cmml" xref="S5.p3.5.m5.1.1.3"><csymbol cd="ambiguous" id="S5.p3.5.m5.1.1.3.1.cmml" xref="S5.p3.5.m5.1.1.3">superscript</csymbol><ci id="S5.p3.5.m5.1.1.3.2.cmml" xref="S5.p3.5.m5.1.1.3.2">ℝ</ci><apply id="S5.p3.5.m5.1.1.3.3.cmml" xref="S5.p3.5.m5.1.1.3.3"><csymbol cd="ambiguous" id="S5.p3.5.m5.1.1.3.3.1.cmml" xref="S5.p3.5.m5.1.1.3.3">subscript</csymbol><ci id="S5.p3.5.m5.1.1.3.3.2.cmml" xref="S5.p3.5.m5.1.1.3.3.2">𝑁</ci><apply id="S5.p3.5.m5.1.1.3.3.3.cmml" xref="S5.p3.5.m5.1.1.3.3.3"><times id="S5.p3.5.m5.1.1.3.3.3.1.cmml" xref="S5.p3.5.m5.1.1.3.3.3.1"></times><ci id="S5.p3.5.m5.1.1.3.3.3.2.cmml" xref="S5.p3.5.m5.1.1.3.3.3.2">𝑡</ci><ci id="S5.p3.5.m5.1.1.3.3.3.3.cmml" xref="S5.p3.5.m5.1.1.3.3.3.3">𝑒</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.5.m5.1c">\hat{Y}\in\mathbb{R}^{N_{te}}</annotation><annotation encoding="application/x-llamapun" id="S5.p3.5.m5.1d">over^ start_ARG italic_Y end_ARG ∈ blackboard_R start_POSTSUPERSCRIPT italic_N start_POSTSUBSCRIPT italic_t italic_e end_POSTSUBSCRIPT end_POSTSUPERSCRIPT</annotation></semantics></math>.
We use Pearson Correlation (PC) which is computed as <math alttext="corr(Y,\hat{Y})" class="ltx_Math" display="inline" id="S5.p3.6.m6.2"><semantics id="S5.p3.6.m6.2a"><mrow id="S5.p3.6.m6.2.3" xref="S5.p3.6.m6.2.3.cmml"><mi id="S5.p3.6.m6.2.3.2" xref="S5.p3.6.m6.2.3.2.cmml">c</mi><mo id="S5.p3.6.m6.2.3.1" xref="S5.p3.6.m6.2.3.1.cmml">⁢</mo><mi id="S5.p3.6.m6.2.3.3" xref="S5.p3.6.m6.2.3.3.cmml">o</mi><mo id="S5.p3.6.m6.2.3.1a" xref="S5.p3.6.m6.2.3.1.cmml">⁢</mo><mi id="S5.p3.6.m6.2.3.4" xref="S5.p3.6.m6.2.3.4.cmml">r</mi><mo id="S5.p3.6.m6.2.3.1b" xref="S5.p3.6.m6.2.3.1.cmml">⁢</mo><mi id="S5.p3.6.m6.2.3.5" xref="S5.p3.6.m6.2.3.5.cmml">r</mi><mo id="S5.p3.6.m6.2.3.1c" xref="S5.p3.6.m6.2.3.1.cmml">⁢</mo><mrow id="S5.p3.6.m6.2.3.6.2" xref="S5.p3.6.m6.2.3.6.1.cmml"><mo id="S5.p3.6.m6.2.3.6.2.1" stretchy="false" xref="S5.p3.6.m6.2.3.6.1.cmml">(</mo><mi id="S5.p3.6.m6.1.1" xref="S5.p3.6.m6.1.1.cmml">Y</mi><mo id="S5.p3.6.m6.2.3.6.2.2" xref="S5.p3.6.m6.2.3.6.1.cmml">,</mo><mover accent="true" id="S5.p3.6.m6.2.2" xref="S5.p3.6.m6.2.2.cmml"><mi id="S5.p3.6.m6.2.2.2" xref="S5.p3.6.m6.2.2.2.cmml">Y</mi><mo id="S5.p3.6.m6.2.2.1" xref="S5.p3.6.m6.2.2.1.cmml">^</mo></mover><mo id="S5.p3.6.m6.2.3.6.2.3" stretchy="false" xref="S5.p3.6.m6.2.3.6.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p3.6.m6.2b"><apply id="S5.p3.6.m6.2.3.cmml" xref="S5.p3.6.m6.2.3"><times id="S5.p3.6.m6.2.3.1.cmml" xref="S5.p3.6.m6.2.3.1"></times><ci id="S5.p3.6.m6.2.3.2.cmml" xref="S5.p3.6.m6.2.3.2">𝑐</ci><ci id="S5.p3.6.m6.2.3.3.cmml" xref="S5.p3.6.m6.2.3.3">𝑜</ci><ci id="S5.p3.6.m6.2.3.4.cmml" xref="S5.p3.6.m6.2.3.4">𝑟</ci><ci id="S5.p3.6.m6.2.3.5.cmml" xref="S5.p3.6.m6.2.3.5">𝑟</ci><interval closure="open" id="S5.p3.6.m6.2.3.6.1.cmml" xref="S5.p3.6.m6.2.3.6.2"><ci id="S5.p3.6.m6.1.1.cmml" xref="S5.p3.6.m6.1.1">𝑌</ci><apply id="S5.p3.6.m6.2.2.cmml" xref="S5.p3.6.m6.2.2"><ci id="S5.p3.6.m6.2.2.1.cmml" xref="S5.p3.6.m6.2.2.1">^</ci><ci id="S5.p3.6.m6.2.2.2.cmml" xref="S5.p3.6.m6.2.2.2">𝑌</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.6.m6.2c">corr(Y,\hat{Y})</annotation><annotation encoding="application/x-llamapun" id="S5.p3.6.m6.2d">italic_c italic_o italic_r italic_r ( italic_Y , over^ start_ARG italic_Y end_ARG )</annotation></semantics></math> where corr is the correlation function.
The final measure of a model’s performance is obtained by calculating Pearson correlation between the model’s predictions and neural recordings.
This correlation is then divided by the estimated cross-subject prediction accuracy and averaged across voxels, regions, and participants, resulting in a standardized measure of performance referred to as normalized brain alignment. For calculating normalized alignment, we select the voxels whose cross-subject prediction accuracy is <math alttext="\geq" class="ltx_Math" display="inline" id="S5.p3.7.m7.1"><semantics id="S5.p3.7.m7.1a"><mo id="S5.p3.7.m7.1.1" xref="S5.p3.7.m7.1.1.cmml">≥</mo><annotation-xml encoding="MathML-Content" id="S5.p3.7.m7.1b"><geq id="S5.p3.7.m7.1.1.cmml" xref="S5.p3.7.m7.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.7.m7.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S5.p3.7.m7.1d">≥</annotation></semantics></math> 0.05.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.11"><span class="ltx_text ltx_font_bold" id="S5.p4.11.1">Variance partitioning.</span>
To disentangle task-specific instruction representations from multimodal instruction-tuned models, we used a variance partitioning approach <cite class="ltx_cite ltx_citemacro_citep">(de Heer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib8" title="">2017</a>; LeBel et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib18" title="">2021</a>)</cite>. This method measures the overlap in brain variance explained by different task-specific instruction representations. Specifically, variance partitioning separates the brain response variance that can be attributed to two models based on their unique and overlapping contributions <cite class="ltx_cite ltx_citemacro_citep">(Vaidya et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib44" title="">2022</a>; Deniz et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib9" title="">2019</a>)</cite>. To perform this, for every pair of instruction representations, we fit separate encoding models for each space as well as a joint encoding model, obtained by concatenating the features. Using set arithmetic, we can then derive the size of the intersection <math alttext="(NBA)^{1\cap 2}_{v}" class="ltx_Math" display="inline" id="S5.p4.1.m1.1"><semantics id="S5.p4.1.m1.1a"><msubsup id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml"><mrow id="S5.p4.1.m1.1.1.1.1.1" xref="S5.p4.1.m1.1.1.1.1.1.1.cmml"><mo id="S5.p4.1.m1.1.1.1.1.1.2" stretchy="false" xref="S5.p4.1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.1.m1.1.1.1.1.1.1" xref="S5.p4.1.m1.1.1.1.1.1.1.cmml"><mi id="S5.p4.1.m1.1.1.1.1.1.1.2" xref="S5.p4.1.m1.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.1.m1.1.1.1.1.1.1.1" xref="S5.p4.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.1.m1.1.1.1.1.1.1.3" xref="S5.p4.1.m1.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.1.m1.1.1.1.1.1.1.1a" xref="S5.p4.1.m1.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.1.m1.1.1.1.1.1.1.4" xref="S5.p4.1.m1.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.1.m1.1.1.1.1.1.3" stretchy="false" xref="S5.p4.1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.1.m1.1.1.3" xref="S5.p4.1.m1.1.1.3.cmml">v</mi><mrow id="S5.p4.1.m1.1.1.1.3" xref="S5.p4.1.m1.1.1.1.3.cmml"><mn id="S5.p4.1.m1.1.1.1.3.2" xref="S5.p4.1.m1.1.1.1.3.2.cmml">1</mn><mo id="S5.p4.1.m1.1.1.1.3.1" xref="S5.p4.1.m1.1.1.1.3.1.cmml">∩</mo><mn id="S5.p4.1.m1.1.1.1.3.3" xref="S5.p4.1.m1.1.1.1.3.3.cmml">2</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><apply id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p4.1.m1.1.1.2.cmml" xref="S5.p4.1.m1.1.1">subscript</csymbol><apply id="S5.p4.1.m1.1.1.1.cmml" xref="S5.p4.1.m1.1.1"><csymbol cd="ambiguous" id="S5.p4.1.m1.1.1.1.2.cmml" xref="S5.p4.1.m1.1.1">superscript</csymbol><apply id="S5.p4.1.m1.1.1.1.1.1.1.cmml" xref="S5.p4.1.m1.1.1.1.1.1"><times id="S5.p4.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1.1"></times><ci id="S5.p4.1.m1.1.1.1.1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.1.m1.1.1.1.1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.1.m1.1.1.1.1.1.1.4.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1.4">𝐴</ci></apply><apply id="S5.p4.1.m1.1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.1.3"><intersect id="S5.p4.1.m1.1.1.1.3.1.cmml" xref="S5.p4.1.m1.1.1.1.3.1"></intersect><cn id="S5.p4.1.m1.1.1.1.3.2.cmml" type="integer" xref="S5.p4.1.m1.1.1.1.3.2">1</cn><cn id="S5.p4.1.m1.1.1.1.3.3.cmml" type="integer" xref="S5.p4.1.m1.1.1.1.3.3">2</cn></apply></apply><ci id="S5.p4.1.m1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">(NBA)^{1\cap 2}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.1.m1.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 1 ∩ 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>=<math alttext="(NBA)^{1}_{v}" class="ltx_Math" display="inline" id="S5.p4.2.m2.1"><semantics id="S5.p4.2.m2.1a"><msubsup id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml"><mrow id="S5.p4.2.m2.1.1.1.1.1" xref="S5.p4.2.m2.1.1.1.1.1.1.cmml"><mo id="S5.p4.2.m2.1.1.1.1.1.2" stretchy="false" xref="S5.p4.2.m2.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.2.m2.1.1.1.1.1.1" xref="S5.p4.2.m2.1.1.1.1.1.1.cmml"><mi id="S5.p4.2.m2.1.1.1.1.1.1.2" xref="S5.p4.2.m2.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.2.m2.1.1.1.1.1.1.1" xref="S5.p4.2.m2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.2.m2.1.1.1.1.1.1.3" xref="S5.p4.2.m2.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.2.m2.1.1.1.1.1.1.1a" xref="S5.p4.2.m2.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.2.m2.1.1.1.1.1.1.4" xref="S5.p4.2.m2.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.2.m2.1.1.1.1.1.3" stretchy="false" xref="S5.p4.2.m2.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.2.m2.1.1.3" xref="S5.p4.2.m2.1.1.3.cmml">v</mi><mn id="S5.p4.2.m2.1.1.1.3" xref="S5.p4.2.m2.1.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><apply id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.2.cmml" xref="S5.p4.2.m2.1.1">subscript</csymbol><apply id="S5.p4.2.m2.1.1.1.cmml" xref="S5.p4.2.m2.1.1"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.1.2.cmml" xref="S5.p4.2.m2.1.1">superscript</csymbol><apply id="S5.p4.2.m2.1.1.1.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1.1.1"><times id="S5.p4.2.m2.1.1.1.1.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.1"></times><ci id="S5.p4.2.m2.1.1.1.1.1.1.2.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.2.m2.1.1.1.1.1.1.3.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.2.m2.1.1.1.1.1.1.4.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.4">𝐴</ci></apply><cn id="S5.p4.2.m2.1.1.1.3.cmml" type="integer" xref="S5.p4.2.m2.1.1.1.3">1</cn></apply><ci id="S5.p4.2.m2.1.1.3.cmml" xref="S5.p4.2.m2.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">(NBA)^{1}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.2.m2.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>+<math alttext="(NBA)^{2}_{v}" class="ltx_Math" display="inline" id="S5.p4.3.m3.1"><semantics id="S5.p4.3.m3.1a"><msubsup id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml"><mrow id="S5.p4.3.m3.1.1.1.1.1" xref="S5.p4.3.m3.1.1.1.1.1.1.cmml"><mo id="S5.p4.3.m3.1.1.1.1.1.2" stretchy="false" xref="S5.p4.3.m3.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.3.m3.1.1.1.1.1.1" xref="S5.p4.3.m3.1.1.1.1.1.1.cmml"><mi id="S5.p4.3.m3.1.1.1.1.1.1.2" xref="S5.p4.3.m3.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.3.m3.1.1.1.1.1.1.1" xref="S5.p4.3.m3.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.3.m3.1.1.1.1.1.1.3" xref="S5.p4.3.m3.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.3.m3.1.1.1.1.1.1.1a" xref="S5.p4.3.m3.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.3.m3.1.1.1.1.1.1.4" xref="S5.p4.3.m3.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.3.m3.1.1.1.1.1.3" stretchy="false" xref="S5.p4.3.m3.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.3.m3.1.1.3" xref="S5.p4.3.m3.1.1.3.cmml">v</mi><mn id="S5.p4.3.m3.1.1.1.3" xref="S5.p4.3.m3.1.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><apply id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p4.3.m3.1.1.2.cmml" xref="S5.p4.3.m3.1.1">subscript</csymbol><apply id="S5.p4.3.m3.1.1.1.cmml" xref="S5.p4.3.m3.1.1"><csymbol cd="ambiguous" id="S5.p4.3.m3.1.1.1.2.cmml" xref="S5.p4.3.m3.1.1">superscript</csymbol><apply id="S5.p4.3.m3.1.1.1.1.1.1.cmml" xref="S5.p4.3.m3.1.1.1.1.1"><times id="S5.p4.3.m3.1.1.1.1.1.1.1.cmml" xref="S5.p4.3.m3.1.1.1.1.1.1.1"></times><ci id="S5.p4.3.m3.1.1.1.1.1.1.2.cmml" xref="S5.p4.3.m3.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.3.m3.1.1.1.1.1.1.3.cmml" xref="S5.p4.3.m3.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.3.m3.1.1.1.1.1.1.4.cmml" xref="S5.p4.3.m3.1.1.1.1.1.1.4">𝐴</ci></apply><cn id="S5.p4.3.m3.1.1.1.3.cmml" type="integer" xref="S5.p4.3.m3.1.1.1.3">2</cn></apply><ci id="S5.p4.3.m3.1.1.3.cmml" xref="S5.p4.3.m3.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">(NBA)^{2}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.3.m3.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>-<math alttext="(NBA)^{1\cup 2}_{v}" class="ltx_Math" display="inline" id="S5.p4.4.m4.1"><semantics id="S5.p4.4.m4.1a"><msubsup id="S5.p4.4.m4.1.1" xref="S5.p4.4.m4.1.1.cmml"><mrow id="S5.p4.4.m4.1.1.1.1.1" xref="S5.p4.4.m4.1.1.1.1.1.1.cmml"><mo id="S5.p4.4.m4.1.1.1.1.1.2" stretchy="false" xref="S5.p4.4.m4.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.4.m4.1.1.1.1.1.1" xref="S5.p4.4.m4.1.1.1.1.1.1.cmml"><mi id="S5.p4.4.m4.1.1.1.1.1.1.2" xref="S5.p4.4.m4.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.4.m4.1.1.1.1.1.1.1" xref="S5.p4.4.m4.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.4.m4.1.1.1.1.1.1.3" xref="S5.p4.4.m4.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.4.m4.1.1.1.1.1.1.1a" xref="S5.p4.4.m4.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.4.m4.1.1.1.1.1.1.4" xref="S5.p4.4.m4.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.4.m4.1.1.1.1.1.3" stretchy="false" xref="S5.p4.4.m4.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.4.m4.1.1.3" xref="S5.p4.4.m4.1.1.3.cmml">v</mi><mrow id="S5.p4.4.m4.1.1.1.3" xref="S5.p4.4.m4.1.1.1.3.cmml"><mn id="S5.p4.4.m4.1.1.1.3.2" xref="S5.p4.4.m4.1.1.1.3.2.cmml">1</mn><mo id="S5.p4.4.m4.1.1.1.3.1" xref="S5.p4.4.m4.1.1.1.3.1.cmml">∪</mo><mn id="S5.p4.4.m4.1.1.1.3.3" xref="S5.p4.4.m4.1.1.1.3.3.cmml">2</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.4.m4.1b"><apply id="S5.p4.4.m4.1.1.cmml" xref="S5.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S5.p4.4.m4.1.1.2.cmml" xref="S5.p4.4.m4.1.1">subscript</csymbol><apply id="S5.p4.4.m4.1.1.1.cmml" xref="S5.p4.4.m4.1.1"><csymbol cd="ambiguous" id="S5.p4.4.m4.1.1.1.2.cmml" xref="S5.p4.4.m4.1.1">superscript</csymbol><apply id="S5.p4.4.m4.1.1.1.1.1.1.cmml" xref="S5.p4.4.m4.1.1.1.1.1"><times id="S5.p4.4.m4.1.1.1.1.1.1.1.cmml" xref="S5.p4.4.m4.1.1.1.1.1.1.1"></times><ci id="S5.p4.4.m4.1.1.1.1.1.1.2.cmml" xref="S5.p4.4.m4.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.4.m4.1.1.1.1.1.1.3.cmml" xref="S5.p4.4.m4.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.4.m4.1.1.1.1.1.1.4.cmml" xref="S5.p4.4.m4.1.1.1.1.1.1.4">𝐴</ci></apply><apply id="S5.p4.4.m4.1.1.1.3.cmml" xref="S5.p4.4.m4.1.1.1.3"><union id="S5.p4.4.m4.1.1.1.3.1.cmml" xref="S5.p4.4.m4.1.1.1.3.1"></union><cn id="S5.p4.4.m4.1.1.1.3.2.cmml" type="integer" xref="S5.p4.4.m4.1.1.1.3.2">1</cn><cn id="S5.p4.4.m4.1.1.1.3.3.cmml" type="integer" xref="S5.p4.4.m4.1.1.1.3.3">2</cn></apply></apply><ci id="S5.p4.4.m4.1.1.3.cmml" xref="S5.p4.4.m4.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.4.m4.1c">(NBA)^{1\cup 2}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.4.m4.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 1 ∪ 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>, where NBA refers to normalized brain alignment, <math alttext="v" class="ltx_Math" display="inline" id="S5.p4.5.m5.1"><semantics id="S5.p4.5.m5.1a"><mi id="S5.p4.5.m5.1.1" xref="S5.p4.5.m5.1.1.cmml">v</mi><annotation-xml encoding="MathML-Content" id="S5.p4.5.m5.1b"><ci id="S5.p4.5.m5.1.1.cmml" xref="S5.p4.5.m5.1.1">𝑣</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.5.m5.1c">v</annotation><annotation encoding="application/x-llamapun" id="S5.p4.5.m5.1d">italic_v</annotation></semantics></math> refers to a specific voxel, <math alttext="(NBA)^{1}_{v}" class="ltx_Math" display="inline" id="S5.p4.6.m6.1"><semantics id="S5.p4.6.m6.1a"><msubsup id="S5.p4.6.m6.1.1" xref="S5.p4.6.m6.1.1.cmml"><mrow id="S5.p4.6.m6.1.1.1.1.1" xref="S5.p4.6.m6.1.1.1.1.1.1.cmml"><mo id="S5.p4.6.m6.1.1.1.1.1.2" stretchy="false" xref="S5.p4.6.m6.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.6.m6.1.1.1.1.1.1" xref="S5.p4.6.m6.1.1.1.1.1.1.cmml"><mi id="S5.p4.6.m6.1.1.1.1.1.1.2" xref="S5.p4.6.m6.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.6.m6.1.1.1.1.1.1.1" xref="S5.p4.6.m6.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.6.m6.1.1.1.1.1.1.3" xref="S5.p4.6.m6.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.6.m6.1.1.1.1.1.1.1a" xref="S5.p4.6.m6.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.6.m6.1.1.1.1.1.1.4" xref="S5.p4.6.m6.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.6.m6.1.1.1.1.1.3" stretchy="false" xref="S5.p4.6.m6.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.6.m6.1.1.3" xref="S5.p4.6.m6.1.1.3.cmml">v</mi><mn id="S5.p4.6.m6.1.1.1.3" xref="S5.p4.6.m6.1.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.6.m6.1b"><apply id="S5.p4.6.m6.1.1.cmml" xref="S5.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S5.p4.6.m6.1.1.2.cmml" xref="S5.p4.6.m6.1.1">subscript</csymbol><apply id="S5.p4.6.m6.1.1.1.cmml" xref="S5.p4.6.m6.1.1"><csymbol cd="ambiguous" id="S5.p4.6.m6.1.1.1.2.cmml" xref="S5.p4.6.m6.1.1">superscript</csymbol><apply id="S5.p4.6.m6.1.1.1.1.1.1.cmml" xref="S5.p4.6.m6.1.1.1.1.1"><times id="S5.p4.6.m6.1.1.1.1.1.1.1.cmml" xref="S5.p4.6.m6.1.1.1.1.1.1.1"></times><ci id="S5.p4.6.m6.1.1.1.1.1.1.2.cmml" xref="S5.p4.6.m6.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.6.m6.1.1.1.1.1.1.3.cmml" xref="S5.p4.6.m6.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.6.m6.1.1.1.1.1.1.4.cmml" xref="S5.p4.6.m6.1.1.1.1.1.1.4">𝐴</ci></apply><cn id="S5.p4.6.m6.1.1.1.3.cmml" type="integer" xref="S5.p4.6.m6.1.1.1.3">1</cn></apply><ci id="S5.p4.6.m6.1.1.3.cmml" xref="S5.p4.6.m6.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.6.m6.1c">(NBA)^{1}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.6.m6.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> denotes alignment of model 1, <math alttext="(NBA)^{2}_{v}" class="ltx_Math" display="inline" id="S5.p4.7.m7.1"><semantics id="S5.p4.7.m7.1a"><msubsup id="S5.p4.7.m7.1.1" xref="S5.p4.7.m7.1.1.cmml"><mrow id="S5.p4.7.m7.1.1.1.1.1" xref="S5.p4.7.m7.1.1.1.1.1.1.cmml"><mo id="S5.p4.7.m7.1.1.1.1.1.2" stretchy="false" xref="S5.p4.7.m7.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.7.m7.1.1.1.1.1.1" xref="S5.p4.7.m7.1.1.1.1.1.1.cmml"><mi id="S5.p4.7.m7.1.1.1.1.1.1.2" xref="S5.p4.7.m7.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.7.m7.1.1.1.1.1.1.1" xref="S5.p4.7.m7.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.7.m7.1.1.1.1.1.1.3" xref="S5.p4.7.m7.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.7.m7.1.1.1.1.1.1.1a" xref="S5.p4.7.m7.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.7.m7.1.1.1.1.1.1.4" xref="S5.p4.7.m7.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.7.m7.1.1.1.1.1.3" stretchy="false" xref="S5.p4.7.m7.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.7.m7.1.1.3" xref="S5.p4.7.m7.1.1.3.cmml">v</mi><mn id="S5.p4.7.m7.1.1.1.3" xref="S5.p4.7.m7.1.1.1.3.cmml">2</mn></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.7.m7.1b"><apply id="S5.p4.7.m7.1.1.cmml" xref="S5.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S5.p4.7.m7.1.1.2.cmml" xref="S5.p4.7.m7.1.1">subscript</csymbol><apply id="S5.p4.7.m7.1.1.1.cmml" xref="S5.p4.7.m7.1.1"><csymbol cd="ambiguous" id="S5.p4.7.m7.1.1.1.2.cmml" xref="S5.p4.7.m7.1.1">superscript</csymbol><apply id="S5.p4.7.m7.1.1.1.1.1.1.cmml" xref="S5.p4.7.m7.1.1.1.1.1"><times id="S5.p4.7.m7.1.1.1.1.1.1.1.cmml" xref="S5.p4.7.m7.1.1.1.1.1.1.1"></times><ci id="S5.p4.7.m7.1.1.1.1.1.1.2.cmml" xref="S5.p4.7.m7.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.7.m7.1.1.1.1.1.1.3.cmml" xref="S5.p4.7.m7.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.7.m7.1.1.1.1.1.1.4.cmml" xref="S5.p4.7.m7.1.1.1.1.1.1.4">𝐴</ci></apply><cn id="S5.p4.7.m7.1.1.1.3.cmml" type="integer" xref="S5.p4.7.m7.1.1.1.3">2</cn></apply><ci id="S5.p4.7.m7.1.1.3.cmml" xref="S5.p4.7.m7.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.7.m7.1c">(NBA)^{2}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.7.m7.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> denotes alignment of model 2 and <math alttext="(NBA)^{1\cup 2}_{v}" class="ltx_Math" display="inline" id="S5.p4.8.m8.1"><semantics id="S5.p4.8.m8.1a"><msubsup id="S5.p4.8.m8.1.1" xref="S5.p4.8.m8.1.1.cmml"><mrow id="S5.p4.8.m8.1.1.1.1.1" xref="S5.p4.8.m8.1.1.1.1.1.1.cmml"><mo id="S5.p4.8.m8.1.1.1.1.1.2" stretchy="false" xref="S5.p4.8.m8.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.8.m8.1.1.1.1.1.1" xref="S5.p4.8.m8.1.1.1.1.1.1.cmml"><mi id="S5.p4.8.m8.1.1.1.1.1.1.2" xref="S5.p4.8.m8.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.8.m8.1.1.1.1.1.1.1" xref="S5.p4.8.m8.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.8.m8.1.1.1.1.1.1.3" xref="S5.p4.8.m8.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.8.m8.1.1.1.1.1.1.1a" xref="S5.p4.8.m8.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.8.m8.1.1.1.1.1.1.4" xref="S5.p4.8.m8.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.8.m8.1.1.1.1.1.3" stretchy="false" xref="S5.p4.8.m8.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.8.m8.1.1.3" xref="S5.p4.8.m8.1.1.3.cmml">v</mi><mrow id="S5.p4.8.m8.1.1.1.3" xref="S5.p4.8.m8.1.1.1.3.cmml"><mn id="S5.p4.8.m8.1.1.1.3.2" xref="S5.p4.8.m8.1.1.1.3.2.cmml">1</mn><mo id="S5.p4.8.m8.1.1.1.3.1" xref="S5.p4.8.m8.1.1.1.3.1.cmml">∪</mo><mn id="S5.p4.8.m8.1.1.1.3.3" xref="S5.p4.8.m8.1.1.1.3.3.cmml">2</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.8.m8.1b"><apply id="S5.p4.8.m8.1.1.cmml" xref="S5.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S5.p4.8.m8.1.1.2.cmml" xref="S5.p4.8.m8.1.1">subscript</csymbol><apply id="S5.p4.8.m8.1.1.1.cmml" xref="S5.p4.8.m8.1.1"><csymbol cd="ambiguous" id="S5.p4.8.m8.1.1.1.2.cmml" xref="S5.p4.8.m8.1.1">superscript</csymbol><apply id="S5.p4.8.m8.1.1.1.1.1.1.cmml" xref="S5.p4.8.m8.1.1.1.1.1"><times id="S5.p4.8.m8.1.1.1.1.1.1.1.cmml" xref="S5.p4.8.m8.1.1.1.1.1.1.1"></times><ci id="S5.p4.8.m8.1.1.1.1.1.1.2.cmml" xref="S5.p4.8.m8.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.8.m8.1.1.1.1.1.1.3.cmml" xref="S5.p4.8.m8.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.8.m8.1.1.1.1.1.1.4.cmml" xref="S5.p4.8.m8.1.1.1.1.1.1.4">𝐴</ci></apply><apply id="S5.p4.8.m8.1.1.1.3.cmml" xref="S5.p4.8.m8.1.1.1.3"><union id="S5.p4.8.m8.1.1.1.3.1.cmml" xref="S5.p4.8.m8.1.1.1.3.1"></union><cn id="S5.p4.8.m8.1.1.1.3.2.cmml" type="integer" xref="S5.p4.8.m8.1.1.1.3.2">1</cn><cn id="S5.p4.8.m8.1.1.1.3.3.cmml" type="integer" xref="S5.p4.8.m8.1.1.1.3.3">2</cn></apply></apply><ci id="S5.p4.8.m8.1.1.3.cmml" xref="S5.p4.8.m8.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.8.m8.1c">(NBA)^{1\cup 2}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.8.m8.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 1 ∪ 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math> denotes alignment of the joint model. Similarly, the unique contribution of model 1’s feature space is computed as <math alttext="(NBA)^{1\backslash 2}_{v}" class="ltx_Math" display="inline" id="S5.p4.9.m9.1"><semantics id="S5.p4.9.m9.1a"><msubsup id="S5.p4.9.m9.1.1" xref="S5.p4.9.m9.1.1.cmml"><mrow id="S5.p4.9.m9.1.1.1.1.1" xref="S5.p4.9.m9.1.1.1.1.1.1.cmml"><mo id="S5.p4.9.m9.1.1.1.1.1.2" stretchy="false" xref="S5.p4.9.m9.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.9.m9.1.1.1.1.1.1" xref="S5.p4.9.m9.1.1.1.1.1.1.cmml"><mi id="S5.p4.9.m9.1.1.1.1.1.1.2" xref="S5.p4.9.m9.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.9.m9.1.1.1.1.1.1.1" xref="S5.p4.9.m9.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.9.m9.1.1.1.1.1.1.3" xref="S5.p4.9.m9.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.9.m9.1.1.1.1.1.1.1a" xref="S5.p4.9.m9.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.9.m9.1.1.1.1.1.1.4" xref="S5.p4.9.m9.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.9.m9.1.1.1.1.1.3" stretchy="false" xref="S5.p4.9.m9.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.9.m9.1.1.3" xref="S5.p4.9.m9.1.1.3.cmml">v</mi><mrow id="S5.p4.9.m9.1.1.1.3" xref="S5.p4.9.m9.1.1.1.3.cmml"><mn id="S5.p4.9.m9.1.1.1.3.2" xref="S5.p4.9.m9.1.1.1.3.2.cmml">1</mn><mo id="S5.p4.9.m9.1.1.1.3.1" lspace="0.222em" rspace="0.222em" xref="S5.p4.9.m9.1.1.1.3.1.cmml">\</mo><mn id="S5.p4.9.m9.1.1.1.3.3" xref="S5.p4.9.m9.1.1.1.3.3.cmml">2</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.9.m9.1b"><apply id="S5.p4.9.m9.1.1.cmml" xref="S5.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S5.p4.9.m9.1.1.2.cmml" xref="S5.p4.9.m9.1.1">subscript</csymbol><apply id="S5.p4.9.m9.1.1.1.cmml" xref="S5.p4.9.m9.1.1"><csymbol cd="ambiguous" id="S5.p4.9.m9.1.1.1.2.cmml" xref="S5.p4.9.m9.1.1">superscript</csymbol><apply id="S5.p4.9.m9.1.1.1.1.1.1.cmml" xref="S5.p4.9.m9.1.1.1.1.1"><times id="S5.p4.9.m9.1.1.1.1.1.1.1.cmml" xref="S5.p4.9.m9.1.1.1.1.1.1.1"></times><ci id="S5.p4.9.m9.1.1.1.1.1.1.2.cmml" xref="S5.p4.9.m9.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.9.m9.1.1.1.1.1.1.3.cmml" xref="S5.p4.9.m9.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.9.m9.1.1.1.1.1.1.4.cmml" xref="S5.p4.9.m9.1.1.1.1.1.1.4">𝐴</ci></apply><apply id="S5.p4.9.m9.1.1.1.3.cmml" xref="S5.p4.9.m9.1.1.1.3"><ci id="S5.p4.9.m9.1.1.1.3.1.cmml" xref="S5.p4.9.m9.1.1.1.3.1">\</ci><cn id="S5.p4.9.m9.1.1.1.3.2.cmml" type="integer" xref="S5.p4.9.m9.1.1.1.3.2">1</cn><cn id="S5.p4.9.m9.1.1.1.3.3.cmml" type="integer" xref="S5.p4.9.m9.1.1.1.3.3">2</cn></apply></apply><ci id="S5.p4.9.m9.1.1.3.cmml" xref="S5.p4.9.m9.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.9.m9.1c">(NBA)^{1\backslash 2}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.9.m9.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 1 \ 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>=<math alttext="(NBA)^{1}_{v}" class="ltx_Math" display="inline" id="S5.p4.10.m10.1"><semantics id="S5.p4.10.m10.1a"><msubsup id="S5.p4.10.m10.1.1" xref="S5.p4.10.m10.1.1.cmml"><mrow id="S5.p4.10.m10.1.1.1.1.1" xref="S5.p4.10.m10.1.1.1.1.1.1.cmml"><mo id="S5.p4.10.m10.1.1.1.1.1.2" stretchy="false" xref="S5.p4.10.m10.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.10.m10.1.1.1.1.1.1" xref="S5.p4.10.m10.1.1.1.1.1.1.cmml"><mi id="S5.p4.10.m10.1.1.1.1.1.1.2" xref="S5.p4.10.m10.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.10.m10.1.1.1.1.1.1.1" xref="S5.p4.10.m10.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.10.m10.1.1.1.1.1.1.3" xref="S5.p4.10.m10.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.10.m10.1.1.1.1.1.1.1a" xref="S5.p4.10.m10.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.10.m10.1.1.1.1.1.1.4" xref="S5.p4.10.m10.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.10.m10.1.1.1.1.1.3" stretchy="false" xref="S5.p4.10.m10.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.10.m10.1.1.3" xref="S5.p4.10.m10.1.1.3.cmml">v</mi><mn id="S5.p4.10.m10.1.1.1.3" xref="S5.p4.10.m10.1.1.1.3.cmml">1</mn></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.10.m10.1b"><apply id="S5.p4.10.m10.1.1.cmml" xref="S5.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S5.p4.10.m10.1.1.2.cmml" xref="S5.p4.10.m10.1.1">subscript</csymbol><apply id="S5.p4.10.m10.1.1.1.cmml" xref="S5.p4.10.m10.1.1"><csymbol cd="ambiguous" id="S5.p4.10.m10.1.1.1.2.cmml" xref="S5.p4.10.m10.1.1">superscript</csymbol><apply id="S5.p4.10.m10.1.1.1.1.1.1.cmml" xref="S5.p4.10.m10.1.1.1.1.1"><times id="S5.p4.10.m10.1.1.1.1.1.1.1.cmml" xref="S5.p4.10.m10.1.1.1.1.1.1.1"></times><ci id="S5.p4.10.m10.1.1.1.1.1.1.2.cmml" xref="S5.p4.10.m10.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.10.m10.1.1.1.1.1.1.3.cmml" xref="S5.p4.10.m10.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.10.m10.1.1.1.1.1.1.4.cmml" xref="S5.p4.10.m10.1.1.1.1.1.1.4">𝐴</ci></apply><cn id="S5.p4.10.m10.1.1.1.3.cmml" type="integer" xref="S5.p4.10.m10.1.1.1.3">1</cn></apply><ci id="S5.p4.10.m10.1.1.3.cmml" xref="S5.p4.10.m10.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.10.m10.1c">(NBA)^{1}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.10.m10.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 1 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>-<math alttext="(NBA)^{1\cap 2}_{v}" class="ltx_Math" display="inline" id="S5.p4.11.m11.1"><semantics id="S5.p4.11.m11.1a"><msubsup id="S5.p4.11.m11.1.1" xref="S5.p4.11.m11.1.1.cmml"><mrow id="S5.p4.11.m11.1.1.1.1.1" xref="S5.p4.11.m11.1.1.1.1.1.1.cmml"><mo id="S5.p4.11.m11.1.1.1.1.1.2" stretchy="false" xref="S5.p4.11.m11.1.1.1.1.1.1.cmml">(</mo><mrow id="S5.p4.11.m11.1.1.1.1.1.1" xref="S5.p4.11.m11.1.1.1.1.1.1.cmml"><mi id="S5.p4.11.m11.1.1.1.1.1.1.2" xref="S5.p4.11.m11.1.1.1.1.1.1.2.cmml">N</mi><mo id="S5.p4.11.m11.1.1.1.1.1.1.1" xref="S5.p4.11.m11.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.11.m11.1.1.1.1.1.1.3" xref="S5.p4.11.m11.1.1.1.1.1.1.3.cmml">B</mi><mo id="S5.p4.11.m11.1.1.1.1.1.1.1a" xref="S5.p4.11.m11.1.1.1.1.1.1.1.cmml">⁢</mo><mi id="S5.p4.11.m11.1.1.1.1.1.1.4" xref="S5.p4.11.m11.1.1.1.1.1.1.4.cmml">A</mi></mrow><mo id="S5.p4.11.m11.1.1.1.1.1.3" stretchy="false" xref="S5.p4.11.m11.1.1.1.1.1.1.cmml">)</mo></mrow><mi id="S5.p4.11.m11.1.1.3" xref="S5.p4.11.m11.1.1.3.cmml">v</mi><mrow id="S5.p4.11.m11.1.1.1.3" xref="S5.p4.11.m11.1.1.1.3.cmml"><mn id="S5.p4.11.m11.1.1.1.3.2" xref="S5.p4.11.m11.1.1.1.3.2.cmml">1</mn><mo id="S5.p4.11.m11.1.1.1.3.1" xref="S5.p4.11.m11.1.1.1.3.1.cmml">∩</mo><mn id="S5.p4.11.m11.1.1.1.3.3" xref="S5.p4.11.m11.1.1.1.3.3.cmml">2</mn></mrow></msubsup><annotation-xml encoding="MathML-Content" id="S5.p4.11.m11.1b"><apply id="S5.p4.11.m11.1.1.cmml" xref="S5.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S5.p4.11.m11.1.1.2.cmml" xref="S5.p4.11.m11.1.1">subscript</csymbol><apply id="S5.p4.11.m11.1.1.1.cmml" xref="S5.p4.11.m11.1.1"><csymbol cd="ambiguous" id="S5.p4.11.m11.1.1.1.2.cmml" xref="S5.p4.11.m11.1.1">superscript</csymbol><apply id="S5.p4.11.m11.1.1.1.1.1.1.cmml" xref="S5.p4.11.m11.1.1.1.1.1"><times id="S5.p4.11.m11.1.1.1.1.1.1.1.cmml" xref="S5.p4.11.m11.1.1.1.1.1.1.1"></times><ci id="S5.p4.11.m11.1.1.1.1.1.1.2.cmml" xref="S5.p4.11.m11.1.1.1.1.1.1.2">𝑁</ci><ci id="S5.p4.11.m11.1.1.1.1.1.1.3.cmml" xref="S5.p4.11.m11.1.1.1.1.1.1.3">𝐵</ci><ci id="S5.p4.11.m11.1.1.1.1.1.1.4.cmml" xref="S5.p4.11.m11.1.1.1.1.1.1.4">𝐴</ci></apply><apply id="S5.p4.11.m11.1.1.1.3.cmml" xref="S5.p4.11.m11.1.1.1.3"><intersect id="S5.p4.11.m11.1.1.1.3.1.cmml" xref="S5.p4.11.m11.1.1.1.3.1"></intersect><cn id="S5.p4.11.m11.1.1.1.3.2.cmml" type="integer" xref="S5.p4.11.m11.1.1.1.3.2">1</cn><cn id="S5.p4.11.m11.1.1.1.3.3.cmml" type="integer" xref="S5.p4.11.m11.1.1.1.3.3">2</cn></apply></apply><ci id="S5.p4.11.m11.1.1.3.cmml" xref="S5.p4.11.m11.1.1.3">𝑣</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.11.m11.1c">(NBA)^{1\cap 2}_{v}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.11.m11.1d">( italic_N italic_B italic_A ) start_POSTSUPERSCRIPT 1 ∩ 2 end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_v end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>MLLM representations align well to human brain activity</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">First, we examine the brain alignment by measuring the similarity of degree of brain predictivity using representations extracted from multimodal instruction-tuned models (MLLMs), focusing on both whole visual cortex and specific visual function localizers. For each MLLM, we calculate the average normalized brain alignment across 10 instructions, multiple subjects, and different MLLM layers, using the NSD dataset. Additionally, we report baseline performance using randomly initialized versions of the InstructBLIP, mPLUG-OWL and IDEFICS models. We further compare the brain alignment performance of these MLLMs with unimodal vision model (ViT-H) and multimodal CLIP-Text model.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">Whole visual cortex analysis.</span>
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F2" title="Figure 2 ‣ 6.1 MLLM representations align well to human brain activity ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">2</span></a> (a) presents the average normalized brain alignment for whole-brain analysis across six different settings. The results demonstrate that MLLMs significantly outperform randomly initialized models in terms of brain alignment. Moreover, we find that multimodal instruction-tuning improves brain alignment over unimodal ViT-H models. Notably, the superior performance of pretrained MLLMs compared to randomly initialized models indicates that natural guided instructions yield more brain-relevant representations, leading to greater alignment with brain activity. Additionally, MLLMs perform on par with, or better than, the CLIP-Text model, despite the latter using ground-truth captions, whereas MLLMs (InstructBLIP, mPLUG-OWL and IDEFICS) use mean pooling over predicted output tokens based on natural instructions.</p>
</div>
<figure class="ltx_figure" id="S6.F2">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="221" id="S6.F2.g1" src="x2.png" width="270"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="221" id="S6.F2.g2" src="x3.png" width="270"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="221" id="S6.F2.g3" src="x4.png" width="270"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>Whole visual cortex and ROI-based normalized brain alignment was computed by averaging across participants, layers, and voxels. Blue: Average across random initialization of the 3 MLLMs. Note that CLIP-text model uses golden oracle captions while instruct models use predicted model generations. <math alttext="\ast" class="ltx_Math" display="inline" id="S6.F2.5.m1.1"><semantics id="S6.F2.5.m1.1b"><mo id="S6.F2.5.m1.1.1" mathcolor="#FF0000" xref="S6.F2.5.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="S6.F2.5.m1.1c"><ci id="S6.F2.5.m1.1.1.cmml" xref="S6.F2.5.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.5.m1.1d">\ast</annotation><annotation encoding="application/x-llamapun" id="S6.F2.5.m1.1e">∗</annotation></semantics></math> indicates cases where MLLM embeddings are statistically significantly better than randomly initialized models, i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="S6.F2.6.m2.1"><semantics id="S6.F2.6.m2.1b"><mrow id="S6.F2.6.m2.1.1" xref="S6.F2.6.m2.1.1.cmml"><mi id="S6.F2.6.m2.1.1.2" xref="S6.F2.6.m2.1.1.2.cmml"></mi><mo id="S6.F2.6.m2.1.1.1" xref="S6.F2.6.m2.1.1.1.cmml">≤</mo><mn id="S6.F2.6.m2.1.1.3" xref="S6.F2.6.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F2.6.m2.1c"><apply id="S6.F2.6.m2.1.1.cmml" xref="S6.F2.6.m2.1.1"><leq id="S6.F2.6.m2.1.1.1.cmml" xref="S6.F2.6.m2.1.1.1"></leq><csymbol cd="latexml" id="S6.F2.6.m2.1.1.2.cmml" xref="S6.F2.6.m2.1.1.2">absent</csymbol><cn id="S6.F2.6.m2.1.1.3.cmml" type="float" xref="S6.F2.6.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.6.m2.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="S6.F2.6.m2.1e">≤ 0.05</annotation></semantics></math>. <math alttext="\wedge" class="ltx_Math" display="inline" id="S6.F2.7.m3.1"><semantics id="S6.F2.7.m3.1b"><mo id="S6.F2.7.m3.1.1" mathcolor="#FF0000" xref="S6.F2.7.m3.1.1.cmml">∧</mo><annotation-xml encoding="MathML-Content" id="S6.F2.7.m3.1c"><and id="S6.F2.7.m3.1.1.cmml" xref="S6.F2.7.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.7.m3.1d">\wedge</annotation><annotation encoding="application/x-llamapun" id="S6.F2.7.m3.1e">∧</annotation></semantics></math> indicates cases where MLLMs are significantly better than unimodal vision models (ViT-H), i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="S6.F2.8.m4.1"><semantics id="S6.F2.8.m4.1b"><mrow id="S6.F2.8.m4.1.1" xref="S6.F2.8.m4.1.1.cmml"><mi id="S6.F2.8.m4.1.1.2" xref="S6.F2.8.m4.1.1.2.cmml"></mi><mo id="S6.F2.8.m4.1.1.1" xref="S6.F2.8.m4.1.1.1.cmml">≤</mo><mn id="S6.F2.8.m4.1.1.3" xref="S6.F2.8.m4.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F2.8.m4.1c"><apply id="S6.F2.8.m4.1.1.cmml" xref="S6.F2.8.m4.1.1"><leq id="S6.F2.8.m4.1.1.1.cmml" xref="S6.F2.8.m4.1.1.1"></leq><csymbol cd="latexml" id="S6.F2.8.m4.1.1.2.cmml" xref="S6.F2.8.m4.1.1.2">absent</csymbol><cn id="S6.F2.8.m4.1.1.3.cmml" type="float" xref="S6.F2.8.m4.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.8.m4.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="S6.F2.8.m4.1e">≤ 0.05</annotation></semantics></math>. Other brain ROI plots are reported in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A6.F10" title="Figure 10 ‣ Appendix F Normalized brain alignment: Visual Functional Localizers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">10</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A6" title="Appendix F Normalized brain alignment: Visual Functional Localizers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">F</span></a>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.2"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.2.1">ROI analysis.</span>
We further examine how instruction-tuning enhances the alignment of MLLM representations with brain activity, focusing on specific brain regions. To do this, we compute the normalized brain alignment separately across five visual functional localizers, as discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S3" title="3 Dataset and Curation ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F2" title="Figure 2 ‣ 6.1 MLLM representations align well to human brain activity ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">2</span></a> (b) and (c) show the normalized alignment across six settings for pRF-Visual ROIs and FLOC-PLACES ROIs (see additional figures in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A6" title="Appendix F Normalized brain alignment: Visual Functional Localizers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">F</span></a>). From Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F2" title="Figure 2 ‣ 6.1 MLLM representations align well to human brain activity ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">2</span></a> (b) and (c), we make the following observations: (1) Similar to the whole visual cortex analysis, MLLMs exhibit significantly better brain alignment compared to baseline (randomly initialized) and unimodal ViT-H models. (2) Interestingly, baseline performance is closer to that of unimodal models in early visual ROIs (i.e. pRF-Visual ROIs), whereas the difference becomes more pronounced in floc-place regions. This trend is also observed in MLLMs, where the normalized alignment reaches <math alttext="\sim" class="ltx_Math" display="inline" id="S6.SS1.p3.1.m1.1"><semantics id="S6.SS1.p3.1.m1.1a"><mo id="S6.SS1.p3.1.m1.1.1" xref="S6.SS1.p3.1.m1.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.1.m1.1b"><csymbol cd="latexml" id="S6.SS1.p3.1.m1.1.1.cmml" xref="S6.SS1.p3.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.1.m1.1d">∼</annotation></semantics></math>0.8 in place regions but drops to <math alttext="\sim" class="ltx_Math" display="inline" id="S6.SS1.p3.2.m2.1"><semantics id="S6.SS1.p3.2.m2.1a"><mo id="S6.SS1.p3.2.m2.1.1" xref="S6.SS1.p3.2.m2.1.1.cmml">∼</mo><annotation-xml encoding="MathML-Content" id="S6.SS1.p3.2.m2.1b"><csymbol cd="latexml" id="S6.SS1.p3.2.m2.1.1.cmml" xref="S6.SS1.p3.2.m2.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S6.SS1.p3.2.m2.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S6.SS1.p3.2.m2.1d">∼</annotation></semantics></math>0.6 in early visual ROIs, suggesting that high-level visual areas provide better brain-relevant representations in MLLMs than early visual areas. This finding agrees with previous
work which suggests that multimodal models better predicts neural responses in the high-level visual cortex than previous vision-only models like CNNs <cite class="ltx_cite ltx_citemacro_citep">(Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib46" title="">2022</a>)</cite>. Similar patterns are observed in other high-level visual areas (faces, bodies, words), as detailed in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A6" title="Appendix F Normalized brain alignment: Visual Functional Localizers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Encoding performances of task-specific instructions from MLLMs</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">While MLLM representations at both whole visual cortex level and in visual functional ROIs demonstrate that multimodal instruction-tuned models improve brain alignment for task-specific instructions, we are also interested in understanding the importance of each task instruction and conducting a layer-wise analysis to examine the trends in brain alignment across different models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">Which task-specific instructions are highly correlated to visual function localizers?</span>
To investigate which instructions are more effective in predicting particular
visual functional localizers, we analyze the voxels as follows. For each voxel, we select the instruction that results in the highest normalized brain alignment and apply the instruction-specific color code to the voxel. The color-scheme corresponding to each instruction is reported in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F3" title="Figure 3 ‣ 6.2 Encoding performances of task-specific instructions from MLLMs ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>. The figure also displays brain maps for the InstructBLIP, and mPLUG-OWL for Subject 1, where the voxel color codes are projected onto the flattened cortical surface of the representative subject. Similar brain maps for other subjects are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A7" title="Appendix G Brain Maps for Task-specific Instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
<figure class="ltx_figure" id="S6.F3">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="212" id="S6.F3.g1" src="x5.png" width="457"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="218" id="S6.F3.g2" src="x6.png" width="224"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Each voxel is color coded with the instruction (out of 10) that led to the highest normalized brain alignment. The color bar highlights color codes for each instruction.
The voxels are projected onto the flattened cortical surface of a representative subject (subject S1) for two MLLMs. Similar brain maps for other subjects are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A7" title="Appendix G Brain Maps for Task-specific Instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">G</span></a>.
</figcaption>
</figure>
<figure class="ltx_figure" id="S6.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="204" id="S6.F4.g1" src="x7.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Voxels specific to visual concepts groups: Counts <span class="ltx_text ltx_font_italic" id="S6.F4.3.1">(</span>Left) and Recognition <span class="ltx_text ltx_font_italic" id="S6.F4.4.2">(</span>Right). The color bar from Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F3" title="Figure 3 ‣ 6.2 Encoding performances of task-specific instructions from MLLMs ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a> highlights color codes for each instruction. The voxels are projected
onto the flattened cortical surface of a representative subject (subject S1). </figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1">From Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F3" title="Figure 3 ‣ 6.2 Encoding performances of task-specific instructions from MLLMs ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>, we make the following observations: (i) Representations related to image captioning show higher brain alignment in the EBA, PPA, and FFA regions for both InstructBLIP and IDEFICS models, while the mPLUG-OWL model demonstrates higher alignment mainly in the EBA region. (ii) Instructions related to image understanding (e.g., “describe the most dominant color”) result in higher brain alignment in early visual regions for both the InstructBLIP and mPLUG-OWL models, while IDEFICS shows only marginal alignment with this instruction. (iii) Visual question-answering instructions, such as “are there people in this image?” and “foreground and background of the image,” contribute significantly to brain alignment in higher visual regions, particularly in the PPA and FFA regions. (iv) In contrast to the above three instructions, other instructions (such as “list any food items available” and “how many animals are there?”) result into some voxels with high alignment predictions, while the remaining instructions do not result in voxels with significant alignment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p4">
<p class="ltx_p" id="S6.SS2.p4.1">In order to draw some more insights of why some instructions might be more pronounced than other in encoding the brain activity, we clustered instructions into five visual concepts - 1) Count 2) Recognition 3) Color/Texture 4) Positional Understanding and 5) General Scene Understanding. This is further motivated by our interest in uncovering which brain regions are responsible for different visual concepts.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p5">
<p class="ltx_p" id="S6.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p5.1.1">Do task-specific instructions from MLLMs account for visual concepts understanding?</span>
To examine whether task-specific instructions from MLLMs capture different visual concepts and to understand how these concepts are processed in the brain’s visual regions, we group the instructions into five categories: (i) <span class="ltx_text ltx_font_italic" id="S6.SS2.p5.1.2">Count</span> (counting objects, animals, people within the image) - IU2, IU3, VQ2; (ii) <span class="ltx_text ltx_font_italic" id="S6.SS2.p5.1.3">Recognition</span> (recognizing objects, animals, or people) - VQ1, VQ2; (iii) <span class="ltx_text ltx_font_italic" id="S6.SS2.p5.1.4">Color/Texture</span> - IU1; (iv) <span class="ltx_text ltx_font_italic" id="S6.SS2.p5.1.5">Positional Understanding</span> (foreground/background, right/left, upper/bottom positions) - VQ3; and (v) <span class="ltx_text ltx_font_italic" id="S6.SS2.p5.1.6">General Scene Understanding</span> - CR.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p6">
<p class="ltx_p" id="S6.SS2.p6.1">Similar to task-specific instructions, we analyze the voxels from visual concept groups as follows. We use the same color scheme corresponding to each instruction as reported in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F3" title="Figure 3 ‣ 6.2 Encoding performances of task-specific instructions from MLLMs ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>. The Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F4" title="Figure 4 ‣ 6.2 Encoding performances of task-specific instructions from MLLMs ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">4</span></a> displays brain maps for the InstructBLIP model for Subject 1, with voxel color codes projected onto the flattened cortical surface of the representative subject for two concept groups: Count and Recognition. We make the following observations: (i) For the visual concept <span class="ltx_text ltx_font_italic" id="S6.SS2.p6.1.1">Count</span>, the VQ2 instruction results in higher brain alignment in high-level visual regions, while IU2 and IU3 instructions show higher alignment in early visual regions. This suggests that MLLMs capture count-related visual concepts effectively across instructions, showing alignment with brain activity. (ii) For <span class="ltx_text ltx_font_italic" id="S6.SS2.p6.1.2">Recognition</span>, both VQ1 and VQ2 instructions exhibit distributed brain alignment across both high-level visual regions and early visual regions. (iii) In contrast to <span class="ltx_text ltx_font_italic" id="S6.SS2.p6.1.3">Count</span> and <span class="ltx_text ltx_font_italic" id="S6.SS2.p6.1.4">Recognition</span>, for visual concepts like <span class="ltx_text ltx_font_italic" id="S6.SS2.p6.1.5">Color</span>, <span class="ltx_text ltx_font_italic" id="S6.SS2.p6.1.6">Positional Understanding</span>, and <span class="ltx_text ltx_font_italic" id="S6.SS2.p6.1.7">Scene Understanding</span>, MLLMs display similar brain alignment patterns, irrespective of the specific visual concept. Therefore, further improvements may be needed for MLLMs to achieve better specificity in processing a broader range of visual concepts.
Similar brain maps for other subjects are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A7" title="Appendix G Brain Maps for Task-specific Instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">G</span></a>.</p>
</div>
<figure class="ltx_figure" id="S6.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="58" id="S6.F5.g1" src="x8.png" width="415"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="177" id="S6.F5.g2" src="x9.png" width="581"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Each voxel is color coded with the MLLM layer number (out of 33) that led to the highest normalized brain alignment. The color bar highlights color codes for each layer. The voxels are projected onto the flattened cortical surface of a representative subject (subject S1) for three MLLMs. Similar brain maps for other subjects are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A8" title="Appendix H Brain Maps for different MLLM Layers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">H</span></a>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS2.p7">
<p class="ltx_p" id="S6.SS2.p7.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p7.1.1">Which layers of MLLMs are highly correlated to visual function localizers?</span>
To explore whether the effectiveness of layer-wise representations from MLLMs varies in relation to visual functional localizers, we analyze the voxels as follows. For each voxel, we select the layer that results in the highest normalized brain alignment and apply a color code for the 33 layers across the three MLLMs. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F5" title="Figure 5 ‣ 6.2 Encoding performances of task-specific instructions from MLLMs ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">5</span></a> presents brain maps for the InstructBLIP, mPLUG-Owl and IDEFICS, where the voxels with their corresponding color codes are projected onto the flattened cortical surface of the representative subject (Subject 1). Similar brain maps for other subjects are in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A8" title="Appendix H Brain Maps for different MLLM Layers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">H</span></a>.
From Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F5" title="Figure 5 ‣ 6.2 Encoding performances of task-specific instructions from MLLMs ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">5</span></a>, we make the following observations: (i) Across all functional localizers, the middle layers of the InstructBLIP and IDEFICS models show greater brain alignment for higher visual regions, whereas the later layers are more aligned with early visual regions. In contrast, the later layers of the mPLUG-Owl model result in higher brain alignment for both higher and early visual regions. This variation across the 3 MLLMs may be due to the difference in the underlying language decoder models, which generate output tokens, capture contextual representations, and influence the alignment trend across the layers. This finding is consistent with studies on brain alignment in language models, which have shown that middle layers of these models tend to exhibit higher brain alignment <cite class="ltx_cite ltx_citemacro_citep">(Toneva &amp; Wehbe, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib41" title="">2019</a>; Caucheteux &amp; King, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib5" title="">2020</a>)</cite>. (ii) Unlike the mPLUG-Owl model, both the InstructBLIP and IDEFICS models show minimal impact of later layer alignment on high-level visual regions.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>Partitioning explained variance between task-specific instructions</h3>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">While the previous analysis reveals that not all instructions result in higher brain alignment across visual ROIs, we disentangle representations of task-specific instructions to measure the overlap in brain variance explained by MLLMs. To accomplish this we use variance partitioning approach discussed in Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S5" title="5 Experimental setup ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">5</span></a>. Using this approach, we measure the brain response variance explained by pairs of instruction representations, separating their unique and overlapping contributions. Variance partitioning quickly becomes intractable as the
number of feature spaces increases. Thus we restrict our analysis to pairwise comparisons that involve layers with max normalized brain alignment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.1">Shared variance across task-specific instructions.</span>
Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F6" title="Figure 6 ‣ 6.3 Partitioning explained variance between task-specific instructions ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">6</span></a> presents the shared variance between task prompts for the InstructBLIP model, averaged across all subjects. We observe the following from this figure: (i) High Overlap of Image Captioning (IC) with other prompts: The IC instruction exhibits a high degree of shared information with prompts VQ1, VQ2, CR, and IU1. The prominent brain alignment of IC across multiple prompts suggests that it acts as the general or umbrella category for the other instruction prompts. This is intuitive since in order to generate an image caption, the responses to other instructions are automatically taken into account. This broad relevance may account for its ability to capture various visual elements that are crucial across different tasks, driving consistent brain alignment. (ii) Lower shared variance with IU2 and SR: Instructions like IU2 and SR demonstrate consistently lower shared variance with other prompts, particularly VQ and IC. This indicates that these instructions may elicit distinct neural responses or emphasize specific aspects of visual stimuli not represented by the broader visual tasks linked to VQ or IC. Further, to verify these results, we plotted the percentage overlap across image categories and show it in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A10.F15" title="Figure 15 ‣ Appendix J Normalized Brain Alignment across different image categories ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">15</span></a>. Based on that, we see that the instruction IU2 which is concerned with “food” category would naturally have low shared variance with other instructions, since the overlap of “food” category is much lower with other categories, except for “kitchen”, “furniture”, and “person”. And, we see similar trends for the instruction SR, which explains its low shared variance, by looking at the “outdoor” and “indoor” image categories.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1">Overall, these observations underscore why IC, IU2, and VQ2 have task-specific brain alignments, exhibiting unique aspects of brain responses to visual regions, as illustrated in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F3" title="Figure 3 ‣ 6.2 Encoding performances of task-specific instructions from MLLMs ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>.</p>
</div>
<figure class="ltx_figure" id="S6.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="159" id="S6.F6.g1" src="x10.png" width="373"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Shared explained variance between pairs of task-specific instructions. A higher shared variance indicates that both instructions share similar features, resulting in greater shared explained variance in the brain. The diagonal cells are empty since the shared variance between pairs of the same task is identical.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p4.1.1">ROI Analysis: Shared and Unique variance across task-specific instructions.</span></p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p5">
<p class="ltx_p" id="S6.SS3.p5.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F7" title="Figure 7 ‣ 6.3 Partitioning explained variance between task-specific instructions ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">7</span></a> presents the unique and shared variance between task prompts—Image Captioning (IC) and Image Understanding 2 (IU2)—for the InstructBLIP model, focusing on representative subject-1. We present similar analysis between Image Captioning (IC) and Visual Question Answering 1 (VQ1) in Appendix Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A9.F14" title="Figure 14 ‣ Appendix I ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">14</span></a>.
From Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F7" title="Figure 7 ‣ 6.3 Partitioning explained variance between task-specific instructions ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">7</span></a>, we observe the following: (i) Between IC and IU2, there is no unique variance for IU2 in the EBA region, while IC retains some unique variance. Additionally, other high-level visual regions show a similar percentage of unique and shared variance for both IC and IU2 instructions.
(ii) In contrast to IC and IU2, between IC and VQ1, there is no unique variance in the high-level visual region EBA, as this region is explained by shared information between the models.
(iii) In other high-level visual regions, a portion of the variance is unique to each model, though the majority is explained by shared variance.
Overall, these findings highlight the role of shared neural processes across task-specific instructions in high-level visual regions while also demonstrating that different task instructions can drive distinct neural responses in specific regions.
However, the fact that the majority of variance is shared between instructions suggests room for improvement in the MLLM models. Enhancing the models’ ability to capture more task-specific information could lead to greater precision in predicting brain responses and better differentiation between various types of instructions. More detailed shared and unique variance analysis across task-specific instructions for whole visual cortex and ROI level are reported in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12" title="Appendix L Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">L</span></a>. Our findings demonstrate that shared variance increases from early visual to higher visual areas, reflecting the hierarchical nature of visual processing.</p>
</div>
<figure class="ltx_figure" id="S6.F7"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="235" id="S6.F7.g1" src="x11.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Unique &amp; shared variances between pairs of task-specific instructions for the InstructBLIP model, focusing on representative subject-1. Image Captioning (IC) vs. Image Understanding (IU2). Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A9.F14" title="Figure 14 ‣ Appendix I ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">14</span></a> in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A9" title="Appendix I ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">I</span></a> shows similar plots for IC vs. Visual Question Answering 1 (VQ1).</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion and Conclusion</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Using instruction-tuned representations from MLLMs for various instructions, we evaluated how well these representations predict fMRI brain activity when participants viewed naturalistic image stimuli. Additionally, we compared different MLLMs’ representations, assessing their alignment with each instruction across five visual brain regions. We show that MLLMs exhibit significantly better brain alignment than vision-only models and perform comparably to multimodal models.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Our analysis of instruction-tuned MLLMs and their brain alignment reveals several key conclusions: (1) The effectiveness of task-specific instructions in predicting visual brain activity across different regions reveals that, although all three MLLMs generate task-specific output tokens based on instructions, not all instructions lead to increased brain alignment across all regions, as expected. Specifically, certain instructions (IC, VQ2, and IU1) may be more effective than others in encoding brain activity. This suggests that these instructions might serve as general or umbrella categories for other instruction prompts. This is further explained through a variance partitioning approach. (2) To uncover which brain regions are responsible for visual concept understanding, we examine how different instructions from MLLMs capture various visual concepts, such as counts and recognition, as well as other concepts like color, positional understanding, and general scene understanding. We find that while MLLMs effectively capture count-related and recognition-related visual concepts across instructions, they exhibit similar brain alignment patterns for other concepts. (3) By employing a variance partitioning approach, we measure the brain response variance explained by pairs of instruction representations, distinguishing between their unique and overlapping contributions. We find that most of the variance is shared across instructions, with a high overlap between Image Captioning (IC) and other prompts but lower overlap with image understanding and
scene recognition. These results highlight a potential area for improvement in MLLMs, particularly in achieving better differentiation between various types of instructions.
(4) The layer-wise representations from MLLMs correlate with visual functional localizers. In examining layer-wise trends in brain alignment, we find that both InstructBLIP and IDEFICS demonstrate better brain alignment in middle layers for higher visual regions, while later layers align more with early visual regions. In contrast, mPLUG-Owl achieves higher brain alignment in the later layers for both high-level and early visual brain regions. This highlights the differences in information processing across the layers of MLLMs. We make the code publicly available<a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#footnote1" title="footnote 1 ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">1</span></a>. Lastly, we discuss limitations of our work in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A14" title="Appendix N Limitations ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">N</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Ethics Statement</h2>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We did not create any new neural recordings data as part of this work. We used the NSD dataset which is publicly available without any restrictions. NSD dataset can be downloaded from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://naturalscenesdataset.org/" title="">https://naturalscenesdataset.org/</a>. Please read their terms of use<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://cvnlab.slite.page/p/IB6BSeW_7o/Terms-and-Conditions" title="">https://cvnlab.slite.page/p/IB6BSeW_7o/Terms-and-Conditions</a></span></span></span> for more details.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">We do not foresee any harmful uses of this technology.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Abdin et al. (2024)</span>
<span class="ltx_bibblock">
Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al.

</span>
<span class="ltx_bibblock">Phi-3 technical report: A highly capable language model locally on your phone.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">arXiv preprint arXiv:2404.14219</em>, August 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Allen et al. (2022)</span>
<span class="ltx_bibblock">
Emily J Allen, Ghislain St-Yves, Yihan Wu, Jesse L Breedlove, Jacob S Prince, Logan T Dowdle, Matthias Nau, Brad Caron, Franco Pestilli, Ian Charest, et al.

</span>
<span class="ltx_bibblock">A massive 7t fmri dataset to bridge cognitive neuroscience and artificial intelligence.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Nature Neuroscience</em>, 25(1):116–126, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aw &amp; Toneva (2023)</span>
<span class="ltx_bibblock">
Khai Loong Aw and Mariya Toneva.

</span>
<span class="ltx_bibblock">Training language models to summarize narratives improves brain alignment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aw et al. (2023)</span>
<span class="ltx_bibblock">
Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut.

</span>
<span class="ltx_bibblock">Instruction-tuning aligns llms to the human brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2312.00575</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caucheteux &amp; King (2020)</span>
<span class="ltx_bibblock">
Charlotte Caucheteux and Jean-Rémi King.

</span>
<span class="ltx_bibblock">Language processing in brains and deep neural networks: computational convergence and its limits.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Nature Communications Biology</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conwell et al. (2022)</span>
<span class="ltx_bibblock">
Colin Conwell, Jacob S Prince, Kendrick N Kay, George A Alvarez, and Talia Konkle.

</span>
<span class="ltx_bibblock">What can 1.8 billion regressions tell us about the pressures shaping high-level visual representation in brains and machines?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">bioRxiv</em>, pp.  2022–03, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dai et al. (2023)</span>
<span class="ltx_bibblock">
Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi.

</span>
<span class="ltx_bibblock">Instructblip: Towards general-purpose vision-language models with instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">de Heer et al. (2017)</span>
<span class="ltx_bibblock">
Wendy A de Heer, Alexander G Huth, Thomas L Griffiths, Jack L Gallant, and Frédéric E Theunissen.

</span>
<span class="ltx_bibblock">The hierarchical cortical organization of human speech processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Journal of Neuroscience</em>, 37(27):6539–6557, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deniz et al. (2019)</span>
<span class="ltx_bibblock">
Fatma Deniz, Anwar O Nunez-Elizalde, Alexander G Huth, and Jack L Gallant.

</span>
<span class="ltx_bibblock">The representation of semantic information across human cerebral cortex during listening versus reading is invariant to stimulus modality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Journal of Neuroscience</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doerig et al. (2022)</span>
<span class="ltx_bibblock">
Adrien Doerig, Tim C Kietzmann, Emily Allen, Yihan Wu, Thomas Naselaris, Kendrick Kay, and Ian Charest.

</span>
<span class="ltx_bibblock">Semantic scene descriptions as an objective of human vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">arXiv preprint arXiv:2209.11737</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy et al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al.

</span>
<span class="ltx_bibblock">The llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2407.21783</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gauthier et al. (2003)</span>
<span class="ltx_bibblock">
Isabel Gauthier, Thomas W James, Kim M Curby, and Michael J Tarr.

</span>
<span class="ltx_bibblock">The influence of conceptual knowledge on visual discrimination.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Cognitive Neuropsychology</em>, 20(3-6):507–523, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldstein et al. (2022)</span>
<span class="ltx_bibblock">
Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi Aubrey, Samuel A Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, et al.

</span>
<span class="ltx_bibblock">Shared computational principles for language processing in humans and deep language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Nature Neuroscience</em>, 25(3):369–380, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain &amp; Huth (2018)</span>
<span class="ltx_bibblock">
Shailee Jain and Alexander G Huth.

</span>
<span class="ltx_bibblock">Incorporating context into language encoding models for fmri.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">NIPS</em>, pp.  6629–6638, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jiang et al. (2023)</span>
<span class="ltx_bibblock">
Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al.

</span>
<span class="ltx_bibblock">Mistral 7b.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">arXiv preprint arXiv:2310.06825</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Laurençon et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Laurençon, Lucile Saulnier, Léo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al.

</span>
<span class="ltx_bibblock">Obelics: An open web-scale filtered dataset of interleaved image-text documents.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">Advances in Neural Information Processing Systems</em>, 36:71683–71702, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">LeBel et al. (2021)</span>
<span class="ltx_bibblock">
Amanda LeBel, Shailee Jain, and Alexander G Huth.

</span>
<span class="ltx_bibblock">Voxelwise encoding models show that cerebellar language representations are highly conceptual.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Journal of Neuroscience</em>, 41(50):10341–10355, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2023)</span>
<span class="ltx_bibblock">
Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.

</span>
<span class="ltx_bibblock">Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">International Conference on Machine Learning</em>, pp.  19730–19742. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lin et al. (2014)</span>
<span class="ltx_bibblock">
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence Zitnick.

</span>
<span class="ltx_bibblock">Microsoft coco: Common objects in context.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Computer Vision–ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13</em>, pp.  740–755. Springer, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2024)</span>
<span class="ltx_bibblock">
Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee.

</span>
<span class="ltx_bibblock">Visual instruction tuning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Logothetis (2008)</span>
<span class="ltx_bibblock">
Nikos K Logothetis.

</span>
<span class="ltx_bibblock">What we can do and what we cannot do with fmri.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Nature</em>, 453(7197):869–878, 2008.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Loong Aw et al. (2024)</span>
<span class="ltx_bibblock">
Khai Loong Aw, Syrielle Montariol, Badr AlKhamissi, Martin Schrimpf, and Antoine Bosselut.

</span>
<span class="ltx_bibblock">Instruction-tuning aligns llms to the human brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">First Conference on Language Modeling</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakagi et al. (2024)</span>
<span class="ltx_bibblock">
Yuko Nakagi, Takuya Matsuyama, Naoko Koide-Majima, Hiroto Yamaguchi, Rieko Kubo, Shinji Nishimoto, and Yu Takagi.

</span>
<span class="ltx_bibblock">The brain tells a story: Unveiling distinct representations of semantic content in speech, objects, and stories in the human brain with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">bioRxiv</em>, pp.  2024–02, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota et al. (2022a)</span>
<span class="ltx_bibblock">
Subba Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish Gupta, and Bapi Surampudi.

</span>
<span class="ltx_bibblock">Neural language taskonomy: Which nlp tasks are the most predictive of fmri brain activity?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pp.  3220–3237, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota et al. (2022b)</span>
<span class="ltx_bibblock">
Subba Reddy Oota, Jashn Arora, Vijay Rowtula, Manish Gupta, and Raju S Bapi.

</span>
<span class="ltx_bibblock">Visio-linguistic brain encoding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">COLING</em>, pp.  116–133, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota et al. (2023)</span>
<span class="ltx_bibblock">
Subba Reddy Oota, Agarwal Veeral, Marreddy Mounika, Gupta Manish, and Raju Surampudi Bapi.

</span>
<span class="ltx_bibblock">Speech taskonomy: Which speech tasks are the most predictive of fmri brain activity?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">24th INTERSPEECH Conference</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota et al. (2024a)</span>
<span class="ltx_bibblock">
Subba Reddy Oota, Emin Çelik, Fatma Deniz, and Mariya Toneva.

</span>
<span class="ltx_bibblock">Speech language models lack important brain-relevant semantics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  8503–8528, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota et al. (2024b)</span>
<span class="ltx_bibblock">
Subba Reddy Oota, Zijiao Chen, Manish Gupta, Bapi Raju Surampudi, Gaël Jobard, Frédéric Alexandre, and Xavier Hinaut.

</span>
<span class="ltx_bibblock">Deep neural networks and brain alignment: Brain encoding and decoding (survey).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Transactions on Machine Learning Research Journal</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota et al. (2024c)</span>
<span class="ltx_bibblock">
Subba Reddy Oota, Manish Gupta, and Mariya Toneva.

</span>
<span class="ltx_bibblock">Joint processing of linguistic properties in brains and language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024c.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popham et al. (2021)</span>
<span class="ltx_bibblock">
Sara F Popham, Alexander G Huth, Natalia Y Bilenko, Fatma Deniz, James S Gao, Anwar O Nunez-Elizalde, and Jack L Gallant.

</span>
<span class="ltx_bibblock">Visual and linguistic semantic representations are aligned at the border of human visual cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Nature Neuroscience</em>, 24(11):1628–1636, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021a)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision, 2021a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2103.00020" title="">https://arxiv.org/abs/2103.00020</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2021b)</span>
<span class="ltx_bibblock">
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Image</em>, 2:T2, 2021b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schrimpf et al. (2021)</span>
<span class="ltx_bibblock">
Martin Schrimpf, Idan Asher Blank, Greta Tuckute, Carina Kauf, Eghbal A Hosseini, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko.

</span>
<span class="ltx_bibblock">The neural architecture of language: Integrative modeling converges on predictive processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the National Academy of Sciences</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scotti et al. (2024a)</span>
<span class="ltx_bibblock">
Paul Scotti, Atmadeep Banerjee, Jimmie Goode, Stepan Shabalin, Alex Nguyen, Aidan Dempster, Nathalie Verlinde, Elad Yundler, David Weisberg, Kenneth Norman, et al.

</span>
<span class="ltx_bibblock">Reconstructing the mind’s eye: fmri-to-image with contrastive learning and diffusion priors.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Scotti et al. (2024b)</span>
<span class="ltx_bibblock">
Paul Steven Scotti, Mihir Tripathy, Cesar Torrico, Reese Kneeland, Tong Chen, Ashutosh Narang, Charan Santhirasegaran, Jonathan Xu, Thomas Naselaris, Kenneth A Norman, et al.

</span>
<span class="ltx_bibblock">Mindeye2: Shared-subject models enable fmri-to-image with 1 hour of data.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">Forty-first International Conference on Machine Learning</em>, 2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun &amp; Moens (2023)</span>
<span class="ltx_bibblock">
Jingyuan Sun and Marie-Francine Moens.

</span>
<span class="ltx_bibblock">Fine-tuned vs. prompt-tuned supervised representations: which better account for brain language representations?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence</em>, pp.  5197–5205, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sun et al. (2023)</span>
<span class="ltx_bibblock">
Jingyuan Sun, Xiaohan Zhang, and Marie-Francine Moens.

</span>
<span class="ltx_bibblock">Tuning in to neural encoding: Linking human brain and artificial supervised representations of language.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">ECAI 2023</em>, pp.  2258–2265. IOS Press, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2024)</span>
<span class="ltx_bibblock">
Jerry Tang, Meng Du, Vy Vo, Vasudev Lal, and Alexander Huth.

</span>
<span class="ltx_bibblock">Brain encoding models based on multimodal transformers can transfer across language and vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Taori et al. (2023)</span>
<span class="ltx_bibblock">
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori B Hashimoto.

</span>
<span class="ltx_bibblock">Stanford alpaca: An instruction-following llama model, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva &amp; Wehbe (2019)</span>
<span class="ltx_bibblock">
Mariya Toneva and Leila Wehbe.

</span>
<span class="ltx_bibblock">Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Advances in Neural Information Processing Systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Touvron et al. (2023)</span>
<span class="ltx_bibblock">
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al.

</span>
<span class="ltx_bibblock">Llama: Open and efficient foundation language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tuckute et al. (2023)</span>
<span class="ltx_bibblock">
Greta Tuckute, Jenelle Feather, Dana Boebinger, and Josh H McDermott.

</span>
<span class="ltx_bibblock">Many but not all deep neural network audio models capture brain responses and exhibit correspondence between model stages and brain regions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Plos Biology</em>, 21(12):e3002366, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaidya et al. (2022)</span>
<span class="ltx_bibblock">
Aditya Vaidya, Shailee Jain, and Alexander Huth.

</span>
<span class="ltx_bibblock">Self-supervised models of audio effectively explain human cortical responses to speech.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">International Conference on Machine Learning</em>, pp.  21927–21944. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Aria Wang, Michael Tarr, and Leila Wehbe.

</span>
<span class="ltx_bibblock">Neural taskonomy: Inferring the similarity of task-derived representations from brain activity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">Advances in Neural Information Processing Systems</em>, 32:15501–15511, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2022)</span>
<span class="ltx_bibblock">
Aria Y Wang, Kendrick Kay, Thomas Naselaris, Michael J Tarr, and Leila Wehbe.

</span>
<span class="ltx_bibblock">Natural language supervision with a large and diverse dataset builds better models of human high-level visual cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">BioRxiv</em>, pp.  2022–09, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf et al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, et al.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations</em>, pp.  38–45, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Xu et al. (2023)</span>
<span class="ltx_bibblock">
Zhiyang Xu, Ying Shen, and Lifu Huang.

</span>
<span class="ltx_bibblock">Multiinstruct: Improving multi-modal zero-shot learning via instruction tuning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  11445–11465, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ye et al. (2023)</span>
<span class="ltx_bibblock">
Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang, Anwen Hu, Pengcheng Shi, Yaya Shi, et al.

</span>
<span class="ltx_bibblock">mplug-owl: Modularization empowers large language models with multimodality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">arXiv preprint arXiv:2304.14178</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhu et al. (2024)</span>
<span class="ltx_bibblock">
Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi.

</span>
<span class="ltx_bibblock">Multimodal c4: An open, billion-scale corpus of images interleaved with text.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Overview of Appendix Sections</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A2" title="Appendix B Visual functional localizers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">B</span></a>: Visual functional localizers</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A3" title="Appendix C Cross-subject brain predictivity ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">C</span></a>: Cross-subject brain predictivity</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A4" title="Appendix D Details of MLLMs with training details and their parameters ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">D</span></a> Details of MLLMs with training details and their parameters</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5" title="Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">E</span></a>: Model generated outputs across instructions</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A6" title="Appendix F Normalized brain alignment: Visual Functional Localizers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">F</span></a>: Normalized brain alignment: Visual functional localizers</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i6.p1">
<p class="ltx_p" id="A1.I1.i6.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A7" title="Appendix G Brain Maps for Task-specific Instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">G</span></a>: Brain Maps for Task-specific Instructions</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i7.p1">
<p class="ltx_p" id="A1.I1.i7.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A8" title="Appendix H Brain Maps for different MLLM Layers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">H</span></a>: Brain Maps for different MLLM Layers</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i8.p1">
<p class="ltx_p" id="A1.I1.i8.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A9" title="Appendix I ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">I</span></a>: ROI Analysis: Shared and Unique variance across task-specific instructions.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i9.p1">
<p class="ltx_p" id="A1.I1.i9.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A10" title="Appendix J Normalized Brain Alignment across different image categories ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">J</span></a>: Normalized Brain Alignment across different image categories.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i10.p1">
<p class="ltx_p" id="A1.I1.i10.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A11" title="Appendix K Comparison of Instruction-tuned MLLMs, Non-Instruction-tuned MLLMs and text-based LLMs ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">K</span></a>: Comparison of Instruction-tuned MLLMs, Non-Instruction-tuned MLLMs and text-based LLMs.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i11.p1">
<p class="ltx_p" id="A1.I1.i11.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12" title="Appendix L Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">L</span></a>: Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A1.I1.i12.p1">
<p class="ltx_p" id="A1.I1.i12.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A13" title="Appendix M Image only / Instruction only input to the instruction-tuned MLLM ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">M</span></a>: Image only / Instruction only input to the instruction-tuned MLLM.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i13" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i13.p1">
<p class="ltx_p" id="A1.I1.i13.p1.1">Section <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A14" title="Appendix N Limitations ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">N</span></a>: Limitations.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Visual functional localizers</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">The NSD data covers five brain regions of interest (ROIs) in the human brain with the following sub-divisions: early visual (pRF-Visual ROIs) and high-level visual (floc-bodies, floc-words, floc-faces and floc-places) <cite class="ltx_cite ltx_citemacro_citep">(Allen et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib2" title="">2022</a>)</cite>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A2.p2">
<ul class="ltx_itemize" id="A2.I1">
<li class="ltx_item" id="A2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i1.p1">
<p class="ltx_p" id="A2.I1.i1.p1.1">floc-bodies is a collection of manually drawn ROIs based on results of the floc experiment. These ROIs consist of EBA, FBA-1, FBA-2, and mTL-bodies (“mid temporal lobe bodies”).</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i2.p1">
<p class="ltx_p" id="A2.I1.i2.p1.1">floc-words is a collection of manually drawn ROIs based on results of the floc experiment. These ROIs consist of OWFA, VWFA-1, VWFA-2, mfs-words (“mid fusiform sulcus words”), and mTL-words (“mid temporal lobe words”).</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i3.p1">
<p class="ltx_p" id="A2.I1.i3.p1.1">floc-faces is a collection of manually drawn ROIs based on results of the floc experiment. These ROIs consist of OFA, FFA-1, FFA-2, mTL-faces (“mid temporal lobe faces”), and aTL-faces (“anterior temporal lobe faces”).</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A2.I1.i4.p1">
<p class="ltx_p" id="A2.I1.i4.p1.1">floc-places is a collection of manually drawn ROIs based on results of the floc experiment. These ROIs consist of OPA, PPA, and RSC.</p>
</div>
</li>
<li class="ltx_item" id="A2.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A2.I1.i5.p1">
<p class="ltx_p" id="A2.I1.i5.p1.1">pRF-Visual ROIs (Retinotopic Early Visual) is a collection of manually drawn ROIs based on results of the prf experiment. These ROIs consist of V1v, V1d, V2v, V2d, V3v, V3d, and hV4.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Cross-subject brain predictivity</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">We estimate cross-subject prediction accuracy for the NSD dataset and present the average accuracy across voxels for each subject in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A3.F8" title="Figure 8 ‣ Appendix C Cross-subject brain predictivity ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">8</span></a>. The figure suggests that the cross-subject prediction accuracy is consistent across subjects, indicating that all subjects share a similar amount of explainable variance.</p>
</div>
<figure class="ltx_figure" id="A3.F8"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="265" id="A3.F8.g1" src="x12.png" width="249"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Cross-subject prediction accuracy for each subject of NSD dataset.</figcaption>
</figure>
<figure class="ltx_figure" id="A3.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A3.F9.1" style="width:397.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="207" id="A3.F9.1.g1" src="extracted/6472677/images/noise_ceiling_s1.jpg" width="449"/>
<p class="ltx_p ltx_align_center" id="A3.F9.1.1">(a) Subject-01</p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A3.F9.2" style="width:397.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="198" id="A3.F9.2.g1" src="extracted/6472677/images/noise_ceiling_s2.jpg" width="449"/>
<p class="ltx_p ltx_align_center" id="A3.F9.2.1">(b) Subject-02</p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A3.F9.3" style="width:397.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="205" id="A3.F9.3.g1" src="extracted/6472677/images/noise_ceiling_s5.jpg" width="449"/>
<p class="ltx_p ltx_align_center" id="A3.F9.3.1">(c) Subject-05</p>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A3.F9.4" style="width:397.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="215" id="A3.F9.4.g1" src="extracted/6472677/images/noise_ceiling_s7.jpg" width="449"/>
<p class="ltx_p ltx_align_center" id="A3.F9.4.1">(d) Subject-07</p>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Contrast of estimated cross-subject prediction accuracy for the participants for the NSD dataset.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Details of MLLMs with training details and their parameters</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">We provide details about model parameters, training dataset details, training procedure details and task-specific instructions for instruction tuning of these models in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A4.T2" title="Table 2 ‣ Appendix D Details of MLLMs with training details and their parameters ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">2</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p2">
<p class="ltx_p" id="A4.p2.1"><span class="ltx_text ltx_font_bold" id="A4.p2.1.1">InstructBLIP</span> <cite class="ltx_cite ltx_citemacro_citep">(Dai et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib7" title="">2023</a>)</cite> is a vision-language instruction-tuned model built upon the pretrained BLIP-2 model <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib19" title="">2023</a>)</cite>. It leverages a diverse set of instruction data (26 different datasets) to train a MLLM, which comprises an image encoder, a large language model (LLM), and a Query Transformer (Q-Former) that serves as a bridge between the two. We utilize the instructblip-vicuna-7b version, which consists of 32 layers and produces 4096-dimensional representations.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p3">
<p class="ltx_p" id="A4.p3.1"><span class="ltx_text ltx_font_bold" id="A4.p3.1.1">mPLUG-Owl</span> <cite class="ltx_cite ltx_citemacro_citep">(Ye et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib49" title="">2023</a>)</cite> is an MLLM designed to perceive and integrate multiple modalities (visual and language) while considering visual context and information to generate corresponding outputs. The model is trained on a language modeling task, which involves learning to generate subsequent tokens based on the preceding context. We utilize the mplug-owl-llama-7b version, which consists of 32 layers and 4096-dimensional representations.</p>
</div>
<div class="ltx_para ltx_noindent" id="A4.p4">
<p class="ltx_p" id="A4.p4.1"><span class="ltx_text ltx_font_bold" id="A4.p4.1.1">IDEFICS</span> <cite class="ltx_cite ltx_citemacro_citep">(Laurençon et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib17" title="">2023</a>)</cite> is an MLLM based on Flamingo <cite class="ltx_cite ltx_citemacro_citep">(Zhu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib50" title="">2024</a>)</cite>, which accepts arbitrary sequences of image and text inputs and generates text tokens. We utilize the idefics-9b-instruct version (the model obtained by further training IDEFICS on supervised fine-tuning and instruction fine-tuning datasets), which consists of 32 layers and 4096-dimensional representations.</p>
</div>
<figure class="ltx_table" id="A4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>MLLM Training Details and Parameters</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A4.T2.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T2.1.1.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A4.T2.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.1.1.1.1.1">Model</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T2.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.1.1.1.2.1">Architecture</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.1.1.3.1">
<span class="ltx_p" id="A4.T2.1.1.1.3.1.1" style="width:56.9pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.1.1.1.3.1.1.1">Training Dataset</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.1.1.4.1">
<span class="ltx_p" id="A4.T2.1.1.1.4.1.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.1.1.1.4.1.1.1">Task-Specific Instructions</span></span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T2.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.1.1.1.5.1"># Parameters</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.1.1.6.1">
<span class="ltx_p" id="A4.T2.1.1.1.6.1.1" style="width:68.3pt;"><span class="ltx_text ltx_font_bold" id="A4.T2.1.1.1.6.1.1.1">Training Procedure</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T2.1.2.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A4.T2.1.2.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">InstructBLIP</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T2.1.2.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">Transformer-based multimodal</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.2.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.2.2.3.1">
<span class="ltx_p" id="A4.T2.1.2.2.3.1.1" style="width:56.9pt;">Large-scale image-text pairs; instruction-following datasets</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.2.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.2.2.4.1">
<span class="ltx_p" id="A4.T2.1.2.2.4.1.1" style="width:68.3pt;">Image captioning, visual question answering, image understanding</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T2.1.2.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">12B</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.2.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.2.2.6.1">
<span class="ltx_p" id="A4.T2.1.2.2.6.1.1" style="width:68.3pt;">Supervised on image-text data, followed by instruction-tuning</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T2.1.3.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A4.T2.1.3.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">mPLUG-OWL</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T2.1.3.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">Vision-language transformer</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.3.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.3.3.3.1">
<span class="ltx_p" id="A4.T2.1.3.3.3.1.1" style="width:56.9pt;">Image-text pairs; multimodal datasets</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.3.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.3.3.4.1">
<span class="ltx_p" id="A4.T2.1.3.3.4.1.1" style="width:68.3pt;">Visual reasoning, image captioning, text generation</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T2.1.3.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">14B</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T2.1.3.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.3.3.6.1">
<span class="ltx_p" id="A4.T2.1.3.3.6.1.1" style="width:68.3pt;">Masked language modeling, next-word prediction, visual grounding, instruction-tuning</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T2.1.4.4">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A4.T2.1.4.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">IDEFICS</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A4.T2.1.4.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">Multimodal transformer</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T2.1.4.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.4.4.3.1">
<span class="ltx_p" id="A4.T2.1.4.4.3.1.1" style="width:56.9pt;">COCO, Visual Genome; instruction-tuning</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T2.1.4.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.4.4.4.1">
<span class="ltx_p" id="A4.T2.1.4.4.4.1.1" style="width:68.3pt;">Image captioning, visual question answering, scene understanding</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A4.T2.1.4.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">10B</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T2.1.4.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T2.1.4.4.6.1">
<span class="ltx_p" id="A4.T2.1.4.4.6.1.1" style="width:68.3pt;">Contrastive learning on image-text pairs, task-specific fine-tuning</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para ltx_noindent" id="A4.p5">
<p class="ltx_p" id="A4.p5.4"><span class="ltx_text ltx_font_bold" id="A4.p5.4.1">Implementation details for reproducibility.</span>
All feature extraction experiments were conducted on a machine equipped with an NVIDIA A100 GPU with 80 GB of GPU RAM, partitioned into two devices of 40 GB each. The voxelwise encoding models were trained on NVIDIA GeForce RTX 3050 GPU with 4GB of GPU RAM. We used bootstrap ridge-regression with the following parameters: MSE loss function; L2-decay (<math alttext="\lambda" class="ltx_Math" display="inline" id="A4.p5.1.m1.1"><semantics id="A4.p5.1.m1.1a"><mi id="A4.p5.1.m1.1.1" xref="A4.p5.1.m1.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A4.p5.1.m1.1b"><ci id="A4.p5.1.m1.1.1.cmml" xref="A4.p5.1.m1.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A4.p5.1.m1.1d">italic_λ</annotation></semantics></math>) varied from 10<sup class="ltx_sup" id="A4.p5.4.2"><span class="ltx_text ltx_font_italic" id="A4.p5.4.2.1">-1</span></sup> to 10<sup class="ltx_sup" id="A4.p5.4.3"><span class="ltx_text ltx_font_italic" id="A4.p5.4.3.1">3</span></sup>; the best <math alttext="\lambda" class="ltx_Math" display="inline" id="A4.p5.4.m4.1"><semantics id="A4.p5.4.m4.1a"><mi id="A4.p5.4.m4.1.1" xref="A4.p5.4.m4.1.1.cmml">λ</mi><annotation-xml encoding="MathML-Content" id="A4.p5.4.m4.1b"><ci id="A4.p5.4.m4.1.1.cmml" xref="A4.p5.4.m4.1.1">𝜆</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p5.4.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A4.p5.4.m4.1d">italic_λ</annotation></semantics></math> was chosen by tuning on validation data that comprised a randomly chosen 10% subset from the train set used only for hyper-parameter tuning.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Model generated outputs across instructions</h2>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">Tables <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T3" title="Table 3 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T4" title="Table 4 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T5" title="Table 5 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T6" title="Table 6 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">6</span></a> show model generated outputs for a sample image from the NSD dataset using InstructBLIP, mPLUG-Owl, IDEFICS and BLIP-2 models respectively.</p>
</div>
<figure class="ltx_table" id="A5.T3">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 3: </span>InstructBLIP generated outputs for a sample image from the NSD dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A5.T3.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T3.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2" id="A5.T3.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="180" id="A5.T3.1.1.1.1.g1" src="extracted/6472677/sample_nsd_images/image_29981.png" width="180"/></td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T3.1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.1.1.1">Instruction</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.2.1.2.1">
<span class="ltx_p" id="A5.T3.1.1.2.1.2.1.1" style="width:284.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T3.1.1.2.1.2.1.1.1">Model Output</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T3.1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T3.1.1.3.2.1.1">Image Captioning</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.3.2.2.1">
<span class="ltx_p" id="A5.T3.1.1.3.2.2.1.1" style="width:284.5pt;">a train is pulling into a train station</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T3.1.1.4.3.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T3.1.1.4.3.1.1">Visual Question Answering</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.4.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.4.3.2.1">
<span class="ltx_p" id="A5.T3.1.1.4.3.2.1.1" style="width:284.5pt;">The image features a blue and yellow train pulling in train station.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.5.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.5.4.1.1">
<span class="ltx_p" id="A5.T3.1.1.5.4.1.1.1" style="width:284.5pt;">No, there are no people in this image.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.6.5.1.1">
<span class="ltx_p" id="A5.T3.1.1.6.5.1.1.1" style="width:284.5pt;">The foreground of the image features a train, while the background consists of a train station.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T3.1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">Visual Relationship</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.7.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.7.6.2.1">
<span class="ltx_p" id="A5.T3.1.1.7.6.2.1.1" style="width:284.5pt;">In the image, the train is moving through a train station and interacting with a person who has their back turned to it.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T3.1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">Commonsense Reasoning</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.8.7.2.1">
<span class="ltx_p" id="A5.T3.1.1.8.7.2.1.1" style="width:284.5pt;">The image shows a train station.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T3.1.1.9.8.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T3.1.1.9.8.1.1">Image Understanding</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.9.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.9.8.2.1">
<span class="ltx_p" id="A5.T3.1.1.9.8.2.1.1" style="width:284.5pt;">The most dominant color in the image is blue.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.10.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.10.9.1.1">
<span class="ltx_p" id="A5.T3.1.1.10.9.1.1.1" style="width:284.5pt;">There are no food items visible in the image.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T3.1.1.11.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.11.10.1.1">
<span class="ltx_p" id="A5.T3.1.1.11.10.1.1.1" style="width:284.5pt;">There are two animals in the image.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T3.1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A5.T3.1.1.12.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">Scene Recognition</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A5.T3.1.1.12.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T3.1.1.12.11.2.1">
<span class="ltx_p" id="A5.T3.1.1.12.11.2.1.1" style="width:284.5pt;">The image features a yellow color.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A5.T4">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 4: </span>mPLUG-Owl generated outputs for a sample image from the NSD dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A5.T4.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T4.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2" id="A5.T4.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="180" id="A5.T4.1.1.1.1.g1" src="extracted/6472677/sample_nsd_images/image_31660.png" width="180"/></td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T4.1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="A5.T4.1.1.2.1.1.1">Instruction</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.2.1.2.1">
<span class="ltx_p" id="A5.T4.1.1.2.1.2.1.1" style="width:284.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T4.1.1.2.1.2.1.1.1">Model Output</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T4.1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T4.1.1.3.2.1.1">Image Captioning</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.3.2.2.1">
<span class="ltx_p" id="A5.T4.1.1.3.2.2.1.1" style="width:284.5pt;">A group of kids is riding their skateboards on a street.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T4.1.1.4.3.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T4.1.1.4.3.1.1">Visual Question Answering</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.4.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.4.3.2.1">
<span class="ltx_p" id="A5.T4.1.1.4.3.2.1.1" style="width:284.5pt;">The image features a young boy wearing a red shirt and a black hat while riding a skateboard in a skate park. There are several other children around, watching or engaging with the skateboarder on their own skateboards.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.5.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.5.4.1.1">
<span class="ltx_p" id="A5.T4.1.1.5.4.1.1.1" style="width:284.5pt;">Yes, there are several people in the image, including a boy who appears to be skateboarding and another boy who is watching the skateboarder, possibly learning to skateboard or observing the skill of another rider.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.6.5.1.1">
<span class="ltx_p" id="A5.T4.1.1.6.5.1.1.1" style="width:284.5pt;">In the foreground of the image, there is a group of young people, including a boy and a few other kids riding skateboards on a dirt lot or park with a skateboard ramp. In the background, a car can be seen parked near the skateboard area.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T4.1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">Visual Relationship</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.7.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.7.6.2.1">
<span class="ltx_p" id="A5.T4.1.1.7.6.2.1.1" style="width:284.5pt;">In the image, a group of young people are playing on skateboards and riding bikes in a city setting. There is both a skateboarder and a person on a bike in focus.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T4.1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">Commonsense Reasoning</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.8.7.2.1">
<span class="ltx_p" id="A5.T4.1.1.8.7.2.1.1" style="width:284.5pt;">The image shows a playground environment, specifically a small skatepark area with a few people engaged in various activities such as rollerblading, skateboarding, and biking.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T4.1.1.9.8.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T4.1.1.9.8.1.1">Image Understanding</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.9.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.9.8.2.1">
<span class="ltx_p" id="A5.T4.1.1.9.8.2.1.1" style="width:284.5pt;">The most dominant color in the image is red.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.10.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.10.9.1.1">
<span class="ltx_p" id="A5.T4.1.1.10.9.1.1.1" style="width:284.5pt;">In the image, a few food items are visible, including a cup of ice cream and a bottle of water.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T4.1.1.11.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.11.10.1.1">
<span class="ltx_p" id="A5.T4.1.1.11.10.1.1.1" style="width:284.5pt;">There are two animals in the image - a dog and a cat.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T4.1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A5.T4.1.1.12.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">Scene Recognition</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A5.T4.1.1.12.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T4.1.1.12.11.2.1">
<span class="ltx_p" id="A5.T4.1.1.12.11.2.1.1" style="width:284.5pt;">The scene in the image showcases a natural outdoor skate park, where several young skateboarders are enjoying their time on the ramps and rails.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A5.T5">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 5: </span>IDEFICS generated outputs for a sample image from the NSD dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A5.T5.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T5.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2" id="A5.T5.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="180" id="A5.T5.1.1.1.1.g1" src="extracted/6472677/sample_nsd_images/image_33814.png" width="180"/></td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T5.1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="A5.T5.1.1.2.1.1.1">Instruction</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.2.1.2.1">
<span class="ltx_p" id="A5.T5.1.1.2.1.2.1.1" style="width:284.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T5.1.1.2.1.2.1.1.1">Model Output</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T5.1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T5.1.1.3.2.1.1">Image Captioning</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.3.2.2.1">
<span class="ltx_p" id="A5.T5.1.1.3.2.2.1.1" style="width:284.5pt;">In a crowded outdoor marketplace, the sun beats down on a man who is standing beneath a large bunch of bright yellow bananas.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T5.1.1.4.3.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T5.1.1.4.3.1.1">Visual Question Answering</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.4.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.4.3.2.1">
<span class="ltx_p" id="A5.T5.1.1.4.3.2.1.1" style="width:284.5pt;">This image features a man in an indoor market area, surrounded by a large number of ripe bananas hanging from a structure.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.5.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.5.4.1.1">
<span class="ltx_p" id="A5.T5.1.1.5.4.1.1.1" style="width:284.5pt;">Yes, there are several people in the image, where a man, who can be seen in the background wearing a blue jacket.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.6.5.1.1">
<span class="ltx_p" id="A5.T5.1.1.6.5.1.1.1" style="width:284.5pt;">In the foreground, there are rows of bananas hanging in a marketplace. In the background, people are walking around in the area.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T5.1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">Visual Relationship</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.7.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.7.6.2.1">
<span class="ltx_p" id="A5.T5.1.1.7.6.2.1.1" style="width:284.5pt;">There is no interaction between animate and inanimate objects.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T5.1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">Commonsense Reasoning</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.8.7.2.1">
<span class="ltx_p" id="A5.T5.1.1.8.7.2.1.1" style="width:284.5pt;">The image shows a market environment with a man standing next to hanging bunches of bananas.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T5.1.1.9.8.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T5.1.1.9.8.1.1">Image Understanding</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.9.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.9.8.2.1">
<span class="ltx_p" id="A5.T5.1.1.9.8.2.1.1" style="width:284.5pt;">The dominant color of the image appears to be yellow, as there is a lot of yellow in the image, possibly from the color of bananas.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.10.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.10.9.1.1">
<span class="ltx_p" id="A5.T5.1.1.10.9.1.1.1" style="width:284.5pt;">In the image, Bananas are visible.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T5.1.1.11.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.11.10.1.1">
<span class="ltx_p" id="A5.T5.1.1.11.10.1.1.1" style="width:284.5pt;">There are two animals in the image.</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T5.1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A5.T5.1.1.12.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">Scene Recognition</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A5.T5.1.1.12.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T5.1.1.12.11.2.1">
<span class="ltx_p" id="A5.T5.1.1.12.11.2.1.1" style="width:284.5pt;">In the image, there is a section of a banana stand with bananas displayed on the wall.</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="A5.T6">
<figcaption class="ltx_caption"><span class="ltx_tag ltx_tag_table">Table 6: </span>BLIP-2 generated outputs for a sample image from the NSD dataset.</figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A5.T6.1.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A5.T6.1.1.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" colspan="2" id="A5.T6.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="180" id="A5.T6.1.1.1.1.g1" src="extracted/6472677/sample_nsd_images/image_29981.png" width="180"/></td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T6.1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text ltx_font_bold" id="A5.T6.1.1.2.1.1.1">Instruction</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.2.1.2.1">
<span class="ltx_p" id="A5.T6.1.1.2.1.2.1.1" style="width:284.5pt;"><span class="ltx_text ltx_font_bold" id="A5.T6.1.1.2.1.2.1.1.1">Model Output</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.3.2">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T6.1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T6.1.1.3.2.1.1">Image Captioning</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.3.2.2.1">
<span class="ltx_p" id="A5.T6.1.1.3.2.2.1.1" style="width:284.5pt;">A passenger train pulling into a train station</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.4.3">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T6.1.1.4.3.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T6.1.1.4.3.1.1">Visual Question Answering</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.4.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.4.3.2.1">
<span class="ltx_p" id="A5.T6.1.1.4.3.2.1.1" style="width:284.5pt;">A train</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.5.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.5.4.1.1">
<span class="ltx_p" id="A5.T6.1.1.5.4.1.1.1" style="width:284.5pt;">No</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.6.5.1.1">
<span class="ltx_p" id="A5.T6.1.1.6.5.1.1.1" style="width:284.5pt;">A blue train on a train track with a platform on the side</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.7.6">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T6.1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">Visual Relationship</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.7.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.7.6.2.1">
<span class="ltx_p" id="A5.T6.1.1.7.6.2.1.1" style="width:284.5pt;">It’s a physical one</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.8.7">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T6.1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">Commonsense Reasoning</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.8.7.2.1">
<span class="ltx_p" id="A5.T6.1.1.8.7.2.1.1" style="width:284.5pt;">A train station</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.9.8">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="A5.T6.1.1.9.8.1" rowspan="3" style="padding-left:2.0pt;padding-right:2.0pt;"><span class="ltx_text" id="A5.T6.1.1.9.8.1.1">Image Understanding</span></td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.9.8.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.9.8.2.1">
<span class="ltx_p" id="A5.T6.1.1.9.8.2.1.1" style="width:284.5pt;">Blue</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.10.9.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.10.9.1.1">
<span class="ltx_p" id="A5.T6.1.1.10.9.1.1.1" style="width:284.5pt;">Sand</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A5.T6.1.1.11.10.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.11.10.1.1">
<span class="ltx_p" id="A5.T6.1.1.11.10.1.1.1" style="width:284.5pt;">No animals</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A5.T6.1.1.12.11">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A5.T6.1.1.12.11.1" style="padding-left:2.0pt;padding-right:2.0pt;">Scene Recognition</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A5.T6.1.1.12.11.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A5.T6.1.1.12.11.2.1">
<span class="ltx_p" id="A5.T6.1.1.12.11.2.1.1" style="width:284.5pt;">This can be a picture, a video or a photograph</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Normalized brain alignment: Visual Functional Localizers</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A6.F10" title="Figure 10 ‣ Appendix F Normalized brain alignment: Visual Functional Localizers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">10</span></a> shows ROI-based normalized brain alignment computed by averaging across participants, layers, and voxels. The figure shows the alignment values for FLOC-BODIES, FLOC-FACES and FLOC-WORDS regions.</p>
</div>
<figure class="ltx_figure" id="A6.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="221" id="A6.F10.g1" src="x13.png" width="270"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="221" id="A6.F10.g2" src="x14.png" width="270"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="221" id="A6.F10.g3" src="x15.png" width="270"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>ROI-based normalized brain alignment was computed by averaging across participants, layers, and voxels. Blue: Average across random initialization of the 3 MLLMs. Note that CLIP-text model uses golden oracle captions while instruct models use predicted model generations. <math alttext="\ast" class="ltx_Math" display="inline" id="A6.F10.5.m1.1"><semantics id="A6.F10.5.m1.1b"><mo id="A6.F10.5.m1.1.1" mathcolor="#FF0000" xref="A6.F10.5.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="A6.F10.5.m1.1c"><ci id="A6.F10.5.m1.1.1.cmml" xref="A6.F10.5.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.F10.5.m1.1d">\ast</annotation><annotation encoding="application/x-llamapun" id="A6.F10.5.m1.1e">∗</annotation></semantics></math> indicates cases where MLLM embeddings are statistically significantly better than randomly initialized models, i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="A6.F10.6.m2.1"><semantics id="A6.F10.6.m2.1b"><mrow id="A6.F10.6.m2.1.1" xref="A6.F10.6.m2.1.1.cmml"><mi id="A6.F10.6.m2.1.1.2" xref="A6.F10.6.m2.1.1.2.cmml"></mi><mo id="A6.F10.6.m2.1.1.1" xref="A6.F10.6.m2.1.1.1.cmml">≤</mo><mn id="A6.F10.6.m2.1.1.3" xref="A6.F10.6.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.F10.6.m2.1c"><apply id="A6.F10.6.m2.1.1.cmml" xref="A6.F10.6.m2.1.1"><leq id="A6.F10.6.m2.1.1.1.cmml" xref="A6.F10.6.m2.1.1.1"></leq><csymbol cd="latexml" id="A6.F10.6.m2.1.1.2.cmml" xref="A6.F10.6.m2.1.1.2">absent</csymbol><cn id="A6.F10.6.m2.1.1.3.cmml" type="float" xref="A6.F10.6.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.F10.6.m2.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="A6.F10.6.m2.1e">≤ 0.05</annotation></semantics></math>. <math alttext="\wedge" class="ltx_Math" display="inline" id="A6.F10.7.m3.1"><semantics id="A6.F10.7.m3.1b"><mo id="A6.F10.7.m3.1.1" mathcolor="#FF0000" xref="A6.F10.7.m3.1.1.cmml">∧</mo><annotation-xml encoding="MathML-Content" id="A6.F10.7.m3.1c"><and id="A6.F10.7.m3.1.1.cmml" xref="A6.F10.7.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="A6.F10.7.m3.1d">\wedge</annotation><annotation encoding="application/x-llamapun" id="A6.F10.7.m3.1e">∧</annotation></semantics></math> indicates cases where MLLMs are significantly better than unimodal vision models (ViT-H), i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="A6.F10.8.m4.1"><semantics id="A6.F10.8.m4.1b"><mrow id="A6.F10.8.m4.1.1" xref="A6.F10.8.m4.1.1.cmml"><mi id="A6.F10.8.m4.1.1.2" xref="A6.F10.8.m4.1.1.2.cmml"></mi><mo id="A6.F10.8.m4.1.1.1" xref="A6.F10.8.m4.1.1.1.cmml">≤</mo><mn id="A6.F10.8.m4.1.1.3" xref="A6.F10.8.m4.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A6.F10.8.m4.1c"><apply id="A6.F10.8.m4.1.1.cmml" xref="A6.F10.8.m4.1.1"><leq id="A6.F10.8.m4.1.1.1.cmml" xref="A6.F10.8.m4.1.1.1"></leq><csymbol cd="latexml" id="A6.F10.8.m4.1.1.2.cmml" xref="A6.F10.8.m4.1.1.2">absent</csymbol><cn id="A6.F10.8.m4.1.1.3.cmml" type="float" xref="A6.F10.8.m4.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.F10.8.m4.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="A6.F10.8.m4.1e">≤ 0.05</annotation></semantics></math>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="A6.p2">
<p class="ltx_p" id="A6.p2.1">We conducted a non-parametric one-way ANOVA test to analyze the normalized brain alignment differences between early and higher visual ROIs for each instruction-tuned MLLM across four subjects.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p3">
<p class="ltx_p" id="A6.p3.1">The results for InstructBLIP Model are as follows. The one-way ANOVA test revealed that higher visual ROIs have significantly higher normalized brain alignment than early visual ROIs, with a p-value of 0.008 and an F-statistic of 14.60.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p4">
<p class="ltx_p" id="A6.p4.1"><span class="ltx_text ltx_font_bold" id="A6.p4.1.1">Pairwise Comparisons Between Early Visual and Higher Visual ROIs:</span> We further performed ANOVA tests between early visual ROIs and each specific higher visual ROI. The results indicate that for most higher visual ROIs, the brain alignment is significantly better compared to early visual ROIs: Early Visual vs. FLOC-PLACES (p-value: 0.006, F-statistic: 41.45), Early Visual vs. FLOC-Bodies (p-value: 0.001, F-statistic: 32.91), Early Visual vs. FLOC-Faces (p-value: 0.006, F-statistic: 41.02), Early Visual vs. FLOC-Words (p-value: 0.14 (not statistically significant), F-statistic: 2.83). These results quantitatively confirm that multimodal models such as InstructBLIP achieve significantly better alignment in higher visual ROIs than in early visual ROIs.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p5">
<p class="ltx_p" id="A6.p5.1">We performed a similar analysis to include all instruction-tuned MLLMs and conducted a one-way ANOVA test to compare normalized brain alignment between early visual and higher visual ROIs. The one-way ANOVA test revealed that higher visual ROIs have significantly higher normalized brain alignment than early visual ROIs, with a p-value of 0.009 and an F-statistic of 13.85.</p>
</div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Brain Maps for Task-specific Instructions</h2>
<div class="ltx_para ltx_noindent" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">The Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A7.F11" title="Figure 11 ‣ Appendix G Brain Maps for Task-specific Instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">11</span></a> displays brain maps for the InstructBLIP, and mPLUG-OWL for Subject 1, where the voxel color codes are projected onto the flattened cortical surface of the representative subject.</p>
</div>
<div class="ltx_para ltx_noindent" id="A7.p2">
<p class="ltx_p" id="A7.p2.1">The Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A7.F12" title="Figure 12 ‣ Appendix G Brain Maps for Task-specific Instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">12</span></a> displays brain maps for the InstructBLIP model for Subject 1, with normalized brain alignment of voxels are projected onto the flattened cortical surface of the representative subject for two concept groups: Color, Position and Scene understanding.</p>
</div>
<figure class="ltx_figure" id="A7.F11">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="305" id="A7.F11.g1" src="x16.png" width="340"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="316" id="A7.F11.g2" src="x17.png" width="340"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="291" id="A7.F11.g3" src="x18.png" width="672"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="294" id="A7.F11.g4" src="x19.png" width="672"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="293" id="A7.F11.g5" src="x20.png" width="672"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Each voxel is color coded with the instruction (out of 10) that led to the highest normalized brain alignment. The color bar highlights color codes for each instruction.
The voxels are projected onto the flattened cortical surface of a representative subject (subject S1, S2, S5 and S7) for three MLLMs.</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="315" id="A7.F12.g1" src="x21.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>
The visual concept specific voxels are projected onto the flattened cortical surface of a representative subject-1.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Brain Maps for different MLLM Layers</h2>
<div class="ltx_para ltx_noindent" id="A8.p1">
<p class="ltx_p" id="A8.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A8.F13" title="Figure 13 ‣ Appendix H Brain Maps for different MLLM Layers ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">13</span></a> presents brain maps for the InstructBLIP, mPLUG-Owl and IDEFICS, where
the voxels with their corresponding color codes are projected onto the flattened cortical surface of
the representative subjects (Subjects 2, 5 and 7).</p>
</div>
<figure class="ltx_figure" id="A8.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="81" id="A8.F13.g1" src="x22.png" width="581"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="221" id="A8.F13.g2" src="x23.png" width="755"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="225" id="A8.F13.g3" src="x24.png" width="756"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="232" id="A8.F13.g4" src="x25.png" width="755"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Each voxel is color coded with the MLLM layer number (out of 33) that led to the highest normalized brain alignment. The color bar highlights color codes for each layer. The voxels are projected onto the flattened cortical surface of a representative subject (S2, S5 and S7) for three MLLMs.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A9">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>ROI Analysis: Shared and Unique variance across task-specific instructions</h2>
<div class="ltx_para ltx_noindent" id="A9.p1">
<p class="ltx_p" id="A9.p1.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A9.F14" title="Figure 14 ‣ Appendix I ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">14</span></a> presents the unique and shared variance between task prompts—Image Captioning (IC) and Visual Question Answering 1 (VQ1)—for the InstructBLIP model, focusing on representative subject-1.
From Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A9.F14" title="Figure 14 ‣ Appendix I ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">14</span></a>, we observe the following: (i)
Between IC and VQ1, there is no unique variance in the high-level visual region EBA, as this region is explained by shared information between the models.
(ii) In other high-level visual regions, a portion of the variance is unique to each model, though the majority is explained by shared variance.
Overall, these findings highlight the role of shared neural processes across task-specific instructions in high-level visual regions while also demonstrating that different task instructions can drive distinct neural responses in specific regions.
However, the fact that the majority of variance is shared between instructions suggests room for improvement in the MLLM models. Enhancing the models’ ability to capture more task-specific information could lead to greater precision in predicting brain responses and better differentiation between various types of instructions.</p>
</div>
<figure class="ltx_figure" id="A9.F14"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="233" id="A9.F14.g1" src="x26.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Unique and shared variances between pairs of task-specific instructions for the InstructBLIP model, focusing on representative subject-1. Image Captioning (IC) vs. Visual Question Answering 1 (VQ1).</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A10">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix J </span>Normalized Brain Alignment across different image categories</h2>
<div class="ltx_para ltx_noindent" id="A10.p1">
<p class="ltx_p" id="A10.p1.1">The NSD dataset contains images from 12 different categories. Each image can be labeled with multiple categories. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A10.F15" title="Figure 15 ‣ Appendix J Normalized Brain Alignment across different image categories ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">15</span></a> shows the percentage of overlap between pair of categories. Some pairs have very high overlap values as expected like (person, sports), (food, kitchen), (furniture, kitchen), (person, vehicle), etc.</p>
</div>
<div class="ltx_para ltx_noindent" id="A10.p2">
<p class="ltx_p" id="A10.p2.1">To understand the effectiveness of task-specific instructions to brain alignment across various image categories, we performed a category-wise analysis, where we computed the normalized brain alignment for voxels in each category by averaging across all task instructions, and across all the 3 MLLMs. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A10.F16" title="Figure 16 ‣ Appendix J Normalized Brain Alignment across different image categories ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">16</span></a> shows the normalized brain alignment for five visual functional localizers: FLOC-BODIES, FLOC-PLACES, FLOC-FACES, FLOC-WORDS and pRF-Visual ROIs.</p>
</div>
<div class="ltx_para ltx_noindent" id="A10.p3">
<p class="ltx_p" id="A10.p3.1"><span class="ltx_text ltx_font_bold" id="A10.p3.1.1">Do the MLLM representations demonstrate better brain predictivity for certain categories of image stimuli?</span></p>
</div>
<div class="ltx_para ltx_noindent" id="A10.p4">
<p class="ltx_p" id="A10.p4.1">To understand the effectiveness of task-specific instructions to brain alignment across various image categories, we performed a category-wise analysis, where we computed the normalized brain alignment for voxels in each category by averaging across all task instructions, and across all the 3 MLLMs. Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A10.F16" title="Figure 16 ‣ Appendix J Normalized Brain Alignment across different image categories ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">16</span></a> shows the normalized brain alignment for FLOC-BODIES and FLOC-PLACES ROIs. From this, we observe the following:
(i) MLLMs exhibit higher normalized brain alignment for person category test images in the FLOC-BODIES region, suggesting that instruction-specific representations effectively capture body-related information, resulting in higher alignment. We also observe higher alignment in the furniture category, likely due to its 15% overlap with person-related images, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A10.F15" title="Figure 15 ‣ Appendix J Normalized Brain Alignment across different image categories ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">15</span></a>.
(ii) For FLOC-PLACES ROI, similar to FLOC-BODIES, both furniture and kitchen categories test images have higher alignment, as these categories share overlapping image content (30%).
(iii) Although the appliance category has partial overlap with furniture and kitchen, MLLMs display lowest normalized brain alignment for this category. This suggests that the appliance category may contain unique features not as well captured by the MLLM’s representations, or it may lack the strong shared features that drive high alignment in other categories.
These trends are consistent across other visual ROIs, such as FLOC-WORDS, FLOC-FACES, and pRF-Visual ROIs, as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A10.F16" title="Figure 16 ‣ Appendix J Normalized Brain Alignment across different image categories ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">16</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="A10.p5">
<p class="ltx_p" id="A10.p5.1">Overall, the analysis shows that MLLMs are most effective at aligning with brain activity when both task-specific and shared category features are present, suggesting that these features play a crucial role in brain alignment. However, in categories with fewer shared visual elements (e.g., appliances), alignment is weaker, indicating that these models may have difficulty fully capturing certain types of visual information.</p>
</div>
<figure class="ltx_figure" id="A10.F15"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="523" id="A10.F15.g1" src="x27.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Percentage of overlap between pair of categories.</figcaption>
</figure>
<figure class="ltx_figure" id="A10.F16">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="287" id="A10.F16.g1" src="x28.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="287" id="A10.F16.g2" src="x29.png" width="407"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="287" id="A10.F16.g3" src="x30.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="287" id="A10.F16.g4" src="x31.png" width="407"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="287" id="A10.F16.g5" src="x32.png" width="407"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 16: </span>Average (across the 3 MLLMs) normalized brain alignment across 12 categories of images for two function localizers (FLOC-BODIES, FLOC-PLACES, FLOC-FACES, FLOC-WORDS and pRF-Visual ROIs). Error bars
indicate the standard error of the mean across participants. </figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A11">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix K </span>Comparison of Instruction-tuned MLLMs, Non-Instruction-tuned MLLMs and text-based LLMs</h2>
<div class="ltx_para ltx_noindent" id="A11.p1">
<p class="ltx_p" id="A11.p1.1">To thoroughly explore the role of instruction tuning, we conducted two additional experiments:
(i) Non-instruction-tuned MLLM (BLIP-2): We passed all task instructions as input.
(ii) Language Model (LLaMA-2-7B): We passed only captions as input without task instructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p2">
<p class="ltx_p" id="A11.p2.1"><span class="ltx_text ltx_font_bold" id="A11.p2.1.1">Generated output tokens from BLIP-2.</span>
Firstly, we present the generated output tokens from the BLIP-2 model in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T6" title="Table 6 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">6</span></a>. This table includes examples of the generated tokens for each task instruction. We already provided predictions from InstructBLIP for the same image in Appendix Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T3" title="Table 3 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>. From the examples in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T6" title="Table 6 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">6</span></a>, we observe that the generated output tokens adhere more closely to captioning instructions, regardless of the specific task instructions provided. The outputs often consist of simple responses, such as “Yes,” “No,” or color names, and lack detailed descriptions. In contrast, instruction-tuned MLLMs excel at providing semantically rich and conceptually grounded descriptions, as shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A5.T3" title="Table 3 ‣ Appendix E Model generated outputs across instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">3</span></a>, demonstrating a significant difference in their ability to follow task-specific instructions effectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p3">
<p class="ltx_p" id="A11.p3.1"><span class="ltx_text ltx_font_bold" id="A11.p3.1.1">Comparison with normalized brain alignment scores across voxels.</span> Second, we present the scatter plot in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A11.F17" title="Figure 17 ‣ Appendix K Comparison of Instruction-tuned MLLMs, Non-Instruction-tuned MLLMs and text-based LLMs ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">17</span></a>, which compares the performance of InstructionBLIP vs. BLIP-2 for brain predictivity. The plot includes all voxels across the visual cortex with normalized brain alignment scores, where the diagonal represents identical performance for both models.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p4">
<p class="ltx_p" id="A11.p4.1">Image Captioning Task (left): The histogram shows a distribution of voxels deviating from the diagonal towards InstructBLIP, indicating that InstructBLIP performs better. However, the deviation is more pronounced for voxels with normalized brain alignment scores <math alttext="&gt;" class="ltx_Math" display="inline" id="A11.p4.1.m1.1"><semantics id="A11.p4.1.m1.1a"><mo id="A11.p4.1.m1.1.1" xref="A11.p4.1.m1.1.1.cmml">&gt;</mo><annotation-xml encoding="MathML-Content" id="A11.p4.1.m1.1b"><gt id="A11.p4.1.m1.1.1.cmml" xref="A11.p4.1.m1.1.1"></gt></annotation-xml><annotation encoding="application/x-tex" id="A11.p4.1.m1.1c">&gt;</annotation><annotation encoding="application/x-llamapun" id="A11.p4.1.m1.1d">&gt;</annotation></semantics></math> 0.2.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p5">
<p class="ltx_p" id="A11.p5.1">Visual Relation Task (right): Similarly, the voxel distribution deviates significantly towards InstructBLIP, demonstrating its superior performance. The deviation is notably larger for this task compared to the image captioning task.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p6">
<p class="ltx_p" id="A11.p6.1"><span class="ltx_text ltx_font_bold" id="A11.p6.1.1">Whole Visual Cortex vs. ROI-level Analysis</span>
Third, we extended the analysis from Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#S6.F2" title="Figure 2 ‣ 6.1 MLLM representations align well to human brain activity ‣ 6 Results ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">2</span></a> of the main paper by computing normalized brain alignment for the following regions:
(a) Whole Visual Cortex,
(b) Early Visual Cortex (pRF-Visual), and
(c) High-level Visual Cortex (Bodies, Faces, Places, Words)
The results in the Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A11.F18" title="Figure 18 ‣ Appendix K Comparison of Instruction-tuned MLLMs, Non-Instruction-tuned MLLMs and text-based LLMs ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">18</span></a>, demonstrate that the non-instruction-tuned MLLM (BLIP-2) achieves brain alignment scores that are marginally below those of the CLIP-Text model. In contrast, the LLaMA-2-7B model demonstrates performance closer to the ViT-H model. This behavior is attributed to the fact that non-instruction-tuned models generate output tokens that adhere more closely to captioning instructions, regardless of specific task instructions.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p7">
<p class="ltx_p" id="A11.p7.1"><span class="ltx_text ltx_font_bold" id="A11.p7.1.1">Key Findings</span></p>
<ul class="ltx_itemize" id="A11.I1">
<li class="ltx_item" id="A11.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="A11.I1.i1.p1">
<p class="ltx_p" id="A11.I1.i1.p1.1">The performance boost in instruction-tuned MLLMs is primarily due to their ability to generate semantic descriptions and conceptually understand the elements of each scene, as opposed to merely generating task-specific answers, as seen in non-instruction-tuned MLLMs.</p>
</div>
</li>
<li class="ltx_item" id="A11.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para ltx_noindent" id="A11.I1.i2.p1">
<p class="ltx_p" id="A11.I1.i2.p1.1">Representations from the LLaMA-2-7B model, which are based solely on captions, exhibit more semantic information but lack visual understanding details. Consequently, its performance aligns more closely with visual model representations (e.g., ViT).</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para ltx_noindent" id="A11.p8">
<p class="ltx_p" id="A11.p8.1">Overall, Instruction-tuned MLLMs demonstrate superior visual understanding when provided with task-specific instructions, as evidenced by their higher brain alignment. This reinforces the value of instruction tuning in enhancing multimodal understanding and alignment with human brain representations.</p>
</div>
<figure class="ltx_figure" id="A11.F17">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="305" id="A11.F17.g1" src="x33.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="305" id="A11.F17.g2" src="x34.png" width="407"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 17: </span>Comparison of MLLMs performance (Instruction-tuned vs. Non-Instruction-tuned) with a 2D scatter plot. Normalized brain alignment of all voxels is represented in the plot. The diagonal corresponds to identical performance for both models. A distribution of voxels deviating from the diagonal towards InstructBLIP means that InstructBLIP is performing better than BLIP-2. Left plot is for Image Captioning task, and right plot is for Visual Relation task. Plot shows that InstructBLIP performs better than BLIP-2.</figcaption>
</figure>
<figure class="ltx_figure" id="A11.F18">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="216" id="A11.F18.g1" src="x35.png" width="270"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="216" id="A11.F18.g2" src="x36.png" width="270"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="216" id="A11.F18.g3" src="x37.png" width="270"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="216" id="A11.F18.g4" src="x38.png" width="270"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="216" id="A11.F18.g5" src="x39.png" width="270"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="216" id="A11.F18.g6" src="x40.png" width="270"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 18: </span>Whole Visual Cortex and ROI-based normalized brain alignment was computed by averaging across participants, layers, and voxels. Blue: Average across random initialization of the 3 MLLMs. Note that CLIP-text model and LLaMA models use golden oracle captions while instruct and non-instruct models use predicted model generations. <math alttext="\ast" class="ltx_Math" display="inline" id="A11.F18.5.m1.1"><semantics id="A11.F18.5.m1.1b"><mo id="A11.F18.5.m1.1.1" mathcolor="#FF0000" xref="A11.F18.5.m1.1.1.cmml">∗</mo><annotation-xml encoding="MathML-Content" id="A11.F18.5.m1.1c"><ci id="A11.F18.5.m1.1.1.cmml" xref="A11.F18.5.m1.1.1">∗</ci></annotation-xml><annotation encoding="application/x-tex" id="A11.F18.5.m1.1d">\ast</annotation><annotation encoding="application/x-llamapun" id="A11.F18.5.m1.1e">∗</annotation></semantics></math> indicates cases where MLLM embeddings are statistically significantly better than randomly initialized models, i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="A11.F18.6.m2.1"><semantics id="A11.F18.6.m2.1b"><mrow id="A11.F18.6.m2.1.1" xref="A11.F18.6.m2.1.1.cmml"><mi id="A11.F18.6.m2.1.1.2" xref="A11.F18.6.m2.1.1.2.cmml"></mi><mo id="A11.F18.6.m2.1.1.1" xref="A11.F18.6.m2.1.1.1.cmml">≤</mo><mn id="A11.F18.6.m2.1.1.3" xref="A11.F18.6.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A11.F18.6.m2.1c"><apply id="A11.F18.6.m2.1.1.cmml" xref="A11.F18.6.m2.1.1"><leq id="A11.F18.6.m2.1.1.1.cmml" xref="A11.F18.6.m2.1.1.1"></leq><csymbol cd="latexml" id="A11.F18.6.m2.1.1.2.cmml" xref="A11.F18.6.m2.1.1.2">absent</csymbol><cn id="A11.F18.6.m2.1.1.3.cmml" type="float" xref="A11.F18.6.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A11.F18.6.m2.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="A11.F18.6.m2.1e">≤ 0.05</annotation></semantics></math>. <math alttext="\wedge" class="ltx_Math" display="inline" id="A11.F18.7.m3.1"><semantics id="A11.F18.7.m3.1b"><mo id="A11.F18.7.m3.1.1" mathcolor="#FF0000" xref="A11.F18.7.m3.1.1.cmml">∧</mo><annotation-xml encoding="MathML-Content" id="A11.F18.7.m3.1c"><and id="A11.F18.7.m3.1.1.cmml" xref="A11.F18.7.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="A11.F18.7.m3.1d">\wedge</annotation><annotation encoding="application/x-llamapun" id="A11.F18.7.m3.1e">∧</annotation></semantics></math> indicates cases where MLLMs are significantly better than unimodal vision models (ViT-H), i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="A11.F18.8.m4.1"><semantics id="A11.F18.8.m4.1b"><mrow id="A11.F18.8.m4.1.1" xref="A11.F18.8.m4.1.1.cmml"><mi id="A11.F18.8.m4.1.1.2" xref="A11.F18.8.m4.1.1.2.cmml"></mi><mo id="A11.F18.8.m4.1.1.1" xref="A11.F18.8.m4.1.1.1.cmml">≤</mo><mn id="A11.F18.8.m4.1.1.3" xref="A11.F18.8.m4.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="A11.F18.8.m4.1c"><apply id="A11.F18.8.m4.1.1.cmml" xref="A11.F18.8.m4.1.1"><leq id="A11.F18.8.m4.1.1.1.cmml" xref="A11.F18.8.m4.1.1.1"></leq><csymbol cd="latexml" id="A11.F18.8.m4.1.1.2.cmml" xref="A11.F18.8.m4.1.1.2">absent</csymbol><cn id="A11.F18.8.m4.1.1.3.cmml" type="float" xref="A11.F18.8.m4.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="A11.F18.8.m4.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="A11.F18.8.m4.1e">≤ 0.05</annotation></semantics></math>.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A12">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix L </span>Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions</h2>
<div class="ltx_para ltx_noindent" id="A12.p1">
<p class="ltx_p" id="A12.p1.1">Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12.F19" title="Figure 19 ‣ Appendix L Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">19</span></a> shows Venn diagrams representing the shared and unique variance between the IC task and nine other task-based instructions (VQ1, VQ2, VR, CR, IU1, IU2, IU3, SR) across the whole visual cortex. We make the following observations from Figure <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12.F19" title="Figure 19 ‣ Appendix L Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">19</span></a>: (1) The higher shared variance across most task pairs (e.g., IC &amp; VQ1, IC &amp; CR), highlighting that tasks involving visual question answering and captioning share significant neural processing mechanisms in the visual cortex. (2) IC retains a notable amount of unique variance in most comparisons, suggesting that certain neural processes underlying image captioning are specific to this task and not shared with other visual tasks. (3) Other tasks also retain unique variance, which might reflect the task-specific nature of these activities (e.g., VQ1–VQ3 could involve question-based reasoning, and IU tasks could involve higher-level image understanding.</p>
</div>
<div class="ltx_para ltx_noindent" id="A12.p2">
<p class="ltx_p" id="A12.p2.1">Similarly, Figures <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12.F20" title="Figure 20 ‣ Appendix L Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">20</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12.F21" title="Figure 21 ‣ Appendix L Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">21</span></a> show Venn diagrams for the early visual and higher visual regions of interest (ROIs), depicting shared and unique variance across these regions. We make the following observations from Figures <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12.F20" title="Figure 20 ‣ Appendix L Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">20</span></a> and  <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A12.F21" title="Figure 21 ‣ Appendix L Whole Visual Cortex and ROI Analysis: Shared and Unique variance across task-specific instructions ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">21</span></a>: (i) Unlike the whole visual cortex, tasks like IC and other task-specific instructions exhibit moderate shared variance in the early visual cortex, while shared variance is significantly higher in higher visual ROIs. This suggests that these tasks depend on similar low-level visual processing mechanisms in early visual areas. (ii) The IU1 instruction (“Describe the most dominant color in the image”) shows greater unique variance than IC in the early visual cortex, indicating that low-level color processing is specific to early visual areas, with MLLMs effectively capturing task-specific representations. In contrast, IC exhibits greater unique variance in the higher visual cortex compared to IU1, reflecting the task’s reliance on higher-order visual processing.</p>
</div>
<div class="ltx_para ltx_noindent" id="A12.p3">
<p class="ltx_p" id="A12.p3.1">Overall, these findings demonstrate that shared variance increases from early visual to higher visual areas, reflecting the hierarchical nature of visual processing. Meanwhile, unique variance decreases in higher areas, as tasks rely more on integrated and shared representations. Tasks such as Visual Question Answering (VQ) and Visual Reasoning (VR) retain distinct processing features across both early and higher ROIs, underscoring their unique demands on visual and cognitive processing. These results highlight the capacity of MLLMs to distinguish task-specific representations and align closely with brain visual processing mechanisms.</p>
</div>
<figure class="ltx_figure" id="A12.F19">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="227" id="A12.F19.g1" src="x41.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="221" id="A12.F19.g2" src="x42.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="219" id="A12.F19.g3" src="x43.png" width="266"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="217" id="A12.F19.g4" src="x44.png" width="265"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="224" id="A12.F19.g5" src="x45.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="218" id="A12.F19.g6" src="x46.png" width="265"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="215" id="A12.F19.g7" src="x47.png" width="265"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="218" id="A12.F19.g8" src="x48.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="216" id="A12.F19.g9" src="x49.png" width="265"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 19: </span>Whole Visual Cortex: Shared and Unique Variance explained between task instructions: Image Captioning (IC) and other task instructions. In each plot, Pink Area (Left Circle - Intersection) represents the unique variance explained by the IC task that is not shared with the corresponding task. Green Area (Right Circle - Intersection) represents the unique variance explained by the corresponding task (e.g., VQ1, CR, etc.) that is not shared with the IC task. Light Brown Intersection (Overlap) represents the shared variance between the IC task and the corresponding task. It indicates the extent to which both tasks explain overlapping neural variance in the visual cortex.</figcaption>
</figure>
<figure class="ltx_figure" id="A12.F20">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="216" id="A12.F20.g1" src="x50.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="207" id="A12.F20.g2" src="x51.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="209" id="A12.F20.g3" src="x52.png" width="266"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="205" id="A12.F20.g4" src="x53.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="213" id="A12.F20.g5" src="x54.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="208" id="A12.F20.g6" src="x55.png" width="266"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="205" id="A12.F20.g7" src="x56.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="209" id="A12.F20.g8" src="x57.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="204" id="A12.F20.g9" src="x58.png" width="266"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 20: </span>Early Visual Cortex: Shared and Unique Variance explained between task instructions: Image Captioning (IC) and other task instructions. In each plot, Pink Area (Left Circle - Intersection) represents the unique variance explained by the IC task that is not shared with the corresponding task. Green Area (Right Circle - Intersection) represents the unique variance explained by the corresponding task (e.g., VQ1, CR, etc.) that is not shared with the IC task. Light brown Intersection (Overlap) represents the shared variance between the IC task and the corresponding task. It indicates the extent to which both tasks explain overlapping neural variance in the visual cortex.</figcaption>
</figure>
<figure class="ltx_figure" id="A12.F21">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="230" id="A12.F21.g1" src="x59.png" width="265"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="226" id="A12.F21.g2" src="x60.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="222" id="A12.F21.g3" src="x61.png" width="265"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="220" id="A12.F21.g4" src="x62.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="230" id="A12.F21.g5" src="x63.png" width="266"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="221" id="A12.F21.g6" src="x64.png" width="266"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="215" id="A12.F21.g7" src="x65.png" width="265"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="219" id="A12.F21.g8" src="x66.png" width="265"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="220" id="A12.F21.g9" src="x67.png" width="266"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 21: </span>Higher Visual Cortex: Shared and Unique Variance explained between task instructions: Image Captioning (IC) and other task instructions. In each plot, Pink Area (Left Circle - Intersection) represents the unique variance explained by the IC task that is not shared with the corresponding task. Green Area (Right Circle - Intersection) represents the unique variance explained by the corresponding task (e.g., VQ1, CR, etc.) that is not shared with the IC task. Light brown Intersection (Overlap) represents the shared variance between the IC task and the corresponding task. It indicates the extent to which both tasks explain overlapping neural variance in the visual cortex.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A13">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix M </span>Image only / Instruction only input to the instruction-tuned MLLM</h2>
<div class="ltx_para ltx_noindent" id="A13.p1">
<p class="ltx_p" id="A13.p1.1">We performed two additional experiments to investigate the behavior of instruction-tuned MLLMs, as reported in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A13.F22" title="Figure 22 ‣ Appendix M Image only / Instruction only input to the instruction-tuned MLLM ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">22</span></a>:
<span class="ltx_text ltx_font_bold" id="A13.p1.1.1">Image-Only Input with Empty Prompt:</span>
Here, the input to the instruction-tuned MLLM consists only of images, with an empty string provided as the instruction prompt. From this experiment, we observed that the embeddings generated by the instruction-tuned MLLM using only images and an empty prompt perform similar to the CLIP-image embeddings. Our hypothesis is that providing an empty instruction effectively reduces the instruction-tuned MLLM to behave similarly to a vision-language model like CLIP, resulting in comparable brain encoding performance to the CLIP model. This supports the notion that the absence of an instruction prompt shifts the model’s behavior towards a more vision-centric embedding generation.
<span class="ltx_text ltx_font_bold" id="A13.p1.1.2">Instruction-Only Input with Empty Image:</span>
In contrast to image only input, when only task instructions are provided with no image input, the embeddings perform below the randomly initialized baseline. This demonstrates that visual input is crucial for achieving meaningful brain alignment, as MLLMs with visual input, such as ”InstructBLIP” (vision+language) and ”InstructBLIP No Prompt” (vision-only), significantly outperform the instruction-only baseline.</p>
</div>
<div class="ltx_para ltx_noindent" id="A13.p2">
<p class="ltx_p" id="A13.p2.1">Fig. <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#A13.F22" title="Figure 22 ‣ Appendix M Image only / Instruction only input to the instruction-tuned MLLM ‣ Correlating instruction-tuning (in multimodal models) with vision-language processing (in the brain)"><span class="ltx_text ltx_ref_tag">22</span></a> illustrates the normalized brain alignment across the whole visual cortex, comparing the InstructBLIP model with and without a prompt, without image, as well as the ViT-H and CLIP-Text models. This figure shows that InstructBLIP with a prompt achieves higher normalized brain alignment compared to the model without a prompt. Furthermore, even when passing a single modality (image) to the InstructBLIP model, it retrieves relevant embeddings from the text modality within its aligned embedding space, similar to the behavior of the CLIP model. This is likely due to the model being pretrained with images and their associated task-specific instructions, enabling it to leverage its multimodal alignment. However, when only task instructions are provided without an image input, the embeddings perform below the randomly initialized baseline, emphasizing the critical role of visual input in achieving meaningful alignment. This occurs because the InstructBLIP model is specifically designed to integrate both visual and textual information. Without the image input, the model loses a critical part of the visual context, severely affecting its ability to comprehend and generate accurate responses. In scenarios with both image and instruction inputs, the model can leverage the visual context to better interpret and respond to the instructions. In contrast, the absence of an image deprives InstructBLIP of this advantage, leading to significantly poorer performance compared to conditions where both inputs are provided.
Overall, these results highlights the dependency of MLLMs on visual input for robust performance and supports the importance of including both visual and task-specific instructions for achieving high brain alignment.</p>
</div>
<figure class="ltx_figure" id="A13.F22"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="300" id="A13.F22.g1" src="x68.png" width="407"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 22: </span>Whole Visual Cortex: Comparison of Instruction-tuned MLLMs performance (InstructBLIP with and without prompt, without image).</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A14">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix N </span>Limitations</h2>
<div class="ltx_para ltx_noindent" id="A14.p1">
<p class="ltx_p" id="A14.p1.1">Apart from the generic limitations of fMRI such as poor temporal resolution (compared to EEG or MEG) and the blood oxygen level dependent (BOLD) signal being an indirect measure of neural activity <cite class="ltx_cite ltx_citemacro_citep">(Logothetis, <a class="ltx_ref" href="https://arxiv.org/html/2505.20029v1#bib.bib22" title="">2008</a>)</cite>, a specific limitation of the current study is that it relies on the NSD dataset, where subjects passively viewed images. As a result, the dataset may not fully capture how brain activity aligns with task-specific instructions. Collecting brain recordings while subjects engage in tasks guided by different instructions could provide a more comprehensive evaluation of whether instruction-tuned MLLMs truly reflect visual information processing in response to natural instructions.
It is also important to note that while we observed several task-specific instructions leading to improved brain alignment between fMRI recordings and MLLMs, not all instructions were relevant for brain alignment. This indicates that our analysis may not have fully captured the range of task-specific instructions that are jointly processed by the brain. Future work could expand on our approach by incorporating additional task-specific instructions to better characterize the joint processing of information between the brain and instruction-tuned MLLMs.</p>
</div>
<div class="ltx_para ltx_noindent" id="A14.p2">
<p class="ltx_p" id="A14.p2.1">Since the NSD dataset was collected while participants engaged in watching scenes, our work primarily focuses on the visual areas of the brain, as these are most relevant for the stimuli and tasks studied. Additionally, leveraging the associated ROI mappings of the NSD dataset, we conducted our analysis across the entire visual cortex. Regions outside the visual cortex, such as the language network, prefrontal cortex, or auditory regions, may not exhibit strong alignment with visual features due to their specialization in non-visual tasks, such as language processing, decision-making, or auditory perception. However, it would be an interesting direction to explore the alignment using instruction-tuned multimodal large language models in scenarios where participants watch videos involving auditory, visual, and language information, enabling a comprehensive whole-brain analysis.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 23 18:20:32 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

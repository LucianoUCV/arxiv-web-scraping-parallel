<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Multi-modal brain encoding models for multi-modal stimuli</title>
<!--Generated on Fri May 23 17:44:10 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2505.20027v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S1" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S2" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S3" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Dataset Curation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S4" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Methodology</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S5" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Experimental Setup</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.SS1" title="In 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span>How effective are multi-modal representations?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.SSx1.SSS0.Px1" title="In Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal models. â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title">Whole brain analysis.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.SSx1.SSS0.Px2" title="In Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal models. â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title">ROI-Level analysis of joint embeddings from multi-modal models.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.SS2" title="In 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span>Which brain regions process uni- and multi-modal information?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.SS3" title="In 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span>How does each modality contribute to the multi-modal brain alignment?</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S7" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span>Discussion</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S8" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span>Ethics Statement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A1" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Overview of Appendix Sections</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A2" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Cross-subject prediction accuracy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A3" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Detailed sub-ROIs of language, visual and auditory regions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A4" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Details of pretrained Transformer models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A5" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Implementation details for reproducibility.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A6" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Effectiveness of multi-modal vs unimodal representations for various brain regions</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A7" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>How is the brain alignment of multi-modal features affected by the elimination of a
particular modality?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A8" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Layerwise brain alignment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A9" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span>Why the choice of ridge regression instead of more complex machine learning models?</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A10" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">J </span>Extended Related Works</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A11" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">K </span>Baseline Analysis: Scrambling Inputs to Multi-modal Models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">L </span>Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A13" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">M </span>Multi-modal versus unimodal effects</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A14" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">N </span>Impact of diverse model architectures on performance comparison</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A15" title="In Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">O </span>Limitations</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Multi-modal brain encoding models for multi-modal stimuli</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Subba Reddy Oota<sup class="ltx_sup" id="id13.13.id1"><span class="ltx_text ltx_font_italic" id="id13.13.id1.1">1âˆ—</span></sup>, Khushbu Pahwa<sup class="ltx_sup" id="id14.14.id2"><span class="ltx_text ltx_font_italic" id="id14.14.id2.1">2âˆ—</span></sup>, Mounika Marreddy<sup class="ltx_sup" id="id15.15.id3"><span class="ltx_text ltx_font_italic" id="id15.15.id3.1">3</span></sup>, Maneesh Singh<sup class="ltx_sup" id="id16.16.id4">4</sup>
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="id7.7.3">Manish Gupta<sup class="ltx_sup" id="id7.7.3.1"><span class="ltx_text ltx_font_medium ltx_font_italic" id="id7.7.3.1.1">5</span></sup>, Bapi S. Raju<sup class="ltx_sup" id="id7.7.3.2"><span class="ltx_text ltx_font_medium" id="id7.7.3.2.1">6</span></sup>
<br class="ltx_break"/><sup class="ltx_sup" id="id7.7.3.3"><span class="ltx_text ltx_font_medium" id="id7.7.3.3.1">1</span></sup></span>Technische UniversitÃ¤t
Berlin, Germany, <sup class="ltx_sup" id="id17.17.id5">2</sup>Rice Univ, USA, <sup class="ltx_sup" id="id18.18.id6">3</sup>Univ of Bonn, Germany 
<br class="ltx_break"/><sup class="ltx_sup" id="id19.19.id7">4</sup>Spector Inc, USA, <sup class="ltx_sup" id="id20.20.id8">5</sup>Microsoft, India, <sup class="ltx_sup" id="id21.21.id9">6</sup>IIIT Hyderabad, India 
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="id22.22.id10">subba.reddy.oota@tu-berlin.de, gmanish@microsoft.com, raju.bapi@iiit.ac.in</span>
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id23.id1">Despite participants engaging in <em class="ltx_emph ltx_font_italic" id="id23.id1.1">unimodal stimuli</em>, such as watching images or silent videos, recent work has demonstrated that multi-modal Transformer models can predict visual brain activity impressively well, even with incongruent modality representations. This raises the question of how accurately these multi-modal models can predict brain activity when participants are engaged in <em class="ltx_emph ltx_font_italic" id="id23.id1.2">multi-modal stimuli</em>.
As these models grow increasingly popular, their use in studying neural activity provides insights into how our brains respond to such multi-modal naturalistic stimuli, i.e., where it separates and integrates information across modalities through a hierarchy of early sensory regions to higher cognition (language regions).
We investigate this question by using multiple unimodal and two types of multi-modal modelsâ€”cross-modal and jointly pretrainedâ€”to determine which type of model is more relevant to fMRI brain activity when participants are engaged in watching movies (videos with audio). We observe that both types of multi-modal models show improved alignment in several language and visual regions. This study also helps in identifying which brain regions process unimodal versus multi-modal information.
We further investigate the contribution of each modality to multi-modal alignment by carefully removing unimodal features one by one from multi-modal representations, and find that there is additional information beyond the unimodal embeddings that is processed in the visual and language regions.
Based on this investigation, we find that while for cross-modal models, their brain alignment is partially attributed to the video modality; for jointly pretrained models, it is partially attributed to both the video and audio modalities.
This serves as a strong motivation for the neuroscience community to investigate the interpretability of these models for deepening our understanding of multi-modal information processing in brain. We make the code publicly available<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/subbareddy248/multi-modal-brain-stimuli" title="">https://github.com/subbareddy248/multi-modal-brain-stimuli</a></span></span></span>.</p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para ltx_noindent" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Brain encoding aims at predicting the neural brain activity recordings from an input stimulus representation.
Recent brain encoding studies use neural models as a powerful approach to better understand the information processing in the brain in response to naturalistic stimuliÂ <cite class="ltx_cite ltx_citemacro_citep">(Oota etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib43" title="">2024b</a>)</cite>.
Current encoding models are trained and tested on brain responses captured from participants who are interacting with <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">unimodal stimuli</span>. Several unimodal pretrained models have been used to obtain stimulus representations for this purpose, such as languageÂ <cite class="ltx_cite ltx_citemacro_citep">(Wehbe etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib63" title="">2014</a>; Jain &amp; Huth, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib32" title="">2018</a>; Toneva &amp; Wehbe, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib55" title="">2019</a>; Caucheteux &amp; King, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib11" title="">2022</a>; Schrimpf etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib49" title="">2021</a>; Toneva etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib56" title="">2022</a>; Aw &amp; Toneva, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib5" title="">2023</a>; Oota etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib38" title="">2022a</a>)</cite>,
visionÂ <cite class="ltx_cite ltx_citemacro_citep">(Yamins etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib66" title="">2014</a>; Eickenberg etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib21" title="">2017</a>; Schrimpf etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib48" title="">2018</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib61" title="">2019</a>)</cite> or speechÂ <cite class="ltx_cite ltx_citemacro_citep">(Millet etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib34" title="">2022</a>; Vaidya etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib59" title="">2022</a>; Tuckute etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib58" title="">2023</a>)</cite>.
In this paper, we build encoding models where participants are engaged with <span class="ltx_text ltx_font_italic" id="S1.p1.1.2">multi-modal stimuli</span> (e.g., watching movies that include audio). We explore multi-modal stimulus representations extracted using Transformer-basedÂ <cite class="ltx_cite ltx_citemacro_citep">(Vaswani etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib60" title="">2017</a>)</cite> multi-modal models. Our analysis focuses on brain alignmentâ€”the degree of similarity when predicting brain activity using both uni-modal and multi-modal models.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="299" id="S1.F1.g1" src="x1.png" width="622"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span><span class="ltx_text ltx_font_bold" id="S1.F1.31.3">(A) Overview of our proposed Multi-modal Brain Encoding Pipeline.</span> Using fMRI recordings from participants watching popular movies included with speech, we align stimulus representations with brain recordings through ridge regression. For uni-modal alignment, we use representations from video models (<span class="ltx_text ltx_font_italic" id="S1.F1.32.4">VM</span>) or speech models (<span class="ltx_text ltx_font_italic" id="S1.F1.33.5">SM</span>), where the input consists exclusively of either videos (without speech) or speech, respectively. For multi-modal alignment, we leverage representations from cross-modal (<span class="ltx_text ltx_font_italic" id="S1.F1.34.6">CM</span>) and jointly-pretrained models (<span class="ltx_text ltx_font_italic" id="S1.F1.35.7">JM</span>), where the input consists of both video and speech. Here, <math alttext="f_{1}" class="ltx_Math" display="inline" id="S1.F1.9.m1.1"><semantics id="S1.F1.9.m1.1b"><msub id="S1.F1.9.m1.1.1" xref="S1.F1.9.m1.1.1.cmml"><mi id="S1.F1.9.m1.1.1.2" xref="S1.F1.9.m1.1.1.2.cmml">f</mi><mn id="S1.F1.9.m1.1.1.3" xref="S1.F1.9.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S1.F1.9.m1.1c"><apply id="S1.F1.9.m1.1.1.cmml" xref="S1.F1.9.m1.1.1"><csymbol cd="ambiguous" id="S1.F1.9.m1.1.1.1.cmml" xref="S1.F1.9.m1.1.1">subscript</csymbol><ci id="S1.F1.9.m1.1.1.2.cmml" xref="S1.F1.9.m1.1.1.2">ğ‘“</ci><cn id="S1.F1.9.m1.1.1.3.cmml" type="integer" xref="S1.F1.9.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.9.m1.1d">f_{1}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.9.m1.1e">italic_f start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="f_{2}" class="ltx_Math" display="inline" id="S1.F1.10.m2.1"><semantics id="S1.F1.10.m2.1b"><msub id="S1.F1.10.m2.1.1" xref="S1.F1.10.m2.1.1.cmml"><mi id="S1.F1.10.m2.1.1.2" xref="S1.F1.10.m2.1.1.2.cmml">f</mi><mn id="S1.F1.10.m2.1.1.3" xref="S1.F1.10.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S1.F1.10.m2.1c"><apply id="S1.F1.10.m2.1.1.cmml" xref="S1.F1.10.m2.1.1"><csymbol cd="ambiguous" id="S1.F1.10.m2.1.1.1.cmml" xref="S1.F1.10.m2.1.1">subscript</csymbol><ci id="S1.F1.10.m2.1.1.2.cmml" xref="S1.F1.10.m2.1.1.2">ğ‘“</ci><cn id="S1.F1.10.m2.1.1.3.cmml" type="integer" xref="S1.F1.10.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.10.m2.1d">f_{2}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.10.m2.1e">italic_f start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="g" class="ltx_Math" display="inline" id="S1.F1.11.m3.1"><semantics id="S1.F1.11.m3.1b"><mi id="S1.F1.11.m3.1.1" xref="S1.F1.11.m3.1.1.cmml">g</mi><annotation-xml encoding="MathML-Content" id="S1.F1.11.m3.1c"><ci id="S1.F1.11.m3.1.1.cmml" xref="S1.F1.11.m3.1.1">ğ‘”</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.11.m3.1d">g</annotation><annotation encoding="application/x-llamapun" id="S1.F1.11.m3.1e">italic_g</annotation></semantics></math> and <math alttext="h" class="ltx_Math" display="inline" id="S1.F1.12.m4.1"><semantics id="S1.F1.12.m4.1b"><mi id="S1.F1.12.m4.1.1" xref="S1.F1.12.m4.1.1.cmml">h</mi><annotation-xml encoding="MathML-Content" id="S1.F1.12.m4.1c"><ci id="S1.F1.12.m4.1.1.cmml" xref="S1.F1.12.m4.1.1">â„</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.12.m4.1d">h</annotation><annotation encoding="application/x-llamapun" id="S1.F1.12.m4.1e">italic_h</annotation></semantics></math> are ridge regression models. <span class="ltx_text ltx_font_bold" id="S1.F1.36.8">(B) Residual Analysis.</span>
First, we remove the uni-modal video model (<span class="ltx_text ltx_font_italic" id="S1.F1.37.9">VM</span>) representations from the cross-modal (<span class="ltx_text ltx_font_italic" id="S1.F1.38.10">CM</span>) representations by learning a simple linear function <math alttext="r" class="ltx_Math" display="inline" id="S1.F1.13.m5.1"><semantics id="S1.F1.13.m5.1b"><mi id="S1.F1.13.m5.1.1" xref="S1.F1.13.m5.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S1.F1.13.m5.1c"><ci id="S1.F1.13.m5.1.1.cmml" xref="S1.F1.13.m5.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.13.m5.1d">r</annotation><annotation encoding="application/x-llamapun" id="S1.F1.13.m5.1e">italic_r</annotation></semantics></math> that maps <span class="ltx_text ltx_font_italic" id="S1.F1.39.11">VM</span> representations to the <span class="ltx_text ltx_font_italic" id="S1.F1.40.12">CM</span> representations, and use this estimated function to obtain the residual representations <span class="ltx_text ltx_font_italic" id="S1.F1.14.1">|CM(X)-<math alttext="r" class="ltx_Math" display="inline" id="S1.F1.14.1.m1.1"><semantics id="S1.F1.14.1.m1.1b"><mi id="S1.F1.14.1.m1.1.1" xref="S1.F1.14.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S1.F1.14.1.m1.1c"><ci id="S1.F1.14.1.m1.1.1.cmml" xref="S1.F1.14.1.m1.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.14.1.m1.1d">r</annotation><annotation encoding="application/x-llamapun" id="S1.F1.14.1.m1.1e">italic_r</annotation></semantics></math>(VM(X))|</span>. In step 2, we learn another ridge regression model (<math alttext="g^{\prime}" class="ltx_Math" display="inline" id="S1.F1.15.m6.1"><semantics id="S1.F1.15.m6.1b"><msup id="S1.F1.15.m6.1.1" xref="S1.F1.15.m6.1.1.cmml"><mi id="S1.F1.15.m6.1.1.2" xref="S1.F1.15.m6.1.1.2.cmml">g</mi><mo id="S1.F1.15.m6.1.1.3" xref="S1.F1.15.m6.1.1.3.cmml">â€²</mo></msup><annotation-xml encoding="MathML-Content" id="S1.F1.15.m6.1c"><apply id="S1.F1.15.m6.1.1.cmml" xref="S1.F1.15.m6.1.1"><csymbol cd="ambiguous" id="S1.F1.15.m6.1.1.1.cmml" xref="S1.F1.15.m6.1.1">superscript</csymbol><ci id="S1.F1.15.m6.1.1.2.cmml" xref="S1.F1.15.m6.1.1.2">ğ‘”</ci><ci id="S1.F1.15.m6.1.1.3.cmml" xref="S1.F1.15.m6.1.1.3">â€²</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.15.m6.1d">g^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S1.F1.15.m6.1e">italic_g start_POSTSUPERSCRIPT â€² end_POSTSUPERSCRIPT</annotation></semantics></math>) to measure the brain alignment between residual representations <span class="ltx_text ltx_font_italic" id="S1.F1.16.2">|CM(X)-<math alttext="r" class="ltx_Math" display="inline" id="S1.F1.16.2.m1.1"><semantics id="S1.F1.16.2.m1.1b"><mi id="S1.F1.16.2.m1.1.1" xref="S1.F1.16.2.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S1.F1.16.2.m1.1c"><ci id="S1.F1.16.2.m1.1.1.cmml" xref="S1.F1.16.2.m1.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.16.2.m1.1d">r</annotation><annotation encoding="application/x-llamapun" id="S1.F1.16.2.m1.1e">italic_r</annotation></semantics></math>(VM(X))|</span> and the fMRI brain recordings. Similarly, residual analysis can also be applied to remove unimodal speech (<span class="ltx_text ltx_font_italic" id="S1.F1.41.13">SM</span>) features from <span class="ltx_text ltx_font_italic" id="S1.F1.42.14">CM</span> or <span class="ltx_text ltx_font_italic" id="S1.F1.43.15">JM</span> representations for a given input <span class="ltx_text ltx_font_italic" id="S1.F1.44.16">X</span>.</figcaption>
</figure>
<div class="ltx_para ltx_noindent" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">There is growing evidence that the human brainâ€™s ability for multi-modal processing is underpinned by synchronized cortical representations of identical concepts across various sensory modalitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Gauthier etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib23" title="">2003</a>; Bracci &amp; OpÂ de Beeck, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib10" title="">2023</a>)</cite>. Reflecting similar principles, the recent advances in AI systems have led to the development of multi-modal models (like CLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib46" title="">2021</a>)</cite>, ImageBindÂ <cite class="ltx_cite ltx_citemacro_citep">(Girdhar etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib25" title="">2023</a>)</cite>, and TVLTÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib54" title="">2022</a>)</cite>) use massive interleaved image-text data, speech-text data or video-audio-text data to represent multi-modal input.
This recent progress in AI has stimulated advancements in brain encoding modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Conwell etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib13" title="">2022</a>)</cite>;Â <cite class="ltx_cite ltx_citemacro_citep">(Doerig etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib17" title="">2022</a>; Oota etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib39" title="">2022b</a>; Popham etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib44" title="">2021</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib62" title="">2023</a>; Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib53" title="">2024</a>; Nakagi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib36" title="">2024</a>)</cite> that learn effectively from multiple input modalities, despite participants being engaged with unimodal stimulus during experiments, e.g., watching natural scene images, or silent movie clips.
However, these studies have experimented with subjects engaged with unimodal stimulus. Only recently there has been some work on training models for true multi-modal stimulus scenarios, but most of them have focused on video+text stimuliÂ <cite class="ltx_cite ltx_citemacro_citep">(Nakagi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib36" title="">2024</a>; Subramaniam etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib51" title="">2024</a>)</cite>. The only exception being Â <cite class="ltx_cite ltx_citemacro_citet">Dong &amp; Toneva (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib18" title="">2023a</a>)</cite> who experiment with video+audio stimuli and explored brain alignment differences with pretrained versus finetuned joint multi-modal transformer representations.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Using brain recordings of participants watching several popular movies included with audioÂ <cite class="ltx_cite ltx_citemacro_citep">(St-Laurent etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib50" title="">2023</a>)</cite>, we investigate several research questions. First, we investigate the effectiveness of stimulus representations obtained using multi-modal models versus unimodal models for brain encoding. Multi-modal models are of two broad types: (i) cross-modal pretrained models, where first intermediate representations for both modalities are computed using individual modality encoders and then combined using contrastive loss,
and (ii) jointly pretrained models, which involve combining data from multiple modalities at token level itself,
and training a single joint encoder. Hence, we also investigate which of the two types (cross-modal versus joint) is better for encoding.
We focus on one cross-modal (ImageBind), one jointly pretrained (TVLT), three video and two speech models.
Additionally, we explore which modality representations are more brain relevant, and identify which brain regions process unimodal versus
multi-modal information.
Overall, this research utilizes multi-modal representations to develop encoding models based on fMRI responses (see Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">1</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">Using our multi-modal brain encoding approach, we examine several insights.
First, we use previous neuroscience findings that have identified brain regions involved in visual, language and auditory processing, and investigate how well our model aligns with these regions when both the model and a human participant watch the same multi-modal video stimuli.
Secondly, we hypothesize that multi-modal models capable of learning cross-modal and joint embeddings across various sensory inputs in a manner that mimics brain processing would likely show significant alignment with these neural regions. However, alignment with these brain regions doesnâ€™t necessarily mean that the model is effectively learning from multiple modalities, as unimodal models for vision or language or audio have also been shown to significantly align with these brain regionsÂ <cite class="ltx_cite ltx_citemacro_citep">(Wehbe etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib63" title="">2014</a>; Toneva etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib56" title="">2022</a>; Schrimpf etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib49" title="">2021</a>; Millet etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib34" title="">2022</a>; Vaidya etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib59" title="">2022</a>)</cite>. To check the second aspect, we investigate this question via a direct approach, closely related to previous studiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Toneva etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib56" title="">2022</a>; Oota etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib40" title="">2023a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib41" title="">b</a>)</cite>. For each modality, we analyze how the alignment between brain recordings and multi-modal model representations is affected by the elimination of information related to that particular modality from the model representation.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Our analysis of multi-modal brain alignment leads to several key conclusions: (1) Both cross-modal and jointly pretrained models demonstrate significantly improved brain alignment with language regions (AG, PCC, PTL, and IFG) and visual regions (EVC and MT) when analyzed against unimodal video data. In contrast, compared to unimodal speech-based models, all multi-modal embeddings show significantly better brain alignment, except in the LOC (object visual processing) region. This highlights the ability of multi-modal models to capture additional informationâ€”either through knowledge transfer or integration between modalitiesâ€”which is crucial for multi-modal brain alignment. (2) Using our residual approach, we find that the improved brain alignment in cross-modal models can be partially attributed to the removal of video features alone, rather than auditory features. On the other hand, the improved brain alignment in jointly pretrained models can be partially attributed to the removal of both video and auditory features.</p>
</div>
<div class="ltx_para ltx_noindent" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">We make the following contributions.
(1) To the best of our knowledge, this study is the first to leverage both cross-modal and jointly pretrained multi-modal models to perform brain alignment while subjects are engaged with multi-modal naturalistic stimuli.
(2) We evaluate the performance of several unimodal Transformer models (three video and two audio) and measure their brain alignment. (3) Additionally, we remove unimodal features from multi-modal representations to explore the impact on brain alignment before and after their removal. We make the code publicly available<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#footnote1" title="footnote 1 â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">1</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<div class="ltx_para ltx_noindent" id="S2.p1">
<p class="ltx_p" id="S2.p1.1"><span class="ltx_text ltx_font_bold" id="S2.p1.1.1">Multi-modal models.</span>
Pretrained Transformer-based models have been found to be very effective in various tasks related to languageÂ <cite class="ltx_cite ltx_citemacro_citep">(Devlin etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib16" title="">2019</a>; Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib45" title="">2019</a>)</cite>, speechÂ <cite class="ltx_cite ltx_citemacro_citep">(Baevski etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib7" title="">2020</a>)</cite>, and imagesÂ <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib20" title="">2020</a>)</cite>. To learn associations between pairs of modalities,
Transformer models have been pretrained on multiple modalities, showing excellent results in multi-modal tasks like visual question answering and visual common-sense reasoning. These multi-modal models are pretrained in two different ways: (i) cross-modal models that integrate information from multiple modalities and learn a joint encoder, such as VisualBERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Li etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib33" title="">2019</a>)</cite> and ImageBindÂ <cite class="ltx_cite ltx_citemacro_citep">(Girdhar etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib25" title="">2023</a>)</cite>, and (ii) jointly pretrained models like LXMERTÂ <cite class="ltx_cite ltx_citemacro_citep">(Tan &amp; Bansal, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib52" title="">2019</a>)</cite>, CLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib46" title="">2021</a>)</cite>, and TVLTÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib54" title="">2022</a>)</cite> which fuse individual modality encoders at different stages, transferring knowledge from one modality to another.
In this work, we investigate how the representations extracted from <em class="ltx_emph ltx_font_italic" id="S2.p1.1.2">cross-modal and jointly-pretrained Transformer models</em> align with human brain recordings when participants engage with multi-modal stimuli.</p>
</div>
<div class="ltx_para ltx_noindent" id="S2.p2">
<p class="ltx_p" id="S2.p2.1"><span class="ltx_text ltx_font_bold" id="S2.p2.1.1">Brain encoding using multi-modal models.</span>
The majority of brain encoding studies tend to focus on vision or language alone, in large part due to the availability of only unimodal datasetsÂ <cite class="ltx_cite ltx_citemacro_citep">(Conwell etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib13" title="">2022</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib62" title="">2023</a>)</cite>. Notably,Â <cite class="ltx_cite ltx_citemacro_citet">Conwell etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib13" title="">2022</a>)</cite> conducted controlled comparisons between visual models with identical architecture and training data, finding no performance improvement from contrastive image-language training. Similarly,Â <cite class="ltx_cite ltx_citemacro_citet">Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib62" title="">2023</a>)</cite> reported that language alignment did not enhance encoding performance across most of the high-level visual cortex in their experiments. However, these studies focused on a single modality of input â€“ vision alone.
Since human brain perceives the environment using information from multiple modalitiesÂ <cite class="ltx_cite ltx_citemacro_citep">(Gauthier etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib23" title="">2003</a>)</cite>, examining the alignment between language and visual representations in the brain by training encoding models on fMRI responses, while extracting joint representations from multi-modal models, can offer insights into the relationship between the two modalities.
For instance, it has been shown that multi-modal models like CLIPÂ <cite class="ltx_cite ltx_citemacro_citep">(Radford etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib46" title="">2021</a>)</cite> better predict neural responses in the high-level visual cortex as compared to previous vision-only modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Doerig etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib17" title="">2022</a>; Wang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib62" title="">2023</a>; Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib53" title="">2024</a>; Nakagi etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib36" title="">2024</a>)</cite>.
However, these studies have experimented with subjects engaged with single-modality stimulus, leaving the full potential of these models in true multi-modal scenarios still unclear. Recently,Â <cite class="ltx_cite ltx_citemacro_citet">Dong &amp; Toneva (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib19" title="">2023b</a>)</cite> interpreted the effectiveness of pretraining versus finetuning using a multi-modal video transformer model, <em class="ltx_emph ltx_font_italic" id="S2.p2.1.2">MERLOT Reserve</em>Â <cite class="ltx_cite ltx_citemacro_citep">(Zellers etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib67" title="">2022</a>)</cite>. Further, they leverage single modality embeddings obtained from the same mulitmodal model for residual analysis.
It should be noted that multi-modal models can be trained broadly using two strategies â€“ cross-modally (dual) or jointlyÂ <cite class="ltx_cite ltx_citemacro_citep">(Frank etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib22" title="">2021</a>)</cite>. It is conceivable that the representations arising from these two kinds of training might have different brain alignmentÂ <cite class="ltx_cite ltx_citemacro_citep">(Oota etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib39" title="">2022b</a>)</cite>.
To address these differences, we experiment with both cross-modal (dual) and joint multi-modal models. In addition, for residual analysis, to avoid any information propagation from one modality to another, we perform a comprehensive study using 3 separate unimodal video and 2 unimodal audio encoders.
We discuss more related studies in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A10" title="Appendix J Extended Related Works â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">J</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Dataset Curation</h2>
<div class="ltx_para ltx_noindent" id="S3.p1">
<p class="ltx_p" id="S3.p1.4"><span class="ltx_text ltx_font_bold" id="S3.p1.4.5">Brain imaging dataset.</span>
We experiment with a multi-modal naturalistic fMRI dataset, Movie10Â <cite class="ltx_cite ltx_citemacro_citep">(St-Laurent etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib50" title="">2023</a>)</cite> obtained from the Courtois NeuroMod databank. This dataset was collected while six human subjects passively watched four different movies: <em class="ltx_emph ltx_font_italic" id="S3.p1.1.1">The Bourne supremacy (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.1.1.m1.1"><semantics id="S3.p1.1.1.m1.1a"><mo id="S3.p1.1.1.m1.1.1" xref="S3.p1.1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.p1.1.1.m1.1b"><csymbol cd="latexml" id="S3.p1.1.1.m1.1.1.cmml" xref="S3.p1.1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.1.m1.1d">âˆ¼</annotation></semantics></math>100 mins)</em>, <em class="ltx_emph ltx_font_italic" id="S3.p1.2.2">The wolf of wall street (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.2.2.m1.1"><semantics id="S3.p1.2.2.m1.1a"><mo id="S3.p1.2.2.m1.1.1" xref="S3.p1.2.2.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.p1.2.2.m1.1b"><csymbol cd="latexml" id="S3.p1.2.2.m1.1.1.cmml" xref="S3.p1.2.2.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.2.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.2.m1.1d">âˆ¼</annotation></semantics></math>170 mins)</em>, <em class="ltx_emph ltx_font_italic" id="S3.p1.3.3">Hidden figures (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.3.3.m1.1"><semantics id="S3.p1.3.3.m1.1a"><mo id="S3.p1.3.3.m1.1.1" xref="S3.p1.3.3.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.p1.3.3.m1.1b"><csymbol cd="latexml" id="S3.p1.3.3.m1.1.1.cmml" xref="S3.p1.3.3.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.3.3.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.p1.3.3.m1.1d">âˆ¼</annotation></semantics></math>120 mins)</em> and <em class="ltx_emph ltx_font_italic" id="S3.p1.4.4">Life (<math alttext="\sim" class="ltx_Math" display="inline" id="S3.p1.4.4.m1.1"><semantics id="S3.p1.4.4.m1.1a"><mo id="S3.p1.4.4.m1.1.1" xref="S3.p1.4.4.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S3.p1.4.4.m1.1b"><csymbol cd="latexml" id="S3.p1.4.4.m1.1.1.cmml" xref="S3.p1.4.4.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.4.4.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S3.p1.4.4.m1.1d">âˆ¼</annotation></semantics></math>50 mins)</em>. Among these, <em class="ltx_emph ltx_font_italic" id="S3.p1.4.6">Hidden figures</em> and <em class="ltx_emph ltx_font_italic" id="S3.p1.4.7">Life</em> are repeated twice, with the repeats used for testing and the remaining movies for training. In this work, we use <em class="ltx_emph ltx_font_italic" id="S3.p1.4.8">Life</em> movie for testing where we average the two repetitions to reduce noise in brain data.
This dataset is one of the largest publicly available multi-modal fMRI dataset in
terms of number of samples per participant. It includes 4024 TRs (Time Repetitions)
for <em class="ltx_emph ltx_font_italic" id="S3.p1.4.9">The Bourne supremacy</em>, 6898 TRs for <em class="ltx_emph ltx_font_italic" id="S3.p1.4.10">The wolf of wall street</em> used in train and 2028 TRs for <em class="ltx_emph ltx_font_italic" id="S3.p1.4.11">Life</em> in test. The fMRI data is collected every 1.49 seconds (= 1 TR).</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">The dataset is already preprocessed and projected onto the surface space (â€œfsaverage6â€).
We use the multi-modal parcellation of the human cerebral cortex based on the Glasser Atlas (which consists of 180 regions of interest in each hemisphere) to report the ROI (region of interest) analysis for the brain mapsÂ <cite class="ltx_cite ltx_citemacro_citep">(Glasser etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib26" title="">2016</a>)</cite>. This includes four visual processing regions (early visual cortex (EVC), object-related areas (LOC), face-related areas (OFA) and scene-related areas (PPA)), one early auditory area (AC), and eight language-relevant regions, encompassing broader language regions: angular gyrus (AG), anterior temporal lobe (ATL), posterior temporal lobe (PTL), inferior frontal gyrus (IFG), inferior frontal gyrus orbital (IFGOrb), middle frontal gyrus (MFG), posterior cingulate cortex (PCC) and dorsal medium prefrontal cortex (dmPFC), based on the Fedorenko labâ€™s language parcelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Milton etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib35" title="">2021</a>; Desai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib15" title="">2023</a>)</cite>.
We list detailed sub-ROIs in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A3" title="Appendix C Detailed sub-ROIs of language, visual and auditory regions â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">C</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S3.p3">
<p class="ltx_p" id="S3.p3.3"><span class="ltx_text ltx_font_bold" id="S3.p3.3.1">Estimating dataset cross-subject prediction accuracy.</span>
To account for the intrinsic noise in biological measurements, we adapt <cite class="ltx_cite ltx_citemacro_citet">Schrimpf etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib49" title="">2021</a>)</cite>â€™s method to estimate the cross-subject prediction accuracy for a modelâ€™s performance for the Movie10 fMRI dataset.
By subsampling fMRI dataset from 6 participants, we generate all possible combinations of <math alttext="s" class="ltx_Math" display="inline" id="S3.p3.1.m1.1"><semantics id="S3.p3.1.m1.1a"><mi id="S3.p3.1.m1.1.1" xref="S3.p3.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p3.1.m1.1b"><ci id="S3.p3.1.m1.1.1.cmml" xref="S3.p3.1.m1.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.p3.1.m1.1d">italic_s</annotation></semantics></math> participants (<math alttext="s" class="ltx_Math" display="inline" id="S3.p3.2.m2.1"><semantics id="S3.p3.2.m2.1a"><mi id="S3.p3.2.m2.1.1" xref="S3.p3.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="S3.p3.2.m2.1b"><ci id="S3.p3.2.m2.1.1.cmml" xref="S3.p3.2.m2.1.1">ğ‘ </ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="S3.p3.2.m2.1d">italic_s</annotation></semantics></math> <math alttext="\in" class="ltx_Math" display="inline" id="S3.p3.3.m3.1"><semantics id="S3.p3.3.m3.1a"><mo id="S3.p3.3.m3.1.1" xref="S3.p3.3.m3.1.1.cmml">âˆˆ</mo><annotation-xml encoding="MathML-Content" id="S3.p3.3.m3.1b"><in id="S3.p3.3.m3.1.1.cmml" xref="S3.p3.3.m3.1.1"></in></annotation-xml><annotation encoding="application/x-tex" id="S3.p3.3.m3.1c">\in</annotation><annotation encoding="application/x-llamapun" id="S3.p3.3.m3.1d">âˆˆ</annotation></semantics></math> [2,6]) for watching movies, and use a voxel-wise encoding model (see Sec. <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S5" title="5 Experimental Setup â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">5</span></a>) to predict one participantâ€™s response from others.
Note that the estimated cross-subject prediction accuracy is based on the assumption of a perfect model, which might differ from real-world scenarios, yet offers valuable insights into modelâ€™s performance.
We estimate cross-subject prediction accuracy by training on the combined brain data from <span class="ltx_text ltx_font_italic" id="S3.p3.3.2">The Bourne supremacy</span> and <span class="ltx_text ltx_font_italic" id="S3.p3.3.3">The wolf of wall street</span> and testing on the brain data from the movie <span class="ltx_text ltx_font_italic" id="S3.p3.3.4">Life</span>.
We present the average cross-subject prediction accuracy across voxels for the <em class="ltx_emph ltx_font_italic" id="S3.p3.3.5">Movie10 fMRI</em> dataset across subjects in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A2" title="Appendix B Cross-subject prediction accuracy â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">B</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Methodology</h2>
<div class="ltx_para ltx_noindent" id="S4.p1">
<p class="ltx_p" id="S4.p1.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.p1.1.1">Multi-modal Models</span>:
To analyse how human brain process information while engaged in multi-modal stimuli, we use recent popular deep learning models to explore multiple modalities information and build the encoding models in two different ways: â€œcross-modality pretrainingâ€ and â€œjoint pretrainingâ€.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p2">
<p class="ltx_p" id="S4.p2.1"><span class="ltx_text ltx_font_bold" id="S4.p2.1.1">Cross-modality pretrained multi-modal models.</span>
Cross-modality representations involve transferring information or learning from one modality to another. For example, in a cross-modal learning scenario, text descriptions can be used to improve the accuracy of image/video recognition tasks. This approach is particularly effective when one modality has limited data or indirect relevance to the task, but can be augmented by information from another modality.
Recently, a cross-modal model called ImageBind (IB)Â <cite class="ltx_cite ltx_citemacro_citep">(Girdhar etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib25" title="">2023</a>)</cite> has shown immense promise in binding data from six modalities at once, without the need for explicit supervision. ImageBind model uses separate encoders for each individual modality and learns a single shared representation space by leveraging multiple types of image-paired data. ImageBind has 12 layers and outputs a 1024-D representation for each modality.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p3">
<p class="ltx_p" id="S4.p3.1"><span class="ltx_text ltx_font_bold" id="S4.p3.1.1">Jointly pretrained multi-modal models.</span>
Jointly pretrained multi-modal model representations, on the other hand, involve combining data from multiple modalities to build a more comprehensive joint understanding to improve decision-making processes. The system processes these diverse inputs concurrently to make more informed and robust decisions.
TVLTÂ <cite class="ltx_cite ltx_citemacro_citep">(Tang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib54" title="">2022</a>)</cite> is an end-to-end Text-less Vision-Language multi-modal Transformer model for learning joint representations of video and speech from YouTube videos. This joint encoder model consists of a 12-layer encoder (hidden size 768) and uses masked autoencoding objective for both videos and speech. Given the video-speech pairs, the TVLT model gives 768D representations for each modality across 12 layers.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p4">
<p class="ltx_p" id="S4.p4.1"><span class="ltx_text ltx_font_bold" id="S4.p4.1.1">Extraction of multi-modal features.</span>
To extract video and audio embedding representations from multi-modal models for the brain encoding task, we input video and audio pairs at each TR simultaneously, and obtain the output embeddings for the two modalities from the last layer.
Here, we first segment the input video and audio into clips corresponding to 1.49 seconds, which matches the fMRI image rate. For both the models, ImageBind and TVLT, we use the pretrained Transformer weights. ImageBind generates an embedding for each modality (IB video and IB audio) at the output. We refer to IB video, IB-audio as modality-specific embeddings extracted from multi-modal models in the remainder of the paper. We concatenate these embeddings to create what we refer to as IB concat embeddings. On the other hand, TVLT provides a joint embedding across all modalities at each layer. Only for the last layer, TVLT provides an embedding for each modality - referred to as modality specific embeddings extracted from multi-modal models, similar to IB-video and IB-audio.</p>
</div>
<div class="ltx_para ltx_noindent" id="S4.p5">
<p class="ltx_p" id="S4.p5.1"><span class="ltx_text ltx_font_bold ltx_framed ltx_framed_underline" id="S4.p5.1.1">Unimodal Models</span>:
To investigate the effectiveness of multi-modal representations in comparison to representations for individual modalities, we use the following methods to obtain embeddings for individual modalities.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p5.1.2">Video-based models.</span>
To extract representations of the video stimulus, we use three popular pretrained Transformer video-based models from HuggingfaceÂ <cite class="ltx_cite ltx_citemacro_citep">(Wolf etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib64" title="">2020</a>)</cite>: (1) Vision Transformer Base (ViT-B)Â <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib20" title="">2020</a>)</cite>,
(2) Video Masked Autoencoders (VideoMAE)Â <cite class="ltx_cite ltx_citemacro_citep">(Tong etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib57" title="">2022</a>)</cite> and
(3) Video Vision Transformer (ViViT)Â <cite class="ltx_cite ltx_citemacro_citep">(Arnab etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib3" title="">2021</a>)</cite>. Details of each model are reported in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A4.T1" title="Table 1 â€£ Appendix D Details of pretrained Transformer models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">1</span></a> in Appendix.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p5.1.3">Speech-based models.</span>
Similar to video-based models, we use two popular pretrained Transformer speech-based models from Huggingface: (1) Wav2Vec2.0Â <cite class="ltx_cite ltx_citemacro_citep">(Baevski etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib7" title="">2020</a>)</cite> and (2) ASTÂ <cite class="ltx_cite ltx_citemacro_citep">(Baade etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib6" title="">2022</a>)</cite>.
Details of each model are reported in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A4.T1" title="Table 1 â€£ Appendix D Details of pretrained Transformer models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">1</span></a> in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A4" title="Appendix D Details of pretrained Transformer models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">D</span></a>.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p5.1.4">Extraction of video features.</span>
ViT-BÂ <cite class="ltx_cite ltx_citemacro_citep">(Dosovitskiy etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib20" title="">2020</a>)</cite>, the underlying video encoder model for ImageBind is used for extracting representations for all frames in each TR for every video. To extract embedding at each TR, we average all frame embeddings and obtain the corresponding video representation. For VideoMAE and ViViT, we directly obtain the video embeddings for each TR. All 3 models provide 768 dimensional representations and all of them are 12-layer Transformer encoders.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S4.p5.1.5">Extraction of speech features.</span>
To explore whether speech models incorporate linguistic information, we extract representations beyond 1.49 secs, i.e., we considered context window of 16 secs with stride of 100 msecs and considered the last token as the representative for each context window. The pretrained speech-based models output token representations at different layers. Both Wav2Vec2.0 and AST models provide 768 dimensional representations and all of them are 12-layer Transformer encoders.
Finally, we align these representations with the fMRI data acquisition rate by downsampling the stimulus features with a 3-lobed Lanczos filter, thus producing chunk-embeddings for each TR.</p>
</div>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Experimental Setup</h2>
<div class="ltx_para ltx_noindent" id="S5.p1">
<p class="ltx_p" id="S5.p1.8"><span class="ltx_text ltx_font_bold" id="S5.p1.8.1">Encoding model.</span>

We train bootstrap ridge regression based voxel-wise encoding modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Deniz etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib14" title="">2019</a>)</cite> to predict the fMRI brain activity associated with the stimulus representations obtained from the individual modalities (speech and video) and multi-modal embeddings from cross-modal and jointly pretrained multi-modal models. We employ z-score thresholding separately for both input stimulus representations and brain recordings for training and test datasets. This helps identify and remove extreme outliers that could disproportionately affect the Pearson Correlation results. For each subject, we account for the delay in the hemodynamic response by modeling hemodynamic response function using a finite response filter (FIR) per voxel with 5 temporal delays (TRs) corresponding to <math alttext="\sim" class="ltx_Math" display="inline" id="S5.p1.1.m1.1"><semantics id="S5.p1.1.m1.1a"><mo id="S5.p1.1.m1.1.1" xref="S5.p1.1.m1.1.1.cmml">âˆ¼</mo><annotation-xml encoding="MathML-Content" id="S5.p1.1.m1.1b"><csymbol cd="latexml" id="S5.p1.1.m1.1.1.cmml" xref="S5.p1.1.m1.1.1">similar-to</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.1.m1.1c">\sim</annotation><annotation encoding="application/x-llamapun" id="S5.p1.1.m1.1d">âˆ¼</annotation></semantics></math>7.5 secondsÂ <cite class="ltx_cite ltx_citemacro_citep">(Huth etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib31" title="">2022</a>)</cite>. Formally, at each time step <math alttext="t" class="ltx_Math" display="inline" id="S5.p1.2.m2.1"><semantics id="S5.p1.2.m2.1a"><mi id="S5.p1.2.m2.1.1" xref="S5.p1.2.m2.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S5.p1.2.m2.1b"><ci id="S5.p1.2.m2.1.1.cmml" xref="S5.p1.2.m2.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.2.m2.1c">t</annotation><annotation encoding="application/x-llamapun" id="S5.p1.2.m2.1d">italic_t</annotation></semantics></math>, we encode the stimuli as <math alttext="X_{t}\in\mathbb{R}^{D}" class="ltx_Math" display="inline" id="S5.p1.3.m3.1"><semantics id="S5.p1.3.m3.1a"><mrow id="S5.p1.3.m3.1.1" xref="S5.p1.3.m3.1.1.cmml"><msub id="S5.p1.3.m3.1.1.2" xref="S5.p1.3.m3.1.1.2.cmml"><mi id="S5.p1.3.m3.1.1.2.2" xref="S5.p1.3.m3.1.1.2.2.cmml">X</mi><mi id="S5.p1.3.m3.1.1.2.3" xref="S5.p1.3.m3.1.1.2.3.cmml">t</mi></msub><mo id="S5.p1.3.m3.1.1.1" xref="S5.p1.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S5.p1.3.m3.1.1.3" xref="S5.p1.3.m3.1.1.3.cmml"><mi id="S5.p1.3.m3.1.1.3.2" xref="S5.p1.3.m3.1.1.3.2.cmml">â„</mi><mi id="S5.p1.3.m3.1.1.3.3" xref="S5.p1.3.m3.1.1.3.3.cmml">D</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.3.m3.1b"><apply id="S5.p1.3.m3.1.1.cmml" xref="S5.p1.3.m3.1.1"><in id="S5.p1.3.m3.1.1.1.cmml" xref="S5.p1.3.m3.1.1.1"></in><apply id="S5.p1.3.m3.1.1.2.cmml" xref="S5.p1.3.m3.1.1.2"><csymbol cd="ambiguous" id="S5.p1.3.m3.1.1.2.1.cmml" xref="S5.p1.3.m3.1.1.2">subscript</csymbol><ci id="S5.p1.3.m3.1.1.2.2.cmml" xref="S5.p1.3.m3.1.1.2.2">ğ‘‹</ci><ci id="S5.p1.3.m3.1.1.2.3.cmml" xref="S5.p1.3.m3.1.1.2.3">ğ‘¡</ci></apply><apply id="S5.p1.3.m3.1.1.3.cmml" xref="S5.p1.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.p1.3.m3.1.1.3.1.cmml" xref="S5.p1.3.m3.1.1.3">superscript</csymbol><ci id="S5.p1.3.m3.1.1.3.2.cmml" xref="S5.p1.3.m3.1.1.3.2">â„</ci><ci id="S5.p1.3.m3.1.1.3.3.cmml" xref="S5.p1.3.m3.1.1.3.3">ğ·</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.3.m3.1c">X_{t}\in\mathbb{R}^{D}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.3.m3.1d">italic_X start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_D end_POSTSUPERSCRIPT</annotation></semantics></math> and brain region voxels <math alttext="Y_{t}\in\mathbb{R}^{V}" class="ltx_Math" display="inline" id="S5.p1.4.m4.1"><semantics id="S5.p1.4.m4.1a"><mrow id="S5.p1.4.m4.1.1" xref="S5.p1.4.m4.1.1.cmml"><msub id="S5.p1.4.m4.1.1.2" xref="S5.p1.4.m4.1.1.2.cmml"><mi id="S5.p1.4.m4.1.1.2.2" xref="S5.p1.4.m4.1.1.2.2.cmml">Y</mi><mi id="S5.p1.4.m4.1.1.2.3" xref="S5.p1.4.m4.1.1.2.3.cmml">t</mi></msub><mo id="S5.p1.4.m4.1.1.1" xref="S5.p1.4.m4.1.1.1.cmml">âˆˆ</mo><msup id="S5.p1.4.m4.1.1.3" xref="S5.p1.4.m4.1.1.3.cmml"><mi id="S5.p1.4.m4.1.1.3.2" xref="S5.p1.4.m4.1.1.3.2.cmml">â„</mi><mi id="S5.p1.4.m4.1.1.3.3" xref="S5.p1.4.m4.1.1.3.3.cmml">V</mi></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p1.4.m4.1b"><apply id="S5.p1.4.m4.1.1.cmml" xref="S5.p1.4.m4.1.1"><in id="S5.p1.4.m4.1.1.1.cmml" xref="S5.p1.4.m4.1.1.1"></in><apply id="S5.p1.4.m4.1.1.2.cmml" xref="S5.p1.4.m4.1.1.2"><csymbol cd="ambiguous" id="S5.p1.4.m4.1.1.2.1.cmml" xref="S5.p1.4.m4.1.1.2">subscript</csymbol><ci id="S5.p1.4.m4.1.1.2.2.cmml" xref="S5.p1.4.m4.1.1.2.2">ğ‘Œ</ci><ci id="S5.p1.4.m4.1.1.2.3.cmml" xref="S5.p1.4.m4.1.1.2.3">ğ‘¡</ci></apply><apply id="S5.p1.4.m4.1.1.3.cmml" xref="S5.p1.4.m4.1.1.3"><csymbol cd="ambiguous" id="S5.p1.4.m4.1.1.3.1.cmml" xref="S5.p1.4.m4.1.1.3">superscript</csymbol><ci id="S5.p1.4.m4.1.1.3.2.cmml" xref="S5.p1.4.m4.1.1.3.2">â„</ci><ci id="S5.p1.4.m4.1.1.3.3.cmml" xref="S5.p1.4.m4.1.1.3.3">ğ‘‰</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.4.m4.1c">Y_{t}\in\mathbb{R}^{V}</annotation><annotation encoding="application/x-llamapun" id="S5.p1.4.m4.1d">italic_Y start_POSTSUBSCRIPT italic_t end_POSTSUBSCRIPT âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_V end_POSTSUPERSCRIPT</annotation></semantics></math>, where <math alttext="D" class="ltx_Math" display="inline" id="S5.p1.5.m5.1"><semantics id="S5.p1.5.m5.1a"><mi id="S5.p1.5.m5.1.1" xref="S5.p1.5.m5.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S5.p1.5.m5.1b"><ci id="S5.p1.5.m5.1.1.cmml" xref="S5.p1.5.m5.1.1">ğ·</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.5.m5.1c">D</annotation><annotation encoding="application/x-llamapun" id="S5.p1.5.m5.1d">italic_D</annotation></semantics></math> denotes the dimension of the concatenation of delayed 5 TRs, and <math alttext="V" class="ltx_Math" display="inline" id="S5.p1.6.m6.1"><semantics id="S5.p1.6.m6.1a"><mi id="S5.p1.6.m6.1.1" xref="S5.p1.6.m6.1.1.cmml">V</mi><annotation-xml encoding="MathML-Content" id="S5.p1.6.m6.1b"><ci id="S5.p1.6.m6.1.1.cmml" xref="S5.p1.6.m6.1.1">ğ‘‰</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.6.m6.1c">V</annotation><annotation encoding="application/x-llamapun" id="S5.p1.6.m6.1d">italic_V</annotation></semantics></math> denotes the number of voxels. Overall, with <math alttext="N" class="ltx_Math" display="inline" id="S5.p1.7.m7.1"><semantics id="S5.p1.7.m7.1a"><mi id="S5.p1.7.m7.1.1" xref="S5.p1.7.m7.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p1.7.m7.1b"><ci id="S5.p1.7.m7.1.1.cmml" xref="S5.p1.7.m7.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.7.m7.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.p1.7.m7.1d">italic_N</annotation></semantics></math> such TRs, we obtain <math alttext="N" class="ltx_Math" display="inline" id="S5.p1.8.m8.1"><semantics id="S5.p1.8.m8.1a"><mi id="S5.p1.8.m8.1.1" xref="S5.p1.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S5.p1.8.m8.1b"><ci id="S5.p1.8.m8.1.1.cmml" xref="S5.p1.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p1.8.m8.1c">N</annotation><annotation encoding="application/x-llamapun" id="S5.p1.8.m8.1d">italic_N</annotation></semantics></math> training examples.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Train-test setup.</span>
We build encoding models where the train and test sets are totally disjoint and the model cannot use any clock relationships from the training data during inference. To be completely clear: independent encoding models are trained for each subject using data concatenated from two movies (<span class="ltx_text ltx_font_italic" id="S5.p2.1.2">The Bourne supremacy</span>: 4024 TRs and The <span class="ltx_text ltx_font_italic" id="S5.p2.1.3">wolf of wall street</span>: 6898 TRs). The test set consisted only data from the <span class="ltx_text ltx_font_italic" id="S5.p2.1.4">â€œLifeâ€ movie</span> (2028 TRs). Thus there is no possibility of any information leakage during inference on the test set. Model details and hyper-parameter settings are in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A5" title="Appendix E Implementation details for reproducibility. â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">E</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p3">
<p class="ltx_p" id="S5.p3.1"><span class="ltx_text ltx_font_bold" id="S5.p3.1.1">Removal of a single modality features from multi-modal representations.</span>
For removing unimodal model representations (<span class="ltx_text ltx_font_italic" id="S5.p3.1.2">VM</span> or <span class="ltx_text ltx_font_italic" id="S5.p3.1.3">SM</span>) from the multi-modal model representations (<span class="ltx_text ltx_font_italic" id="S5.p3.1.4">CM</span> or <span class="ltx_text ltx_font_italic" id="S5.p3.1.5">JM</span>), we employ the direct or residual approach, as outlined byÂ <cite class="ltx_cite ltx_citemacro_citet">Toneva etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib56" title="">2022</a>); Oota etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib40" title="">2023a</a>); Oota &amp; Toneva (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib37" title="">2023</a>); Dong &amp; Toneva (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib19" title="">2023b</a>); Oota etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib42" title="">2024a</a>)</cite>. This method estimates the impact of specific modality features on the alignment between the model and brain recordings by comparing the alignment before and after computationally removing the targeted modality features from the multi-modal representations. To remove features corresponding to a particular modality (<span class="ltx_text ltx_font_italic" id="S5.p3.1.6">VM</span> or <span class="ltx_text ltx_font_italic" id="S5.p3.1.7">SM</span>) from multi-modal model representations, we
remove the linear contribution of the unimodal features by training a ridge regression model (<math alttext="r" class="ltx_Math" display="inline" id="S5.p3.1.m1.1"><semantics id="S5.p3.1.m1.1a"><mi id="S5.p3.1.m1.1.1" xref="S5.p3.1.m1.1.1.cmml">r</mi><annotation-xml encoding="MathML-Content" id="S5.p3.1.m1.1b"><ci id="S5.p3.1.m1.1.1.cmml" xref="S5.p3.1.m1.1.1">ğ‘Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p3.1.m1.1c">r</annotation><annotation encoding="application/x-llamapun" id="S5.p3.1.m1.1d">italic_r</annotation></semantics></math>), where the unimodal feature vector is the input and the multi-modal representation serves as the target. Since our encoding model (ridge regression) is also a linear function, this linear removal limits the contribution of features for the particular modality to the eventual brain alignment. The approach is illustrated in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S1.F1" title="Figure 1 â€£ 1 Introduction â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">1</span></a> (B).</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p4">
<p class="ltx_p" id="S5.p4.5"><span class="ltx_text ltx_font_bold" id="S5.p4.5.1">Evaluation metrics.</span>
We evaluate our models using Pearson Correlation (PC) which is a standard metric for evaluating brain alignment <cite class="ltx_cite ltx_citemacro_citep">(Jain &amp; Huth, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib32" title="">2018</a>; Schrimpf etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib49" title="">2021</a>; Goldstein etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib27" title="">2022</a>)</cite>. Let TR be the number of time repetitions in the test set. Let <math alttext="Y=\{Y_{i}\}_{i=1}^{TR}" class="ltx_Math" display="inline" id="S5.p4.1.m1.1"><semantics id="S5.p4.1.m1.1a"><mrow id="S5.p4.1.m1.1.1" xref="S5.p4.1.m1.1.1.cmml"><mi id="S5.p4.1.m1.1.1.3" xref="S5.p4.1.m1.1.1.3.cmml">Y</mi><mo id="S5.p4.1.m1.1.1.2" xref="S5.p4.1.m1.1.1.2.cmml">=</mo><msubsup id="S5.p4.1.m1.1.1.1" xref="S5.p4.1.m1.1.1.1.cmml"><mrow id="S5.p4.1.m1.1.1.1.1.1.1" xref="S5.p4.1.m1.1.1.1.1.1.2.cmml"><mo id="S5.p4.1.m1.1.1.1.1.1.1.2" stretchy="false" xref="S5.p4.1.m1.1.1.1.1.1.2.cmml">{</mo><msub id="S5.p4.1.m1.1.1.1.1.1.1.1" xref="S5.p4.1.m1.1.1.1.1.1.1.1.cmml"><mi id="S5.p4.1.m1.1.1.1.1.1.1.1.2" xref="S5.p4.1.m1.1.1.1.1.1.1.1.2.cmml">Y</mi><mi id="S5.p4.1.m1.1.1.1.1.1.1.1.3" xref="S5.p4.1.m1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.p4.1.m1.1.1.1.1.1.1.3" stretchy="false" xref="S5.p4.1.m1.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S5.p4.1.m1.1.1.1.1.3" xref="S5.p4.1.m1.1.1.1.1.3.cmml"><mi id="S5.p4.1.m1.1.1.1.1.3.2" xref="S5.p4.1.m1.1.1.1.1.3.2.cmml">i</mi><mo id="S5.p4.1.m1.1.1.1.1.3.1" xref="S5.p4.1.m1.1.1.1.1.3.1.cmml">=</mo><mn id="S5.p4.1.m1.1.1.1.1.3.3" xref="S5.p4.1.m1.1.1.1.1.3.3.cmml">1</mn></mrow><mrow id="S5.p4.1.m1.1.1.1.3" xref="S5.p4.1.m1.1.1.1.3.cmml"><mi id="S5.p4.1.m1.1.1.1.3.2" xref="S5.p4.1.m1.1.1.1.3.2.cmml">T</mi><mo id="S5.p4.1.m1.1.1.1.3.1" xref="S5.p4.1.m1.1.1.1.3.1.cmml">â¢</mo><mi id="S5.p4.1.m1.1.1.1.3.3" xref="S5.p4.1.m1.1.1.1.3.3.cmml">R</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.1.m1.1b"><apply id="S5.p4.1.m1.1.1.cmml" xref="S5.p4.1.m1.1.1"><eq id="S5.p4.1.m1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.2"></eq><ci id="S5.p4.1.m1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.3">ğ‘Œ</ci><apply id="S5.p4.1.m1.1.1.1.cmml" xref="S5.p4.1.m1.1.1.1"><csymbol cd="ambiguous" id="S5.p4.1.m1.1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.1">superscript</csymbol><apply id="S5.p4.1.m1.1.1.1.1.cmml" xref="S5.p4.1.m1.1.1.1"><csymbol cd="ambiguous" id="S5.p4.1.m1.1.1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.1">subscript</csymbol><set id="S5.p4.1.m1.1.1.1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1"><apply id="S5.p4.1.m1.1.1.1.1.1.1.1.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p4.1.m1.1.1.1.1.1.1.1.1.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S5.p4.1.m1.1.1.1.1.1.1.1.2.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1.1.2">ğ‘Œ</ci><ci id="S5.p4.1.m1.1.1.1.1.1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S5.p4.1.m1.1.1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.1.1.3"><eq id="S5.p4.1.m1.1.1.1.1.3.1.cmml" xref="S5.p4.1.m1.1.1.1.1.3.1"></eq><ci id="S5.p4.1.m1.1.1.1.1.3.2.cmml" xref="S5.p4.1.m1.1.1.1.1.3.2">ğ‘–</ci><cn id="S5.p4.1.m1.1.1.1.1.3.3.cmml" type="integer" xref="S5.p4.1.m1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S5.p4.1.m1.1.1.1.3.cmml" xref="S5.p4.1.m1.1.1.1.3"><times id="S5.p4.1.m1.1.1.1.3.1.cmml" xref="S5.p4.1.m1.1.1.1.3.1"></times><ci id="S5.p4.1.m1.1.1.1.3.2.cmml" xref="S5.p4.1.m1.1.1.1.3.2">ğ‘‡</ci><ci id="S5.p4.1.m1.1.1.1.3.3.cmml" xref="S5.p4.1.m1.1.1.1.3.3">ğ‘…</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.1.m1.1c">Y=\{Y_{i}\}_{i=1}^{TR}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.1.m1.1d">italic_Y = { italic_Y start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T italic_R end_POSTSUPERSCRIPT</annotation></semantics></math> and <math alttext="\hat{Y}=\{\hat{Y}_{i}\}_{i=1}^{TR}" class="ltx_Math" display="inline" id="S5.p4.2.m2.1"><semantics id="S5.p4.2.m2.1a"><mrow id="S5.p4.2.m2.1.1" xref="S5.p4.2.m2.1.1.cmml"><mover accent="true" id="S5.p4.2.m2.1.1.3" xref="S5.p4.2.m2.1.1.3.cmml"><mi id="S5.p4.2.m2.1.1.3.2" xref="S5.p4.2.m2.1.1.3.2.cmml">Y</mi><mo id="S5.p4.2.m2.1.1.3.1" xref="S5.p4.2.m2.1.1.3.1.cmml">^</mo></mover><mo id="S5.p4.2.m2.1.1.2" xref="S5.p4.2.m2.1.1.2.cmml">=</mo><msubsup id="S5.p4.2.m2.1.1.1" xref="S5.p4.2.m2.1.1.1.cmml"><mrow id="S5.p4.2.m2.1.1.1.1.1.1" xref="S5.p4.2.m2.1.1.1.1.1.2.cmml"><mo id="S5.p4.2.m2.1.1.1.1.1.1.2" stretchy="false" xref="S5.p4.2.m2.1.1.1.1.1.2.cmml">{</mo><msub id="S5.p4.2.m2.1.1.1.1.1.1.1" xref="S5.p4.2.m2.1.1.1.1.1.1.1.cmml"><mover accent="true" id="S5.p4.2.m2.1.1.1.1.1.1.1.2" xref="S5.p4.2.m2.1.1.1.1.1.1.1.2.cmml"><mi id="S5.p4.2.m2.1.1.1.1.1.1.1.2.2" xref="S5.p4.2.m2.1.1.1.1.1.1.1.2.2.cmml">Y</mi><mo id="S5.p4.2.m2.1.1.1.1.1.1.1.2.1" xref="S5.p4.2.m2.1.1.1.1.1.1.1.2.1.cmml">^</mo></mover><mi id="S5.p4.2.m2.1.1.1.1.1.1.1.3" xref="S5.p4.2.m2.1.1.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S5.p4.2.m2.1.1.1.1.1.1.3" stretchy="false" xref="S5.p4.2.m2.1.1.1.1.1.2.cmml">}</mo></mrow><mrow id="S5.p4.2.m2.1.1.1.1.3" xref="S5.p4.2.m2.1.1.1.1.3.cmml"><mi id="S5.p4.2.m2.1.1.1.1.3.2" xref="S5.p4.2.m2.1.1.1.1.3.2.cmml">i</mi><mo id="S5.p4.2.m2.1.1.1.1.3.1" xref="S5.p4.2.m2.1.1.1.1.3.1.cmml">=</mo><mn id="S5.p4.2.m2.1.1.1.1.3.3" xref="S5.p4.2.m2.1.1.1.1.3.3.cmml">1</mn></mrow><mrow id="S5.p4.2.m2.1.1.1.3" xref="S5.p4.2.m2.1.1.1.3.cmml"><mi id="S5.p4.2.m2.1.1.1.3.2" xref="S5.p4.2.m2.1.1.1.3.2.cmml">T</mi><mo id="S5.p4.2.m2.1.1.1.3.1" xref="S5.p4.2.m2.1.1.1.3.1.cmml">â¢</mo><mi id="S5.p4.2.m2.1.1.1.3.3" xref="S5.p4.2.m2.1.1.1.3.3.cmml">R</mi></mrow></msubsup></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.2.m2.1b"><apply id="S5.p4.2.m2.1.1.cmml" xref="S5.p4.2.m2.1.1"><eq id="S5.p4.2.m2.1.1.2.cmml" xref="S5.p4.2.m2.1.1.2"></eq><apply id="S5.p4.2.m2.1.1.3.cmml" xref="S5.p4.2.m2.1.1.3"><ci id="S5.p4.2.m2.1.1.3.1.cmml" xref="S5.p4.2.m2.1.1.3.1">^</ci><ci id="S5.p4.2.m2.1.1.3.2.cmml" xref="S5.p4.2.m2.1.1.3.2">ğ‘Œ</ci></apply><apply id="S5.p4.2.m2.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.1.2.cmml" xref="S5.p4.2.m2.1.1.1">superscript</csymbol><apply id="S5.p4.2.m2.1.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.1.1.2.cmml" xref="S5.p4.2.m2.1.1.1">subscript</csymbol><set id="S5.p4.2.m2.1.1.1.1.1.2.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1"><apply id="S5.p4.2.m2.1.1.1.1.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S5.p4.2.m2.1.1.1.1.1.1.1.1.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.1">subscript</csymbol><apply id="S5.p4.2.m2.1.1.1.1.1.1.1.2.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.1.2"><ci id="S5.p4.2.m2.1.1.1.1.1.1.1.2.1.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.1.2.1">^</ci><ci id="S5.p4.2.m2.1.1.1.1.1.1.1.2.2.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.1.2.2">ğ‘Œ</ci></apply><ci id="S5.p4.2.m2.1.1.1.1.1.1.1.3.cmml" xref="S5.p4.2.m2.1.1.1.1.1.1.1.3">ğ‘–</ci></apply></set><apply id="S5.p4.2.m2.1.1.1.1.3.cmml" xref="S5.p4.2.m2.1.1.1.1.3"><eq id="S5.p4.2.m2.1.1.1.1.3.1.cmml" xref="S5.p4.2.m2.1.1.1.1.3.1"></eq><ci id="S5.p4.2.m2.1.1.1.1.3.2.cmml" xref="S5.p4.2.m2.1.1.1.1.3.2">ğ‘–</ci><cn id="S5.p4.2.m2.1.1.1.1.3.3.cmml" type="integer" xref="S5.p4.2.m2.1.1.1.1.3.3">1</cn></apply></apply><apply id="S5.p4.2.m2.1.1.1.3.cmml" xref="S5.p4.2.m2.1.1.1.3"><times id="S5.p4.2.m2.1.1.1.3.1.cmml" xref="S5.p4.2.m2.1.1.1.3.1"></times><ci id="S5.p4.2.m2.1.1.1.3.2.cmml" xref="S5.p4.2.m2.1.1.1.3.2">ğ‘‡</ci><ci id="S5.p4.2.m2.1.1.1.3.3.cmml" xref="S5.p4.2.m2.1.1.1.3.3">ğ‘…</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.2.m2.1c">\hat{Y}=\{\hat{Y}_{i}\}_{i=1}^{TR}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.2.m2.1d">over^ start_ARG italic_Y end_ARG = { over^ start_ARG italic_Y end_ARG start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT } start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_T italic_R end_POSTSUPERSCRIPT</annotation></semantics></math> denote the actual and predicted value vectors for a single voxel. Thus, <math alttext="Y~{}and~{}\hat{Y}~{}\in\mathbb{R}^{TR}" class="ltx_Math" display="inline" id="S5.p4.3.m3.1"><semantics id="S5.p4.3.m3.1a"><mrow id="S5.p4.3.m3.1.1" xref="S5.p4.3.m3.1.1.cmml"><mrow id="S5.p4.3.m3.1.1.2" xref="S5.p4.3.m3.1.1.2.cmml"><mi id="S5.p4.3.m3.1.1.2.2" xref="S5.p4.3.m3.1.1.2.2.cmml">Y</mi><mo id="S5.p4.3.m3.1.1.2.1" lspace="0.330em" xref="S5.p4.3.m3.1.1.2.1.cmml">â¢</mo><mi id="S5.p4.3.m3.1.1.2.3" xref="S5.p4.3.m3.1.1.2.3.cmml">a</mi><mo id="S5.p4.3.m3.1.1.2.1a" xref="S5.p4.3.m3.1.1.2.1.cmml">â¢</mo><mi id="S5.p4.3.m3.1.1.2.4" xref="S5.p4.3.m3.1.1.2.4.cmml">n</mi><mo id="S5.p4.3.m3.1.1.2.1b" xref="S5.p4.3.m3.1.1.2.1.cmml">â¢</mo><mi id="S5.p4.3.m3.1.1.2.5" xref="S5.p4.3.m3.1.1.2.5.cmml">d</mi><mo id="S5.p4.3.m3.1.1.2.1c" lspace="0.330em" xref="S5.p4.3.m3.1.1.2.1.cmml">â¢</mo><mover accent="true" id="S5.p4.3.m3.1.1.2.6" xref="S5.p4.3.m3.1.1.2.6.cmml"><mi id="S5.p4.3.m3.1.1.2.6.2" xref="S5.p4.3.m3.1.1.2.6.2.cmml">Y</mi><mo id="S5.p4.3.m3.1.1.2.6.1" xref="S5.p4.3.m3.1.1.2.6.1.cmml">^</mo></mover></mrow><mo id="S5.p4.3.m3.1.1.1" xref="S5.p4.3.m3.1.1.1.cmml">âˆˆ</mo><msup id="S5.p4.3.m3.1.1.3" xref="S5.p4.3.m3.1.1.3.cmml"><mi id="S5.p4.3.m3.1.1.3.2" xref="S5.p4.3.m3.1.1.3.2.cmml">â„</mi><mrow id="S5.p4.3.m3.1.1.3.3" xref="S5.p4.3.m3.1.1.3.3.cmml"><mi id="S5.p4.3.m3.1.1.3.3.2" xref="S5.p4.3.m3.1.1.3.3.2.cmml">T</mi><mo id="S5.p4.3.m3.1.1.3.3.1" xref="S5.p4.3.m3.1.1.3.3.1.cmml">â¢</mo><mi id="S5.p4.3.m3.1.1.3.3.3" xref="S5.p4.3.m3.1.1.3.3.3.cmml">R</mi></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.3.m3.1b"><apply id="S5.p4.3.m3.1.1.cmml" xref="S5.p4.3.m3.1.1"><in id="S5.p4.3.m3.1.1.1.cmml" xref="S5.p4.3.m3.1.1.1"></in><apply id="S5.p4.3.m3.1.1.2.cmml" xref="S5.p4.3.m3.1.1.2"><times id="S5.p4.3.m3.1.1.2.1.cmml" xref="S5.p4.3.m3.1.1.2.1"></times><ci id="S5.p4.3.m3.1.1.2.2.cmml" xref="S5.p4.3.m3.1.1.2.2">ğ‘Œ</ci><ci id="S5.p4.3.m3.1.1.2.3.cmml" xref="S5.p4.3.m3.1.1.2.3">ğ‘</ci><ci id="S5.p4.3.m3.1.1.2.4.cmml" xref="S5.p4.3.m3.1.1.2.4">ğ‘›</ci><ci id="S5.p4.3.m3.1.1.2.5.cmml" xref="S5.p4.3.m3.1.1.2.5">ğ‘‘</ci><apply id="S5.p4.3.m3.1.1.2.6.cmml" xref="S5.p4.3.m3.1.1.2.6"><ci id="S5.p4.3.m3.1.1.2.6.1.cmml" xref="S5.p4.3.m3.1.1.2.6.1">^</ci><ci id="S5.p4.3.m3.1.1.2.6.2.cmml" xref="S5.p4.3.m3.1.1.2.6.2">ğ‘Œ</ci></apply></apply><apply id="S5.p4.3.m3.1.1.3.cmml" xref="S5.p4.3.m3.1.1.3"><csymbol cd="ambiguous" id="S5.p4.3.m3.1.1.3.1.cmml" xref="S5.p4.3.m3.1.1.3">superscript</csymbol><ci id="S5.p4.3.m3.1.1.3.2.cmml" xref="S5.p4.3.m3.1.1.3.2">â„</ci><apply id="S5.p4.3.m3.1.1.3.3.cmml" xref="S5.p4.3.m3.1.1.3.3"><times id="S5.p4.3.m3.1.1.3.3.1.cmml" xref="S5.p4.3.m3.1.1.3.3.1"></times><ci id="S5.p4.3.m3.1.1.3.3.2.cmml" xref="S5.p4.3.m3.1.1.3.3.2">ğ‘‡</ci><ci id="S5.p4.3.m3.1.1.3.3.3.cmml" xref="S5.p4.3.m3.1.1.3.3.3">ğ‘…</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.3.m3.1c">Y~{}and~{}\hat{Y}~{}\in\mathbb{R}^{TR}</annotation><annotation encoding="application/x-llamapun" id="S5.p4.3.m3.1d">italic_Y italic_a italic_n italic_d over^ start_ARG italic_Y end_ARG âˆˆ blackboard_R start_POSTSUPERSCRIPT italic_T italic_R end_POSTSUPERSCRIPT</annotation></semantics></math>.
We use Pearson Correlation (PC) which is computed as <math alttext="corr(Y,\hat{Y})" class="ltx_Math" display="inline" id="S5.p4.4.m4.2"><semantics id="S5.p4.4.m4.2a"><mrow id="S5.p4.4.m4.2.3" xref="S5.p4.4.m4.2.3.cmml"><mi id="S5.p4.4.m4.2.3.2" xref="S5.p4.4.m4.2.3.2.cmml">c</mi><mo id="S5.p4.4.m4.2.3.1" xref="S5.p4.4.m4.2.3.1.cmml">â¢</mo><mi id="S5.p4.4.m4.2.3.3" xref="S5.p4.4.m4.2.3.3.cmml">o</mi><mo id="S5.p4.4.m4.2.3.1a" xref="S5.p4.4.m4.2.3.1.cmml">â¢</mo><mi id="S5.p4.4.m4.2.3.4" xref="S5.p4.4.m4.2.3.4.cmml">r</mi><mo id="S5.p4.4.m4.2.3.1b" xref="S5.p4.4.m4.2.3.1.cmml">â¢</mo><mi id="S5.p4.4.m4.2.3.5" xref="S5.p4.4.m4.2.3.5.cmml">r</mi><mo id="S5.p4.4.m4.2.3.1c" xref="S5.p4.4.m4.2.3.1.cmml">â¢</mo><mrow id="S5.p4.4.m4.2.3.6.2" xref="S5.p4.4.m4.2.3.6.1.cmml"><mo id="S5.p4.4.m4.2.3.6.2.1" stretchy="false" xref="S5.p4.4.m4.2.3.6.1.cmml">(</mo><mi id="S5.p4.4.m4.1.1" xref="S5.p4.4.m4.1.1.cmml">Y</mi><mo id="S5.p4.4.m4.2.3.6.2.2" xref="S5.p4.4.m4.2.3.6.1.cmml">,</mo><mover accent="true" id="S5.p4.4.m4.2.2" xref="S5.p4.4.m4.2.2.cmml"><mi id="S5.p4.4.m4.2.2.2" xref="S5.p4.4.m4.2.2.2.cmml">Y</mi><mo id="S5.p4.4.m4.2.2.1" xref="S5.p4.4.m4.2.2.1.cmml">^</mo></mover><mo id="S5.p4.4.m4.2.3.6.2.3" stretchy="false" xref="S5.p4.4.m4.2.3.6.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S5.p4.4.m4.2b"><apply id="S5.p4.4.m4.2.3.cmml" xref="S5.p4.4.m4.2.3"><times id="S5.p4.4.m4.2.3.1.cmml" xref="S5.p4.4.m4.2.3.1"></times><ci id="S5.p4.4.m4.2.3.2.cmml" xref="S5.p4.4.m4.2.3.2">ğ‘</ci><ci id="S5.p4.4.m4.2.3.3.cmml" xref="S5.p4.4.m4.2.3.3">ğ‘œ</ci><ci id="S5.p4.4.m4.2.3.4.cmml" xref="S5.p4.4.m4.2.3.4">ğ‘Ÿ</ci><ci id="S5.p4.4.m4.2.3.5.cmml" xref="S5.p4.4.m4.2.3.5">ğ‘Ÿ</ci><interval closure="open" id="S5.p4.4.m4.2.3.6.1.cmml" xref="S5.p4.4.m4.2.3.6.2"><ci id="S5.p4.4.m4.1.1.cmml" xref="S5.p4.4.m4.1.1">ğ‘Œ</ci><apply id="S5.p4.4.m4.2.2.cmml" xref="S5.p4.4.m4.2.2"><ci id="S5.p4.4.m4.2.2.1.cmml" xref="S5.p4.4.m4.2.2.1">^</ci><ci id="S5.p4.4.m4.2.2.2.cmml" xref="S5.p4.4.m4.2.2.2">ğ‘Œ</ci></apply></interval></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.4.m4.2c">corr(Y,\hat{Y})</annotation><annotation encoding="application/x-llamapun" id="S5.p4.4.m4.2d">italic_c italic_o italic_r italic_r ( italic_Y , over^ start_ARG italic_Y end_ARG )</annotation></semantics></math> where corr is the correlation function.
The final measure of a modelâ€™s performance is obtained by calculating Pearsonâ€™s correlation between the modelâ€™s predictions and neural recordings.
To quantify the model predictions, the resulting model prediction correlations are divided by the estimated cross-subject prediction accuracy and averaged across voxels, regions, and participants, resulting in a standardized measure of performance referred to as normalized brain alignment. For calculating normalized alignment, we select the voxels whose cross-subject prediction accuracy is <math alttext="\geq" class="ltx_Math" display="inline" id="S5.p4.5.m5.1"><semantics id="S5.p4.5.m5.1a"><mo id="S5.p4.5.m5.1.1" xref="S5.p4.5.m5.1.1.cmml">â‰¥</mo><annotation-xml encoding="MathML-Content" id="S5.p4.5.m5.1b"><geq id="S5.p4.5.m5.1.1.cmml" xref="S5.p4.5.m5.1.1"></geq></annotation-xml><annotation encoding="application/x-tex" id="S5.p4.5.m5.1c">\geq</annotation><annotation encoding="application/x-llamapun" id="S5.p4.5.m5.1d">â‰¥</annotation></semantics></math> 0.05.</p>
</div>
<div class="ltx_para ltx_noindent" id="S5.p5">
<p class="ltx_p" id="S5.p5.3"><span class="ltx_text ltx_font_bold" id="S5.p5.3.1">Statistical significance.</span>
To determine if normalized predictivity scores are significantly higher than chance, we run a permutation test using blocks of 10 contiguous fMRI TRs (considering the slowness of hemodynamic response) rather than individual TRs. By permuting predictions 5000 times, we create an empirical distribution for chance performance, from which we estimate p-value of the actual performance. The choice of these specific permutation test configurations is based on established methodologies in previous researchÂ <cite class="ltx_cite ltx_citemacro_citep">(Deniz etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib14" title="">2019</a>; Reddy &amp; Wehbe, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib47" title="">2021</a>; Oota etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib42" title="">2024a</a>)</cite>.
To estimate the statistical significance of performance differences, such as between the modelâ€™s predictions and chance or residual predictions and chance, we utilized the Wilcoxon signed-rank testÂ <cite class="ltx_cite ltx_citemacro_citep">(Conover, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib12" title="">1999</a>)</cite>, applying it to the mean normalized predictivity for the participants.
Finally, the Benjamini-Hochberg False Discovery Rate (FDR) correction for multiple comparisonsÂ <cite class="ltx_cite ltx_citemacro_citep">(Benjamini &amp; Hochberg, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib9" title="">1995</a>)</cite> is used for all the tests (appropriate because fMRI data is considered to have positive
dependenceÂ <cite class="ltx_cite ltx_citemacro_citep">(Genovese, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib24" title="">2000</a>)</cite>).
In all cases, we denote significant differences (p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="S5.p5.1.m1.1"><semantics id="S5.p5.1.m1.1a"><mrow id="S5.p5.1.m1.1.1" xref="S5.p5.1.m1.1.1.cmml"><mi id="S5.p5.1.m1.1.1.2" xref="S5.p5.1.m1.1.1.2.cmml"></mi><mo id="S5.p5.1.m1.1.1.1" xref="S5.p5.1.m1.1.1.1.cmml">â‰¤</mo><mn id="S5.p5.1.m1.1.1.3" xref="S5.p5.1.m1.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S5.p5.1.m1.1b"><apply id="S5.p5.1.m1.1.1.cmml" xref="S5.p5.1.m1.1.1"><leq id="S5.p5.1.m1.1.1.1.cmml" xref="S5.p5.1.m1.1.1.1"></leq><csymbol cd="latexml" id="S5.p5.1.m1.1.1.2.cmml" xref="S5.p5.1.m1.1.1.2">absent</csymbol><cn id="S5.p5.1.m1.1.1.3.cmml" type="float" xref="S5.p5.1.m1.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.1.m1.1c">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="S5.p5.1.m1.1d">â‰¤ 0.05</annotation></semantics></math>) with a <math alttext="\ast" class="ltx_Math" display="inline" id="S5.p5.2.m2.1"><semantics id="S5.p5.2.m2.1a"><mo id="S5.p5.2.m2.1.1" mathcolor="#4B0082" xref="S5.p5.2.m2.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S5.p5.2.m2.1b"><ci id="S5.p5.2.m2.1.1.cmml" xref="S5.p5.2.m2.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.2.m2.1c">\ast</annotation><annotation encoding="application/x-llamapun" id="S5.p5.2.m2.1d">âˆ—</annotation></semantics></math> or <math alttext="\wedge" class="ltx_Math" display="inline" id="S5.p5.3.m3.1"><semantics id="S5.p5.3.m3.1a"><mo id="S5.p5.3.m3.1.1" mathcolor="#4B0082" xref="S5.p5.3.m3.1.1.cmml">âˆ§</mo><annotation-xml encoding="MathML-Content" id="S5.p5.3.m3.1b"><and id="S5.p5.3.m3.1.1.cmml" xref="S5.p5.3.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S5.p5.3.m3.1c">\wedge</annotation><annotation encoding="application/x-llamapun" id="S5.p5.3.m3.1d">âˆ§</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span>Results</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span>How effective are multi-modal representations?</h3>
<div class="ltx_para ltx_noindent" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">In Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a>, we present the average normalized brain alignment scores for both multi-modal and individual modality features. Specifically, we show the normalized brain alignment for cross-modality (ImageBind), jointly pretrained multi-modal (TVLT), and the average from individual video and speech models. The results are shown for whole brain (Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a> <span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.1">Left</span>), and also for average across language and visual ROIs (Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a> <span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.2">Right</span>). Results for individual ROIs are in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F3" title="Figure 3 â€£ Whole brain analysis. â€£ Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal models. â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">3</span></a>.
<br class="ltx_break"/><span class="ltx_text ltx_font_bold" id="S6.SS1.p1.1.3">Baseline comparison.</span>
To compare the brain predictivity of multi-modal and unimodal models against baseline performance, we employ two baselines: (i) randomly generated vector embeddings to predict brain activity, and (ii) randomly initialized models for ImageBind, TVLT, Unimodal VM, and Unimodal SM.
Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a> (<span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.4">Left</span>) displays the comparison of average normalized brain alignment of randomly generated vectors, pretrained and randomly initialized models.
From Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a> (<span class="ltx_text ltx_font_italic" id="S6.SS1.p1.1.5">Left</span>), we observe that randomly initialized models show significantly better alignment than random vectors. However, the pretrained model embedding brain alignment is significantly better than randomly initialized models. This shows that the representations from these multi-modal models are significant enough for learning non-trivial alignment with the fMRI recordings of multi-modal stimuli.</p>
</div>
<figure class="ltx_figure" id="S6.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="166" id="S6.F2.g1" src="x2.png" width="639"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>(<span class="ltx_text ltx_font_italic" id="S6.F2.21.1">Left</span>) Avg normalized brain alignment of pretrained vs randomly initialized multi-modal and unimodal models across whole brain. <math alttext="\times" class="ltx_Math" display="inline" id="S6.F2.10.m1.1"><semantics id="S6.F2.10.m1.1b"><mo id="S6.F2.10.m1.1.1" mathcolor="#4B0082" xref="S6.F2.10.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S6.F2.10.m1.1c"><times id="S6.F2.10.m1.1.1.cmml" xref="S6.F2.10.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.10.m1.1d">\times</annotation><annotation encoding="application/x-llamapun" id="S6.F2.10.m1.1e">Ã—</annotation></semantics></math><math alttext="\implies" class="ltx_Math" display="inline" id="S6.F2.11.m2.1"><semantics id="S6.F2.11.m2.1b"><mo id="S6.F2.11.m2.1.1" stretchy="false" xref="S6.F2.11.m2.1.1.cmml">âŸ¹</mo><annotation-xml encoding="MathML-Content" id="S6.F2.11.m2.1c"><implies id="S6.F2.11.m2.1.1.cmml" xref="S6.F2.11.m2.1.1"></implies></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.11.m2.1d">\implies</annotation><annotation encoding="application/x-llamapun" id="S6.F2.11.m2.1e">âŸ¹</annotation></semantics></math> pretrained model embeddings are significantly better than randomly initialized models, i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="S6.F2.12.m3.1"><semantics id="S6.F2.12.m3.1b"><mrow id="S6.F2.12.m3.1.1" xref="S6.F2.12.m3.1.1.cmml"><mi id="S6.F2.12.m3.1.1.2" xref="S6.F2.12.m3.1.1.2.cmml"></mi><mo id="S6.F2.12.m3.1.1.1" xref="S6.F2.12.m3.1.1.1.cmml">â‰¤</mo><mn id="S6.F2.12.m3.1.1.3" xref="S6.F2.12.m3.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F2.12.m3.1c"><apply id="S6.F2.12.m3.1.1.cmml" xref="S6.F2.12.m3.1.1"><leq id="S6.F2.12.m3.1.1.1.cmml" xref="S6.F2.12.m3.1.1.1"></leq><csymbol cd="latexml" id="S6.F2.12.m3.1.1.2.cmml" xref="S6.F2.12.m3.1.1.2">absent</csymbol><cn id="S6.F2.12.m3.1.1.3.cmml" type="float" xref="S6.F2.12.m3.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.12.m3.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="S6.F2.12.m3.1e">â‰¤ 0.05</annotation></semantics></math>. (<span class="ltx_text ltx_font_italic" id="S6.F2.22.2">Right</span>) Avg normalized brain alignment for both multi-modal and unimodal model features specifically within language and visual regions.
Blue bar represents the normalized alignment using randomly generated vector embeddings. Error bars indicate the standard error of the mean across participants. <math alttext="\ast" class="ltx_Math" display="inline" id="S6.F2.13.m4.1"><semantics id="S6.F2.13.m4.1b"><mo id="S6.F2.13.m4.1.1" mathcolor="#4B0082" xref="S6.F2.13.m4.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S6.F2.13.m4.1c"><ci id="S6.F2.13.m4.1.1.cmml" xref="S6.F2.13.m4.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.13.m4.1d">\ast</annotation><annotation encoding="application/x-llamapun" id="S6.F2.13.m4.1e">âˆ—</annotation></semantics></math><math alttext="\implies" class="ltx_Math" display="inline" id="S6.F2.14.m5.1"><semantics id="S6.F2.14.m5.1b"><mo id="S6.F2.14.m5.1.1" stretchy="false" xref="S6.F2.14.m5.1.1.cmml">âŸ¹</mo><annotation-xml encoding="MathML-Content" id="S6.F2.14.m5.1c"><implies id="S6.F2.14.m5.1.1.cmml" xref="S6.F2.14.m5.1.1"></implies></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.14.m5.1d">\implies</annotation><annotation encoding="application/x-llamapun" id="S6.F2.14.m5.1e">âŸ¹</annotation></semantics></math> multi-modal embeddings are significantly better than unimodal video models (VM), i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="S6.F2.15.m6.1"><semantics id="S6.F2.15.m6.1b"><mrow id="S6.F2.15.m6.1.1" xref="S6.F2.15.m6.1.1.cmml"><mi id="S6.F2.15.m6.1.1.2" xref="S6.F2.15.m6.1.1.2.cmml"></mi><mo id="S6.F2.15.m6.1.1.1" xref="S6.F2.15.m6.1.1.1.cmml">â‰¤</mo><mn id="S6.F2.15.m6.1.1.3" xref="S6.F2.15.m6.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F2.15.m6.1c"><apply id="S6.F2.15.m6.1.1.cmml" xref="S6.F2.15.m6.1.1"><leq id="S6.F2.15.m6.1.1.1.cmml" xref="S6.F2.15.m6.1.1.1"></leq><csymbol cd="latexml" id="S6.F2.15.m6.1.1.2.cmml" xref="S6.F2.15.m6.1.1.2">absent</csymbol><cn id="S6.F2.15.m6.1.1.3.cmml" type="float" xref="S6.F2.15.m6.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.15.m6.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="S6.F2.15.m6.1e">â‰¤ 0.05</annotation></semantics></math>. <math alttext="\wedge" class="ltx_Math" display="inline" id="S6.F2.16.m7.1"><semantics id="S6.F2.16.m7.1b"><mo id="S6.F2.16.m7.1.1" mathcolor="#4B0082" xref="S6.F2.16.m7.1.1.cmml">âˆ§</mo><annotation-xml encoding="MathML-Content" id="S6.F2.16.m7.1c"><and id="S6.F2.16.m7.1.1.cmml" xref="S6.F2.16.m7.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.16.m7.1d">\wedge</annotation><annotation encoding="application/x-llamapun" id="S6.F2.16.m7.1e">âˆ§</annotation></semantics></math> <math alttext="\implies" class="ltx_Math" display="inline" id="S6.F2.17.m8.1"><semantics id="S6.F2.17.m8.1b"><mo id="S6.F2.17.m8.1.1" stretchy="false" xref="S6.F2.17.m8.1.1.cmml">âŸ¹</mo><annotation-xml encoding="MathML-Content" id="S6.F2.17.m8.1c"><implies id="S6.F2.17.m8.1.1.cmml" xref="S6.F2.17.m8.1.1"></implies></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.17.m8.1d">\implies</annotation><annotation encoding="application/x-llamapun" id="S6.F2.17.m8.1e">âŸ¹</annotation></semantics></math> multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="S6.F2.18.m9.1"><semantics id="S6.F2.18.m9.1b"><mrow id="S6.F2.18.m9.1.1" xref="S6.F2.18.m9.1.1.cmml"><mi id="S6.F2.18.m9.1.1.2" xref="S6.F2.18.m9.1.1.2.cmml"></mi><mo id="S6.F2.18.m9.1.1.1" xref="S6.F2.18.m9.1.1.1.cmml">â‰¤</mo><mn id="S6.F2.18.m9.1.1.3" xref="S6.F2.18.m9.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F2.18.m9.1c"><apply id="S6.F2.18.m9.1.1.cmml" xref="S6.F2.18.m9.1.1"><leq id="S6.F2.18.m9.1.1.1.cmml" xref="S6.F2.18.m9.1.1.1"></leq><csymbol cd="latexml" id="S6.F2.18.m9.1.1.2.cmml" xref="S6.F2.18.m9.1.1.2">absent</csymbol><cn id="S6.F2.18.m9.1.1.3.cmml" type="float" xref="S6.F2.18.m9.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F2.18.m9.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="S6.F2.18.m9.1e">â‰¤ 0.05</annotation></semantics></math>.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SSx1">
<h3 class="ltx_title ltx_title_subsection">Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal models.</h3>
<section class="ltx_paragraph" id="S6.SSx1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Whole brain analysis.</h4>
<div class="ltx_para ltx_noindent" id="S6.SSx1.SSS0.Px1.p1">
<p class="ltx_p" id="S6.SSx1.SSS0.Px1.p1.1">Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a> (<span class="ltx_text ltx_font_italic" id="S6.SSx1.SSS0.Px1.p1.1.1">Left</span>) displays results for whole brain analysis, where the IB Concat bar plot corresponds to results for representations from a cross-modal model, while TVLT Joint bar plot corresponds to results for representations from a jointly pretrained multi-modal model. From Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a> (<span class="ltx_text ltx_font_italic" id="S6.SSx1.SSS0.Px1.p1.1.2">Left</span>), we make the following observations: (i) At the whole brain level, the Wilcoxon signed-rank test shows that the differences in embeddings from the IB Concat and TVLT models are not statistically significant. (ii) The multi-modal embeddings show improved brain alignment compared to unimodal models. Specifically, cross-modal embeddings are significantly better than both unimodal video and speech models, while jointly pretrained embeddings are significantly better than speech models.
This implies that cross-modal embeddings contain additional information beyond the two modalities, while embeddings from a jointly pretrained model do not provide extra information beyond unimodal visual information but do contain additional information beyond unimodal speech.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx1.SSS0.Px1.p2">
<p class="ltx_p" id="S6.SSx1.SSS0.Px1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SSx1.SSS0.Px1.p2.1.1">Whole language and visual region analysis.</span>
We also present average normalized brain alignment results across language and visual regions in Figs.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a>
(<span class="ltx_text ltx_font_italic" id="S6.SSx1.SSS0.Px1.p2.1.2">Right</span>). The Wilcoxon signed-rank test shows that the differences in embeddings from the IB Concat and TVLT models are not statistically significant when averaged across language and visual regions.
Similar to whole brain performance, in the language regions, cross-modal embeddings are significantly better than both unimodal video and speech models, while jointly pretrained embeddings are significantly better than unimodal speech models.
In contrast, for the visual regions, the normalized brain alignment of cross-modal and jointly pretrained embeddings is similar to the performance of unimodal video models. This implies that when we average across visual regions, there is no additional information beyond unimodal video features. However, when compared to unimodal speech features, both multi-modal embeddings show significant improvement.</p>
</div>
<figure class="ltx_figure" id="S6.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="271" id="S6.F3.g1" src="x3.png" width="664"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>Average normalized brain alignment for video and audio modalities from multi-modal and individual modality features across whole brain and several ROIs of language (AG, PTL and IFG), visual (EVC, PPA and MT) and auditory cortex (AC). Error bars
indicate the standard error of the mean across participants. <math alttext="\ast" class="ltx_Math" display="inline" id="S6.F3.5.m1.1"><semantics id="S6.F3.5.m1.1b"><mo id="S6.F3.5.m1.1.1" mathcolor="#4B0082" xref="S6.F3.5.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="S6.F3.5.m1.1c"><ci id="S6.F3.5.m1.1.1.cmml" xref="S6.F3.5.m1.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="S6.F3.5.m1.1d">\ast</annotation><annotation encoding="application/x-llamapun" id="S6.F3.5.m1.1e">âˆ—</annotation></semantics></math> indicates cases where multi-modal embeddings are significantly better than unimodal video models (VM), i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="S6.F3.6.m2.1"><semantics id="S6.F3.6.m2.1b"><mrow id="S6.F3.6.m2.1.1" xref="S6.F3.6.m2.1.1.cmml"><mi id="S6.F3.6.m2.1.1.2" xref="S6.F3.6.m2.1.1.2.cmml"></mi><mo id="S6.F3.6.m2.1.1.1" xref="S6.F3.6.m2.1.1.1.cmml">â‰¤</mo><mn id="S6.F3.6.m2.1.1.3" xref="S6.F3.6.m2.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F3.6.m2.1c"><apply id="S6.F3.6.m2.1.1.cmml" xref="S6.F3.6.m2.1.1"><leq id="S6.F3.6.m2.1.1.1.cmml" xref="S6.F3.6.m2.1.1.1"></leq><csymbol cd="latexml" id="S6.F3.6.m2.1.1.2.cmml" xref="S6.F3.6.m2.1.1.2">absent</csymbol><cn id="S6.F3.6.m2.1.1.3.cmml" type="float" xref="S6.F3.6.m2.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F3.6.m2.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="S6.F3.6.m2.1e">â‰¤ 0.05</annotation></semantics></math>. <math alttext="\wedge" class="ltx_Math" display="inline" id="S6.F3.7.m3.1"><semantics id="S6.F3.7.m3.1b"><mo id="S6.F3.7.m3.1.1" mathcolor="#4B0082" xref="S6.F3.7.m3.1.1.cmml">âˆ§</mo><annotation-xml encoding="MathML-Content" id="S6.F3.7.m3.1c"><and id="S6.F3.7.m3.1.1.cmml" xref="S6.F3.7.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="S6.F3.7.m3.1d">\wedge</annotation><annotation encoding="application/x-llamapun" id="S6.F3.7.m3.1e">âˆ§</annotation></semantics></math> indicates cases where multi-modal embeddings are significantly better than unimodal speech models (SM), i.e., p<math alttext="\leq 0.05" class="ltx_Math" display="inline" id="S6.F3.8.m4.1"><semantics id="S6.F3.8.m4.1b"><mrow id="S6.F3.8.m4.1.1" xref="S6.F3.8.m4.1.1.cmml"><mi id="S6.F3.8.m4.1.1.2" xref="S6.F3.8.m4.1.1.2.cmml"></mi><mo id="S6.F3.8.m4.1.1.1" xref="S6.F3.8.m4.1.1.1.cmml">â‰¤</mo><mn id="S6.F3.8.m4.1.1.3" xref="S6.F3.8.m4.1.1.3.cmml">0.05</mn></mrow><annotation-xml encoding="MathML-Content" id="S6.F3.8.m4.1c"><apply id="S6.F3.8.m4.1.1.cmml" xref="S6.F3.8.m4.1.1"><leq id="S6.F3.8.m4.1.1.1.cmml" xref="S6.F3.8.m4.1.1.1"></leq><csymbol cd="latexml" id="S6.F3.8.m4.1.1.2.cmml" xref="S6.F3.8.m4.1.1.2">absent</csymbol><cn id="S6.F3.8.m4.1.1.3.cmml" type="float" xref="S6.F3.8.m4.1.1.3">0.05</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S6.F3.8.m4.1d">\leq 0.05</annotation><annotation encoding="application/x-llamapun" id="S6.F3.8.m4.1e">â‰¤ 0.05</annotation></semantics></math></figcaption>
</figure>
</section>
<section class="ltx_paragraph" id="S6.SSx1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">ROI-Level analysis of joint embeddings from multi-modal models.</h4>
<div class="ltx_para ltx_noindent" id="S6.SSx1.SSS0.Px2.p1">
<p class="ltx_p" id="S6.SSx1.SSS0.Px2.p1.1">Since we didnâ€™t observe any significant difference at the whole brain level and when averaged across language and visual regions, between cross-modal and jointly pretrained multi-modal models, we attempt to seek if there are any differences when we pay a closer look at the individual ROIs.
We present results for language regions such as Angular gyrus (AG), the posterior temporal lobe (PTL), and the inferior frontal gyrus (IFG) in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F3" title="Figure 3 â€£ Whole brain analysis. â€£ Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal models. â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">3</span></a>. Additionally, we cover visual regions like early visual cortex (EVC), scene visual areas (PPA) and middle temporal gyrus (MT), as well as early auditory cortex (AC).
In this figure, we also report the average normalized brain alignment of each modality obtained from multi-modal models. Unlike the whole brain analysis, we observe some differences between cross-modal and jointly pretrained models in several language and visual ROIs. Results for other ROIs are in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A5.F7" title="Figure 7 â€£ Appendix E Implementation details for reproducibility. â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">7</span></a> in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A6" title="Appendix F Effectiveness of multi-modal vs unimodal representations for various brain regions â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">F</span></a>. Our observations are as follows: (i) Cross-modal IB Concat embeddings are significantly better than TVLT Joint embeddings in semantic regions such as AG and PCC, as well as the multi-modal processing region MT. (ii) Conversely, TVLT Joint embeddings are significantly better than IB Concat embeddings in dmPFC regions. This indicates that while both cross-modal and jointly pretrained multi-modal models perform similarly at a macro level, there are individual differences at micro level. This observation motivated us to do further detailed analysis in Sec.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.SS2" title="6.2 Which brain regions process uni- and multi-modal information? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">6.2</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.SS3" title="6.3 How does each modality contribute to the multi-modal brain alignment? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">6.3</span></a>. Results for each unimodal video and each unimodal speech model for individual ROIs are in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A5.F8" title="Figure 8 â€£ Appendix E Implementation details for reproducibility. â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">8</span></a> in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A6" title="Appendix F Effectiveness of multi-modal vs unimodal representations for various brain regions â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">F</span></a>.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SSx1.SSS0.Px2.p2">
<p class="ltx_p" id="S6.SSx1.SSS0.Px2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SSx1.SSS0.Px2.p2.1.1">ROI-Level analysis of modality-specific embeddings extracted from multi-modal models.</span>
While considering both joint and each modality embeddings from multi-modal models, we make the following observations from Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F3" title="Figure 3 â€£ Whole brain analysis. â€£ Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal models. â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">3</span></a>: (1) Cross-modal IB video embeddings exhibit improved brain alignment compared to unimodal video in the AG and MT regions with the exceptions of the PTL and AC regions. Notably, in the AC, which is an early auditory processing area primarily handling sound-related information rather than higher cognitive functions (such as language processing), audio embeddings yield higher brain predictivity than video embeddings. Interestingly, this differential effect is not observed when comparing ImageBind audio embeddings to unimodal audio embeddings. These findings suggest that video modality information contributes more significantly to brain alignment in the context of ImageBind concatenated embeddings derived from cross-modal models.
(2) TVLT video embeddings show improved brain alignment in the AG, PTL, PCC, dmPFC and EVC regions, with other regions displaying similar normalized brain alignment unimodal video embeddings. Interestingly, while the PTL is known for auditory processing, it also contributes to the integration of visual and auditory inputs. The improved alignment of video embeddings here indicates that visual information enhances the processing capabilities in this region.
(3) Consistent with the cross-modality models, in jointly pretrained TVLT models, TVLT video embeddings significantly outperform TVLT audio embeddings, except in PTL region. These observations indicate that video information is advantageous for both cross-modal and jointly pretrained models, whereas audio embeddings mainly benefit the PTL region.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span>Which brain regions process uni- and multi-modal information?</h3>
<div class="ltx_para ltx_noindent" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">From Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F3" title="Figure 3 â€£ Whole brain analysis. â€£ Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal models. â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">3</span></a>, we observe that multi-modal video embeddings exhibit improved brain alignment not only in the whole brain but also in various language, visual and multi-modal regions.
For instance, the cross-modal IB Concat embeddings demonstrate superior brain alignment compared to unimodal video-based models in areas such as the AG, PTL, IFG, and PCC. Moreover, TVLT-joint embeddings show notable enhancements in the AG, PTL, IFG, PCC, dmPFC and EVC regions. In contrast, compared to unimodal speech-based models, all multi-modal embeddings display significantly better brain alignment, except the LOC (object visual processing) region.
The LOC region is highly specialized for processing visual information related to object recognition. When audio information is integrated, the specific visual features crucial for LOC region alignment may become less pronounced in the embeddings, leading to slightly reduced alignment compared to unimodal visual models.
However, overall, our findings suggests that integrating multiple modalities leads to transferring information from one modality to another, resulting in improved brain predictability.
Hence, it can be inferred that multi-modal models can indeed learn multi-modal linkages that are relevant to the brain.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1">When subjects engage with multi-modality stimuli, we observe that multi-modal embeddings show improvements in semantic regions such as the AG, PCC and dmPFC, and syntactic regions such as the PTL and IFG. Overall, we find that multi-modal information is processed in only a few regions. Furthermore, several regions, including the PPA (scene visual area), EVC (early visual cortex), ATL (anterior temporal lobe), IFGOrb, MFG, and dmPFC, exhibit similar brain alignment with both unimodal and multi-modal embeddings.</p>
</div>
<figure class="ltx_figure" id="S6.F4">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S6.F4.1" style="width:277.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="217" id="S6.F4.1.g1" src="x4.png" width="831"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S6.F4.2" style="width:151.8pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="203" id="S6.F4.2.g1" src="x5.png" width="830"/>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>Residual analysis: Average normalized brain alignment was computed across participants before and after removal of video and audio embeddings from both jointly pretrained and cross-modality models. Error bars
indicate the standard error of the mean across participants. â€œ-â€ symbol represents residuals.
</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span>How does each modality contribute to the multi-modal brain alignment?</h3>
<div class="ltx_para ltx_noindent" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">To understand the contribution of each modality to the multi-modal brain alignment for multi-modality naturalistic stimulus, we perform residual analyses by removing the unimodal model features from multi-modal joint representations as well as multi-modal video or audio representations from joint representations and measure the differences in brain alignment before and after removal modality-specific features. To check the quality of information removal using residual analysis, We computed Pearson correlation where unimodal video features are projected onto the multi-modal IB Concat feature space using the residual approach. We found correlation to be as low as 0.56 which implies that unimodal video features are successfully removed from multi-modal representations. Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F4" title="Figure 4 â€£ 6.2 Which brain regions process uni- and multi-modal information? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">4</span></a> shows normalized alignment for language (AG) and visual regions (MT).</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p2">
<p class="ltx_p" id="S6.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p2.1.1">Cross-modal multi-modal models.</span> The alignment in regions AG and MT is extremely high, and this alignment is only partially explained by video features. This implies that significant unexplained alignment remains after the removal of video features. Conversely, the removal of speech features does not lead to a drop in brain alignment, indicating that there is additional information beyond speech features that is processed in these regions.
This means that in cross-modal models, when transferring knowledge from one modality to another, the model relies more heavily on visual information. As a result, the model becomes more focused on video inputs rather than audio inputs. This likely reflects the modelâ€™s preference for using the detailed visual features that align closely with brain activity in regions AG and MT, leading to the observed high alignment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p3">
<p class="ltx_p" id="S6.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p3.1.1">Jointly pretrained multi-modal models.</span>
The alignment in regions AG and MT is extremely high, and this alignment is partially explained by both video and audio features. Unlike cross-modal representations, the TVLT model learns a more balanced representation of both video and audio features. This leads to integrated information from both modalities, making the model less sensitive to the loss of features from a specific modality.
As a result, we observe only a small drop in brain alignment when either modality is removed. This suggests that the model is capturing more high-level abstract and semantic information that goes beyond the specific features of just one modality.
We observe similar findings for language ROIs such as PTL, MFG, ATL, PCC and visual regions EVC, LOC and OFA, as shown in Figs.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A7.F9" title="Figure 9 â€£ Appendix G How is the brain alignment of multi-modal features affected by the elimination of a particular modality? â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">9</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A7.F10" title="Figure 10 â€£ Appendix G How is the brain alignment of multi-modal features affected by the elimination of a particular modality? â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">10</span></a> in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A7" title="Appendix G How is the brain alignment of multi-modal features affected by the elimination of a particular modality? â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">G</span></a>.
These results suggest that there is additional information beyond the unimodal embeddings considered in this study that is processed in the visual and language regions.
</p>
</div>
<div class="ltx_para ltx_noindent" id="S6.SS3.p4">
<p class="ltx_p" id="S6.SS3.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS3.p4.1.1">Qualitative analysis.</span>
We compute the percentage decrease in alignment for each voxel following the removal of unimodal video embeddings from the IB Concat (cross-modality) and the TVLT Joint (jointly pretrained model), with projections onto the brain surface averaged across participants, as depicted in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F5" title="Figure 5 â€£ 6.3 How does each modality contribute to the multi-modal brain alignment? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">5</span></a>. The colorbar shows the percentage decrease in brain alignment, where red voxels indicate a higher percentage decrease and white voxels indicate areas where unimodal video features do not contribute any shared information within the multi-modal context.
We observe that removal of unimodal video features leads to a significant drop (40-50%) in performance in the visual regions for IB Concat, and in language regions (PTL &amp; MFG) for TVLT Joint.</p>
</div>
<figure class="ltx_figure" id="S6.F5">
<div class="ltx_block" id="S6.F5.2">
<div class="ltx_inline-block ltx_align_center ltx_transformed_outer" id="S6.F5.1" style="width:16.6pt;height:78.1pt;vertical-align:-0.0pt;"><span class="ltx_transformed_inner" style="width:78.1pt;transform:translate(-30.71pt,-30.71pt) rotate(-90deg) ;"><img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="31" id="S6.F5.1.g1" src="x6.png" width="149"/>
</span></div>
<img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="156" id="S6.F5.g1" src="x7.png" width="581"/>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Percent decrease of brain alignment after removal of unimodal embeddings from different multimodal models. (Left) Removal of unimodal VM embeddings from IB-Concat. (Middle) Removal of unimodal VM embeddings from jointly pretrained TVLT. (Right) Removal of unimodal SM embeddings from TVLT Joint. The color bar indicates the percent of decrease where darker shade of red denotes higher and white denotes zero. LH: Left Hemisphere and RH: Right Hemisphere.
</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span>Discussion</h2>
<div class="ltx_para ltx_noindent" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">Using multi-modal model representations, including both cross-modal and jointly pretrained types, we evaluated how these representations can predict fMRI brain activity when participants are engaged in multi-modality naturalistic stimuli. Further, we compared both multi-modal and unimodal representations and observed their alignment with both unimodal and multi-modal brain regions. This is achieved by removing information related to unimodal stimulus features (audio and video) and observing how this perturbation affects the alignment with fMRI brain recordings acquired while participants are engaged in watching multi-modality naturalistic movies.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p2">
<p class="ltx_p" id="S7.p2.1">Our analysis of multi-modal brain alignment yields several important conclusions: (1) The improved brain alignment of the multi-modal models over unimodal models, across several language, visual, and auditory regions is only partially attributable to the video and audio stimulus features presented to the model. A deeper understanding of these models is required to shed light on the underlying information processing of both unimodal and multi-modal information.
(2) Cross-modal representations have significantly improved brain alignment in language regions such as AG, PCC and PTL. This variance can be partially attributed to the video features alone, rather than removal of auditory features.
(3) Video embeddings from multi-modal models exhibit higher brain alignment than audio embeddings, except in the PTL and AC regions. This suggests that audio-based models may encode weaker brain-relevant semantics, as similar findings are observed in a recent studyÂ <cite class="ltx_cite ltx_citemacro_citep">(Oota etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib42" title="">2024a</a>)</cite>.
(4) Both cross-modal and jointly pretrained models demonstrate significantly improved brain alignment with language regions (AG, PCC, PTL and IFG) compared to visual regions when analyzed against unimodal video data. In contrast, when compared to unimodal audio-based models, all multi-modal embeddings display significantly better brain alignment, with the exception of the LOC region. This underscores the capability of multi-modal models to capture additional informationâ€”either through knowledge transfer or integration between modalitiesâ€”crucial for multi-modal brain alignment.</p>
</div>
<div class="ltx_para ltx_noindent" id="S7.p3">
<p class="ltx_p" id="S7.p3.1">The model training protocol of TVLT appears more in line with how humans learn during development when they experience multiple modalities simultaneously and the learning is mediated by the experience of joint inter-modal associations.
It is unlikely that the human system experiences these modalities in isolation, except in cases of congenital conditions where the inputs from a specific modality are not accessible. Given that the brain alignment observed in TVLT model in a language region like AG is less sensitive to loss of information from specific modalities, we believe that AG serves as a multi-modal convergent buffer integrating spatio-temporal information from multiple sensory modalities to process narrativesÂ <cite class="ltx_cite ltx_citemacro_citep">(Dong &amp; Toneva, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib19" title="">2023b</a>; Humphreys &amp; Tibon, <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib29" title="">2023</a>)</cite>.
The results of high alignment found in AG even in IB-Concat but more brittle with respect to loss of information from a specific modality are also interesting. It would be interesting to study patterns of activation in AG in patients who acquired visual or auditory function later in their lifeÂ <cite class="ltx_cite ltx_citemacro_citep">(HÃ¶lig etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib28" title="">2023</a>)</cite> to see if one observes such brittleness in the representations acquired. We make the code publicly available<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#footnote1" title="footnote 1 â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">1</span></a>. Lastly, we discuss limitations of our work in AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A15" title="Appendix O Limitations â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">O</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span>Ethics Statement</h2>
<div class="ltx_para ltx_noindent" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">We did not create any new neural recordings data as part of this work. We used the Movie10 dataset which is publicly available without any restrictions. Movie10 dataset can be downloaded from <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/courtois-neuromod/movie10/tree/33a97c01503315e5e09b3ac07c6ccadb8b887dcf" title="">https://github.com/courtois-neuromod/movie10/tree/33a97c01503315e5e09b3ac07c6ccadb8b887dcf</a>. Please read their terms of use<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://docs.cneuromod.ca/en/latest/ACCESS.html" title="">https://docs.cneuromod.ca/en/latest/ACCESS.html</a></span></span></span> for more details.</p>
</div>
<div class="ltx_para ltx_noindent" id="S8.p2">
<p class="ltx_p" id="S8.p2.1">We do not foresee any harmful uses of this technology.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antonello etÂ al. (2021)</span>
<span class="ltx_bibblock">
Richard Antonello, JavierÂ S Turek, VyÂ Vo, and Alexander Huth.

</span>
<span class="ltx_bibblock">Low-dimensional structure in the space of language representations is
reflected in brain responses.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in Neural Information Processing Systems</em>,
34:8332â€“8344, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Antonello etÂ al. (2024)</span>
<span class="ltx_bibblock">
Richard Antonello, Aditya Vaidya, and Alexander Huth.

</span>
<span class="ltx_bibblock">Scaling laws for language encoding models in fmri.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Arnab etÂ al. (2021)</span>
<span class="ltx_bibblock">
Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen Sun, Mario LuÄiÄ‡,
and Cordelia Schmid.

</span>
<span class="ltx_bibblock">Vivit: A video vision transformer.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">Proceedings of the IEEE/CVF International Conference on
Computer Vision</em>, pp.Â  6836â€“6846, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Ataallah etÂ al. (2024)</span>
<span class="ltx_bibblock">
Kirolos Ataallah, Xiaoqian Shen, Eslam Abdelrahman, Essam Sleiman, Deyao Zhu,
Jian Ding, and Mohamed Elhoseiny.

</span>
<span class="ltx_bibblock">Minigpt4-video: Advancing multimodal llms for video understanding
with interleaved visual-textual tokens.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">arXiv preprint arXiv:2404.03413</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Aw &amp; Toneva (2023)</span>
<span class="ltx_bibblock">
KhaiÂ Loong Aw and Mariya Toneva.

</span>
<span class="ltx_bibblock">Training language models to summarize narratives improves brain
alignment.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">The Eleventh International Conference on Learning
Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baade etÂ al. (2022)</span>
<span class="ltx_bibblock">
Alan Baade, Puyuan Peng, and David Harwath.

</span>
<span class="ltx_bibblock">Mae-ast: Masked autoencoding audio spectrogram transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Interspeech 2022</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baevski etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli.

</span>
<span class="ltx_bibblock">wav2vec 2.0: A framework for self-supervised learning of speech
representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Advances in Neural Information Processing Systems</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Baker etÂ al. (2018)</span>
<span class="ltx_bibblock">
CordellÂ M Baker, JoshuaÂ D Burks, RobertÂ G Briggs, AndrewÂ K Conner, ChadÂ A
Glenn, KathleenÂ N Taylor, Goksel Sali, TressieÂ M McCoy, JamesÂ D Battiste,
DanielÂ L Oâ€™Donoghue, etÂ al.

</span>
<span class="ltx_bibblock">A connectomic atlas of the human cerebrumâ€”chapter 7: the lateral
parietal lobe.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">Operative Neurosurgery</em>, 15(suppl_1):S295â€“S349, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Benjamini &amp; Hochberg (1995)</span>
<span class="ltx_bibblock">
Yoav Benjamini and Yosef Hochberg.

</span>
<span class="ltx_bibblock">Controlling the false discovery rate: a practical and powerful
approach to multiple testing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Journal of the Royal statistical society: series B
(Methodological)</em>, 57(1):289â€“300, 1995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bracci &amp; OpÂ de Beeck (2023)</span>
<span class="ltx_bibblock">
Stefania Bracci and HansÂ P OpÂ de Beeck.

</span>
<span class="ltx_bibblock">Understanding human object vision: a picture is worth a thousand
representations.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">Annual Review of Psychology</em>, 74:113â€“135, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Caucheteux &amp; King (2022)</span>
<span class="ltx_bibblock">
Charlotte Caucheteux and Jean-RÃ©mi King.

</span>
<span class="ltx_bibblock">Brains and algorithms partially converge in natural language
processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Communications Biology</em>, 5(1):134, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conover (1999)</span>
<span class="ltx_bibblock">
WilliamÂ Jay Conover.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">Practical nonparametric statistics</em>, volume 350.

</span>
<span class="ltx_bibblock">john wiley &amp; sons, 1999.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Conwell etÂ al. (2022)</span>
<span class="ltx_bibblock">
Colin Conwell, JacobÂ S Prince, KendrickÂ N Kay, GeorgeÂ A Alvarez, and Talia
Konkle.

</span>
<span class="ltx_bibblock">What can 1.8 billion regressions tell us about the pressures shaping
high-level visual representation in brains and machines?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">bioRxiv</em>, pp.Â  2022â€“03, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Deniz etÂ al. (2019)</span>
<span class="ltx_bibblock">
Fatma Deniz, AnwarÂ O Nunez-Elizalde, AlexanderÂ G Huth, and JackÂ L Gallant.

</span>
<span class="ltx_bibblock">The representation of semantic information across human cerebral
cortex during listening versus reading is invariant to stimulus modality.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Journal of Neuroscience</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Desai etÂ al. (2023)</span>
<span class="ltx_bibblock">
RutvikÂ H Desai, Usha Tadimeti, and Nicholas Riccardi.

</span>
<span class="ltx_bibblock">Proper and common names in the semantic system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Brain Structure and Function</em>, 228(1):239â€“254, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin etÂ al. (2019)</span>
<span class="ltx_bibblock">
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova.

</span>
<span class="ltx_bibblock">Bert: Pre-training of deep bidirectional transformers for language
understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">Proceedings of the 2019 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies, Volume 1 (Long and Short Papers)</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Doerig etÂ al. (2022)</span>
<span class="ltx_bibblock">
Adrien Doerig, TimÂ C Kietzmann, Emily Allen, Yihan Wu, Thomas Naselaris,
Kendrick Kay, and Ian Charest.

</span>
<span class="ltx_bibblock">Semantic scene descriptions as an objective of human vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">arXiv preprint arXiv:2209.11737</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong &amp; Toneva (2023a)</span>
<span class="ltx_bibblock">
DotaÂ Tianai Dong and Mariya Toneva.

</span>
<span class="ltx_bibblock">Interpreting multimodal video transformers using brain recordings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">ICLR 2023 Workshop on Multimodal Representation Learning:
Perks and Pitfalls</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dong &amp; Toneva (2023b)</span>
<span class="ltx_bibblock">
DotaÂ Tianai Dong and Mariya Toneva.

</span>
<span class="ltx_bibblock">Vision-language integration in multimodal video transformers
(partially) aligns with the brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2311.07766</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dosovitskiy etÂ al. (2020)</span>
<span class="ltx_bibblock">
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg
Heigold, Sylvain Gelly, etÂ al.

</span>
<span class="ltx_bibblock">An image is worth 16x16 words: Transformers for image recognition at
scale.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">International Conference on Learning Representations</em>, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Eickenberg etÂ al. (2017)</span>
<span class="ltx_bibblock">
Michael Eickenberg, Alexandre Gramfort, GaÃ«l Varoquaux, and Bertrand
Thirion.

</span>
<span class="ltx_bibblock">Seeing it all: Convolutional network layers map the function of the
human visual system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">NeuroImage</em>, 152:184â€“194, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Frank etÂ al. (2021)</span>
<span class="ltx_bibblock">
Stella Frank, Emanuele Bugliarello, and Desmond Elliott.

</span>
<span class="ltx_bibblock">Vision-and-language or vision-for-language? on cross-modal influence
in multimodal transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 2021 Conference on Empirical Methods in
Natural Language Processing</em>, pp.Â  9847â€“9857, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gauthier etÂ al. (2003)</span>
<span class="ltx_bibblock">
Isabel Gauthier, ThomasÂ W James, KimÂ M Curby, and MichaelÂ J Tarr.

</span>
<span class="ltx_bibblock">The influence of conceptual knowledge on visual discrimination.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Cognitive Neuropsychology</em>, 20(3-6):507â€“523, 2003.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Genovese (2000)</span>
<span class="ltx_bibblock">
ChristopherÂ R Genovese.

</span>
<span class="ltx_bibblock">A bayesian time-course model for functional magnetic resonance
imaging data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">Journal of the American Statistical Association</em>, 95(451):691â€“703, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Girdhar etÂ al. (2023)</span>
<span class="ltx_bibblock">
Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, KalyanÂ Vasudev
Alwala, Armand Joulin, and Ishan Misra.

</span>
<span class="ltx_bibblock">Imagebind: One embedding space to bind them all.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.Â  15180â€“15190, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Glasser etÂ al. (2016)</span>
<span class="ltx_bibblock">
MatthewÂ F Glasser, TimothyÂ S Coalson, EmmaÂ C Robinson, CarlÂ D Hacker, John
Harwell, Essa Yacoub, Kamil Ugurbil, Jesper Andersson, ChristianÂ F Beckmann,
Mark Jenkinson, etÂ al.

</span>
<span class="ltx_bibblock">A multi-modal parcellation of human cerebral cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Nature</em>, 536(7615):171â€“178, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goldstein etÂ al. (2022)</span>
<span class="ltx_bibblock">
Ariel Goldstein, Zaid Zada, Eliav Buchnik, Mariano Schain, Amy Price, Bobbi
Aubrey, SamuelÂ A Nastase, Amir Feder, Dotan Emanuel, Alon Cohen, etÂ al.

</span>
<span class="ltx_bibblock">Shared computational principles for language processing in humans and
deep language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">Nature Neuroscience</em>, 25(3):369â€“380, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">HÃ¶lig etÂ al. (2023)</span>
<span class="ltx_bibblock">
Cordula HÃ¶lig, MariaÂ JS Guerreiro, Sunitha Lingareddy, Ramesh Kekunnaya,
and Brigitte RÃ¶der.

</span>
<span class="ltx_bibblock">Sight restoration in congenitally blind humans does not restore
visual brain structure.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">Cerebral Cortex</em>, 33(5):2152â€“2161, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Humphreys &amp; Tibon (2023)</span>
<span class="ltx_bibblock">
GinaÂ F Humphreys and Roni Tibon.

</span>
<span class="ltx_bibblock">Dual-axes of functional organisation across lateral parietal cortex:
the angular gyrus forms part of a multi-modal buffering system.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Brain Structure and Function</em>, 228(1):341â€“352, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huth etÂ al. (2016)</span>
<span class="ltx_bibblock">
AlexanderÂ G Huth, WendyÂ A DeÂ Heer, ThomasÂ L Griffiths, FrÃ©dÃ©ricÂ E
Theunissen, and JackÂ L Gallant.

</span>
<span class="ltx_bibblock">Natural speech reveals the semantic maps that tile human cerebral
cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Nature</em>, 532(7600):453â€“458, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Huth etÂ al. (2022)</span>
<span class="ltx_bibblock">
AlexanderÂ G Huth, Shinji Nishimoto, AnÂ T Vu, and TÂ Dupre LaÂ Tour.

</span>
<span class="ltx_bibblock">Gallant lab natural short clips 3t fmri data.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">G-Node doi</em>, 10, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain &amp; Huth (2018)</span>
<span class="ltx_bibblock">
Shailee Jain and AlexanderÂ G Huth.

</span>
<span class="ltx_bibblock">Incorporating context into language encoding models for fmri.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">NIPS</em>, pp.Â  6629â€“6638, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li etÂ al. (2019)</span>
<span class="ltx_bibblock">
LiunianÂ Harold Li, Mark Yatskar, DaÂ Yin, Cho-Jui Hsieh, and Kai-Wei Chang.

</span>
<span class="ltx_bibblock">Visualbert: A simple and performant baseline for vision and language.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">arXiv preprint arXiv:1908.03557</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Millet etÂ al. (2022)</span>
<span class="ltx_bibblock">
Juliette Millet, Charlotte Caucheteux, Yves Boubenec, Alexandre Gramfort, Ewan
Dunbar, Christophe Pallier, Jean-Remi King, etÂ al.

</span>
<span class="ltx_bibblock">Toward a realistic model of speech processing in the brain with
self-supervised learning.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Advances in Neural Information Processing Systems</em>,
35:33428â€“33443, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Milton etÂ al. (2021)</span>
<span class="ltx_bibblock">
CamilleÂ K Milton, Vukshitha Dhanaraj, IsabellaÂ M Young, HughÂ M Taylor, PeterÂ J
Nicholas, RobertÂ G Briggs, MichaelÂ Y Bai, RannuluÂ D Fonseka, Jorge Hormovas,
Yueh-Hsin Lin, etÂ al.

</span>
<span class="ltx_bibblock">Parcellation-based anatomic model of the semantic network.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">Brain and Behavior</em>, 11(4):e02065, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Nakagi etÂ al. (2024)</span>
<span class="ltx_bibblock">
Yuko Nakagi, Takuya Matsuyama, Naoko Koide-Majima, Hiroto Yamaguchi, Rieko
Kubo, Shinji Nishimoto, and YuÂ Takagi.

</span>
<span class="ltx_bibblock">The brain tells a story: Unveiling distinct representations of
semantic content in speech, objects, and stories in the human brain with
large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">bioRxiv</em>, pp.Â  2024â€“02, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota &amp; Toneva (2023)</span>
<span class="ltx_bibblock">
SubbaÂ Reddy Oota and Mariya Toneva.

</span>
<span class="ltx_bibblock">What aspects of nlp models and brain datasets affect brain-nlp
alignment?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">2023 Conference on Cognitive Computational Neuroscience</em>,
2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota etÂ al. (2022a)</span>
<span class="ltx_bibblock">
SubbaÂ Reddy Oota, Jashn Arora, Veeral Agarwal, Mounika Marreddy, Manish Gupta,
and Bapi Surampudi.

</span>
<span class="ltx_bibblock">Neural language taskonomy: Which nlp tasks are the most predictive of
fmri brain activity?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the 2022 Conference of the North American
Chapter of the Association for Computational Linguistics: Human Language
Technologies</em>, pp.Â  3220â€“3237, 2022a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota etÂ al. (2022b)</span>
<span class="ltx_bibblock">
SubbaÂ Reddy Oota, Jashn Arora, Vijay Rowtula, Manish Gupta, and RajuÂ S Bapi.

</span>
<span class="ltx_bibblock">Visio-linguistic brain encoding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">COLING</em>, pp.Â  116â€“133, 2022b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota etÂ al. (2023a)</span>
<span class="ltx_bibblock">
SubbaÂ Reddy Oota, Manish Gupta, and Mariya Toneva.

</span>
<span class="ltx_bibblock">Joint processing of linguistic properties in brains and language
models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">NeurIPS</em>, 2023a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota etÂ al. (2023b)</span>
<span class="ltx_bibblock">
SubbaÂ Reddy Oota, Agarwal Veeral, Marreddy Mounika, Gupta Manish, and
RajuÂ Surampudi Bapi.

</span>
<span class="ltx_bibblock">Speech taskonomy: Which speech tasks are the most predictive of fmri
brain activity?

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">24th INTERSPEECH Conference</em>, 2023b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota etÂ al. (2024a)</span>
<span class="ltx_bibblock">
SubbaÂ Reddy Oota, Emin Ã‡elik, Fatma Deniz, and Mariya Toneva.

</span>
<span class="ltx_bibblock">Speech language models lack important brain-relevant semantics.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Proceedings of the 62nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers)</em>, pp.Â  8503â€“8528.
Association for Computational Linguistics, 2024a.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2024.acl-long.462" title="">https://aclanthology.org/2024.acl-long.462</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Oota etÂ al. (2024b)</span>
<span class="ltx_bibblock">
SubbaÂ Reddy Oota, Zijiao Chen, Manish Gupta, BapiÂ Raju Surampudi, GaÃ«l
Jobard, FrÃ©dÃ©ric Alexandre, and Xavier Hinaut.

</span>
<span class="ltx_bibblock">Deep neural networks and brain alignment: Brain encoding and decoding
(survey).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Transactions on Machine Learning Research Journal</em>,
2024b.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Popham etÂ al. (2021)</span>
<span class="ltx_bibblock">
SaraÂ F Popham, AlexanderÂ G Huth, NataliaÂ Y Bilenko, Fatma Deniz, JamesÂ S Gao,
AnwarÂ O Nunez-Elizalde, and JackÂ L Gallant.

</span>
<span class="ltx_bibblock">Visual and linguistic semantic representations are aligned at the
border of human visual cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Nature Neuroscience</em>, 24(11):1628â€“1636,
2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2019)</span>
<span class="ltx_bibblock">
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya
Sutskever, etÂ al.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">OpenAI</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford etÂ al. (2021)</span>
<span class="ltx_bibblock">
Alec Radford, JongÂ Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh,
Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark,
etÂ al.

</span>
<span class="ltx_bibblock">Learning transferable visual models from natural language
supervision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Image</em>, 2:T2, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reddy &amp; Wehbe (2021)</span>
<span class="ltx_bibblock">
AnikethÂ Janardhan Reddy and Leila Wehbe.

</span>
<span class="ltx_bibblock">Can fmri reveal the representation of syntactic structure in the
brain?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">Advances in Neural Information Processing Systems</em>,
34:9843â€“9856, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schrimpf etÂ al. (2018)</span>
<span class="ltx_bibblock">
Martin Schrimpf, Jonas Kubilius, HaÂ Hong, NajibÂ J Majaj, Rishi Rajalingham,
EliasÂ B Issa, Kohitij Kar, Pouya Bashivan, Jonathan Prescott-Roy, Franziska
Geiger, etÂ al.

</span>
<span class="ltx_bibblock">Brain-score: Which artificial neural network for object recognition
is most brain-like?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">BioRxiv</em>, pp.Â  407007, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schrimpf etÂ al. (2021)</span>
<span class="ltx_bibblock">
Martin Schrimpf, IdanÂ Asher Blank, Greta Tuckute, Carina Kauf, EghbalÂ A
Hosseini, Nancy Kanwisher, JoshuaÂ B Tenenbaum, and Evelina Fedorenko.

</span>
<span class="ltx_bibblock">The neural architecture of language: Integrative modeling converges
on predictive processing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">Proceedings of the National Academy of Sciences</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">St-Laurent etÂ al. (2023)</span>
<span class="ltx_bibblock">
Marie St-Laurent, Basile Pinsard, Oliver Contier, Katja Seeliger, Valentina
Borghesani, Julie Boyle, Pierre Bellec, and Martin Hebart.

</span>
<span class="ltx_bibblock">cneuromod-things: a large-scale fmri dataset for task-and data-driven
assessment of object representation and visual memory recognition in the
human brain.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">Journal of Vision</em>, 23(9):5424â€“5424, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Subramaniam etÂ al. (2024)</span>
<span class="ltx_bibblock">
VÂ Subramaniam, CÂ Wang, AÂ Barbu, GÂ Kreiman, and BÂ Katz.

</span>
<span class="ltx_bibblock">Revealing vision-language integration in the brain with multimodal
networks.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">International Conference on Machine Learning</em>. International
Conference on Machine Learning (ICML), 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tan &amp; Bansal (2019)</span>
<span class="ltx_bibblock">
Hao Tan and Mohit Bansal.

</span>
<span class="ltx_bibblock">Lxmert: Learning cross-modality encoder representations from
transformers.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">Proceedings of the 2019 Conference on Empirical Methods in
Natural Language Processing and the 9th International Joint Conference on
Natural Language Processing (EMNLP-IJCNLP)</em>, pp.Â  5100â€“5111, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2024)</span>
<span class="ltx_bibblock">
Jerry Tang, Meng Du, VyÂ Vo, Vasudev Lal, and Alexander Huth.

</span>
<span class="ltx_bibblock">Brain encoding models based on multimodal transformers can transfer
across language and vision.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zineng Tang, Jaemin Cho, Yixin Nie, and Mohit Bansal.

</span>
<span class="ltx_bibblock">Tvlt: Textless vision-language transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Advances in Neural Information Processing Systems</em>,
35:9617â€“9632, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva &amp; Wehbe (2019)</span>
<span class="ltx_bibblock">
Mariya Toneva and Leila Wehbe.

</span>
<span class="ltx_bibblock">Interpreting and improving natural-language processing (in machines)
with natural language-processing (in the brain).

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Advances in Neural Information Processing Systems</em>, 32, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Toneva etÂ al. (2022)</span>
<span class="ltx_bibblock">
Mariya Toneva, TomÂ M Mitchell, and Leila Wehbe.

</span>
<span class="ltx_bibblock">Combining computational controls with natural text reveals aspects of
meaning composition.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Nature Computational Science</em>, 2(11):745â€“757, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tong etÂ al. (2022)</span>
<span class="ltx_bibblock">
Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.

</span>
<span class="ltx_bibblock">Videomae: Masked autoencoders are data-efficient learners for
self-supervised video pre-training.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Advances in Neural Information Processing Systems</em>,
35:10078â€“10093, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tuckute etÂ al. (2023)</span>
<span class="ltx_bibblock">
Greta Tuckute, Jenelle Feather, Dana Boebinger, and JoshÂ H McDermott.

</span>
<span class="ltx_bibblock">Many but not all deep neural network audio models capture brain
responses and exhibit correspondence between model stages and brain regions.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Plos Biology</em>, 21(12):e3002366, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaidya etÂ al. (2022)</span>
<span class="ltx_bibblock">
Aditya Vaidya, Shailee Jain, and Alexander Huth.

</span>
<span class="ltx_bibblock">Self-supervised models of audio effectively explain human cortical
responses to speech.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">International Conference on Machine Learning</em>, pp.Â 21927â€“21944. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Vaswani etÂ al. (2017)</span>
<span class="ltx_bibblock">
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,
AidanÂ N Gomez, Åukasz Kaiser, and Illia Polosukhin.

</span>
<span class="ltx_bibblock">Attention is all you need.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">Advances in Neural Information Processing Systems</em>, 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2019)</span>
<span class="ltx_bibblock">
Aria Wang, Michael Tarr, and Leila Wehbe.

</span>
<span class="ltx_bibblock">Neural taskonomy: Inferring the similarity of task-derived
representations from brain activity.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">NeurIPS</em>, 32:15501â€“15511, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang etÂ al. (2023)</span>
<span class="ltx_bibblock">
AriaÂ Y Wang, Kendrick Kay, Thomas Naselaris, MichaelÂ J Tarr, and Leila Wehbe.

</span>
<span class="ltx_bibblock">Better models of human high-level visual cortex emerge from natural
language supervision with a large and diverse dataset.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Nature Machine Intelligence</em>, 5(12):1415â€“1426, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wehbe etÂ al. (2014)</span>
<span class="ltx_bibblock">
Leila Wehbe, Brian Murphy, Partha Talukdar, Alona Fyshe, Aaditya Ramdas, and
Tom Mitchell.

</span>
<span class="ltx_bibblock">Simultaneously uncovering the patterns of brain regions involved in
different story reading subprocesses.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">PloS one</em>, 11, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wolf etÂ al. (2020)</span>
<span class="ltx_bibblock">
Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue,
Anthony Moi, Pierric Cistac, Tim Rault, RÃ©mi Louf, Morgan Funtowicz,
etÂ al.

</span>
<span class="ltx_bibblock">Transformers: State-of-the-art natural language processing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">Proceedings of the 2020 conference on empirical methods in
natural language processing: system demonstrations</em>, pp.Â  38â€“45, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wu etÂ al. (2024)</span>
<span class="ltx_bibblock">
Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua.

</span>
<span class="ltx_bibblock">Next-gpt: Any-to-any multimodal llm.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">Forty-first International Conference on Machine Learning</em>,
2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yamins etÂ al. (2014)</span>
<span class="ltx_bibblock">
DanielÂ LK Yamins, HaÂ Hong, CharlesÂ F Cadieu, EthanÂ A Solomon, Darren Seibert,
and JamesÂ J DiCarlo.

</span>
<span class="ltx_bibblock">Performance-optimized hierarchical models predict neural responses in
higher visual cortex.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">PNAS</em>, 111(23):8619â€“8624, 2014.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zellers etÂ al. (2022)</span>
<span class="ltx_bibblock">
Rowan Zellers, Jiasen Lu, Ximing Lu, Youngjae Yu, Yanpeng Zhao, Mohammadreza
Salehi, Aditya Kusupati, Jack Hessel, Ali Farhadi, and Yejin Choi.

</span>
<span class="ltx_bibblock">Merlot reserve: Neural script knowledge through vision and language
and sound.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision
and Pattern Recognition</em>, pp.Â  16375â€“16387, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang etÂ al. (2023)</span>
<span class="ltx_bibblock">
Hang Zhang, Xin Li, and Lidong Bing.

</span>
<span class="ltx_bibblock">Video-llama: An instruction-tuned audio-visual language model for
video understanding.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Proceedings of the 2023 Conference on Empirical Methods in
Natural Language Processing: System Demonstrations</em>, pp.Â  543â€“553, 2023.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Overview of Appendix Sections</h2>
<div class="ltx_para ltx_noindent" id="A1.p1">
<ul class="ltx_itemize" id="A1.I1">
<li class="ltx_item" id="A1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i1.p1">
<p class="ltx_p" id="A1.I1.i1.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A2" title="Appendix B Cross-subject prediction accuracy â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">B</span></a>: Cross-subject prediction accuracy</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i2.p1">
<p class="ltx_p" id="A1.I1.i2.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A3" title="Appendix C Detailed sub-ROIs of language, visual and auditory regions â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">C</span></a>: Detailed sub-ROIs of language, visual and auditory regions</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i3.p1">
<p class="ltx_p" id="A1.I1.i3.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A4" title="Appendix D Details of pretrained Transformer models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">D</span></a>: Details of pretrained Transformer models</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i4.p1">
<p class="ltx_p" id="A1.I1.i4.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A5" title="Appendix E Implementation details for reproducibility. â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">E</span></a>:
Implementation details for reproducibility.</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i5.p1">
<p class="ltx_p" id="A1.I1.i5.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A6" title="Appendix F Effectiveness of multi-modal vs unimodal representations for various brain regions â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">F</span></a>: Effectiveness of multi-modal vs unimodal representations for various brain regions</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i6.p1">
<p class="ltx_p" id="A1.I1.i6.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A7" title="Appendix G How is the brain alignment of multi-modal features affected by the elimination of a particular modality? â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">G</span></a>: How is the brain alignment of multi-modal features affected by the elimination of a
particular modality?</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i7.p1">
<p class="ltx_p" id="A1.I1.i7.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A8" title="Appendix H Layerwise brain alignment â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">H</span></a>: Layerwise brain alignment</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i8.p1">
<p class="ltx_p" id="A1.I1.i8.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A9" title="Appendix I Why the choice of ridge regression instead of more complex machine learning models? â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">I</span></a>: Why the choice of ridge regression instead of more complex machine learning models?</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i9" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i9.p1">
<p class="ltx_p" id="A1.I1.i9.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A10" title="Appendix J Extended Related Works â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">J</span></a>: Extended Related Works</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i10" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i10.p1">
<p class="ltx_p" id="A1.I1.i10.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A11" title="Appendix K Baseline Analysis: Scrambling Inputs to Multi-modal Models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">K</span></a>: Baseline Analysis: Scrambling Inputs to Multi-modal Models</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i11" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i11.p1">
<p class="ltx_p" id="A1.I1.i11.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12" title="Appendix L Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">L</span></a>: Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i12" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i12.p1">
<p class="ltx_p" id="A1.I1.i12.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A13" title="Appendix M Multi-modal versus unimodal effects â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">M</span></a>: Multi-modal versus unimodal effects</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i13" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="A1.I1.i13.p1">
<p class="ltx_p" id="A1.I1.i13.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A14" title="Appendix N Impact of diverse model architectures on performance comparison â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">N</span></a>: Impact of diverse model architectures on performance comparison</p>
</div>
</li>
<li class="ltx_item" id="A1.I1.i14" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para ltx_noindent" id="A1.I1.i14.p1">
<p class="ltx_p" id="A1.I1.i14.p1.1">SectionÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A15" title="Appendix O Limitations â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">O</span></a>: Limitations.</p>
</div>
</li>
</ul>
</div>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Cross-subject prediction accuracy</h2>
<div class="ltx_para ltx_noindent" id="A2.p1">
<p class="ltx_p" id="A2.p1.1">We estimate cross-subject prediction accuracy in three settings: (i) training with <span class="ltx_text ltx_font_italic" id="A2.p1.1.1">The Bourne supremacy</span> and testing with <span class="ltx_text ltx_font_italic" id="A2.p1.1.2">Life</span> data, (ii) training with <span class="ltx_text ltx_font_italic" id="A2.p1.1.3">The wolf of wall street</span> and testing with <span class="ltx_text ltx_font_italic" id="A2.p1.1.4">Life</span> data, and (iii) training with both <span class="ltx_text ltx_font_italic" id="A2.p1.1.5">The Bourne supremacy</span> and <span class="ltx_text ltx_font_italic" id="A2.p1.1.6">The wolf of wall street</span> and testing with Life data.
We present the average cross-subject prediction accuracy across voxels for the <em class="ltx_emph ltx_font_italic" id="A2.p1.1.7">Movie10 fMRI</em> dataset and across the three settings in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A2.F6" title="Figure 6 â€£ Appendix B Cross-subject prediction accuracy â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">6</span></a>.</p>
</div>
<figure class="ltx_figure" id="A2.F6">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A2.F6.1" style="width:397.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="384" id="A2.F6.1.g1" src="x8.png" width="415"/>
</div>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="A2.F6.2" style="width:397.5pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="296" id="A2.F6.2.g1" src="x9.png" width="415"/>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Cross-subject prediction accuracy: (top) across whole brain, (bottom) across language, visual and auditory regions.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Detailed sub-ROIs of language, visual and auditory regions</h2>
<div class="ltx_para ltx_noindent" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">The data covers seven brain regions of interest (ROIs) in the human brain with the following sub-divisions: (i) early visual (EV: V1, V2, V3, V3B, and V4); (ii) object-related areas (LO1 and LO2); (iii) face-related areas (OFA), (iv) scene-related areas (PPA), (v) middle temporal (MT: MT, MST, LO3, FST and V3CD), (vi) late language regions, encompassing broader language regions: angular gyrus (AG: PFm, PGs, PGi, TPOJ2, TPOJ3), lateral temporal cortex (LTC: STSda, STSva, STGa, TE1a, TE2a, TGv, TGd, A5, STSdp, STSvp, PSL, STV, TPOJ1), inferior frontal gyrus (IFG: 44, 45, IFJa, IFSp) and middle frontal gyrus (MFG: 55b)Â <cite class="ltx_cite ltx_citemacro_citep">(Baker etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib8" title="">2018</a>; Milton etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib35" title="">2021</a>; Desai etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib15" title="">2023</a>)</cite>.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Details of pretrained Transformer models</h2>
<div class="ltx_para ltx_noindent" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">Details of each pretrained Transformer model are reported in TableÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A4.T1" title="Table 1 â€£ Appendix D Details of pretrained Transformer models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">1</span></a> in Appendix. From the table, we can clearly observe that both multi-modal models (ImageBind and TVLT) maintain similar backbone architectures for videos but differ in their backbone architecture for embedding audio as well as in the training strategies. For the TVLT model, video embeddings are captured from the ViT model, while audio embeddings are generated from Mel Spectrograms and jointly pretrained within a single Transformer encoder.
In contrast, the ImageBind model uses the ViT model as the backbone for Images and Videos, while the AST model is used for Audio; these individual encoders are used and learn a common embedding space.</p>
</div>
<figure class="ltx_table" id="A4.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Pretrained Transformer-based Encoder Models. All models have 12 layers.</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T1.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T1.1.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.1.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.1.1.1.1">
<span class="ltx_p" id="A4.T1.1.1.1.1.1.1" style="width:31.8pt;">Model Name</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A4.T1.1.1.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.1.1.2.1">
<span class="ltx_p" id="A4.T1.1.1.1.2.1.1" style="width:59.6pt;">Pretraining data modality</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A4.T1.1.1.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.1.1.3.1">
<span class="ltx_p" id="A4.T1.1.1.1.3.1.1" style="width:79.5pt;">Pretraining Method</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A4.T1.1.1.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.1.1.4.1">
<span class="ltx_p" id="A4.T1.1.1.1.4.1.1" style="width:23.8pt;"># Parameters</span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A4.T1.1.1.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.1.1.5.1">
<span class="ltx_p" id="A4.T1.1.1.1.5.1.1" style="width:79.5pt;">Dataset</span>
</span>
</th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A4.T1.1.1.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">Layers</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r ltx_border_t" id="A4.T1.1.1.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.1.1.7.1">
<span class="ltx_p" id="A4.T1.1.1.1.7.1.1" style="width:79.5pt;">Backbone</span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T1.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_tt" id="A4.T1.1.2.1.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.2.1.1.1">
<span class="ltx_p" id="A4.T1.1.2.1.1.1.1" style="width:31.8pt;">ImageBind</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="A4.T1.1.2.1.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.2.1.2.1">
<span class="ltx_p" id="A4.T1.1.2.1.2.1.1" style="width:59.6pt;">Video &amp; Audio</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="A4.T1.1.2.1.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.2.1.3.1">
<span class="ltx_p" id="A4.T1.1.2.1.3.1.1" style="width:79.5pt;">Cross-model multi-modal Transformer</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="A4.T1.1.2.1.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.2.1.4.1">
<span class="ltx_p" id="A4.T1.1.2.1.4.1.1" style="width:23.8pt;">132 M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="A4.T1.1.2.1.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.2.1.5.1">
<span class="ltx_p" id="A4.T1.1.2.1.5.1.1" style="width:79.5pt;">Audioset, Ego4D, SUN RGB-D</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="A4.T1.1.2.1.6" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="A4.T1.1.2.1.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.2.1.7.1">
<span class="ltx_p" id="A4.T1.1.2.1.7.1.1" style="width:79.5pt;">ViT for Images and Videos, AST for audio</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T1.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.3.2.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.3.2.1.1">
<span class="ltx_p" id="A4.T1.1.3.2.1.1.1" style="width:31.8pt;">TVLT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.3.2.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.3.2.2.1">
<span class="ltx_p" id="A4.T1.1.3.2.2.1.1" style="width:59.6pt;">Video &amp; Audio</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.3.2.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.3.2.3.1">
<span class="ltx_p" id="A4.T1.1.3.2.3.1.1" style="width:79.5pt;">Jointly pretrained on video and audio (Masked auto encoder)</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.3.2.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.3.2.4.1">
<span class="ltx_p" id="A4.T1.1.3.2.4.1.1" style="width:23.8pt;">88 M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.3.2.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.3.2.5.1">
<span class="ltx_p" id="A4.T1.1.3.2.5.1.1" style="width:79.5pt;">HowTo100M, YTTemporal180M</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T1.1.3.2.6" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.3.2.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.3.2.7.1">
<span class="ltx_p" id="A4.T1.1.3.2.7.1.1" style="width:79.5pt;">ViT for video embeddings, and Spectrogram for audio embeddings</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T1.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.4.3.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.4.3.1.1">
<span class="ltx_p" id="A4.T1.1.4.3.1.1.1" style="width:31.8pt;">ViT-B</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.4.3.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.4.3.2.1">
<span class="ltx_p" id="A4.T1.1.4.3.2.1.1" style="width:59.6pt;">Image</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.4.3.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.4.3.3.1">
<span class="ltx_p" id="A4.T1.1.4.3.3.1.1" style="width:79.5pt;">Vision Transformer</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.4.3.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.4.3.4.1">
<span class="ltx_p" id="A4.T1.1.4.3.4.1.1" style="width:23.8pt;">86 M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.4.3.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.4.3.5.1">
<span class="ltx_p" id="A4.T1.1.4.3.5.1.1" style="width:79.5pt;">ImageNet</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T1.1.4.3.6" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.4.3.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.4.3.7.1">
<span class="ltx_p" id="A4.T1.1.4.3.7.1.1" style="width:79.5pt;">Transformer encoder</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T1.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.5.4.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.5.4.1.1">
<span class="ltx_p" id="A4.T1.1.5.4.1.1.1" style="width:31.8pt;">VideoMAE</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.5.4.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.5.4.2.1">
<span class="ltx_p" id="A4.T1.1.5.4.2.1.1" style="width:59.6pt;">Video</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.5.4.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.5.4.3.1">
<span class="ltx_p" id="A4.T1.1.5.4.3.1.1" style="width:79.5pt;">Masked autoencoder for video inputs</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.5.4.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.5.4.4.1">
<span class="ltx_p" id="A4.T1.1.5.4.4.1.1" style="width:23.8pt;">87 M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.5.4.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.5.4.5.1">
<span class="ltx_p" id="A4.T1.1.5.4.5.1.1" style="width:79.5pt;">Kinetics, Epic Kitchens 100, Something-Something v2</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T1.1.5.4.6" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.5.4.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.5.4.7.1">
<span class="ltx_p" id="A4.T1.1.5.4.7.1.1" style="width:79.5pt;">ViT-B</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T1.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.6.5.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.6.5.1.1">
<span class="ltx_p" id="A4.T1.1.6.5.1.1.1" style="width:31.8pt;">ViViT</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.6.5.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.6.5.2.1">
<span class="ltx_p" id="A4.T1.1.6.5.2.1.1" style="width:59.6pt;">Video</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.6.5.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.6.5.3.1">
<span class="ltx_p" id="A4.T1.1.6.5.3.1.1" style="width:79.5pt;">Video vision Transformer</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.6.5.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.6.5.4.1">
<span class="ltx_p" id="A4.T1.1.6.5.4.1.1" style="width:23.8pt;">86 M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.6.5.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.6.5.5.1">
<span class="ltx_p" id="A4.T1.1.6.5.5.1.1" style="width:79.5pt;">Kinetics, Epic Kitchens 100, Something-Something v2</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T1.1.6.5.6" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.6.5.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.6.5.7.1">
<span class="ltx_p" id="A4.T1.1.6.5.7.1.1" style="width:79.5pt;">ViT-B</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T1.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.7.6.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.7.6.1.1">
<span class="ltx_p" id="A4.T1.1.7.6.1.1.1" style="width:31.8pt;">Wav2Vec 2.0-base</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.7.6.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.7.6.2.1">
<span class="ltx_p" id="A4.T1.1.7.6.2.1.1" style="width:59.6pt;">Speech</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.7.6.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.7.6.3.1">
<span class="ltx_p" id="A4.T1.1.7.6.3.1.1" style="width:79.5pt;">Speech-based Transformer model</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.7.6.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.7.6.4.1">
<span class="ltx_p" id="A4.T1.1.7.6.4.1.1" style="width:23.8pt;">95 M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.7.6.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.7.6.5.1">
<span class="ltx_p" id="A4.T1.1.7.6.5.1.1" style="width:79.5pt;">Librispeech</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="A4.T1.1.7.6.6" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A4.T1.1.7.6.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.7.6.7.1">
<span class="ltx_p" id="A4.T1.1.7.6.7.1.1" style="width:79.5pt;">Transformer encoder</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A4.T1.1.8.7">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="A4.T1.1.8.7.1" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.8.7.1.1">
<span class="ltx_p" id="A4.T1.1.8.7.1.1.1" style="width:31.8pt;">AST</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T1.1.8.7.2" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.8.7.2.1">
<span class="ltx_p" id="A4.T1.1.8.7.2.1.1" style="width:59.6pt;">Speech</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T1.1.8.7.3" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.8.7.3.1">
<span class="ltx_p" id="A4.T1.1.8.7.3.1.1" style="width:79.5pt;">Audio Spectrogram Transformer</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T1.1.8.7.4" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.8.7.4.1">
<span class="ltx_p" id="A4.T1.1.8.7.4.1.1" style="width:23.8pt;">86 M</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T1.1.8.7.5" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.8.7.5.1">
<span class="ltx_p" id="A4.T1.1.8.7.5.1.1" style="width:79.5pt;">AudioSet, ESC-50 and Speech commands</span>
</span>
</td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r ltx_border_t" id="A4.T1.1.8.7.6" style="padding-left:2.0pt;padding-right:2.0pt;">12</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A4.T1.1.8.7.7" style="padding-left:2.0pt;padding-right:2.0pt;">
<span class="ltx_inline-block ltx_align_top" id="A4.T1.1.8.7.7.1">
<span class="ltx_p" id="A4.T1.1.8.7.7.1.1" style="width:79.5pt;">Initialized with ViT-B weights</span>
</span>
</td>
</tr>
</tbody>
</table>
</figure>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Implementation details for reproducibility.</h2>
<div class="ltx_para ltx_noindent" id="A5.p1">
<p class="ltx_p" id="A5.p1.4">All experiments were conducted on a machine with 1 NVIDIA GeForce-GTX GPU with 16GB GPU RAM. We used bootstrap ridge-regression (AppendixÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A9" title="Appendix I Why the choice of ridge regression instead of more complex machine learning models? â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">I</span></a>) with MSE loss function; L2-decay (<math alttext="\lambda" class="ltx_Math" display="inline" id="A5.p1.1.m1.1"><semantics id="A5.p1.1.m1.1a"><mi id="A5.p1.1.m1.1.1" xref="A5.p1.1.m1.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="A5.p1.1.m1.1b"><ci id="A5.p1.1.m1.1.1.cmml" xref="A5.p1.1.m1.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.1.m1.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A5.p1.1.m1.1d">italic_Î»</annotation></semantics></math>) varied from 10<sup class="ltx_sup" id="A5.p1.4.1"><span class="ltx_text ltx_font_italic" id="A5.p1.4.1.1">1</span></sup> to 10<sup class="ltx_sup" id="A5.p1.4.2"><span class="ltx_text ltx_font_italic" id="A5.p1.4.2.1">3</span></sup>. Best <math alttext="\lambda" class="ltx_Math" display="inline" id="A5.p1.4.m4.1"><semantics id="A5.p1.4.m4.1a"><mi id="A5.p1.4.m4.1.1" xref="A5.p1.4.m4.1.1.cmml">Î»</mi><annotation-xml encoding="MathML-Content" id="A5.p1.4.m4.1b"><ci id="A5.p1.4.m4.1.1.cmml" xref="A5.p1.4.m4.1.1">ğœ†</ci></annotation-xml><annotation encoding="application/x-tex" id="A5.p1.4.m4.1c">\lambda</annotation><annotation encoding="application/x-llamapun" id="A5.p1.4.m4.1d">italic_Î»</annotation></semantics></math> was chosen by tuning on validation data that comprised a randomly chosen 10% subset from train set used only for hyper-parameter tuning.</p>
</div>
<figure class="ltx_figure" id="A5.F7">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="72" id="A5.F7.g1" src="x10.png" width="415"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="181" id="A5.F7.g2" src="x11.png" width="203"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="181" id="A5.F7.g3" src="x12.png" width="203"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="181" id="A5.F7.g4" src="x13.png" width="203"/></div>
<div class="ltx_flex_cell ltx_flex_size_4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="181" id="A5.F7.g5" src="x14.png" width="203"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="181" id="A5.F7.g6" src="x15.png" width="203"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="181" id="A5.F7.g7" src="x16.png" width="203"/></div>
<div class="ltx_flex_cell ltx_flex_size_3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_square" height="181" id="A5.F7.g8" src="x17.png" width="203"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 7: </span>Average normalized brain alignment for per video and audio modalities from multi-modal and individual modality features across whole brain and several ROIs of language (ATL, IFGOrb, MFG, PCC, dmPFC) and visual (LOC, OFA). The points overlaid on the bars represent the normalized brain alignment scores of the six participants.
</figcaption>
</figure>
<figure class="ltx_figure" id="A5.F8">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="A5.F8.g1" src="x18.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="A5.F8.g2" src="x19.png" width="407"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="A5.F8.g3" src="x20.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="A5.F8.g4" src="x21.png" width="407"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="A5.F8.g5" src="x22.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="A5.F8.g6" src="x23.png" width="407"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="A5.F8.g7" src="x24.png" width="407"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="280" id="A5.F8.g8" src="x25.png" width="407"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 8: </span>Average normalized brain alignment for video and audio modalities from multi-modal and individual modality features across whole brain and several ROIs of language (AG, ATL, PTL, IFG, PCC, dmPFC) and visual (EVC, MT). Error bars
indicate the standard error of the mean across participants.
</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Effectiveness of multi-modal vs unimodal representations for various brain regions</h2>
<div class="ltx_para ltx_noindent" id="A6.p1">
<p class="ltx_p" id="A6.p1.1">Average normalized brain alignment for per video and audio modalities from multi-modal and individual modality features across whole brain and several ROIs of language (ATL, IFGOrb, MFG, PCC, dmPFC) and visual (LOC, OFA) are shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A5.F7" title="Figure 7 â€£ Appendix E Implementation details for reproducibility. â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">7</span></a>. Our observations are as follows: (i) Cross-modal IB Concat embeddings are significantly better than TVLT Joint embeddings in semantic regions such as AG and PCC, as well as the multi-modal processing region MT. (ii) Conversely, TVLT Joint embeddings are significantly better than IB Concat embeddings in dmPFC regions. This indicates that while both cross-modal and jointly pretrained multi-modal models perform similarly at a macro level, there are individual differences at micro level.</p>
</div>
<div class="ltx_para ltx_noindent" id="A6.p2">
<p class="ltx_p" id="A6.p2.1">We now present the results for per unimodal video model and per speech model in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A5.F8" title="Figure 8 â€£ Appendix E Implementation details for reproducibility. â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">8</span></a>. Similar to the average results of unimodal video and speech models, we observe that multi-modal models exhibit better normalized brain alignment than individual unimodal video and speech models across language and visual regions. Among unimodal speech models, the AST model shows better normalized brain alignment than the Wav2vec2.0 model. Among unimodal video models, each unimodal video model displays notably consistent performance across regions.</p>
</div>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>How is the brain alignment of multi-modal features affected by the elimination of a
particular modality?</h2>
<div class="ltx_para ltx_noindent" id="A7.p1">
<p class="ltx_p" id="A7.p1.1">To understand the contribution of each modality to the multi-modal brain alignment for multi-modal naturalistic stimulus, we perform residual analyses by removing the unimodality features from multi-modal joint representations as well as multi-modal video or audio representations from joint representations and measure the differences in brain alignment before and after removal modality-specific features.
Figs.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A7.F9" title="Figure 9 â€£ Appendix G How is the brain alignment of multi-modal features affected by the elimination of a particular modality? â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">9</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A7.F10" title="Figure 10 â€£ Appendix G How is the brain alignment of multi-modal features affected by the elimination of a particular modality? â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">10</span></a> display the normalized brain alignment for language ROIs such as PTL, MFG, ATL, PCC and visual regions EVC, LOC and OFA. We note a decrease in brain alignment for these regions following the removal of video embeddings from cross-modality models, whereas the removal of audio embeddings does not affect the brain alignment. On the other hand, for jointly pretrained models, removal of both video and audio embeddings partially impacts the brain alignment.</p>
</div>
<figure class="ltx_figure" id="A7.F9">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="275" id="A7.F9.g1" src="x26.png" width="365"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="275" id="A7.F9.g2" src="x27.png" width="365"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="275" id="A7.F9.g3" src="x28.png" width="365"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="275" id="A7.F9.g4" src="x29.png" width="365"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="275" id="A7.F9.g5" src="x30.png" width="365"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="275" id="A7.F9.g6" src="x31.png" width="365"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="275" id="A7.F9.g7" src="x32.png" width="365"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 9: </span>Residual analysis for ATL, PTL, IFG, MFG, IFGOrb, PCC and dmPFC regions: Average normalized brain alignment was computed across participants before and after removal of video and audio embeddings from both jointly pretrained and cross-modality models. The points overlaid on the bars represent the normalized brain alignment scores of the six participants. â€œ-â€ symbol represents residuals.
</figcaption>
</figure>
<figure class="ltx_figure" id="A7.F10">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="310" id="A7.F10.g1" src="x33.png" width="411"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="310" id="A7.F10.g2" src="x34.png" width="411"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="310" id="A7.F10.g3" src="x35.png" width="411"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="310" id="A7.F10.g4" src="x36.png" width="411"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="310" id="A7.F10.g5" src="x37.png" width="411"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 10: </span>Residual analysis for EVC, LOC, PPA, OFA and AC regions: Average normalized brain alignment was computed across participants before and after removal of video and audio embeddings from both jointly pretrained and cross-modality models. The points overlaid on the bars represent the normalized brain alignment scores of the six participants. â€œ-â€ symbol represents residuals.
</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Layerwise brain alignment</h2>
<div class="ltx_para ltx_noindent" id="A8.p1">
<p class="ltx_p" id="A8.p1.1">We now plot the layer-wise normalized brain alignment for the Unimodal models and TVLT joint model, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A8.F11" title="Figure 11 â€£ Appendix H Layerwise brain alignment â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">11</span></a>.
Observation from Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A8.F11" title="Figure 11 â€£ Appendix H Layerwise brain alignment â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">11</span></a> indicates a consistent drop in performance from early to lower layers, specifically for both TVLT joint and unimodal video models.
The key finding here is that our results that TVLT joint embeddings showcase improved brain alignment across all the layers compared to unimodal video and speech embeddings.</p>
</div>
<figure class="ltx_figure" id="A8.F11"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="426" id="A8.F11.g1" src="x38.png" width="581"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 11: </span>Normalized brain alignment across layers for multi-modal model (TVLT joint embeddings) and unimodal video and speech models.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A9">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix I </span>Why the choice of ridge regression instead of more complex machine learning models?</h2>
<div class="ltx_para ltx_noindent" id="A9.p1">
<p class="ltx_p" id="A9.p1.1">Since fMRI brain recordings have a low signal-to-noise ratio, and pretrained language models are trained in a non-linear fashion, the model representations are rich and complex.
To understand the relationship between brain activity and various stimuli, a large body of brain encoding literature over the past two decadesÂ <cite class="ltx_cite ltx_citemacro_cite">Wehbe etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib63" title="">2014</a>); Huth etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib30" title="">2016</a>); Jain &amp; Huth (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib32" title="">2018</a>); Toneva &amp; Wehbe (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib55" title="">2019</a>); Schrimpf etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib49" title="">2021</a>); Caucheteux &amp; King (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib11" title="">2022</a>); Antonello etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib2" title="">2024</a>); Vaidya etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib59" title="">2022</a>); Millet etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib34" title="">2022</a>); Oota etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib40" title="">2023a</a>; <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib42" title="">2024a</a>); Wang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib62" title="">2023</a>)</cite> has preferred ridge regression due to its simplicity.
Ridge regression is a linear model, making it easier to interpret and understand compared to more complex models. Further, the regularization in ridge regression helps manage the noise effectively, leading to more robust and reliable models.</p>
</div>
</section>
<section class="ltx_appendix" id="A10">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix J </span>Extended Related Works</h2>
<div class="ltx_para ltx_noindent" id="A10.p1">
<p class="ltx_p" id="A10.p1.1"><cite class="ltx_cite ltx_citemacro_citet">Tang etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib53" title="">2024</a>)</cite> demonstrate the use of multi-modal models in a cross-modal experiment to assess how well the language encoding models can predict movie-fMRI responses and how well the vision encoding models can predict narrative story-fMRI.
Â <cite class="ltx_cite ltx_citemacro_citet">Nakagi etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib36" title="">2024</a>)</cite> analyzed fMRI related to video content viewing and found distinct brain regions associated with different semantic levels, highlighting the significance of modeling various levels of semantic content simultaneously.</p>
</div>
<div class="ltx_para ltx_noindent" id="A10.p2">
<p class="ltx_p" id="A10.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Subramaniam etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib51" title="">2024</a>)</cite> utilized vision-language models based on image frame-text pairs, despite the stimuli being continuous movies. While effective for certain tasks, this approach may overlook the temporal dynamics inherent in videos. Additionally, their use of stereoencephalography (sEEG), although offers high temporal resolution, is limited in spatial resolution and typically restricted to specific brain regions where the electrodes are implanted. Finally, their study focused on multi-modal integration in a cross-modal model setting and did not explore jointly pretrained settings. Additionally, each participant watched a different movie while their sEEG activity was recorded, therefore the input stimuli varied widely across participants.
In contrast, our study leverages video-audio models that capture the temporal events in videos, providing richer and more dynamic representations of the stimuli. By incorporating audio data, we preserve acoustic information that may be lost in text-based transcriptions. Moreover, we utilize fMRI data, which offers whole-brain coverage and higher spatial resolution, enabling a more comprehensive analysis of the brain activity. Our approach also considers both cross-modal and jointly pretrained multi-modal models, offering a more nuanced understanding of how different modalities interact and integrate information in the brain.</p>
</div>
</section>
<section class="ltx_appendix" id="A11">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix K </span>Baseline Analysis: Scrambling Inputs to Multi-modal Models</h2>
<div class="ltx_para ltx_noindent" id="A11.p1">
<p class="ltx_p" id="A11.p1.1">We conducted an additional baseline experiment where we kept the trained weights unchanged and shuffled the movie stimuli into a scrambled order as input to the two multi-modal models: cross-modal and jointly-pretrained models, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A11.F12" title="Figure 12 â€£ Appendix K Baseline Analysis: Scrambling Inputs to Multi-modal Models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">12</span></a> as â€œIB Concat Shuffleâ€ and â€œTVLT Joint Shuffleâ€ respectively.</p>
</div>
<div class="ltx_para ltx_noindent" id="A11.p2">
<p class="ltx_p" id="A11.p2.1">From Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A11.F12" title="Figure 12 â€£ Appendix K Baseline Analysis: Scrambling Inputs to Multi-modal Models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">12</span></a>, we observe that embeddings from multi-modal models exhibit significantly better brain alignment compared to both randomly initialized models and when passing scrambled clips as input. Furthermore, when comparing scrambled input to pretrained models with randomly initialized models, the scrambled input shows improved alignment over random initialization.
Overall, this sanity check confirms that representations from multi-modal models maintain meaningful alignment with brain activity, even when the stimulus order is scrambled, highlighting their robustness and effectiveness.</p>
</div>
<figure class="ltx_figure" id="A11.F12"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="276" id="A11.F12.g1" src="x39.png" width="415"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 12: </span>The plot compares the average normalized brain alignment across the whole brain under three conditions: (i) pretrained models, (ii) randomly initialized models, and (iii) pretrained multi-modal models using scrambled videos as input. Error bars represent the standard error of the mean across participants. <math alttext="\ast" class="ltx_Math" display="inline" id="A11.F12.5.m1.1"><semantics id="A11.F12.5.m1.1b"><mo id="A11.F12.5.m1.1.1" xref="A11.F12.5.m1.1.1.cmml">âˆ—</mo><annotation-xml encoding="MathML-Content" id="A11.F12.5.m1.1c"><ci id="A11.F12.5.m1.1.1.cmml" xref="A11.F12.5.m1.1.1">âˆ—</ci></annotation-xml><annotation encoding="application/x-tex" id="A11.F12.5.m1.1d">\ast</annotation><annotation encoding="application/x-llamapun" id="A11.F12.5.m1.1e">âˆ—</annotation></semantics></math> indicates cases where multi-modal embeddings are significantly better
than randomly initialized models, i.e., p<math alttext="\leq" class="ltx_Math" display="inline" id="A11.F12.6.m2.1"><semantics id="A11.F12.6.m2.1b"><mo id="A11.F12.6.m2.1.1" xref="A11.F12.6.m2.1.1.cmml">â‰¤</mo><annotation-xml encoding="MathML-Content" id="A11.F12.6.m2.1c"><leq id="A11.F12.6.m2.1.1.cmml" xref="A11.F12.6.m2.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="A11.F12.6.m2.1d">\leq</annotation><annotation encoding="application/x-llamapun" id="A11.F12.6.m2.1e">â‰¤</annotation></semantics></math> 0.05. <math alttext="\wedge" class="ltx_Math" display="inline" id="A11.F12.7.m3.1"><semantics id="A11.F12.7.m3.1b"><mo id="A11.F12.7.m3.1.1" xref="A11.F12.7.m3.1.1.cmml">âˆ§</mo><annotation-xml encoding="MathML-Content" id="A11.F12.7.m3.1c"><and id="A11.F12.7.m3.1.1.cmml" xref="A11.F12.7.m3.1.1"></and></annotation-xml><annotation encoding="application/x-tex" id="A11.F12.7.m3.1d">\wedge</annotation><annotation encoding="application/x-llamapun" id="A11.F12.7.m3.1e">âˆ§</annotation></semantics></math> indicates cases where multi-modal embeddings
are significantly better than scrambled videos as input to multi-modal models, i.e., p<math alttext="\leq" class="ltx_Math" display="inline" id="A11.F12.8.m4.1"><semantics id="A11.F12.8.m4.1b"><mo id="A11.F12.8.m4.1.1" xref="A11.F12.8.m4.1.1.cmml">â‰¤</mo><annotation-xml encoding="MathML-Content" id="A11.F12.8.m4.1c"><leq id="A11.F12.8.m4.1.1.cmml" xref="A11.F12.8.m4.1.1"></leq></annotation-xml><annotation encoding="application/x-tex" id="A11.F12.8.m4.1d">\leq</annotation><annotation encoding="application/x-llamapun" id="A11.F12.8.m4.1e">â‰¤</annotation></semantics></math> 0.05. </figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A12">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix L </span>Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models</h2>
<div class="ltx_para ltx_noindent" id="A12.p1">
<p class="ltx_p" id="A12.p1.1"><span class="ltx_text ltx_font_bold" id="A12.p1.1.1">Residual Analysis on the Brain:</span> The features can be removed from the model representations, or from the brain recordings. Conceptually, the results of these approaches should be the same because when the feature is removed completely from either the input or/and the target, it would not be able to further impact the observed alignment. However, practically, brain recordings are noisier than model representations and so estimating the removal regression model will be more difficult especially with fMRI data of low SNR. Thus residual analysis on the brain is less effective. Therefore, we opt to remove features from the model representations where we can exercise more control.</p>
</div>
<div class="ltx_para ltx_noindent" id="A12.p2">
<p class="ltx_p" id="A12.p2.1"><span class="ltx_text ltx_font_bold" id="A12.p2.1.1">Unique Variance Calculation:</span> We now build two voxelwise joint encoding models: (i) that includes both unimodal and cross-modal features, (ii) that includes both unimodal and jointly-pretrained features. Using these joint encoding models and prior encoding models, we compute the unique variance explained by each model i.e. unimodal and cross-modal, unimodal and jointly-pretrained models. Also, we compute the shared variance between these models. The results are shown in Figs.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12.F13" title="Figure 13 â€£ Appendix L Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">13</span></a>,Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12.F14" title="Figure 14 â€£ Appendix L Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">14</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12.F15" title="Figure 15 â€£ Appendix L Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">15</span></a> for the whole brain, language network and visual network, respectively. We make the following observations from Figs.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12.F13" title="Figure 13 â€£ Appendix L Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">13</span></a>,Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12.F14" title="Figure 14 â€£ Appendix L Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">14</span></a> andÂ <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12.F15" title="Figure 15 â€£ Appendix L Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">15</span></a>: (i) It can be clearly seen that the shared variance between jointly-pretrained (TVLT) model and the unimodal models is significantly lower than that observed between cross-modal (IB Concat) versus unimodal models. This result is in line with the earlier results on residual analysis where IB Concat showed larger drop in performance when unimodal information is removed as compared to that of TVLT model (see Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F4" title="Figure 4 â€£ 6.2 Which brain regions process uni- and multi-modal information? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">4</span></a>). Specifically, we can see that there is a larger drop in performance for removal of video features as compared to that with removal of speech features.
(ii) For the visual network, as shown in Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#A12.F15" title="Figure 15 â€£ Appendix L Whole Brain, Language and Visual ROIs analysis: Shared and Unique variance between Multi-modal and Unimodal models â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">15</span></a>, we observe that the unique explained variance between TVLT and unimodal VMs is comparable, while the cross-modal model IB Concat exhibits a higher unique explained variance compared to unimodal VMs. In contrast to unimodal VMs, both cross-modal and jointly pre-trained models demonstrate higher unique explained variance compared to unimodal SMs.</p>
</div>
<figure class="ltx_figure" id="A12.F13">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="232" id="A12.F13.g1" src="x40.png" width="290"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="231" id="A12.F13.g2" src="x41.png" width="291"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="200" id="A12.F13.g3" src="x42.png" width="291"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="207" id="A12.F13.g4" src="x43.png" width="290"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 13: </span>Whole Brain Analysis: Shared and Unique Variance explained between Cross-modal (IBConcat) and Unimodal models (VM, SM), Jointly-pretrained (TVLT) and Unimodal models (VM, SM). In each plot, Pink Area (Left Circle - Intersection) represents the unique variance explained by the multi-modal model that is not shared with the unimodal model. Green Area (Right Circle - Intersection) represents the unique variance explained by the unimodal model that is not shared with the Multi-modal model. Light Brown Intersection (Overlap) represents the shared variance between the multi-modal and unimodal model. It indicates the extent to which both models explain overlapping neural variance in the whole brain.</figcaption>
</figure>
<figure class="ltx_figure" id="A12.F14">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="233" id="A12.F14.g1" src="x44.png" width="291"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="180" id="A12.F14.g2" src="x45.png" width="291"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="203" id="A12.F14.g3" src="x46.png" width="291"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="209" id="A12.F14.g4" src="x47.png" width="290"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 14: </span>Language Network: Shared and Unique Variance explained between Cross-modal (IB Concat) and Unimodal models (VM, SM), Jointly-pretrained (TVLT) and Unimodal models (VM, SM). In each plot, Pink Area (Left Circle - Intersection) represents the unique variance explained by the multi-modal model that is not shared with the unimodal model. Green Area (Right Circle - Intersection) represents the unique variance explained by the unimodal model that is not shared with the Multi-modal model. Light Brown Intersection (Overlap) represents the shared variance between the multi-modal and unimodal model. It indicates the extent to which both models explain overlapping neural variance in the whole brain.</figcaption>
</figure>
<figure class="ltx_figure" id="A12.F15">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="225" id="A12.F15.g1" src="x48.png" width="291"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="197" id="A12.F15.g2" src="x49.png" width="290"/></div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="191" id="A12.F15.g3" src="x50.png" width="291"/></div>
<div class="ltx_flex_cell ltx_flex_size_2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_figure_panel ltx_img_landscape" height="204" id="A12.F15.g4" src="x51.png" width="291"/></div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 15: </span>Visual Network: Shared and Unique Variance explained between Cross-modal (IB Concat) and Unimodal models (VM, SM), Jointly-pretrained (TVLT) and Unimodal models (VM, SM). In each plot, Pink Area (Left Circle - Intersection) represents the unique variance explained by the multi-modal model that is not shared with the unimodal model. Green Area (Right Circle - Intersection) represents the unique variance explained by the unimodal model that is not shared with the Multi-modal model. Light Brown Intersection (Overlap) represents the shared variance between the multi-modal and unimodal model. It indicates the extent to which both models explain overlapping neural variance in the whole brain.</figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A13">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix M </span>Multi-modal versus unimodal effects</h2>
<div class="ltx_para ltx_noindent" id="A13.p1">
<p class="ltx_p" id="A13.p1.1"><span class="ltx_text ltx_font_bold" id="A13.p1.1.1">Multi-modal effects:</span>
In general, multi-modal models have better predictivity in the language regions (see Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F2" title="Figure 2 â€£ 6.1 How effective are multi-modal representations? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">2</span></a>).</p>
</div>
<div class="ltx_para ltx_noindent" id="A13.p2">
<p class="ltx_p" id="A13.p2.1"><span class="ltx_text ltx_font_bold" id="A13.p2.1.1">Unimodal effects:</span> Unimodal models have higher predictivity in the early sensory regions (visual and auditory). Such patterns of results are expected.</p>
</div>
<div class="ltx_para ltx_noindent" id="A13.p3">
<p class="ltx_p" id="A13.p3.1"><span class="ltx_text ltx_font_bold" id="A13.p3.1.1">Critical Differences:</span>
However, critical differences are also observed. Although multi-modal models perform with around 50% alignment (Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F3" title="Figure 3 â€£ Whole brain analysis. â€£ Cross-modal vs. Jointly pretrained multi-modal models vs. Unimodal models. â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">3</span></a>) in high-level visual regions (PPA, MT), they seem to perform less (around 40% alignment) in the early visual region (EVC). These patterns are also seen in the auditory regions (AC versus PTL). Thus, there seem to be both patterns of similarity as well as critical dissimilarities observed in the comparative analyses.</p>
</div>
<div class="ltx_para ltx_noindent" id="A13.p4">
<p class="ltx_p" id="A13.p4.1"><span class="ltx_text ltx_font_bold" id="A13.p4.1.1">Residual analysis:</span>
For cross-modality models, the alignment in regions AG and MT is extremely high, and this alignment is only partially explained by video features (Fig.Â <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#S6.F4" title="Figure 4 â€£ 6.2 Which brain regions process uni- and multi-modal information? â€£ 6 Results â€£ Multi-modal brain encoding models for multi-modal stimuli"><span class="ltx_text ltx_ref_tag">4</span></a>). This implies that significant unexplained alignment remains after the removal of video features. Conversely, the removal of speech features does not lead to a drop in brain alignment, indicating that there is additional information beyond speech features that is processed in these regions.
The EVC region is well-predicted by unimodal video models, showing similar levels of predictivity with multi-modal models. A recent study by [Oota et al. 2024] explored the type of information in language models that predicts brain activity and found that low-level features in these models drive predictivity, such as speech-based language models predicting the visual cortex or text models predicting the auditory cortex.</p>
</div>
<div class="ltx_para ltx_noindent" id="A13.p5">
<p class="ltx_p" id="A13.p5.1">When unimodal video model (VM) features are regressed out of multi-modal models, there is no performance drop in EVC but a drop in regions like PPA and LOC, suggesting that multi-modal models do not solely rely on corresponding unimodal features but also contain unexplained variance in EVC. Additionally, removing low-level features like motion energy may impact EVC performance. Interestingly, regressing out unimodal VM features does not affect speech-related information in multi-modal models, as speech models also exhibit brain predictivity in EVC.</p>
</div>
</section>
<section class="ltx_appendix" id="A14">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix N </span>Impact of diverse model architectures on performance comparison</h2>
<div class="ltx_para ltx_noindent" id="A14.p1">
<p class="ltx_p" id="A14.p1.1">Several prior brain encoding studies in the literature compared a variety of language/speech models (differing in their size, architecture, training dataset, etc.) and their brain alignment.</p>
</div>
<div class="ltx_para ltx_noindent" id="A14.p2">
<p class="ltx_p" id="A14.p2.1"><cite class="ltx_cite ltx_citemacro_citet">Schrimpf etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib49" title="">2021</a>)</cite> investigated 43 language models ranging from distributed word embeddings models like Word2Vec, GloVe, FastText, to Sequence models such as RNN, LSTM, Contextualized models like ELMo, Transformer models like BERT, GPT-2 and Transformer-XL with its variations such as base, small, and larger models. Although all these models have different architectures, training datasets, Â <cite class="ltx_cite ltx_citemacro_citet">Schrimpf etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib49" title="">2021</a>)</cite> considered each model as a subject and computed normalized brain predictivity, i.e., what percentage the model explains the variance given a ceiling value for each voxel.
Similarly,Â <cite class="ltx_cite ltx_citemacro_citet">Toneva &amp; Wehbe (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib55" title="">2019</a>)</cite> used four different language models such as ELMo, BERT, USE and Transformer-XL and compared the explained variance of each model while doing brain alignment.
Further,Â <cite class="ltx_cite ltx_citemacro_citet">Aw &amp; Toneva (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib5" title="">2023</a>)</cite> use four longer-context language models such as BART, Big-Bird, Longformer and Long-T5 and verify the deeper understanding of language models and brain alignment by the amount of variance explained by each model.
Similarly,Â <cite class="ltx_cite ltx_citemacro_citet">Antonello etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib1" title="">2021</a>)</cite> used 101 language models including both pretrained and task-based language models and compared the amount of explained variance in the brain by extracting the semantic representations and whether these representations are closer to brain-level semantics.
Recently,Â <cite class="ltx_cite ltx_citemacro_citet">Oota &amp; Toneva (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib37" title="">2023</a>); Oota etÂ al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib42" title="">2024a</a>)</cite> used four text-based language models such as BERT, GPT-2, FLAN-T5 and BART, speech-based language models such as Wav2Vec2.0 and Whisper and verified the amount of explained variance in the brain at different language regions.</p>
</div>
<div class="ltx_para ltx_noindent" id="A14.p3">
<p class="ltx_p" id="A14.p3.1">It is important to observe that all the above studies utilize a number of language models that are different in training architecture and training datasets, however the primary goal of all these studies is to investigate how close the semantic representations captured by each model aligns with brain-relevant semantics.</p>
</div>
<div class="ltx_para ltx_noindent" id="A14.p4">
<p class="ltx_p" id="A14.p4.1">The extensive precedent in the literature, from studies comparing 43 modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Schrimpf etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib49" title="">2021</a>)</cite> to those examining 101 modelsÂ <cite class="ltx_cite ltx_citemacro_citep">(Antonello etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib1" title="">2021</a>)</cite>, demonstrates that this approach is both valid and valuable for understanding the relationship between artificial and biological language processing.</p>
</div>
</section>
<section class="ltx_appendix" id="A15">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix O </span>Limitations</h2>
<div class="ltx_para ltx_noindent" id="A15.p1">
<p class="ltx_p" id="A15.p1.1">The low alignment scores clearly show that despite the increasing popularity of multi-modal models in tackling complex tasks such as visual question answering,
we are still far from developing a model that fully encapsulates the complete information processing steps involved in handling multi-modal naturalistic information in the brain.
In the future, by fine-tuning these multi-modal models on specific tasks such as generating captions for videos, we can better leverage their alignment strengths.
Further, multi-modal large language models (MLLMs) <cite class="ltx_cite ltx_citemacro_citep">(Zhang etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib68" title="">2023</a>; Ataallah etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib4" title="">2024</a>; Wu etÂ al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20027v1#bib.bib65" title="">2024</a>)</cite> that align visual features from video frames into the LLM embedding space via a trainable linear projection layer, offer promise for enhanced multi-modal capabilities. Lastly, although we observe differences between the models (that we experimented with in this work) in terms of architectural variability and variability in pretraining methods, this suggests that future work could benefit from more tightly controlled comparisons to better isolate the effects of these factors. Addressing this limitation would provide deeper insights into how model design and training strategies impact brain alignment, particularly in the context of multi-modal stimuli datasets.</p>
</div>
<div class="ltx_para ltx_noindent" id="A15.p2">
<p class="ltx_p" id="A15.p2.1">It is to be noted that there exist a relatively large number of vision-language or vision-alone or language-alone or speech-alone models as compared to video-audio models that is the primary focus of the current investigation. Therefore the current effort needs to take into consideration the sparsity of video-audio model availability. Despite this limitation, the current paper reports comparison across three video-only, two speech-only, one jointly-pretrained video-audio, and one cross-modal multi-modal model â€“ to the best of our knowledge, this is by far the largest cohort of comparative analysis of multi-modal models and their alignment with brain representations resulting from multi-modal stimuli.</p>
</div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Fri May 23 17:44:10 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

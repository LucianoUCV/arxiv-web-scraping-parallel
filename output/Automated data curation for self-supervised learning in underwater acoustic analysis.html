<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis</title>
<!--Generated on Mon May 26 14:48:00 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<base href="/html/2505.20066v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S1" title="In Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S2" title="In Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S2.SS1" title="In 2 Related Work â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>SSL in underwater acoustics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S2.SS2" title="In 2 Related Work â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Data curation methods</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3" title="In Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Methods</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.SS1" title="In 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>PAM data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.SS2" title="In 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>AIS data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.SS3" title="In 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>AIS data curation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.SS4" title="In 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Audio preprocessing</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.SS5" title="In 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>PAM data curation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.SS6" title="In 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.6 </span>SSL algorithm training and evaluation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S4" title="In Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Results</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S4.SS1" title="In 4 Results â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span>Data curation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S4.SS2" title="In 4 Results â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span>SSL results curated dataset</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S5" title="In Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis</h1>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The sustainability of the ocean ecosystem is threatened
by increased levels of sound pollution, making monitoring crucial to
understand its variability and impact. <span class="ltx_text ltx_font_italic" id="id1.id1.1">Passive acoustic monitoring</span> (PAM) systems collect a large amount of underwater sound recordings, but the large volume of data makes manual analysis impossible, creating the need for automation. Although machine learning offers a potential solution, most underwater acoustic recordings are unlabeled. Self-supervised learning models have demonstrated success in learning from large-scale unlabeled data in various domains like computer vision, Natural Language Processing, and audio. However, these models require large, diverse, and balanced datasets for training in order to generalize well. To address this, a fully automated self-supervised data curation pipeline is proposed to create a diverse and balanced dataset from raw PAM data. It integrates <span class="ltx_text ltx_font_italic" id="id1.id1.2">Automatic Identification System</span> (AIS) data with recordings from various hydrophones in the U.S. waters. Using hierarchical k-means clustering, the raw audio data is sampled and then combined with AIS samples to create a balanced and diverse dataset. The resulting curated dataset enables the development of self-supervised learning models, facilitating various tasks such as monitoring marine mammals and assessing sound pollution.</p>
</div>
<div class="ltx_para" id="p1">
<p class="ltx_p" id="p1.1"><span class="ltx_text ltx_font_bold" id="p1.1.1" style="font-size:90%;">Keywords:<span class="ltx_text ltx_font_medium" id="p1.1.1.1"> <span class="ltx_text ltx_font_italic" id="p1.1.1.1.1">Data curation, Underwater Acoustics, Self-Supervised Learning</span></span></span></p>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1"><span class="ltx_text" id="S1.p1.1.1" style="font-size:90%;">The increasing levels of sound pollution threaten the preservation of ocean ecosystems, necessitating the monitoring of underwater sounds </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib1" title="">1</a><span class="ltx_text" id="S1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.4" style="font-size:90%;">. </span><span class="ltx_text ltx_font_italic" id="S1.p1.1.5" style="font-size:90%;">Passive Acoustic Monitoring</span><span class="ltx_text" id="S1.p1.1.6" style="font-size:90%;"> (PAM) systems are globally deployed and collect a vast amount of diverse underwater sound recordings. The complexity of the marine environment, combined with the large volume of data, makes manual analysis impractical. As a result, annotating such data is time-consuming and expensive, leaving most PAM recordings unlabeled </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.7.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib1" title="">1</a><span class="ltx_text" id="S1.p1.1.8.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.9" style="font-size:90%;">. Although </span><span class="ltx_text ltx_font_italic" id="S1.p1.1.10" style="font-size:90%;">machine learning</span><span class="ltx_text" id="S1.p1.1.11" style="font-size:90%;"> (ML) could potentially facilitate automatic analysis, the performance of supervised methods is limited by the scarce label availability. </span><span class="ltx_text ltx_font_italic" id="S1.p1.1.12" style="font-size:90%;">Self-Supervised Learning</span><span class="ltx_text" id="S1.p1.1.13" style="font-size:90%;"> (SSL) has successfully handled large-scale unlabeled data in domains like computer vision, Natural Language Processing </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib2" title="">2</a><span class="ltx_text" id="S1.p1.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.16" style="font-size:90%;">, and audio </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.17.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib3" title="">3</a><span class="ltx_text" id="S1.p1.1.18.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.19" style="font-size:90%;">. However, the performance of SSL models is reduced when they are optimized on uncurated data </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p1.1.20.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib4" title="">4</a><span class="ltx_text" id="S1.p1.1.21.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p1.1.22" style="font-size:90%;">. This makes balancing of the training data by data curation a key aspect for SSL models to generalize on downstream tasks. Despite the widespread availability of uncurated PAM data, no prior research has focused on curating these data to support the development of SSL-driven models for automatic underwater acoustic analysis. To address this gap, this article presents a fully automated pipeline for the curation of large-scale, freely accessible PAM recordings. This pipeline integrates </span><span class="ltx_text ltx_font_italic" id="S1.p1.1.23" style="font-size:90%;">Automatic Identification System</span><span class="ltx_text" id="S1.p1.1.24" style="font-size:90%;"> (AIS) data with PAM data to balance both the ship distribution and the raw audio distributions. The key contributions of this paper are:</span></p>
<ul class="ltx_itemize" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1"><span class="ltx_text" id="S1.I1.i1.p1.1.1" style="font-size:90%;">A simple AIS curation method based on ship occurrence;</span></p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">â€¢</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1"><span class="ltx_text" id="S1.I1.i2.p1.1.1" style="font-size:90%;">An online hierarchical clustering approach for raw large-scale PAM data curation.</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1"><span class="ltx_text" id="S1.p2.1.1" style="font-size:90%;">The curation method is evaluated by training an SSL algorithm on the curated dataset. The embedding space will be evaluated on a downstream task. The learned embeddings serve as input for the linear regression-based classification, to classify ship types from the reference datasets Deepship </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib5" title="">5</a><span class="ltx_text" id="S1.p2.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.4" style="font-size:90%;"> and ShipsEar </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S1.p2.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib6" title="">6</a><span class="ltx_text" id="S1.p2.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S1.p2.1.7" style="font-size:90%;"> containing underwater recordings of ships labeled by the type of ship.</span></p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">2 </span>Related Work</h2>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>SSL in underwater acoustics</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1"><span class="ltx_text" id="S2.SS1.p1.1.1" style="font-size:90%;">Several previous studies have shown the potential of SSL in the automatic recognition of ship types </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib1" title="">1</a><span class="ltx_text" id="S2.SS1.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.4" style="font-size:90%;">. With no large, curated underwater acoustic dataset publicly available yet, </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib7" title="">7</a><span class="ltx_text" id="S2.SS1.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.7" style="font-size:90%;"> and </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib8" title="">8</a><span class="ltx_text" id="S2.SS1.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.10" style="font-size:90%;"> proposed pretraining on AudioSet applying a mix-up strategy. Next, a Swin Transformer encoder is optimized with masked log-Melspectrogram, while a decoder is optimized to reconstruct the full spectrogram. A high masking rate was needed because of the limited contrast in these spectrograms. The fine-tuning on Deepship </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib5" title="">5</a><span class="ltx_text" id="S2.SS1.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.13" style="font-size:90%;"> yielded an accuracy of 80% - 86%. However, because of the complexity of the marine environment, tasks related to acoustics above the water may not be representative of the marine environment. To address this, </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib9" title="">9</a><span class="ltx_text" id="S2.SS1.p1.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.16" style="font-size:90%;"> proposed a generative transformer model pre-trained solely on underwater acoustic data. To reduce the GPU demand during training, they proposed hierarchical token-convolutions and masked these for reconstruction. Deepship model pre-training and fine-tuning on both Deepship and ShipsEar </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS1.p1.1.17.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib6" title="">6</a><span class="ltx_text" id="S2.SS1.p1.1.18.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS1.p1.1.19" style="font-size:90%;"> improved performance over a supervised transformer baseline. However, their method is limited in real-world applications because of the lack of diversity of ocean environments in their training data. Our study overcomes this issue by representing a data curation pipeline to extract a diverse and balanced unlabeled dataset for SSL algorithms.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Data curation methods</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1"><span class="ltx_text" id="S2.SS2.p1.1.1" style="font-size:90%;">SSL models need a balanced and diverse dataset to compete with supervised methods. The simple scraping of raw data from the web results in an unbalanced dataset, creating the need for data curation </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib4" title="">4</a><span class="ltx_text" id="S2.SS2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.4" style="font-size:90%;">. Previous research proposed an automatic data curation pipeline for data curation from images scraped from the web </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib10" title="">10</a><span class="ltx_text" id="S2.SS2.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.7" style="font-size:90%;">. They suggested converting both a labeled curated dataset and an unlabeled uncurated dataset to the embedding space and performed deduplication by removing near-duplicate images in the uncurated dataset. Finally, they retrieved images from the uncurated dataset by calculating the relative distance to the curated samples. Although they state that this method is self-supervised, it still requires a large labeled dataset for matching. In a follow-up article, </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.8.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib11" title="">11</a><span class="ltx_text" id="S2.SS2.p1.1.9.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.10" style="font-size:90%;">, they proposed a hierarchical KMeans algorithm to retrieve images from an uncurated dataset. Here, they overcome the problem that web-scraped big data is mostly long-tailed. Sampling from traditional KMeans does not encounter this, and therefore, hierarchical KMeans clustering and sampling are proposed. In addition, the DINOv2 algorithm was applied to text-image pairs </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.11.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib4" title="">4</a><span class="ltx_text" id="S2.SS2.p1.1.12.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.13" style="font-size:90%;">. In this paper, they performed data curation. Here, they aligned the images to the captions and sampled from the aligned captions to create the image dataset. The addition of metadata for data curation has also been proposed for audio-visual video representations in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S2.SS2.p1.1.14.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib12" title="">12</a><span class="ltx_text" id="S2.SS2.p1.1.15.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S2.SS2.p1.1.16" style="font-size:90%;">. In this work, they tried to maximize the estimated mutual information between the video imagery and the corresponding audio. Both studies showed that the incorporation of metadata for data curation can improve the quality of the curated data.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">3 </span>Methods</h2>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>PAM data</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1"><span class="ltx_text" id="S3.SS1.p1.1.1" style="font-size:90%;">The raw audio data is collected from NOAA</span><span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://console.cloud.google.com/storage/browser/noaa-passive-bioacoustic;tab=objects?inv=1&amp;invt=AbnmXQ&amp;prefix=&amp;forceOnObjectsSortingFiltering=false</span></span></span></span><span class="ltx_text" id="S3.SS1.p1.1.2" style="font-size:90%;">, selecting hydrophones which started recording in 2023 or later. All these PAM audio recordings are combined into the resulting dataset </span><math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS1.p1.1.m1.1"><semantics id="S3.SS1.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS1.p1.1.m1.1.1" mathsize="90%" xref="S3.SS1.p1.1.m1.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.p1.1.m1.1b"><ci id="S3.SS1.p1.1.m1.1.1.cmml" xref="S3.SS1.p1.1.m1.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.p1.1.m1.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.p1.1.m1.1d">caligraphic_D</annotation></semantics></math><span class="ltx_text" id="S3.SS1.p1.1.3" style="font-size:90%;">. In total, this combination covers the duration of 8 years, 6 months, 9 days, 15 hours, 19 minutes, and 49 seconds from 11 individual hydrophones. These hydrophones are visualized in Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.F1" style="font-size:90%;" title="Figure 1 â€£ 3.1 PAM data â€£ 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S3.SS1.p1.1.4" style="font-size:90%;">.</span></p>
</div>
<figure class="ltx_figure" id="S3.F1">
<p class="ltx_p ltx_align_center" id="S3.F1.1"><span class="ltx_text" id="S3.F1.1.1" style="font-size:90%;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="274" id="S3.F1.1.1.g1" src="x1.png" width="425"/></span></p>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S3.F1.5.1.1">FigureÂ 1</span>: </span>The locations of the hydrophones from the web scraped PAM data, indicated in red pointers.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>AIS data</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.3"><span class="ltx_text" id="S3.SS2.p1.3.1" style="font-size:90%;">Track records of ships are recovered by AIS. This system captures information about ship characteristics, the location, and movements of almost all ships worldwide. The NOAA also provides freely accessible AIS data</span><span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span><span class="ltx_ref ltx_nolink ltx_url ltx_font_typewriter ltx_ref_self">https://marinecadastre.gov/accessais/</span></span></span></span><span class="ltx_text" id="S3.SS2.p1.3.2" style="font-size:90%;"> covering the same regions as raw PAM data. The raw PAM data were aligned with the AIS data by drawing a 4 km </span><math alttext="\times" class="ltx_Math" display="inline" id="S3.SS2.p1.1.m1.1"><semantics id="S3.SS2.p1.1.m1.1a"><mo id="S3.SS2.p1.1.m1.1.1" mathsize="90%" xref="S3.SS2.p1.1.m1.1.1.cmml">Ã—</mo><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.1.m1.1b"><times id="S3.SS2.p1.1.m1.1.1.cmml" xref="S3.SS2.p1.1.m1.1.1"></times></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.1.m1.1c">\times</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.1.m1.1d">Ã—</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p1.3.3" style="font-size:90%;"> 4 km square with the hydrophone in the center (Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.F2" style="font-size:90%;" title="Figure 2 â€£ 3.2 AIS data â€£ 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_tag">2</span></a><span class="ltx_text" id="S3.SS2.p1.3.4" style="font-size:90%;">). This square defines the range of the hydrophone, and therefore, every AIS pulse within this range was considered recorded. The data set containing the AIS pulses will be referred to as </span><math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S3.SS2.p1.2.m2.1"><semantics id="S3.SS2.p1.2.m2.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.2.m2.1.1" mathsize="90%" xref="S3.SS2.p1.2.m2.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.2.m2.1b"><ci id="S3.SS2.p1.2.m2.1.1.cmml" xref="S3.SS2.p1.2.m2.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.2.m2.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.2.m2.1d">caligraphic_A</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p1.3.5" style="font-size:90%;">. All recordings, with AIS information, were gathered to create the dataset </span><math alttext="\mathcal{D}_{s}" class="ltx_Math" display="inline" id="S3.SS2.p1.3.m3.1"><semantics id="S3.SS2.p1.3.m3.1a"><msub id="S3.SS2.p1.3.m3.1.1" xref="S3.SS2.p1.3.m3.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS2.p1.3.m3.1.1.2" mathsize="90%" xref="S3.SS2.p1.3.m3.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS2.p1.3.m3.1.1.3" mathsize="90%" xref="S3.SS2.p1.3.m3.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.p1.3.m3.1b"><apply id="S3.SS2.p1.3.m3.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS2.p1.3.m3.1.1.1.cmml" xref="S3.SS2.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS2.p1.3.m3.1.1.2.cmml" xref="S3.SS2.p1.3.m3.1.1.2">ğ’Ÿ</ci><ci id="S3.SS2.p1.3.m3.1.1.3.cmml" xref="S3.SS2.p1.3.m3.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.p1.3.m3.1c">\mathcal{D}_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.p1.3.m3.1d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS2.p1.3.6" style="font-size:90%;">.</span></p>
</div>
<figure class="ltx_figure" id="S3.F2">
<p class="ltx_p ltx_align_center" id="S3.F2.1"><span class="ltx_text" id="S3.F2.1.1" style="font-size:90%;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="398" id="S3.F2.1.1.g1" src="x2.png" width="425"/></span></p>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S3.F2.5.1.1">FigureÂ 2</span>: </span>The defined range of the selected hydrophones to align raw PAM recordings with the AIS pulses.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>AIS data curation</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.11"><span class="ltx_text" id="S3.SS3.p1.11.1" style="font-size:90%;">The low contrast of radiated ship noise against background noise makes it difficult for simple machine learning methods to encounter ship noise in a large volume of data. To ensure a diverse representation of the acoustic profiles of the ships in the curated dataset, the AIS data </span><math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.1.m1.1.1" mathsize="90%" xref="S3.SS3.p1.1.m1.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">caligraphic_A</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.2" style="font-size:90%;"> is also curated. A simple approach inspired by </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS3.p1.11.3.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib4" title="">4</a><span class="ltx_text" id="S3.SS3.p1.11.4.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS3.p1.11.5" style="font-size:90%;"> is proposed. Here, they proposed to curate the aligned caption to curate an image dataset from a caption-image pair dataset. In this work, the AIS data are aligned with the PAM data, creating a dataset </span><math alttext="\mathcal{D}_{s}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.1"><semantics id="S3.SS3.p1.2.m2.1a"><msub id="S3.SS3.p1.2.m2.1.1" xref="S3.SS3.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.2.m2.1.1.2" mathsize="90%" xref="S3.SS3.p1.2.m2.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS3.p1.2.m2.1.1.3" mathsize="90%" xref="S3.SS3.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.1b"><apply id="S3.SS3.p1.2.m2.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.2">ğ’Ÿ</ci><ci id="S3.SS3.p1.2.m2.1.1.3.cmml" xref="S3.SS3.p1.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.1c">\mathcal{D}_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.1d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.6" style="font-size:90%;"> holding the aligned PAM recordings and a dataset </span><math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S3.SS3.p1.3.m3.1"><semantics id="S3.SS3.p1.3.m3.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.3.m3.1.1" mathsize="90%" xref="S3.SS3.p1.3.m3.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.3.m3.1b"><ci id="S3.SS3.p1.3.m3.1.1.cmml" xref="S3.SS3.p1.3.m3.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.3.m3.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.3.m3.1d">caligraphic_A</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.7" style="font-size:90%;"> holding the aligned AIS pulses. These pulses are then sampled based on the occurrence of individual ships, using a threshold value </span><math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p1.4.m4.1"><semantics id="S3.SS3.p1.4.m4.1a"><mi id="S3.SS3.p1.4.m4.1.1" mathsize="90%" xref="S3.SS3.p1.4.m4.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.4.m4.1b"><ci id="S3.SS3.p1.4.m4.1.1.cmml" xref="S3.SS3.p1.4.m4.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.4.m4.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.4.m4.1d">italic_t</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.8" style="font-size:90%;">. This threshold is defined based on the incidents of individual ships in the aligned PAM dataset </span><math alttext="\mathcal{D}_{s}" class="ltx_Math" display="inline" id="S3.SS3.p1.5.m5.1"><semantics id="S3.SS3.p1.5.m5.1a"><msub id="S3.SS3.p1.5.m5.1.1" xref="S3.SS3.p1.5.m5.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.5.m5.1.1.2" mathsize="90%" xref="S3.SS3.p1.5.m5.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS3.p1.5.m5.1.1.3" mathsize="90%" xref="S3.SS3.p1.5.m5.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.5.m5.1b"><apply id="S3.SS3.p1.5.m5.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.5.m5.1.1.1.cmml" xref="S3.SS3.p1.5.m5.1.1">subscript</csymbol><ci id="S3.SS3.p1.5.m5.1.1.2.cmml" xref="S3.SS3.p1.5.m5.1.1.2">ğ’Ÿ</ci><ci id="S3.SS3.p1.5.m5.1.1.3.cmml" xref="S3.SS3.p1.5.m5.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.5.m5.1c">\mathcal{D}_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.5.m5.1d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.9" style="font-size:90%;">. Ships with a lower incidence than </span><math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p1.6.m6.1"><semantics id="S3.SS3.p1.6.m6.1a"><mi id="S3.SS3.p1.6.m6.1.1" mathsize="90%" xref="S3.SS3.p1.6.m6.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.6.m6.1b"><ci id="S3.SS3.p1.6.m6.1.1.cmml" xref="S3.SS3.p1.6.m6.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.6.m6.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.6.m6.1d">italic_t</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.10" style="font-size:90%;"> will not be sampled and kept complete in the curated dataset, while ships that exceed </span><math alttext="t" class="ltx_Math" display="inline" id="S3.SS3.p1.7.m7.1"><semantics id="S3.SS3.p1.7.m7.1a"><mi id="S3.SS3.p1.7.m7.1.1" mathsize="90%" xref="S3.SS3.p1.7.m7.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.7.m7.1b"><ci id="S3.SS3.p1.7.m7.1.1.cmml" xref="S3.SS3.p1.7.m7.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.7.m7.1c">t</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.7.m7.1d">italic_t</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.11" style="font-size:90%;"> are sampled according to their sample probability. This probability is inversely proportional to the occurrence of the ship in </span><math alttext="\mathcal{D}_{s}" class="ltx_Math" display="inline" id="S3.SS3.p1.8.m8.1"><semantics id="S3.SS3.p1.8.m8.1a"><msub id="S3.SS3.p1.8.m8.1.1" xref="S3.SS3.p1.8.m8.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.8.m8.1.1.2" mathsize="90%" xref="S3.SS3.p1.8.m8.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS3.p1.8.m8.1.1.3" mathsize="90%" xref="S3.SS3.p1.8.m8.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.8.m8.1b"><apply id="S3.SS3.p1.8.m8.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.8.m8.1.1.1.cmml" xref="S3.SS3.p1.8.m8.1.1">subscript</csymbol><ci id="S3.SS3.p1.8.m8.1.1.2.cmml" xref="S3.SS3.p1.8.m8.1.1.2">ğ’Ÿ</ci><ci id="S3.SS3.p1.8.m8.1.1.3.cmml" xref="S3.SS3.p1.8.m8.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.8.m8.1c">\mathcal{D}_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.8.m8.1d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.12" style="font-size:90%;">, the higher the occurrence, the lower the sampling probability and vice versa. Here, the objective is to balance the long-tailed distribution of </span><math alttext="\mathcal{D}_{s}" class="ltx_Math" display="inline" id="S3.SS3.p1.9.m9.1"><semantics id="S3.SS3.p1.9.m9.1a"><msub id="S3.SS3.p1.9.m9.1.1" xref="S3.SS3.p1.9.m9.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.9.m9.1.1.2" mathsize="90%" xref="S3.SS3.p1.9.m9.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.SS3.p1.9.m9.1.1.3" mathsize="90%" xref="S3.SS3.p1.9.m9.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.9.m9.1b"><apply id="S3.SS3.p1.9.m9.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.9.m9.1.1.1.cmml" xref="S3.SS3.p1.9.m9.1.1">subscript</csymbol><ci id="S3.SS3.p1.9.m9.1.1.2.cmml" xref="S3.SS3.p1.9.m9.1.1.2">ğ’Ÿ</ci><ci id="S3.SS3.p1.9.m9.1.1.3.cmml" xref="S3.SS3.p1.9.m9.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.9.m9.1c">\mathcal{D}_{s}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.9.m9.1d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.13" style="font-size:90%;"> given </span><math alttext="\mathcal{A}" class="ltx_Math" display="inline" id="S3.SS3.p1.10.m10.1"><semantics id="S3.SS3.p1.10.m10.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.10.m10.1.1" mathsize="90%" xref="S3.SS3.p1.10.m10.1.1.cmml">ğ’œ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.10.m10.1b"><ci id="S3.SS3.p1.10.m10.1.1.cmml" xref="S3.SS3.p1.10.m10.1.1">ğ’œ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.10.m10.1c">\mathcal{A}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.10.m10.1d">caligraphic_A</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.14" style="font-size:90%;">, resulting in a more balanced dataset </span><math alttext="\mathcal{D}_{s}^{*}" class="ltx_Math" display="inline" id="S3.SS3.p1.11.m11.1"><semantics id="S3.SS3.p1.11.m11.1a"><msubsup id="S3.SS3.p1.11.m11.1.1" xref="S3.SS3.p1.11.m11.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.SS3.p1.11.m11.1.1.2.2" mathsize="90%" xref="S3.SS3.p1.11.m11.1.1.2.2.cmml">ğ’Ÿ</mi><mi id="S3.SS3.p1.11.m11.1.1.2.3" mathsize="90%" xref="S3.SS3.p1.11.m11.1.1.2.3.cmml">s</mi><mo id="S3.SS3.p1.11.m11.1.1.3" mathsize="90%" xref="S3.SS3.p1.11.m11.1.1.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.11.m11.1b"><apply id="S3.SS3.p1.11.m11.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m11.1.1.1.cmml" xref="S3.SS3.p1.11.m11.1.1">superscript</csymbol><apply id="S3.SS3.p1.11.m11.1.1.2.cmml" xref="S3.SS3.p1.11.m11.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.11.m11.1.1.2.1.cmml" xref="S3.SS3.p1.11.m11.1.1">subscript</csymbol><ci id="S3.SS3.p1.11.m11.1.1.2.2.cmml" xref="S3.SS3.p1.11.m11.1.1.2.2">ğ’Ÿ</ci><ci id="S3.SS3.p1.11.m11.1.1.2.3.cmml" xref="S3.SS3.p1.11.m11.1.1.2.3">ğ‘ </ci></apply><times id="S3.SS3.p1.11.m11.1.1.3.cmml" xref="S3.SS3.p1.11.m11.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.11.m11.1c">\mathcal{D}_{s}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.11.m11.1d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S3.SS3.p1.11.15" style="font-size:90%;">:</span></p>
</div>
<div class="ltx_para" id="S3.SS3.p2">
<table class="ltx_equation ltx_eqn_table" id="S3.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}_{s}^{*}\leftarrow f(\mathcal{D}_{s};\mathcal{A},t)" class="ltx_Math" display="block" id="S3.E1.m1.3"><semantics id="S3.E1.m1.3a"><mrow id="S3.E1.m1.3.3" xref="S3.E1.m1.3.3.cmml"><msubsup id="S3.E1.m1.3.3.3" xref="S3.E1.m1.3.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.3.2.2" mathsize="90%" xref="S3.E1.m1.3.3.3.2.2.cmml">ğ’Ÿ</mi><mi id="S3.E1.m1.3.3.3.2.3" mathsize="90%" xref="S3.E1.m1.3.3.3.2.3.cmml">s</mi><mo id="S3.E1.m1.3.3.3.3" mathsize="90%" xref="S3.E1.m1.3.3.3.3.cmml">âˆ—</mo></msubsup><mo id="S3.E1.m1.3.3.2" mathsize="90%" stretchy="false" xref="S3.E1.m1.3.3.2.cmml">â†</mo><mrow id="S3.E1.m1.3.3.1" xref="S3.E1.m1.3.3.1.cmml"><mi id="S3.E1.m1.3.3.1.3" mathsize="90%" xref="S3.E1.m1.3.3.1.3.cmml">f</mi><mo id="S3.E1.m1.3.3.1.2" xref="S3.E1.m1.3.3.1.2.cmml">â¢</mo><mrow id="S3.E1.m1.3.3.1.1.1" xref="S3.E1.m1.3.3.1.1.2.cmml"><mo id="S3.E1.m1.3.3.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E1.m1.3.3.1.1.2.cmml">(</mo><msub id="S3.E1.m1.3.3.1.1.1.1" xref="S3.E1.m1.3.3.1.1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.3.3.1.1.1.1.2" mathsize="90%" xref="S3.E1.m1.3.3.1.1.1.1.2.cmml">ğ’Ÿ</mi><mi id="S3.E1.m1.3.3.1.1.1.1.3" mathsize="90%" xref="S3.E1.m1.3.3.1.1.1.1.3.cmml">s</mi></msub><mo id="S3.E1.m1.3.3.1.1.1.3" mathsize="90%" xref="S3.E1.m1.3.3.1.1.2.cmml">;</mo><mi class="ltx_font_mathcaligraphic" id="S3.E1.m1.1.1" mathsize="90%" xref="S3.E1.m1.1.1.cmml">ğ’œ</mi><mo id="S3.E1.m1.3.3.1.1.1.4" mathsize="90%" xref="S3.E1.m1.3.3.1.1.2.cmml">,</mo><mi id="S3.E1.m1.2.2" mathsize="90%" xref="S3.E1.m1.2.2.cmml">t</mi><mo id="S3.E1.m1.3.3.1.1.1.5" maxsize="90%" minsize="90%" xref="S3.E1.m1.3.3.1.1.2.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E1.m1.3b"><apply id="S3.E1.m1.3.3.cmml" xref="S3.E1.m1.3.3"><ci id="S3.E1.m1.3.3.2.cmml" xref="S3.E1.m1.3.3.2">â†</ci><apply id="S3.E1.m1.3.3.3.cmml" xref="S3.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.1.cmml" xref="S3.E1.m1.3.3.3">superscript</csymbol><apply id="S3.E1.m1.3.3.3.2.cmml" xref="S3.E1.m1.3.3.3"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.3.2.1.cmml" xref="S3.E1.m1.3.3.3">subscript</csymbol><ci id="S3.E1.m1.3.3.3.2.2.cmml" xref="S3.E1.m1.3.3.3.2.2">ğ’Ÿ</ci><ci id="S3.E1.m1.3.3.3.2.3.cmml" xref="S3.E1.m1.3.3.3.2.3">ğ‘ </ci></apply><times id="S3.E1.m1.3.3.3.3.cmml" xref="S3.E1.m1.3.3.3.3"></times></apply><apply id="S3.E1.m1.3.3.1.cmml" xref="S3.E1.m1.3.3.1"><times id="S3.E1.m1.3.3.1.2.cmml" xref="S3.E1.m1.3.3.1.2"></times><ci id="S3.E1.m1.3.3.1.3.cmml" xref="S3.E1.m1.3.3.1.3">ğ‘“</ci><list id="S3.E1.m1.3.3.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1"><apply id="S3.E1.m1.3.3.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S3.E1.m1.3.3.1.1.1.1.1.cmml" xref="S3.E1.m1.3.3.1.1.1.1">subscript</csymbol><ci id="S3.E1.m1.3.3.1.1.1.1.2.cmml" xref="S3.E1.m1.3.3.1.1.1.1.2">ğ’Ÿ</ci><ci id="S3.E1.m1.3.3.1.1.1.1.3.cmml" xref="S3.E1.m1.3.3.1.1.1.1.3">ğ‘ </ci></apply><ci id="S3.E1.m1.1.1.cmml" xref="S3.E1.m1.1.1">ğ’œ</ci><ci id="S3.E1.m1.2.2.cmml" xref="S3.E1.m1.2.2">ğ‘¡</ci></list></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E1.m1.3c">\mathcal{D}_{s}^{*}\leftarrow f(\mathcal{D}_{s};\mathcal{A},t)</annotation><annotation encoding="application/x-llamapun" id="S3.E1.m1.3d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT â† italic_f ( caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT ; caligraphic_A , italic_t )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Audio preprocessing</h3>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.1"><span class="ltx_text" id="S3.SS4.p1.1.1" style="font-size:90%;">First, the raw audio was resampled to a sample rate of 16 kHz to ensure a common sample rate for all recordings. Next, the audio was windowed using a 10-second window size without overlap. From this windowed audio, embeddings were generated using the model presented in </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS4.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib13" title="">13</a><span class="ltx_text" id="S3.SS4.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS4.p1.1.4" style="font-size:90%;">. In this research, they state that this model is optimized on a large quantity of unlabeled underwater acoustic data and generates generalized embeddings with dimension 2048.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>PAM data curation</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.9"><span class="ltx_text" id="S3.SS5.p1.9.1" style="font-size:90%;">The objective of the PAM data curation method is to retrieve a dataset with the target distribution </span><math alttext="U" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><mi id="S3.SS5.p1.1.m1.1.1" mathsize="90%" xref="S3.SS5.p1.1.m1.1.1.cmml">U</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><ci id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">ğ‘ˆ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">U</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_U</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.2" style="font-size:90%;">. This distribution is defined as a uniform distribution with the support of the distribution </span><math alttext="P" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><mi id="S3.SS5.p1.2.m2.1.1" mathsize="90%" xref="S3.SS5.p1.2.m2.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><ci id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_P</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.3" style="font-size:90%;">, where </span><math alttext="P" class="ltx_Math" display="inline" id="S3.SS5.p1.3.m3.1"><semantics id="S3.SS5.p1.3.m3.1a"><mi id="S3.SS5.p1.3.m3.1.1" mathsize="90%" xref="S3.SS5.p1.3.m3.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><ci id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">ğ‘ƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">P</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.3.m3.1d">italic_P</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.4" style="font-size:90%;"> represents the distribution of dataset </span><math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS5.p1.4.m4.1"><semantics id="S3.SS5.p1.4.m4.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p1.4.m4.1.1" mathsize="90%" xref="S3.SS5.p1.4.m4.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.4.m4.1b"><ci id="S3.SS5.p1.4.m4.1.1.cmml" xref="S3.SS5.p1.4.m4.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.4.m4.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.4.m4.1d">caligraphic_D</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.5" style="font-size:90%;">. The data curation method is inspired by the method proposed by </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS5.p1.9.6.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib11" title="">11</a><span class="ltx_text" id="S3.SS5.p1.9.7.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS5.p1.9.8" style="font-size:90%;">. The entire dataset </span><math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS5.p1.5.m5.1"><semantics id="S3.SS5.p1.5.m5.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p1.5.m5.1.1" mathsize="90%" xref="S3.SS5.p1.5.m5.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.5.m5.1b"><ci id="S3.SS5.p1.5.m5.1.1.cmml" xref="S3.SS5.p1.5.m5.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.5.m5.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.5.m5.1d">caligraphic_D</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.9" style="font-size:90%;"> is employed to optimize a hierarchical KMeans model using the resampling-clustering technique. This model consists of four levels with cluster sizes of </span><math alttext="[6000,400,40,10]" class="ltx_Math" display="inline" id="S3.SS5.p1.6.m6.4"><semantics id="S3.SS5.p1.6.m6.4a"><mrow id="S3.SS5.p1.6.m6.4.5.2" xref="S3.SS5.p1.6.m6.4.5.1.cmml"><mo id="S3.SS5.p1.6.m6.4.5.2.1" maxsize="90%" minsize="90%" xref="S3.SS5.p1.6.m6.4.5.1.cmml">[</mo><mn id="S3.SS5.p1.6.m6.1.1" mathsize="90%" xref="S3.SS5.p1.6.m6.1.1.cmml">6000</mn><mo id="S3.SS5.p1.6.m6.4.5.2.2" mathsize="90%" xref="S3.SS5.p1.6.m6.4.5.1.cmml">,</mo><mn id="S3.SS5.p1.6.m6.2.2" mathsize="90%" xref="S3.SS5.p1.6.m6.2.2.cmml">400</mn><mo id="S3.SS5.p1.6.m6.4.5.2.3" mathsize="90%" xref="S3.SS5.p1.6.m6.4.5.1.cmml">,</mo><mn id="S3.SS5.p1.6.m6.3.3" mathsize="90%" xref="S3.SS5.p1.6.m6.3.3.cmml">40</mn><mo id="S3.SS5.p1.6.m6.4.5.2.4" mathsize="90%" xref="S3.SS5.p1.6.m6.4.5.1.cmml">,</mo><mn id="S3.SS5.p1.6.m6.4.4" mathsize="90%" xref="S3.SS5.p1.6.m6.4.4.cmml">10</mn><mo id="S3.SS5.p1.6.m6.4.5.2.5" maxsize="90%" minsize="90%" xref="S3.SS5.p1.6.m6.4.5.1.cmml">]</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.6.m6.4b"><list id="S3.SS5.p1.6.m6.4.5.1.cmml" xref="S3.SS5.p1.6.m6.4.5.2"><cn id="S3.SS5.p1.6.m6.1.1.cmml" type="integer" xref="S3.SS5.p1.6.m6.1.1">6000</cn><cn id="S3.SS5.p1.6.m6.2.2.cmml" type="integer" xref="S3.SS5.p1.6.m6.2.2">400</cn><cn id="S3.SS5.p1.6.m6.3.3.cmml" type="integer" xref="S3.SS5.p1.6.m6.3.3">40</cn><cn id="S3.SS5.p1.6.m6.4.4.cmml" type="integer" xref="S3.SS5.p1.6.m6.4.4">10</cn></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.6.m6.4c">[6000,400,40,10]</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.6.m6.4d">[ 6000 , 400 , 40 , 10 ]</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.10" style="font-size:90%;">. Here, the lower cluster levels focus on detailed information, while the upper levels capture global features. Audio samples of 10 seconds are then drawn from each hierarchical layer. Given the large size of the dataset </span><math alttext="\mathcal{D}" class="ltx_Math" display="inline" id="S3.SS5.p1.7.m7.1"><semantics id="S3.SS5.p1.7.m7.1a"><mi class="ltx_font_mathcaligraphic" id="S3.SS5.p1.7.m7.1.1" mathsize="90%" xref="S3.SS5.p1.7.m7.1.1.cmml">ğ’Ÿ</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.7.m7.1b"><ci id="S3.SS5.p1.7.m7.1.1.cmml" xref="S3.SS5.p1.7.m7.1.1">ğ’Ÿ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.7.m7.1c">\mathcal{D}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.7.m7.1d">caligraphic_D</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.11" style="font-size:90%;">, the hierarchical KMeans is optimized in a streaming manner, which approximates the optimal solution </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS5.p1.9.12.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib14" title="">14</a><span class="ltx_text" id="S3.SS5.p1.9.13.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS5.p1.9.14" style="font-size:90%;">. After training, the optimal HKmeans is utilized to sample the data from each cluster level. A maximum of target size </span><math alttext="N" class="ltx_Math" display="inline" id="S3.SS5.p1.8.m8.1"><semantics id="S3.SS5.p1.8.m8.1a"><mi id="S3.SS5.p1.8.m8.1.1" mathsize="90%" xref="S3.SS5.p1.8.m8.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.8.m8.1b"><ci id="S3.SS5.p1.8.m8.1.1.cmml" xref="S3.SS5.p1.8.m8.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.8.m8.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.8.m8.1d">italic_N</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.15" style="font-size:90%;"> samples is selected from the first datastream based on the relative distance of the datapoints to the assigned cluster centers. As additional datastreams are processed, once the target number of </span><math alttext="N" class="ltx_Math" display="inline" id="S3.SS5.p1.9.m9.1"><semantics id="S3.SS5.p1.9.m9.1a"><mi id="S3.SS5.p1.9.m9.1.1" mathsize="90%" xref="S3.SS5.p1.9.m9.1.1.cmml">N</mi><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.9.m9.1b"><ci id="S3.SS5.p1.9.m9.1.1.cmml" xref="S3.SS5.p1.9.m9.1.1">ğ‘</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.9.m9.1c">N</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.9.m9.1d">italic_N</annotation></semantics></math><span class="ltx_text" id="S3.SS5.p1.9.16" style="font-size:90%;"> samples is exceeded, the samples with the greatest distance to the cluster center are replaced by those closer to the center.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS6">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">3.6 </span>SSL algorithm training and evaluation</h3>
<div class="ltx_para" id="S3.SS6.p1">
<p class="ltx_p" id="S3.SS6.p1.6"><span class="ltx_text" id="S3.SS6.p1.6.1" style="font-size:90%;">The proposed method is evaluated by training the Data2vec base framework</span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S3.SS6.p1.6.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib15" title="">15</a><span class="ltx_text" id="S3.SS6.p1.6.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S3.SS6.p1.6.4" style="font-size:90%;">. The base model pretrained on speech is completely fine-tuned using the curated dataset. This dataset is defined as:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\mathcal{D}^{*}=\mathcal{D}_{a}+\mathcal{D}_{s}^{*}" class="ltx_Math" display="block" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msup id="S3.E2.m1.1.1.2" xref="S3.E2.m1.1.1.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.2.2" mathsize="90%" xref="S3.E2.m1.1.1.2.2.cmml">ğ’Ÿ</mi><mo id="S3.E2.m1.1.1.2.3" mathsize="90%" xref="S3.E2.m1.1.1.2.3.cmml">âˆ—</mo></msup><mo id="S3.E2.m1.1.1.1" mathsize="90%" xref="S3.E2.m1.1.1.1.cmml">=</mo><mrow id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><msub id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.3.2.2" mathsize="90%" xref="S3.E2.m1.1.1.3.2.2.cmml">ğ’Ÿ</mi><mi id="S3.E2.m1.1.1.3.2.3" mathsize="90%" xref="S3.E2.m1.1.1.3.2.3.cmml">a</mi></msub><mo id="S3.E2.m1.1.1.3.1" mathsize="90%" xref="S3.E2.m1.1.1.3.1.cmml">+</mo><msubsup id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml"><mi class="ltx_font_mathcaligraphic" id="S3.E2.m1.1.1.3.3.2.2" mathsize="90%" xref="S3.E2.m1.1.1.3.3.2.2.cmml">ğ’Ÿ</mi><mi id="S3.E2.m1.1.1.3.3.2.3" mathsize="90%" xref="S3.E2.m1.1.1.3.3.2.3.cmml">s</mi><mo id="S3.E2.m1.1.1.3.3.3" mathsize="90%" xref="S3.E2.m1.1.1.3.3.3.cmml">âˆ—</mo></msubsup></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><eq id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"></eq><apply id="S3.E2.m1.1.1.2.cmml" xref="S3.E2.m1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.2">superscript</csymbol><ci id="S3.E2.m1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.2.2">ğ’Ÿ</ci><times id="S3.E2.m1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.2.3"></times></apply><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><plus id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3.1"></plus><apply id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.2.1.cmml" xref="S3.E2.m1.1.1.3.2">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.2.cmml" xref="S3.E2.m1.1.1.3.2.2">ğ’Ÿ</ci><ci id="S3.E2.m1.1.1.3.2.3.cmml" xref="S3.E2.m1.1.1.3.2.3">ğ‘</ci></apply><apply id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.1.cmml" xref="S3.E2.m1.1.1.3.3">superscript</csymbol><apply id="S3.E2.m1.1.1.3.3.2.cmml" xref="S3.E2.m1.1.1.3.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.3.2.1.cmml" xref="S3.E2.m1.1.1.3.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.3.2.2.cmml" xref="S3.E2.m1.1.1.3.3.2.2">ğ’Ÿ</ci><ci id="S3.E2.m1.1.1.3.3.2.3.cmml" xref="S3.E2.m1.1.1.3.3.2.3">ğ‘ </ci></apply><times id="S3.E2.m1.1.1.3.3.3.cmml" xref="S3.E2.m1.1.1.3.3.3"></times></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\mathcal{D}^{*}=\mathcal{D}_{a}+\mathcal{D}_{s}^{*}</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">caligraphic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT = caligraphic_D start_POSTSUBSCRIPT italic_a end_POSTSUBSCRIPT + caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS6.p1.2"><span class="ltx_text" id="S3.SS6.p1.2.1" style="font-size:90%;">The model is optimized using a batch size of 64, where each input sample for the student model is masked for 15% while the teacher model receives the full audio. The teacher model weights </span><math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS6.p1.1.m1.1"><semantics id="S3.SS6.p1.1.m1.1a"><mi id="S3.SS6.p1.1.m1.1.1" mathsize="90%" mathvariant="normal" xref="S3.SS6.p1.1.m1.1.1.cmml">Î”</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.1.m1.1b"><ci id="S3.SS6.p1.1.m1.1.1.cmml" xref="S3.SS6.p1.1.m1.1.1">Î”</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.1.m1.1d">roman_Î”</annotation></semantics></math><span class="ltx_text" id="S3.SS6.p1.2.2" style="font-size:90%;"> keep track of the student model weights </span><math alttext="\theta" class="ltx_Math" display="inline" id="S3.SS6.p1.2.m2.1"><semantics id="S3.SS6.p1.2.m2.1a"><mi id="S3.SS6.p1.2.m2.1.1" mathsize="90%" xref="S3.SS6.p1.2.m2.1.1.cmml">Î¸</mi><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.2.m2.1b"><ci id="S3.SS6.p1.2.m2.1.1.cmml" xref="S3.SS6.p1.2.m2.1.1">ğœƒ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.2.m2.1c">\theta</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.2.m2.1d">italic_Î¸</annotation></semantics></math><span class="ltx_text" id="S3.SS6.p1.2.3" style="font-size:90%;"> by:</span></p>
<table class="ltx_equation ltx_eqn_table" id="S3.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="\Delta=\tau\Delta+(1-\tau)\theta" class="ltx_Math" display="block" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><mi id="S3.E3.m1.1.1.3" mathsize="90%" mathvariant="normal" xref="S3.E3.m1.1.1.3.cmml">Î”</mi><mo id="S3.E3.m1.1.1.2" mathsize="90%" xref="S3.E3.m1.1.1.2.cmml">=</mo><mrow id="S3.E3.m1.1.1.1" xref="S3.E3.m1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.3" xref="S3.E3.m1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.3.2" mathsize="90%" xref="S3.E3.m1.1.1.1.3.2.cmml">Ï„</mi><mo id="S3.E3.m1.1.1.1.3.1" xref="S3.E3.m1.1.1.1.3.1.cmml">â¢</mo><mi id="S3.E3.m1.1.1.1.3.3" mathsize="90%" mathvariant="normal" xref="S3.E3.m1.1.1.1.3.3.cmml">Î”</mi></mrow><mo id="S3.E3.m1.1.1.1.2" mathsize="90%" xref="S3.E3.m1.1.1.1.2.cmml">+</mo><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.cmml"><mrow id="S3.E3.m1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml"><mn id="S3.E3.m1.1.1.1.1.1.1.1.2" mathsize="90%" xref="S3.E3.m1.1.1.1.1.1.1.1.2.cmml">1</mn><mo id="S3.E3.m1.1.1.1.1.1.1.1.1" mathsize="90%" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">âˆ’</mo><mi id="S3.E3.m1.1.1.1.1.1.1.1.3" mathsize="90%" xref="S3.E3.m1.1.1.1.1.1.1.1.3.cmml">Ï„</mi></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.E3.m1.1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.2.cmml">â¢</mo><mi id="S3.E3.m1.1.1.1.1.3" mathsize="90%" xref="S3.E3.m1.1.1.1.1.3.cmml">Î¸</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><eq id="S3.E3.m1.1.1.2.cmml" xref="S3.E3.m1.1.1.2"></eq><ci id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3">Î”</ci><apply id="S3.E3.m1.1.1.1.cmml" xref="S3.E3.m1.1.1.1"><plus id="S3.E3.m1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.2"></plus><apply id="S3.E3.m1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.3"><times id="S3.E3.m1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.3.1"></times><ci id="S3.E3.m1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.3.2">ğœ</ci><ci id="S3.E3.m1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.3.3">Î”</ci></apply><apply id="S3.E3.m1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><times id="S3.E3.m1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.2"></times><apply id="S3.E3.m1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1"><minus id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1"></minus><cn id="S3.E3.m1.1.1.1.1.1.1.1.2.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.1.1.2">1</cn><ci id="S3.E3.m1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.3">ğœ</ci></apply><ci id="S3.E3.m1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.3">ğœƒ</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\Delta=\tau\Delta+(1-\tau)\theta</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">roman_Î” = italic_Ï„ roman_Î” + ( 1 - italic_Ï„ ) italic_Î¸</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S3.SS6.p1.5"><span class="ltx_text" id="S3.SS6.p1.5.1" style="font-size:90%;">The parameter </span><math alttext="\tau~{}(\tau_{0}\leq\tau\leq\tau_{e})" class="ltx_Math" display="inline" id="S3.SS6.p1.3.m1.1"><semantics id="S3.SS6.p1.3.m1.1a"><mrow id="S3.SS6.p1.3.m1.1.1" xref="S3.SS6.p1.3.m1.1.1.cmml"><mi id="S3.SS6.p1.3.m1.1.1.3" mathsize="90%" xref="S3.SS6.p1.3.m1.1.1.3.cmml">Ï„</mi><mo id="S3.SS6.p1.3.m1.1.1.2" lspace="0.300em" xref="S3.SS6.p1.3.m1.1.1.2.cmml">â¢</mo><mrow id="S3.SS6.p1.3.m1.1.1.1.1" xref="S3.SS6.p1.3.m1.1.1.1.1.1.cmml"><mo id="S3.SS6.p1.3.m1.1.1.1.1.2" maxsize="90%" minsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.SS6.p1.3.m1.1.1.1.1.1" xref="S3.SS6.p1.3.m1.1.1.1.1.1.cmml"><msub id="S3.SS6.p1.3.m1.1.1.1.1.1.2" xref="S3.SS6.p1.3.m1.1.1.1.1.1.2.cmml"><mi id="S3.SS6.p1.3.m1.1.1.1.1.1.2.2" mathsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.2.2.cmml">Ï„</mi><mn id="S3.SS6.p1.3.m1.1.1.1.1.1.2.3" mathsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS6.p1.3.m1.1.1.1.1.1.3" mathsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.3.cmml">â‰¤</mo><mi id="S3.SS6.p1.3.m1.1.1.1.1.1.4" mathsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.4.cmml">Ï„</mi><mo id="S3.SS6.p1.3.m1.1.1.1.1.1.5" mathsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.5.cmml">â‰¤</mo><msub id="S3.SS6.p1.3.m1.1.1.1.1.1.6" xref="S3.SS6.p1.3.m1.1.1.1.1.1.6.cmml"><mi id="S3.SS6.p1.3.m1.1.1.1.1.1.6.2" mathsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.6.2.cmml">Ï„</mi><mi id="S3.SS6.p1.3.m1.1.1.1.1.1.6.3" mathsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.6.3.cmml">e</mi></msub></mrow><mo id="S3.SS6.p1.3.m1.1.1.1.1.3" maxsize="90%" minsize="90%" xref="S3.SS6.p1.3.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.3.m1.1b"><apply id="S3.SS6.p1.3.m1.1.1.cmml" xref="S3.SS6.p1.3.m1.1.1"><times id="S3.SS6.p1.3.m1.1.1.2.cmml" xref="S3.SS6.p1.3.m1.1.1.2"></times><ci id="S3.SS6.p1.3.m1.1.1.3.cmml" xref="S3.SS6.p1.3.m1.1.1.3">ğœ</ci><apply id="S3.SS6.p1.3.m1.1.1.1.1.1.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1"><and id="S3.SS6.p1.3.m1.1.1.1.1.1a.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1"></and><apply id="S3.SS6.p1.3.m1.1.1.1.1.1b.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1"><leq id="S3.SS6.p1.3.m1.1.1.1.1.1.3.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.3"></leq><apply id="S3.SS6.p1.3.m1.1.1.1.1.1.2.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m1.1.1.1.1.1.2.1.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.3.m1.1.1.1.1.1.2.2.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.2.2">ğœ</ci><cn id="S3.SS6.p1.3.m1.1.1.1.1.1.2.3.cmml" type="integer" xref="S3.SS6.p1.3.m1.1.1.1.1.1.2.3">0</cn></apply><ci id="S3.SS6.p1.3.m1.1.1.1.1.1.4.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.4">ğœ</ci></apply><apply id="S3.SS6.p1.3.m1.1.1.1.1.1c.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1"><leq id="S3.SS6.p1.3.m1.1.1.1.1.1.5.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.5"></leq><share href="https://arxiv.org/html/2505.20066v1#S3.SS6.p1.3.m1.1.1.1.1.1.4.cmml" id="S3.SS6.p1.3.m1.1.1.1.1.1d.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1"></share><apply id="S3.SS6.p1.3.m1.1.1.1.1.1.6.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.6"><csymbol cd="ambiguous" id="S3.SS6.p1.3.m1.1.1.1.1.1.6.1.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.6">subscript</csymbol><ci id="S3.SS6.p1.3.m1.1.1.1.1.1.6.2.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.6.2">ğœ</ci><ci id="S3.SS6.p1.3.m1.1.1.1.1.1.6.3.cmml" xref="S3.SS6.p1.3.m1.1.1.1.1.1.6.3">ğ‘’</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.3.m1.1c">\tau~{}(\tau_{0}\leq\tau\leq\tau_{e})</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.3.m1.1d">italic_Ï„ ( italic_Ï„ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT â‰¤ italic_Ï„ â‰¤ italic_Ï„ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT )</annotation></semantics></math><span class="ltx_text" id="S3.SS6.p1.5.2" style="font-size:90%;"> is gradually increased from </span><math alttext="\tau_{0}=0.999" class="ltx_Math" display="inline" id="S3.SS6.p1.4.m2.1"><semantics id="S3.SS6.p1.4.m2.1a"><mrow id="S3.SS6.p1.4.m2.1.1" xref="S3.SS6.p1.4.m2.1.1.cmml"><msub id="S3.SS6.p1.4.m2.1.1.2" xref="S3.SS6.p1.4.m2.1.1.2.cmml"><mi id="S3.SS6.p1.4.m2.1.1.2.2" mathsize="90%" xref="S3.SS6.p1.4.m2.1.1.2.2.cmml">Ï„</mi><mn id="S3.SS6.p1.4.m2.1.1.2.3" mathsize="90%" xref="S3.SS6.p1.4.m2.1.1.2.3.cmml">0</mn></msub><mo id="S3.SS6.p1.4.m2.1.1.1" mathsize="90%" xref="S3.SS6.p1.4.m2.1.1.1.cmml">=</mo><mn id="S3.SS6.p1.4.m2.1.1.3" mathsize="90%" xref="S3.SS6.p1.4.m2.1.1.3.cmml">0.999</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.4.m2.1b"><apply id="S3.SS6.p1.4.m2.1.1.cmml" xref="S3.SS6.p1.4.m2.1.1"><eq id="S3.SS6.p1.4.m2.1.1.1.cmml" xref="S3.SS6.p1.4.m2.1.1.1"></eq><apply id="S3.SS6.p1.4.m2.1.1.2.cmml" xref="S3.SS6.p1.4.m2.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.4.m2.1.1.2.1.cmml" xref="S3.SS6.p1.4.m2.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.4.m2.1.1.2.2.cmml" xref="S3.SS6.p1.4.m2.1.1.2.2">ğœ</ci><cn id="S3.SS6.p1.4.m2.1.1.2.3.cmml" type="integer" xref="S3.SS6.p1.4.m2.1.1.2.3">0</cn></apply><cn id="S3.SS6.p1.4.m2.1.1.3.cmml" type="float" xref="S3.SS6.p1.4.m2.1.1.3">0.999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.4.m2.1c">\tau_{0}=0.999</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.4.m2.1d">italic_Ï„ start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT = 0.999</annotation></semantics></math><span class="ltx_text" id="S3.SS6.p1.5.3" style="font-size:90%;"> to </span><math alttext="\tau_{e}=0.9999" class="ltx_Math" display="inline" id="S3.SS6.p1.5.m3.1"><semantics id="S3.SS6.p1.5.m3.1a"><mrow id="S3.SS6.p1.5.m3.1.1" xref="S3.SS6.p1.5.m3.1.1.cmml"><msub id="S3.SS6.p1.5.m3.1.1.2" xref="S3.SS6.p1.5.m3.1.1.2.cmml"><mi id="S3.SS6.p1.5.m3.1.1.2.2" mathsize="90%" xref="S3.SS6.p1.5.m3.1.1.2.2.cmml">Ï„</mi><mi id="S3.SS6.p1.5.m3.1.1.2.3" mathsize="90%" xref="S3.SS6.p1.5.m3.1.1.2.3.cmml">e</mi></msub><mo id="S3.SS6.p1.5.m3.1.1.1" mathsize="90%" xref="S3.SS6.p1.5.m3.1.1.1.cmml">=</mo><mn id="S3.SS6.p1.5.m3.1.1.3" mathsize="90%" xref="S3.SS6.p1.5.m3.1.1.3.cmml">0.9999</mn></mrow><annotation-xml encoding="MathML-Content" id="S3.SS6.p1.5.m3.1b"><apply id="S3.SS6.p1.5.m3.1.1.cmml" xref="S3.SS6.p1.5.m3.1.1"><eq id="S3.SS6.p1.5.m3.1.1.1.cmml" xref="S3.SS6.p1.5.m3.1.1.1"></eq><apply id="S3.SS6.p1.5.m3.1.1.2.cmml" xref="S3.SS6.p1.5.m3.1.1.2"><csymbol cd="ambiguous" id="S3.SS6.p1.5.m3.1.1.2.1.cmml" xref="S3.SS6.p1.5.m3.1.1.2">subscript</csymbol><ci id="S3.SS6.p1.5.m3.1.1.2.2.cmml" xref="S3.SS6.p1.5.m3.1.1.2.2">ğœ</ci><ci id="S3.SS6.p1.5.m3.1.1.2.3.cmml" xref="S3.SS6.p1.5.m3.1.1.2.3">ğ‘’</ci></apply><cn id="S3.SS6.p1.5.m3.1.1.3.cmml" type="float" xref="S3.SS6.p1.5.m3.1.1.3">0.9999</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS6.p1.5.m3.1c">\tau_{e}=0.9999</annotation><annotation encoding="application/x-llamapun" id="S3.SS6.p1.5.m3.1d">italic_Ï„ start_POSTSUBSCRIPT italic_e end_POSTSUBSCRIPT = 0.9999</annotation></semantics></math><span class="ltx_text" id="S3.SS6.p1.5.4" style="font-size:90%;"> over 20 updates. The performance of the model is compared by training the same framework on a randomly curated dataset of the same size. A simple logistic regression is optimized using the learned embeddings for the classification of the type of ships.</span></p>
</div>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">4 </span>Results</h2>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.1 </span>Data curation</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.4"><span class="ltx_text" id="S4.SS1.p1.4.1" style="font-size:90%;">For the AIS curation method, the optimal threshold value </span><math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.p1.1.m1.1"><semantics id="S4.SS1.p1.1.m1.1a"><mi id="S4.SS1.p1.1.m1.1.1" mathsize="90%" xref="S4.SS1.p1.1.m1.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.1.m1.1b"><ci id="S4.SS1.p1.1.m1.1.1.cmml" xref="S4.SS1.p1.1.m1.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.1.m1.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.1.m1.1d">italic_t</annotation></semantics></math><span class="ltx_text" id="S4.SS1.p1.4.2" style="font-size:90%;"> is defined to correspond to the knee of the skewed distribution of </span><math alttext="\mathcal{D}_{s}" class="ltx_Math" display="inline" id="S4.SS1.p1.2.m2.1"><semantics id="S4.SS1.p1.2.m2.1a"><msub id="S4.SS1.p1.2.m2.1.1" xref="S4.SS1.p1.2.m2.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.2.m2.1.1.2" mathsize="90%" xref="S4.SS1.p1.2.m2.1.1.2.cmml">ğ’Ÿ</mi><mi id="S4.SS1.p1.2.m2.1.1.3" mathsize="90%" xref="S4.SS1.p1.2.m2.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.2.m2.1b"><apply id="S4.SS1.p1.2.m2.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.2.m2.1.1.1.cmml" xref="S4.SS1.p1.2.m2.1.1">subscript</csymbol><ci id="S4.SS1.p1.2.m2.1.1.2.cmml" xref="S4.SS1.p1.2.m2.1.1.2">ğ’Ÿ</ci><ci id="S4.SS1.p1.2.m2.1.1.3.cmml" xref="S4.SS1.p1.2.m2.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.2.m2.1c">\mathcal{D}_{s}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.2.m2.1d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS1.p1.4.3" style="font-size:90%;"> </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS1.p1.4.4.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib4" title="">4</a><span class="ltx_text" id="S4.SS1.p1.4.5.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS1.p1.4.6" style="font-size:90%;">. Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S4.F3" style="font-size:90%;" title="Figure 3 â€£ 4.1 Data curation â€£ 4 Results â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_tag">3</span></a><span class="ltx_text" id="S4.SS1.p1.4.7" style="font-size:90%;"> illustrates the number of 10-second audio windows per individual ship, revealing a skewed distribution. The optimal threshold, aligning with the knee of the distribution, is around 250. For this reason, the threshold value </span><math alttext="t" class="ltx_Math" display="inline" id="S4.SS1.p1.3.m3.1"><semantics id="S4.SS1.p1.3.m3.1a"><mi id="S4.SS1.p1.3.m3.1.1" mathsize="90%" xref="S4.SS1.p1.3.m3.1.1.cmml">t</mi><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.3.m3.1b"><ci id="S4.SS1.p1.3.m3.1.1.cmml" xref="S4.SS1.p1.3.m3.1.1">ğ‘¡</ci></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.3.m3.1c">t</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.3.m3.1d">italic_t</annotation></semantics></math><span class="ltx_text" id="S4.SS1.p1.4.8" style="font-size:90%;"> was set to 250, resulting in a dataset </span><math alttext="\mathcal{D}_{s}^{*}" class="ltx_Math" display="inline" id="S4.SS1.p1.4.m4.1"><semantics id="S4.SS1.p1.4.m4.1a"><msubsup id="S4.SS1.p1.4.m4.1.1" xref="S4.SS1.p1.4.m4.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p1.4.m4.1.1.2.2" mathsize="90%" xref="S4.SS1.p1.4.m4.1.1.2.2.cmml">ğ’Ÿ</mi><mi id="S4.SS1.p1.4.m4.1.1.2.3" mathsize="90%" xref="S4.SS1.p1.4.m4.1.1.2.3.cmml">s</mi><mo id="S4.SS1.p1.4.m4.1.1.3" mathsize="90%" xref="S4.SS1.p1.4.m4.1.1.3.cmml">âˆ—</mo></msubsup><annotation-xml encoding="MathML-Content" id="S4.SS1.p1.4.m4.1b"><apply id="S4.SS1.p1.4.m4.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.1.cmml" xref="S4.SS1.p1.4.m4.1.1">superscript</csymbol><apply id="S4.SS1.p1.4.m4.1.1.2.cmml" xref="S4.SS1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S4.SS1.p1.4.m4.1.1.2.1.cmml" xref="S4.SS1.p1.4.m4.1.1">subscript</csymbol><ci id="S4.SS1.p1.4.m4.1.1.2.2.cmml" xref="S4.SS1.p1.4.m4.1.1.2.2">ğ’Ÿ</ci><ci id="S4.SS1.p1.4.m4.1.1.2.3.cmml" xref="S4.SS1.p1.4.m4.1.1.2.3">ğ‘ </ci></apply><times id="S4.SS1.p1.4.m4.1.1.3.cmml" xref="S4.SS1.p1.4.m4.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p1.4.m4.1c">\mathcal{D}_{s}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p1.4.m4.1d">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS1.p1.4.9" style="font-size:90%;"> capturing 25,021 audio samples.</span></p>
</div>
<figure class="ltx_figure" id="S4.F3">
<p class="ltx_p ltx_align_center" id="S4.F3.1"><span class="ltx_text" id="S4.F3.1.1" style="font-size:90%;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="319" id="S4.F3.1.1.g1" src="x3.png" width="425"/></span></p>
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text ltx_font_bold" id="S4.F3.17.1.1">FigureÂ 3</span>: </span> AIS distribution of individual ships in datset <math alttext="\mathcal{D}_{s}" class="ltx_Math" display="inline" id="S4.F3.6.m1.1"><semantics id="S4.F3.6.m1.1b"><msub id="S4.F3.6.m1.1.1" xref="S4.F3.6.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.F3.6.m1.1.1.2" xref="S4.F3.6.m1.1.1.2.cmml">ğ’Ÿ</mi><mi id="S4.F3.6.m1.1.1.3" xref="S4.F3.6.m1.1.1.3.cmml">s</mi></msub><annotation-xml encoding="MathML-Content" id="S4.F3.6.m1.1c"><apply id="S4.F3.6.m1.1.1.cmml" xref="S4.F3.6.m1.1.1"><csymbol cd="ambiguous" id="S4.F3.6.m1.1.1.1.cmml" xref="S4.F3.6.m1.1.1">subscript</csymbol><ci id="S4.F3.6.m1.1.1.2.cmml" xref="S4.F3.6.m1.1.1.2">ğ’Ÿ</ci><ci id="S4.F3.6.m1.1.1.3.cmml" xref="S4.F3.6.m1.1.1.3">ğ‘ </ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.6.m1.1d">\mathcal{D}_{s}</annotation><annotation encoding="application/x-llamapun" id="S4.F3.6.m1.1e">caligraphic_D start_POSTSUBSCRIPT italic_s end_POSTSUBSCRIPT</annotation></semantics></math> with threshold value <math alttext="t=250" class="ltx_Math" display="inline" id="S4.F3.7.m2.1"><semantics id="S4.F3.7.m2.1b"><mrow id="S4.F3.7.m2.1.1" xref="S4.F3.7.m2.1.1.cmml"><mi id="S4.F3.7.m2.1.1.2" xref="S4.F3.7.m2.1.1.2.cmml">t</mi><mo id="S4.F3.7.m2.1.1.1" xref="S4.F3.7.m2.1.1.1.cmml">=</mo><mn id="S4.F3.7.m2.1.1.3" xref="S4.F3.7.m2.1.1.3.cmml">250</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.7.m2.1c"><apply id="S4.F3.7.m2.1.1.cmml" xref="S4.F3.7.m2.1.1"><eq id="S4.F3.7.m2.1.1.1.cmml" xref="S4.F3.7.m2.1.1.1"></eq><ci id="S4.F3.7.m2.1.1.2.cmml" xref="S4.F3.7.m2.1.1.2">ğ‘¡</ci><cn id="S4.F3.7.m2.1.1.3.cmml" type="integer" xref="S4.F3.7.m2.1.1.3">250</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.7.m2.1d">t=250</annotation><annotation encoding="application/x-llamapun" id="S4.F3.7.m2.1e">italic_t = 250</annotation></semantics></math> in red, <math alttext="t=500" class="ltx_Math" display="inline" id="S4.F3.8.m3.1"><semantics id="S4.F3.8.m3.1b"><mrow id="S4.F3.8.m3.1.1" xref="S4.F3.8.m3.1.1.cmml"><mi id="S4.F3.8.m3.1.1.2" xref="S4.F3.8.m3.1.1.2.cmml">t</mi><mo id="S4.F3.8.m3.1.1.1" xref="S4.F3.8.m3.1.1.1.cmml">=</mo><mn id="S4.F3.8.m3.1.1.3" xref="S4.F3.8.m3.1.1.3.cmml">500</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.8.m3.1c"><apply id="S4.F3.8.m3.1.1.cmml" xref="S4.F3.8.m3.1.1"><eq id="S4.F3.8.m3.1.1.1.cmml" xref="S4.F3.8.m3.1.1.1"></eq><ci id="S4.F3.8.m3.1.1.2.cmml" xref="S4.F3.8.m3.1.1.2">ğ‘¡</ci><cn id="S4.F3.8.m3.1.1.3.cmml" type="integer" xref="S4.F3.8.m3.1.1.3">500</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.8.m3.1d">t=500</annotation><annotation encoding="application/x-llamapun" id="S4.F3.8.m3.1e">italic_t = 500</annotation></semantics></math> in blue, <math alttext="t=1000" class="ltx_Math" display="inline" id="S4.F3.9.m4.1"><semantics id="S4.F3.9.m4.1b"><mrow id="S4.F3.9.m4.1.1" xref="S4.F3.9.m4.1.1.cmml"><mi id="S4.F3.9.m4.1.1.2" xref="S4.F3.9.m4.1.1.2.cmml">t</mi><mo id="S4.F3.9.m4.1.1.1" xref="S4.F3.9.m4.1.1.1.cmml">=</mo><mn id="S4.F3.9.m4.1.1.3" xref="S4.F3.9.m4.1.1.3.cmml">1000</mn></mrow><annotation-xml encoding="MathML-Content" id="S4.F3.9.m4.1c"><apply id="S4.F3.9.m4.1.1.cmml" xref="S4.F3.9.m4.1.1"><eq id="S4.F3.9.m4.1.1.1.cmml" xref="S4.F3.9.m4.1.1.1"></eq><ci id="S4.F3.9.m4.1.1.2.cmml" xref="S4.F3.9.m4.1.1.2">ğ‘¡</ci><cn id="S4.F3.9.m4.1.1.3.cmml" type="integer" xref="S4.F3.9.m4.1.1.3">1000</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.F3.9.m4.1d">t=1000</annotation><annotation encoding="application/x-llamapun" id="S4.F3.9.m4.1e">italic_t = 1000</annotation></semantics></math> in green</figcaption>
</figure>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1"><span class="ltx_text" id="S4.SS1.p2.1.1" style="font-size:90%;">In addition to AIS samples, 323,532 samples were selected during PAM data curation. These samples were combined to create the dataset </span><math alttext="\mathcal{D}^{*}" class="ltx_Math" display="inline" id="S4.SS1.p2.1.m1.1"><semantics id="S4.SS1.p2.1.m1.1a"><msup id="S4.SS1.p2.1.m1.1.1" xref="S4.SS1.p2.1.m1.1.1.cmml"><mi class="ltx_font_mathcaligraphic" id="S4.SS1.p2.1.m1.1.1.2" mathsize="90%" xref="S4.SS1.p2.1.m1.1.1.2.cmml">ğ’Ÿ</mi><mo id="S4.SS1.p2.1.m1.1.1.3" mathsize="90%" xref="S4.SS1.p2.1.m1.1.1.3.cmml">âˆ—</mo></msup><annotation-xml encoding="MathML-Content" id="S4.SS1.p2.1.m1.1b"><apply id="S4.SS1.p2.1.m1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS1.p2.1.m1.1.1.1.cmml" xref="S4.SS1.p2.1.m1.1.1">superscript</csymbol><ci id="S4.SS1.p2.1.m1.1.1.2.cmml" xref="S4.SS1.p2.1.m1.1.1.2">ğ’Ÿ</ci><times id="S4.SS1.p2.1.m1.1.1.3.cmml" xref="S4.SS1.p2.1.m1.1.1.3"></times></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS1.p2.1.m1.1c">\mathcal{D}^{*}</annotation><annotation encoding="application/x-llamapun" id="S4.SS1.p2.1.m1.1d">caligraphic_D start_POSTSUPERSCRIPT âˆ— end_POSTSUPERSCRIPT</annotation></semantics></math><span class="ltx_text" id="S4.SS1.p2.1.2" style="font-size:90%;"> holding roughly 970 hours of PAM recordings.</span></p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection" style="font-size:90%;">
<span class="ltx_tag ltx_tag_subsection">4.2 </span>SSL results curated dataset</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1"><span class="ltx_text" id="S4.SS2.p1.1.1" style="font-size:90%;">The proposed method is evaluated using the labeled benchmark datasets Deepship </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.2.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib5" title="">5</a><span class="ltx_text" id="S4.SS2.p1.1.3.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.4" style="font-size:90%;"> and ShipsEar </span><cite class="ltx_cite ltx_citemacro_cite"><span class="ltx_text" id="S4.SS2.p1.1.5.1" style="font-size:90%;">[</span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#bib.bib6" title="">6</a><span class="ltx_text" id="S4.SS2.p1.1.6.2" style="font-size:90%;">]</span></cite><span class="ltx_text" id="S4.SS2.p1.1.7" style="font-size:90%;">. Both datasets hold ship types recorded in two distinct regions. Table </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S4.T1" style="font-size:90%;" title="Table 1 â€£ 4.2 SSL results curated dataset â€£ 4 Results â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S4.SS2.p1.1.8" style="font-size:90%;"> presents the accuracy scores of the Data2Vec model trained on curated and random datasets. The results indicate that the curated model outperforms the random model on both benchmark datasets. However, the performance gain is smaller for ShipsEar, likely due to environmental factors. The ocean environment of ShipsEar is quite different from the environment of the hydrophones in the raw PAM data (Figure </span><a class="ltx_ref" href="https://arxiv.org/html/2505.20066v1#S3.F1" style="font-size:90%;" title="Figure 1 â€£ 3.1 PAM data â€£ 3 Methods â€£ Automated Data Curation for Self-Supervised Learning in Underwater Acoustic Analysis"><span class="ltx_text ltx_ref_tag">1</span></a><span class="ltx_text" id="S4.SS2.p1.1.9" style="font-size:90%;">), making classification more challenging. In contrast, the ocean environment of Deepship is more comparable with the PAM data, leading to a performance increase of more than 7%.</span></p>
</div>
<figure class="ltx_table" id="S4.T1">
<figcaption class="ltx_caption" style="font-size:90%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text ltx_font_bold" id="S4.T1.4.1.1">Table 1</span>: </span>Accuracy scores ship type classification</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T1.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T1.5.1.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.5.1.1.1"></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.5.1.1.2"><span class="ltx_text" id="S4.T1.5.1.1.2.1" style="font-size:90%;">ShipsEar</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_t" id="S4.T1.5.1.1.3"><span class="ltx_text" id="S4.T1.5.1.1.3.1" style="font-size:90%;">Deepship</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T1.5.2.1">
<td class="ltx_td ltx_align_left ltx_border_l ltx_border_r ltx_border_t" id="S4.T1.5.2.1.1"><span class="ltx_text" id="S4.T1.5.2.1.1.1" style="font-size:90%;">Random</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.5.2.1.2"><span class="ltx_text" id="S4.T1.5.2.1.2.1" style="font-size:90%;">51.98%</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T1.5.2.1.3"><span class="ltx_text" id="S4.T1.5.2.1.3.1" style="font-size:90%;">49.16%</span></td>
</tr>
<tr class="ltx_tr" id="S4.T1.5.3.2">
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_l ltx_border_r" id="S4.T1.5.3.2.1"><span class="ltx_text" id="S4.T1.5.3.2.1.1" style="font-size:90%;">Curated</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.5.3.2.2"><span class="ltx_text" id="S4.T1.5.3.2.2.1" style="font-size:90%;">53.11%</span></td>
<td class="ltx_td ltx_align_left ltx_border_b ltx_border_r" id="S4.T1.5.3.2.3"><span class="ltx_text" id="S4.T1.5.3.2.3.1" style="font-size:90%;">56.72%</span></td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section" style="font-size:90%;">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1"><span class="ltx_text" id="S5.p1.1.1" style="font-size:90%;">This work describes the first automatic data curation pipeline to curate large web-scraped PAM data. The study demonstrates that curation is a key aspect in extracting accurate SSL model representations from unlabeled underwater recordings. Although this work focuses on data curation, more research is still required on SSL methods applied to underwater acoustics. Due to the stationarity of the data, the masking strategy may be suboptimal, and the results may benefit from a more contrastive approach. In addition to the raw web-scraped PAM data used in this study, more diverse data is publicly available. Expanding the diversity by incorporating more hydrophones from various regions would make the representations more robust for various other underwater acoustic-related tasks. This offers the possibility of training large-scale SSL models from scratch, enabling more advanced underwater acoustic applications.</span></p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
H.Â I. Hummel, R.Â D. vanÂ der Mei, and S.Â Bhulai, â€œA survey on machine learning in ship radiated noise,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib1.2.2" style="font-size:90%;">Ocean Engineering</span><span class="ltx_text" id="bib.bib1.3.3" style="font-size:90%;">, vol.Â 298, p.Â 117252, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
J.Â Gui, T.Â Chen, J.Â Zhang, Q.Â Cao, Z.Â Sun, H.Â Luo, and D.Â Tao, â€œA survey on self-supervised learning: Algorithms, applications, and future trends,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib2.2.2" style="font-size:90%;">IEEE Trans. Pattern Anal. Mach. Intell.</span><span class="ltx_text" id="bib.bib2.3.3" style="font-size:90%;">, vol.Â 46, no.Â 12, p.Â 9052â€“9071, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
S.Â Liu, A.Â Mallol-Ragolta, E.Â Parada-Cabaleiro, K.Â Qian, X.Â Jing, A.Â Kathan, B.Â Hu, and B.Â W. Schuller, â€œAudio self-supervised learning: A survey,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib3.2.2" style="font-size:90%;">Patterns</span><span class="ltx_text" id="bib.bib3.3.3" style="font-size:90%;">, vol.Â 3, no.Â 12, p.Â 100616, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
C.Â Jose, T.Â Moutakanni, D.Â Kang, F.Â Baldassarre, T.Â Darcet, H.Â Xu, D.Â Li, M.Â Szafraniec, M.Â Ramamonjisoa, M.Â Oquab, </span><span class="ltx_text ltx_font_italic" id="bib.bib4.2.2" style="font-size:90%;">etÂ al.</span><span class="ltx_text" id="bib.bib4.3.3" style="font-size:90%;">, â€œDinov2 meets text: A unified framework for image-and pixel-level vision-language alignment,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib4.4.4" style="font-size:90%;">arXiv preprint arXiv:2412.16334</span><span class="ltx_text" id="bib.bib4.5.5" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
M.Â Irfan, Z.Â Jiangbin, S.Â Ali, M.Â Iqbal, Z.Â Masood, and U.Â Hamid, â€œDeepship: An underwater acoustic benchmark dataset and a separable convolution based autoencoder for classification,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib5.2.2" style="font-size:90%;">Expert Systems with Applications</span><span class="ltx_text" id="bib.bib5.3.3" style="font-size:90%;">, vol.Â 183, p.Â 115270, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
D.Â Santos-DomÃ­nguez, S.Â Torres-Guijarro, A.Â Cardenal-LÃ³pez, and A.Â Pena-Gimenez, â€œShipsear: An underwater vessel noise database,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib6.2.2" style="font-size:90%;">Applied Acoustics</span><span class="ltx_text" id="bib.bib6.3.3" style="font-size:90%;">, vol.Â 113, pp.Â 64â€“69, 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
K.Â Xu, Q.Â Xu, K.Â You, B.Â Zhu, M.Â Feng, D.Â Feng, and B.Â Liu, â€œSelf-supervised learningâ€“based underwater acoustical signal classification via mask modeling,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib7.2.2" style="font-size:90%;">The Journal of the Acoustical Society of America</span><span class="ltx_text" id="bib.bib7.3.3" style="font-size:90%;">, vol.Â 154, no.Â 1, pp.Â 5â€“15, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
Q.Â Xu, J.Â Jiang, K.Â Xu, Y.Â Dou, C.Â Gao, B.Â Zhu, K.Â You, and Q.Â Zhu, â€œSelf-supervised learning-for underwater acoustic signal classification with mixup,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib8.2.2" style="font-size:90%;">IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing</span><span class="ltx_text" id="bib.bib8.3.3" style="font-size:90%;">, vol.Â 17, pp.Â 3530â€“3542, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
S.Â Feng, X.Â Zhu, and S.Â Ma, â€œMasking hierarchical tokens for underwater acoustic target recognition with self-supervised learning,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib9.2.2" style="font-size:90%;">IEEE/ACM Transactions on Audio, Speech, and Language Processing</span><span class="ltx_text" id="bib.bib9.3.3" style="font-size:90%;">, vol.Â 32, pp.Â 1365â€“1379, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
M.Â Oquab, T.Â Darcet, T.Â Moutakanni, H.Â Vo, M.Â Szafraniec, V.Â Khalidov, P.Â Fernandez, D.Â Haziza, F.Â Massa, A.Â El-Nouby, M.Â Assran, N.Â Ballas, W.Â Galuba, R.Â Howes, P.-Y. Huang, S.-W. Li, I.Â Misra, M.Â Rabbat, V.Â Sharma, G.Â Synnaeve, H.Â Xu, H.Â Jegou, J.Â Mairal, P.Â Labatut, A.Â Joulin, and P.Â Bojanowski, â€œDINOv2: Learning Robust Visual Features without Supervision,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib10.2.2" style="font-size:90%;">Transactions on Machine Learning Research Journal</span><span class="ltx_text" id="bib.bib10.3.3" style="font-size:90%;">, pp.Â 1â€“31, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
H.Â V. Vo, V.Â Khalidov, T.Â Darcet, T.Â Moutakanni, N.Â Smetanin, M.Â Szafraniec, H.Â Touvron, C.Â Couprie, M.Â Oquab, A.Â Joulin, </span><span class="ltx_text ltx_font_italic" id="bib.bib11.2.2" style="font-size:90%;">etÂ al.</span><span class="ltx_text" id="bib.bib11.3.3" style="font-size:90%;">, â€œAutomatic data curation for self-supervised learning: A clustering-based approach,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib11.4.4" style="font-size:90%;">arXiv preprint arXiv:2405.15613</span><span class="ltx_text" id="bib.bib11.5.5" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
S.Â Lee, J.Â Chung, Y.Â Yu, G.Â Kim, T.Â Breuel, G.Â Chechik, and Y.Â Song, â€œAcav100m: Automatic curation of large-scale datasets for audio-visual video representation learning,â€ in </span><span class="ltx_text ltx_font_italic" id="bib.bib12.2.2" style="font-size:90%;">Proceedings of the IEEE/CVF International Conference on Computer Vision</span><span class="ltx_text" id="bib.bib12.3.3" style="font-size:90%;">, pp.Â 10274â€“10284, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
H.Â I. Hummel, A.Â Gansekoele, R.Â D. vanÂ der Mei, and S.Â Bhulai, â€œThe computation of generalized embeddings for underwater acoustic target recognition using contrastive learning,â€ </span><span class="ltx_text ltx_font_italic" id="bib.bib13.2.2" style="font-size:90%;">Available at SSRN: http://dx.doi.org/10.2139/ssrn.5112948</span><span class="ltx_text" id="bib.bib13.3.3" style="font-size:90%;">, 2025.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
D.Â Sculley, â€œWeb-scale k-means clustering,â€ in </span><span class="ltx_text ltx_font_italic" id="bib.bib14.2.2" style="font-size:90%;">Proceedings of the 19th international conference on World wide web</span><span class="ltx_text" id="bib.bib14.3.3" style="font-size:90%;">, (New York, NY, USA), pp.Â 1177â€“1178, 2010.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
A.Â Baevski, W.-N. Hsu, Q.Â Xu, A.Â Babu, J.Â Gu, and M.Â Auli, â€œData2vec: A general framework for self-supervised learning in speech, vision and language,â€ in </span><span class="ltx_text ltx_font_italic" id="bib.bib15.2.2" style="font-size:90%;">International conference on machine learning</span><span class="ltx_text" id="bib.bib15.3.3" style="font-size:90%;">, (Maryland, USA), pp.Â 1298â€“1312, 2022.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<div class="ltx_pagination ltx_role_newpage"></div>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 26 14:48:00 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

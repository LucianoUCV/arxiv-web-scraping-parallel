<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Learning Extrapolative Sequence Transformations from Markov Chains</title>
<!--Generated on Mon May 26 17:24:12 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="Machine Learning,  ICML" lang="en" name="keywords"/>
<base href="/html/2505.20251v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S1" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span>Introduction</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S1.SS0.SSS0.Px1" title="In 1 Introduction ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Summary of contributions</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span>Proposed Method</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS0.SSS0.Px1" title="In 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Toy example</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS0.SSS0.Px2" title="In 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Oracle scoring</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS1" title="In 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span>Generating Markov chains</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS1.SSS0.Px1" title="In 2.1 Generating Markov chains ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Surrogate model</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS1.SSS0.Px2" title="In 2.1 Generating Markov chains ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Sampler</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS2" title="In 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span>Training the extrapolative model</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS2.SSS0.Px1" title="In 2.2 Training the extrapolative model ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Parametrization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS2.SSS0.Px2" title="In 2.2 Training the extrapolative model ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Autoregressive refinement</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS2.SSS0.Px3" title="In 2.2 Training the extrapolative model ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS3" title="In 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.3 </span>Creating training episodes</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS3.SSS0.Px1" title="In 2.3 Creating training episodes ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Uniform thinning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS3.SSS0.Px2" title="In 2.3 Creating training episodes ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">First and best</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S2.SS3.SSS0.Px3" title="In 2.3 Creating training episodes ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Changes in energy</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span>Experiments</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS1" title="In 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.1 </span>Protein engineering</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS1.SSS0.Px1" title="In 3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS1.SSS0.Px2" title="In 3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS1.SSS0.Px3" title="In 3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS2" title="In 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.2 </span>Sentiment extrapolation</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS2.SSS0.Px1" title="In 3.2 Sentiment extrapolation ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS2.SSS0.Px2" title="In 3.2 Sentiment extrapolation ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS2.SSS0.Px3" title="In 3.2 Sentiment extrapolation ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS3" title="In 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.3 </span>Anonymization</span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS3.SSS0.Px1" title="In 3.3 Anonymization ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Baselines</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS3.SSS0.Px2" title="In 3.3 Anonymization ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Metrics</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS3.SSS0.Px3" title="In 3.3 Anonymization ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Results</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS4" title="In 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.4 </span>Analysis of episode creation strategy</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.SS5" title="In 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3.5 </span>Approximating <math alttext="q_{\theta}" class="ltx_Math" display="inline"><semantics><msub><mi>q</mi><mi>θ</mi></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝑞</ci><ci>𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex">q_{\theta}</annotation><annotation encoding="application/x-llamapun">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> through further MCMC exploration</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S4" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span>Related Work</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S4.SS0.SSS0.Px1" title="In 4 Related Work ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Controllable generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S4.SS0.SSS0.Px2" title="In 4 Related Work ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Editing models</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S4.SS0.SSS0.Px3" title="In 4 Related Work ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Reinforcement Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S4.SS0.SSS0.Px4" title="In 4 Related Work ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Inference-time scaling</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S5" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span>Conclusion</span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S5.SS0.SSS0.Px1" title="In 5 Conclusion ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Main findings</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S5.SS0.SSS0.Px2" title="In 5 Conclusion ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Limitations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S5.SS0.SSS0.Px3" title="In 5 Conclusion ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Future work</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A1" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">A </span>Reward choice</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A2" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B </span>Extrapolation experimental details</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A2.SS1" title="In Appendix B Extrapolation experimental details ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.1 </span>Protein engineering</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A2.SS2" title="In Appendix B Extrapolation experimental details ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">B.2 </span>Sentiment</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A3" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">C </span>Analysis of episode length</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A4" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">D </span>Ablation of MCMC exploration</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A5" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">E </span>Diversity of generations</span></a></li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A6" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">F </span>Hyperparameters</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A6.SS0.SSS0.Px1" title="In Appendix F Hyperparameters ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Protein engineering energy function</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A6.SS0.SSS0.Px2" title="In Appendix F Hyperparameters ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title">Sentiment energy function</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A7" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G </span>Text Anonymization Implementation</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A7.SS1" title="In Appendix G Text Anonymization Implementation ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.1 </span>Baseline Systems</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A7.SS2" title="In Appendix G Text Anonymization Implementation ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.2 </span>Data</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A7.SS3" title="In Appendix G Text Anonymization Implementation ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">G.3 </span><math alttext="q_{\theta}" class="ltx_Math" display="inline"><semantics><msub><mi>q</mi><mi>θ</mi></msub><annotation-xml encoding="MathML-Content"><apply><csymbol cd="ambiguous">subscript</csymbol><ci>𝑞</ci><ci>𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex">q_{\theta}</annotation><annotation encoding="application/x-llamapun">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> and Inference</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_appendix">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A8" title="In Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H </span>Example generations</span></a>
<ol class="ltx_toclist ltx_toclist_appendix">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A8.SS1" title="In Appendix H Example generations ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H.1 </span>Sentiment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#A8.SS2" title="In Appendix H Example generations ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">H.2 </span>Anonymization</span></a></li>
</ol>
</li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_pruned_first">
<h1 class="ltx_title ltx_title_document">Learning Extrapolative Sequence Transformations from Markov Chains</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Sophia Hager
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Aleem Khan
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Andrew Wang
</span></span>
<span class="ltx_author_before">  </span><span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Nicholas Andrews
</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">Most successful applications of deep learning involve similar training and test conditions. However, tasks such as biological sequence design involve searching for sequences that improve desirable properties beyond previously known values, which requires novel hypotheses that <em class="ltx_emph ltx_font_italic" id="id1.id1.1">extrapolate</em> beyond training data.
In these settings, extrapolation may be achieved by using random search methods such as Markov chain Monte Carlo (MCMC), which, given an initial state, sample local transformations to approximate a target density that rewards states with the desired properties. However, even with a well-designed proposal, MCMC may struggle to explore large structured state spaces efficiently.
Rather than relying on stochastic search, it would be desirable to have a model that greedily optimizes the properties of interest, successfully extrapolating in as few steps as possible.
We propose to learn such a model from the Markov chains resulting from MCMC search.
Specifically, our approach uses selected states from Markov chains as a source of training data for an autoregressive model, which is then able to efficiently generate novel sequences that extrapolate along the sequence-level properties of interest.
The proposed approach is validated on three problems: protein sequence design, text sentiment control, and text anonymization.
We find that the autoregressive model can extrapolate as well or better than MCMC, but with the additional benefits of scalability and significantly higher sample efficiency.</p>
</div>
<div class="ltx_keywords">Machine Learning, ICML
</div>
<div class="ltx_para" id="p2">
<br class="ltx_break"/>
</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span>Introduction</h2>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="355" id="S1.F1.g1" src="x1.png" width="298"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>The sentiment extrapolation task (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.SS2" title="3.2 Sentiment extrapolation ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3.2</span></a>) requires generating reviews with ratings beyond the range observed at training time. The search process is illustrated using a toy 1D representation of the features (x-axis) and rating (y-axis). Monte Carlo exploration can produce reviews that extrapolate, but many steps are required. However, once good state sequences have been discovered, we can sub-sample the transitions that decrease the rating (A <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.F1.3.m1.1"><semantics id="S1.F1.3.m1.1b"><mo id="S1.F1.3.m1.1.1" stretchy="false" xref="S1.F1.3.m1.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F1.3.m1.1c"><ci id="S1.F1.3.m1.1.1.cmml" xref="S1.F1.3.m1.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.3.m1.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.F1.3.m1.1e">→</annotation></semantics></math> C <math alttext="\rightarrow" class="ltx_Math" display="inline" id="S1.F1.4.m2.1"><semantics id="S1.F1.4.m2.1b"><mo id="S1.F1.4.m2.1.1" stretchy="false" xref="S1.F1.4.m2.1.1.cmml">→</mo><annotation-xml encoding="MathML-Content" id="S1.F1.4.m2.1c"><ci id="S1.F1.4.m2.1.1.cmml" xref="S1.F1.4.m2.1.1">→</ci></annotation-xml><annotation encoding="application/x-tex" id="S1.F1.4.m2.1d">\rightarrow</annotation><annotation encoding="application/x-llamapun" id="S1.F1.4.m2.1e">→</annotation></semantics></math> N) and use them to learn an extrapolative model. The reviews shown to the right for states B, C, and N are actual reviews generated by our method, while A is a genuine review from the validation data.</figcaption>
</figure>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">In creative tasks such as scientific discovery, a key requirement is the ability to <span class="ltx_text ltx_font_italic" id="S1.p1.1.1">extrapolate</span> beyond existing knowledge. For example, automating generation of novel hypotheses is central to mathematical discovery, biological sequence design, molecular optimization, and the creation of new materials <cite class="ltx_cite ltx_citemacro_citep">(Romera-Paredes et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib41" title="">2024</a>; Fu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib13" title="">2023</a>; Jain et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib20" title="">2022</a>; Trabucco et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib52" title="">2022</a>; Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib15" title="">2022</a>)</cite>. Extrapolation is also necessary in many creative applications, such as writing assistants for creative writing <cite class="ltx_cite ltx_citemacro_citep">(Swanson et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib48" title="">2021</a>; Gómez-Rodríguez &amp; Williams, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib16" title="">2023</a>)</cite>.
It is natural to wonder if extrapolation is an emergent ability of large-scale generative models <cite class="ltx_cite ltx_citemacro_citep">(Schaeffer et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib42" title="">2024</a>)</cite>.
However, prior work has found that state-of-the-art foundation models can struggle on tasks requiring extrapolation <cite class="ltx_cite ltx_citemacro_citep">(Dziri et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib11" title="">2023</a>; Chakrabarty et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib5" title="">2024</a>)</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Scaling test-time compute may offer an alternate approach to the problem of extrapolation.
Instead of increasing the number of model parameters or amount of training data, a generative model can simply produce samples repeatedly to find the optimal solution. When guided by a verifier, this process can improve model performance substantially <cite class="ltx_cite ltx_citemacro_citep">(Snell et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib47" title="">2025</a>)</cite>.
Notably, <cite class="ltx_cite ltx_citemacro_citet">Lu et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib29" title="">2025</a>)</cite> compare reasoning and inference strategies for foundation models, finding that the only strategy that successfully increases sample diversity is Monte Carlo search.
However, while such approaches may offer good results for extrapolation, sampling at inference-time may be too slow in practice.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">How can we <em class="ltx_emph ltx_font_italic" id="S1.p3.1.1">efficiently</em> extrapolate beyond the training data, taking advantage of performance gains brought by scaling test-time compute without paying the cost at inference? We build on Iterative Controllable Extrapolation (ICE), a recent approach which leverages the de-noising ability of masked language models (MLMs) to extrapolate <cite class="ltx_cite ltx_citemacro_citep">(Padmakumar et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>. ICE uses random masking and infilling to generate sequence transformations that improve the target objective as evaluated by a trained scorer, and then supplies these transformations as training data for an autoregressive model. The assumption is that at inference time, composing several transformations with this model may lead to effective extrapolation.
While this method was found to be successful in extrapolating beyond the training region for some tasks, its success is critically dependent on the choice of a number of sensitive hyper-parameters, including a threshold on the relative improvement from different transformations and a fixed number of iterative decoding steps.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In this paper, we seek to better utilize the implicit knowledge of generative models trained using in-filling objectives <cite class="ltx_cite ltx_citemacro_citep">(Bavarian et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib3" title="">2022</a>; Tay et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib51" title="">2023</a>)</cite> for extrapolative generation.
Rather than employ a heuristic search, we use Metropolis-Hastings (MH) to generate correlated samples where an MLM is used as a proposal distribution.
While MCMC provides theoretical guarantees, it is inefficient in high-dimensional state spaces such as natural language.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">To address this inefficiency, we prune the set of states drawn by the sampler and finetune language models on these pruned states to autoregressively predict the transition from one state to the next.
Our objective in doing so is to generate sequences that achieve scores in the extrapolation range in <em class="ltx_emph ltx_font_italic" id="S1.p5.1.1">as few steps as possible</em>. This is illustrated in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Figure 1</span></a> for the controlled task of review generation (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.SS2" title="3.2 Sentiment extrapolation ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3.2</span></a>). Not all transitions in the Markov chains are equivalently useful as training data, since some transitions may fail to improve the score or result in <em class="ltx_emph ltx_font_italic" id="S1.p5.1.2">worse</em> scores. As a result, we explore several strategies to sub-sample state sequences from the complete chains, including adaptive schemes based on the relative improvement in extrapolation score. While the model we fine-tune has an autoregressive parametrization (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S2" title="2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 2</span></a>), by selecting transitions from the Markov chains, we implicitly learn a non-autoregressive model that iteratively transforms an initial sequence (token-by-token) to improve the score beyond the training range. By further incorporating a sequence-level score at each step of generation—similar to reward-to-go in sequence modeling approaches to reinforcement learning <cite class="ltx_cite ltx_citemacro_citep">(Janner et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib21" title="">2021</a>)</cite>—the model can learn to incorporate this feedback.</p>
</div>
<section class="ltx_paragraph" id="S1.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Summary of contributions</h4>
<div class="ltx_para" id="S1.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S1.SS0.SSS0.Px1.p1.1">We propose a framework to extrapolate beyond a given training dataset given an arbitrary scoring function. Our approach leverages existing components, namely pre-trained language models trained using de-noising objectives, to explore the space of sequence-to-sequence transformations and their impact on the target objective, a process formalized as MCMC. We consider a variety of strategies to select training data from the resulting Markov chains to fine-tune a model to generate novel sequences. In particular, we propose a multi-step generative process in which, starting from an initial state, the properties of interest are optimized in multiple rounds, similar to non-autoregressive generation. We evaluate our model on three tasks: protein engineering, sentiment style transfer, and anonymization<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span>Code made available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/sophia-hager/learning-MCMC-extrapolation" title="">https://github.com/sophia-hager/learning-MCMC-extrapolation</a></span></span></span>.
In some cases, we find that our model, <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S1.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S1.SS0.SSS0.Px1.p1.1.m1.1a"><msub id="S1.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S1.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S1.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">q</mi><mi id="S1.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S1.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S1.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S1.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S1.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S1.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S1.SS0.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S1.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S1.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝑞</ci><ci id="S1.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S1.SS0.SSS0.Px1.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S1.SS0.SSS0.Px1.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S1.SS0.SSS0.Px1.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, can achieve competitive results with MCMC and other baselines using a significantly smaller number of steps (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3" title="3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3</span></a>). In other cases, we find that the fine-tuned model extrapolates beyond the best value achieved during sampling.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span>Proposed Method</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">The objective of extrapolative generation is to produce samples that optimize properties of interest, such as sentiment in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Figure 1</span></a>, beyond previously observed values.
Our approach proceeds in two phases. In the first phase, we use MCMC to sample from a surrogate density that assigns higher probability to samples likely to contain the desired properties. Unlike typical applications of MCMC, our objective is not to approximate the density itself, but to derive training data from the resulting Markov chains. The training data is used to fit a model that iteratively improves the property of interest, given an initial state. Note that the learned model is not an inference network amortizing the sampling process <cite class="ltx_cite ltx_citemacro_citep">(Li et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib27" title="">2017</a>)</cite>; the goal is efficient extrapolation.</p>
</div>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Toy example</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.3">To illustrate the main idea, we provide a simple example of training an iterative extrapolation model on Markov chains. Consider the space of binary sequences of fixed length <math alttext="L" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.1.m1.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.1.m1.1c">L</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.1.m1.1d">italic_L</annotation></semantics></math>. Given an initial sequence <math alttext="x^{(0)}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px1.p1.2.m2.1a"><msup id="S2.SS0.SSS0.Px1.p1.2.m2.1.2" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.2.m2.1.2.2" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.2.2.cmml">x</mi><mrow id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.2.cmml"><mo id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.3.1" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.2.cmml">(</mo><mn id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.1" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.cmml">0</mn><mo id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.3.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS0.SSS0.Px1.p1.2.m2.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.2.m2.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.2">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.2.m2.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.2.2">𝑥</ci><cn id="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.2.m2.1.1.1.1">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.2.m2.1c">x^{(0)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.2.m2.1d">italic_x start_POSTSUPERSCRIPT ( 0 ) end_POSTSUPERSCRIPT</annotation></semantics></math> of all zeros, the objective is to search for sequences that maximize a scalar score function <math alttext="s(x)=\exp{\sum_{i}^{L}r_{i}}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px1.p1.3.m3.1a"><mrow id="S2.SS0.SSS0.Px1.p1.3.m3.1.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.cmml"><mrow id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.2.cmml">s</mi><mo id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.1" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.3.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.cmml"><mo id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.cmml">)</mo></mrow></mrow><mo id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.1" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.1.cmml">=</mo><mrow id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.2.cmml">exp</mi><mo id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.1" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.cmml"><msubsup id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.cmml"><mo id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.2.cmml">∑</mo><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.3" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.3.cmml">i</mi><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.3" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.3.cmml">L</mi></msubsup><msub id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.2" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.2.cmml">r</mi><mi id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.3" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.3.cmml">i</mi></msub></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.3.m3.1b"><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2"><eq id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.1"></eq><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2"><times id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.1"></times><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.2.2">𝑠</ci><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.1">𝑥</ci></apply><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3"><times id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.1"></times><exp id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.2"></exp><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3"><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1">superscript</csymbol><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1">subscript</csymbol><sum id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.2"></sum><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.3.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.2.3">𝑖</ci></apply><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.1.3">𝐿</ci></apply><apply id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.2">𝑟</ci><ci id="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.3.cmml" xref="S2.SS0.SSS0.Px1.p1.3.m3.1.2.3.3.2.3">𝑖</ci></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.3.m3.1c">s(x)=\exp{\sum_{i}^{L}r_{i}}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.3.m3.1d">italic_s ( italic_x ) = roman_exp ∑ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_L end_POSTSUPERSCRIPT italic_r start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> where</p>
<table class="ltx_equation ltx_eqn_table" id="S2.Ex1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s_{i}=\begin{cases}ix_{i}/L&amp;\quad i&gt;L/2\\
-ix_{i}/L&amp;\quad\text{otherwise}\end{cases}" class="ltx_Math" display="block" id="S2.Ex1.m1.4"><semantics id="S2.Ex1.m1.4a"><mrow id="S2.Ex1.m1.4.5" xref="S2.Ex1.m1.4.5.cmml"><msub id="S2.Ex1.m1.4.5.2" xref="S2.Ex1.m1.4.5.2.cmml"><mi id="S2.Ex1.m1.4.5.2.2" xref="S2.Ex1.m1.4.5.2.2.cmml">s</mi><mi id="S2.Ex1.m1.4.5.2.3" xref="S2.Ex1.m1.4.5.2.3.cmml">i</mi></msub><mo id="S2.Ex1.m1.4.5.1" xref="S2.Ex1.m1.4.5.1.cmml">=</mo><mrow id="S2.Ex1.m1.4.4" xref="S2.Ex1.m1.4.5.3.1.cmml"><mo id="S2.Ex1.m1.4.4.5" xref="S2.Ex1.m1.4.5.3.1.1.cmml">{</mo><mtable columnspacing="5pt" displaystyle="true" id="S2.Ex1.m1.4.4.4" rowspacing="0pt" xref="S2.Ex1.m1.4.5.3.1.cmml"><mtr id="S2.Ex1.m1.4.4.4a" xref="S2.Ex1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.Ex1.m1.4.4.4b" xref="S2.Ex1.m1.4.5.3.1.cmml"><mrow id="S2.Ex1.m1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.cmml"><mrow id="S2.Ex1.m1.1.1.1.1.1.1.2" xref="S2.Ex1.m1.1.1.1.1.1.1.2.cmml"><mi id="S2.Ex1.m1.1.1.1.1.1.1.2.2" xref="S2.Ex1.m1.1.1.1.1.1.1.2.2.cmml">i</mi><mo id="S2.Ex1.m1.1.1.1.1.1.1.2.1" xref="S2.Ex1.m1.1.1.1.1.1.1.2.1.cmml">⁢</mo><msub id="S2.Ex1.m1.1.1.1.1.1.1.2.3" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S2.Ex1.m1.1.1.1.1.1.1.2.3.2" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3.2.cmml">x</mi><mi id="S2.Ex1.m1.1.1.1.1.1.1.2.3.3" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3.3.cmml">i</mi></msub></mrow><mo id="S2.Ex1.m1.1.1.1.1.1.1.1" xref="S2.Ex1.m1.1.1.1.1.1.1.1.cmml">/</mo><mi id="S2.Ex1.m1.1.1.1.1.1.1.3" xref="S2.Ex1.m1.1.1.1.1.1.1.3.cmml">L</mi></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.Ex1.m1.4.4.4c" xref="S2.Ex1.m1.4.5.3.1.cmml"><mrow id="S2.Ex1.m1.2.2.2.2.2.1" xref="S2.Ex1.m1.2.2.2.2.2.1.cmml"><mi id="S2.Ex1.m1.2.2.2.2.2.1.2" xref="S2.Ex1.m1.2.2.2.2.2.1.2.cmml">i</mi><mo id="S2.Ex1.m1.2.2.2.2.2.1.1" xref="S2.Ex1.m1.2.2.2.2.2.1.1.cmml">&gt;</mo><mrow id="S2.Ex1.m1.2.2.2.2.2.1.3" xref="S2.Ex1.m1.2.2.2.2.2.1.3.cmml"><mi id="S2.Ex1.m1.2.2.2.2.2.1.3.2" xref="S2.Ex1.m1.2.2.2.2.2.1.3.2.cmml">L</mi><mo id="S2.Ex1.m1.2.2.2.2.2.1.3.1" xref="S2.Ex1.m1.2.2.2.2.2.1.3.1.cmml">/</mo><mn id="S2.Ex1.m1.2.2.2.2.2.1.3.3" xref="S2.Ex1.m1.2.2.2.2.2.1.3.3.cmml">2</mn></mrow></mrow></mtd></mtr><mtr id="S2.Ex1.m1.4.4.4d" xref="S2.Ex1.m1.4.5.3.1.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.Ex1.m1.4.4.4e" xref="S2.Ex1.m1.4.5.3.1.cmml"><mrow id="S2.Ex1.m1.3.3.3.3.1.1" xref="S2.Ex1.m1.3.3.3.3.1.1.cmml"><mo id="S2.Ex1.m1.3.3.3.3.1.1a" xref="S2.Ex1.m1.3.3.3.3.1.1.cmml">−</mo><mrow id="S2.Ex1.m1.3.3.3.3.1.1.2" xref="S2.Ex1.m1.3.3.3.3.1.1.2.cmml"><mrow id="S2.Ex1.m1.3.3.3.3.1.1.2.2" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.cmml"><mi id="S2.Ex1.m1.3.3.3.3.1.1.2.2.2" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.2.cmml">i</mi><mo id="S2.Ex1.m1.3.3.3.3.1.1.2.2.1" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.1.cmml">⁢</mo><msub id="S2.Ex1.m1.3.3.3.3.1.1.2.2.3" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.cmml"><mi id="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.2" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.2.cmml">x</mi><mi id="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.3" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.3.cmml">i</mi></msub></mrow><mo id="S2.Ex1.m1.3.3.3.3.1.1.2.1" xref="S2.Ex1.m1.3.3.3.3.1.1.2.1.cmml">/</mo><mi id="S2.Ex1.m1.3.3.3.3.1.1.2.3" xref="S2.Ex1.m1.3.3.3.3.1.1.2.3.cmml">L</mi></mrow></mrow></mtd><mtd class="ltx_align_left" columnalign="left" id="S2.Ex1.m1.4.4.4f" xref="S2.Ex1.m1.4.5.3.1.cmml"><mtext id="S2.Ex1.m1.4.4.4.4.2.1" xref="S2.Ex1.m1.4.4.4.4.2.1a.cmml">otherwise</mtext></mtd></mtr></mtable></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.Ex1.m1.4b"><apply id="S2.Ex1.m1.4.5.cmml" xref="S2.Ex1.m1.4.5"><eq id="S2.Ex1.m1.4.5.1.cmml" xref="S2.Ex1.m1.4.5.1"></eq><apply id="S2.Ex1.m1.4.5.2.cmml" xref="S2.Ex1.m1.4.5.2"><csymbol cd="ambiguous" id="S2.Ex1.m1.4.5.2.1.cmml" xref="S2.Ex1.m1.4.5.2">subscript</csymbol><ci id="S2.Ex1.m1.4.5.2.2.cmml" xref="S2.Ex1.m1.4.5.2.2">𝑠</ci><ci id="S2.Ex1.m1.4.5.2.3.cmml" xref="S2.Ex1.m1.4.5.2.3">𝑖</ci></apply><apply id="S2.Ex1.m1.4.5.3.1.cmml" xref="S2.Ex1.m1.4.4"><csymbol cd="latexml" id="S2.Ex1.m1.4.5.3.1.1.cmml" xref="S2.Ex1.m1.4.4.5">cases</csymbol><apply id="S2.Ex1.m1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1"><divide id="S2.Ex1.m1.1.1.1.1.1.1.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.1"></divide><apply id="S2.Ex1.m1.1.1.1.1.1.1.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2"><times id="S2.Ex1.m1.1.1.1.1.1.1.2.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.1"></times><ci id="S2.Ex1.m1.1.1.1.1.1.1.2.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.2">𝑖</ci><apply id="S2.Ex1.m1.1.1.1.1.1.1.2.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3">subscript</csymbol><ci id="S2.Ex1.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3.2">𝑥</ci><ci id="S2.Ex1.m1.1.1.1.1.1.1.2.3.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.2.3.3">𝑖</ci></apply></apply><ci id="S2.Ex1.m1.1.1.1.1.1.1.3.cmml" xref="S2.Ex1.m1.1.1.1.1.1.1.3">𝐿</ci></apply><apply id="S2.Ex1.m1.2.2.2.2.2.1.cmml" xref="S2.Ex1.m1.2.2.2.2.2.1"><gt id="S2.Ex1.m1.2.2.2.2.2.1.1.cmml" xref="S2.Ex1.m1.2.2.2.2.2.1.1"></gt><ci id="S2.Ex1.m1.2.2.2.2.2.1.2.cmml" xref="S2.Ex1.m1.2.2.2.2.2.1.2">𝑖</ci><apply id="S2.Ex1.m1.2.2.2.2.2.1.3.cmml" xref="S2.Ex1.m1.2.2.2.2.2.1.3"><divide id="S2.Ex1.m1.2.2.2.2.2.1.3.1.cmml" xref="S2.Ex1.m1.2.2.2.2.2.1.3.1"></divide><ci id="S2.Ex1.m1.2.2.2.2.2.1.3.2.cmml" xref="S2.Ex1.m1.2.2.2.2.2.1.3.2">𝐿</ci><cn id="S2.Ex1.m1.2.2.2.2.2.1.3.3.cmml" type="integer" xref="S2.Ex1.m1.2.2.2.2.2.1.3.3">2</cn></apply></apply><apply id="S2.Ex1.m1.3.3.3.3.1.1.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1"><minus id="S2.Ex1.m1.3.3.3.3.1.1.1.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1"></minus><apply id="S2.Ex1.m1.3.3.3.3.1.1.2.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2"><divide id="S2.Ex1.m1.3.3.3.3.1.1.2.1.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.1"></divide><apply id="S2.Ex1.m1.3.3.3.3.1.1.2.2.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2"><times id="S2.Ex1.m1.3.3.3.3.1.1.2.2.1.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.1"></times><ci id="S2.Ex1.m1.3.3.3.3.1.1.2.2.2.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.2">𝑖</ci><apply id="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.3"><csymbol cd="ambiguous" id="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.1.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.3">subscript</csymbol><ci id="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.2.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.2">𝑥</ci><ci id="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.3.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.2.3.3">𝑖</ci></apply></apply><ci id="S2.Ex1.m1.3.3.3.3.1.1.2.3.cmml" xref="S2.Ex1.m1.3.3.3.3.1.1.2.3">𝐿</ci></apply></apply><ci id="S2.Ex1.m1.4.4.4.4.2.1a.cmml" xref="S2.Ex1.m1.4.4.4.4.2.1"><mtext id="S2.Ex1.m1.4.4.4.4.2.1.cmml" xref="S2.Ex1.m1.4.4.4.4.2.1">otherwise</mtext></ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.Ex1.m1.4c">s_{i}=\begin{cases}ix_{i}/L&amp;\quad i&gt;L/2\\
-ix_{i}/L&amp;\quad\text{otherwise}\end{cases}</annotation><annotation encoding="application/x-llamapun" id="S2.Ex1.m1.4d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT = { start_ROW start_CELL italic_i italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_L end_CELL start_CELL italic_i &gt; italic_L / 2 end_CELL end_ROW start_ROW start_CELL - italic_i italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT / italic_L end_CELL start_CELL otherwise end_CELL end_ROW</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
</tr></tbody>
</table>
<p class="ltx_p" id="S2.SS0.SSS0.Px1.p1.29">which is maximized by placing <math alttext="0" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.4.m1.1"><semantics id="S2.SS0.SSS0.Px1.p1.4.m1.1a"><mn id="S2.SS0.SSS0.Px1.p1.4.m1.1.1" xref="S2.SS0.SSS0.Px1.p1.4.m1.1.1.cmml">0</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.4.m1.1b"><cn id="S2.SS0.SSS0.Px1.p1.4.m1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.4.m1.1.1">0</cn></annotation-xml></semantics></math>’s in the first <math alttext="L/2" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.5.m2.1"><semantics id="S2.SS0.SSS0.Px1.p1.5.m2.1a"><mrow id="S2.SS0.SSS0.Px1.p1.5.m2.1.1" xref="S2.SS0.SSS0.Px1.p1.5.m2.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.5.m2.1.1.2" xref="S2.SS0.SSS0.Px1.p1.5.m2.1.1.2.cmml">L</mi><mo id="S2.SS0.SSS0.Px1.p1.5.m2.1.1.1" xref="S2.SS0.SSS0.Px1.p1.5.m2.1.1.1.cmml">/</mo><mn id="S2.SS0.SSS0.Px1.p1.5.m2.1.1.3" xref="S2.SS0.SSS0.Px1.p1.5.m2.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.5.m2.1b"><apply id="S2.SS0.SSS0.Px1.p1.5.m2.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m2.1.1"><divide id="S2.SS0.SSS0.Px1.p1.5.m2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m2.1.1.1"></divide><ci id="S2.SS0.SSS0.Px1.p1.5.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.5.m2.1.1.2">𝐿</ci><cn id="S2.SS0.SSS0.Px1.p1.5.m2.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.5.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.5.m2.1c">L/2</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.5.m2.1d">italic_L / 2</annotation></semantics></math> positions followed by <math alttext="1" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.6.m3.1"><semantics id="S2.SS0.SSS0.Px1.p1.6.m3.1a"><mn id="S2.SS0.SSS0.Px1.p1.6.m3.1.1" xref="S2.SS0.SSS0.Px1.p1.6.m3.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.6.m3.1b"><cn id="S2.SS0.SSS0.Px1.p1.6.m3.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.6.m3.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.6.m3.1c">1</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.6.m3.1d">1</annotation></semantics></math>’s in the last <math alttext="L/2" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.7.m4.1"><semantics id="S2.SS0.SSS0.Px1.p1.7.m4.1a"><mrow id="S2.SS0.SSS0.Px1.p1.7.m4.1.1" xref="S2.SS0.SSS0.Px1.p1.7.m4.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.7.m4.1.1.2" xref="S2.SS0.SSS0.Px1.p1.7.m4.1.1.2.cmml">L</mi><mo id="S2.SS0.SSS0.Px1.p1.7.m4.1.1.1" xref="S2.SS0.SSS0.Px1.p1.7.m4.1.1.1.cmml">/</mo><mn id="S2.SS0.SSS0.Px1.p1.7.m4.1.1.3" xref="S2.SS0.SSS0.Px1.p1.7.m4.1.1.3.cmml">2</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.7.m4.1b"><apply id="S2.SS0.SSS0.Px1.p1.7.m4.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.7.m4.1.1"><divide id="S2.SS0.SSS0.Px1.p1.7.m4.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.7.m4.1.1.1"></divide><ci id="S2.SS0.SSS0.Px1.p1.7.m4.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.7.m4.1.1.2">𝐿</ci><cn id="S2.SS0.SSS0.Px1.p1.7.m4.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.7.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.7.m4.1c">L/2</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.7.m4.1d">italic_L / 2</annotation></semantics></math> positions (for even <math alttext="L" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.8.m5.1"><semantics id="S2.SS0.SSS0.Px1.p1.8.m5.1a"><mi id="S2.SS0.SSS0.Px1.p1.8.m5.1.1" xref="S2.SS0.SSS0.Px1.p1.8.m5.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.8.m5.1b"><ci id="S2.SS0.SSS0.Px1.p1.8.m5.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.8.m5.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.8.m5.1c">L</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.8.m5.1d">italic_L</annotation></semantics></math>). To explore the state space, we use a Metropolis sampler with block size <math alttext="L" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.9.m6.1"><semantics id="S2.SS0.SSS0.Px1.p1.9.m6.1a"><mi id="S2.SS0.SSS0.Px1.p1.9.m6.1.1" xref="S2.SS0.SSS0.Px1.p1.9.m6.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.9.m6.1b"><ci id="S2.SS0.SSS0.Px1.p1.9.m6.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.9.m6.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.9.m6.1c">L</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.9.m6.1d">italic_L</annotation></semantics></math> that flips a fair coin for each position.
We consider the space of sequences of length <math alttext="L=16" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.10.m7.1"><semantics id="S2.SS0.SSS0.Px1.p1.10.m7.1a"><mrow id="S2.SS0.SSS0.Px1.p1.10.m7.1.1" xref="S2.SS0.SSS0.Px1.p1.10.m7.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.10.m7.1.1.2" xref="S2.SS0.SSS0.Px1.p1.10.m7.1.1.2.cmml">L</mi><mo id="S2.SS0.SSS0.Px1.p1.10.m7.1.1.1" xref="S2.SS0.SSS0.Px1.p1.10.m7.1.1.1.cmml">=</mo><mn id="S2.SS0.SSS0.Px1.p1.10.m7.1.1.3" xref="S2.SS0.SSS0.Px1.p1.10.m7.1.1.3.cmml">16</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.10.m7.1b"><apply id="S2.SS0.SSS0.Px1.p1.10.m7.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.10.m7.1.1"><eq id="S2.SS0.SSS0.Px1.p1.10.m7.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.10.m7.1.1.1"></eq><ci id="S2.SS0.SSS0.Px1.p1.10.m7.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.10.m7.1.1.2">𝐿</ci><cn id="S2.SS0.SSS0.Px1.p1.10.m7.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.10.m7.1.1.3">16</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.10.m7.1c">L=16</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.10.m7.1d">italic_L = 16</annotation></semantics></math>, which has a maximum reward of <math alttext="314.2" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.11.m8.1"><semantics id="S2.SS0.SSS0.Px1.p1.11.m8.1a"><mn id="S2.SS0.SSS0.Px1.p1.11.m8.1.1" xref="S2.SS0.SSS0.Px1.p1.11.m8.1.1.cmml">314.2</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.11.m8.1b"><cn id="S2.SS0.SSS0.Px1.p1.11.m8.1.1.cmml" type="float" xref="S2.SS0.SSS0.Px1.p1.11.m8.1.1">314.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.11.m8.1c">314.2</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.11.m8.1d">314.2</annotation></semantics></math>. Starting from the initial state, we run the Metropolis sampler for <math alttext="10000" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.12.m9.1"><semantics id="S2.SS0.SSS0.Px1.p1.12.m9.1a"><mn id="S2.SS0.SSS0.Px1.p1.12.m9.1.1" xref="S2.SS0.SSS0.Px1.p1.12.m9.1.1.cmml">10000</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.12.m9.1b"><cn id="S2.SS0.SSS0.Px1.p1.12.m9.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.12.m9.1.1">10000</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.12.m9.1c">10000</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.12.m9.1d">10000</annotation></semantics></math> steps. The sampler had an acceptance rate of <math alttext="43.7\%" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.13.m10.1"><semantics id="S2.SS0.SSS0.Px1.p1.13.m10.1a"><mrow id="S2.SS0.SSS0.Px1.p1.13.m10.1.1" xref="S2.SS0.SSS0.Px1.p1.13.m10.1.1.cmml"><mn id="S2.SS0.SSS0.Px1.p1.13.m10.1.1.2" xref="S2.SS0.SSS0.Px1.p1.13.m10.1.1.2.cmml">43.7</mn><mo id="S2.SS0.SSS0.Px1.p1.13.m10.1.1.1" xref="S2.SS0.SSS0.Px1.p1.13.m10.1.1.1.cmml">%</mo></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.13.m10.1b"><apply id="S2.SS0.SSS0.Px1.p1.13.m10.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m10.1.1"><csymbol cd="latexml" id="S2.SS0.SSS0.Px1.p1.13.m10.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.13.m10.1.1.1">percent</csymbol><cn id="S2.SS0.SSS0.Px1.p1.13.m10.1.1.2.cmml" type="float" xref="S2.SS0.SSS0.Px1.p1.13.m10.1.1.2">43.7</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.13.m10.1c">43.7\%</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.13.m10.1d">43.7 %</annotation></semantics></math> and the highest achieved reward was <math alttext="244.7" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.14.m11.1"><semantics id="S2.SS0.SSS0.Px1.p1.14.m11.1a"><mn id="S2.SS0.SSS0.Px1.p1.14.m11.1.1" xref="S2.SS0.SSS0.Px1.p1.14.m11.1.1.cmml">244.7</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.14.m11.1b"><cn id="S2.SS0.SSS0.Px1.p1.14.m11.1.1.cmml" type="float" xref="S2.SS0.SSS0.Px1.p1.14.m11.1.1">244.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.14.m11.1c">244.7</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.14.m11.1d">244.7</annotation></semantics></math>. Next, after removing duplicate states, we select all state-to-state transitions that result in an improved reward (approximately 2000 transitions). This data is used to train a sequence-to-sequence model <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.15.m12.1"><semantics id="S2.SS0.SSS0.Px1.p1.15.m12.1a"><msub id="S2.SS0.SSS0.Px1.p1.15.m12.1.1" xref="S2.SS0.SSS0.Px1.p1.15.m12.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.15.m12.1.1.2" xref="S2.SS0.SSS0.Px1.p1.15.m12.1.1.2.cmml">q</mi><mi id="S2.SS0.SSS0.Px1.p1.15.m12.1.1.3" xref="S2.SS0.SSS0.Px1.p1.15.m12.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.15.m12.1b"><apply id="S2.SS0.SSS0.Px1.p1.15.m12.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.15.m12.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.15.m12.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.15.m12.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.15.m12.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.15.m12.1.1.2">𝑞</ci><ci id="S2.SS0.SSS0.Px1.p1.15.m12.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.15.m12.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.15.m12.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.15.m12.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> parametrized as a two-layer multi-layer perceptron (MLP) <span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>We use hidden dimensions <math alttext="16" class="ltx_Math" display="inline" id="footnote2.m1.1"><semantics id="footnote2.m1.1b"><mn id="footnote2.m1.1.1" xref="footnote2.m1.1.1.cmml">16</mn><annotation-xml encoding="MathML-Content" id="footnote2.m1.1c"><cn id="footnote2.m1.1.1.cmml" type="integer" xref="footnote2.m1.1.1">16</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m1.1d">16</annotation><annotation encoding="application/x-llamapun" id="footnote2.m1.1e">16</annotation></semantics></math> for the embedding matrix and two <math alttext="128" class="ltx_Math" display="inline" id="footnote2.m2.1"><semantics id="footnote2.m2.1b"><mn id="footnote2.m2.1.1" xref="footnote2.m2.1.1.cmml">128</mn><annotation-xml encoding="MathML-Content" id="footnote2.m2.1c"><cn id="footnote2.m2.1.1.cmml" type="integer" xref="footnote2.m2.1.1">128</cn></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m2.1d">128</annotation><annotation encoding="application/x-llamapun" id="footnote2.m2.1e">128</annotation></semantics></math> dimensional layers with <span class="ltx_text ltx_font_typewriter" id="footnote2.1">relu</span> activations. The MLP is fit to the selected transitions using a multi-label sigmoid cross-entropy loss for 20 epochs using an Adam optimizer with <math alttext="1e^{-2}" class="ltx_Math" display="inline" id="footnote2.m3.1"><semantics id="footnote2.m3.1b"><mrow id="footnote2.m3.1.1" xref="footnote2.m3.1.1.cmml"><mn id="footnote2.m3.1.1.2" xref="footnote2.m3.1.1.2.cmml">1</mn><mo id="footnote2.m3.1.1.1" xref="footnote2.m3.1.1.1.cmml">⁢</mo><msup id="footnote2.m3.1.1.3" xref="footnote2.m3.1.1.3.cmml"><mi id="footnote2.m3.1.1.3.2" xref="footnote2.m3.1.1.3.2.cmml">e</mi><mrow id="footnote2.m3.1.1.3.3" xref="footnote2.m3.1.1.3.3.cmml"><mo id="footnote2.m3.1.1.3.3b" xref="footnote2.m3.1.1.3.3.cmml">−</mo><mn id="footnote2.m3.1.1.3.3.2" xref="footnote2.m3.1.1.3.3.2.cmml">2</mn></mrow></msup></mrow><annotation-xml encoding="MathML-Content" id="footnote2.m3.1c"><apply id="footnote2.m3.1.1.cmml" xref="footnote2.m3.1.1"><times id="footnote2.m3.1.1.1.cmml" xref="footnote2.m3.1.1.1"></times><cn id="footnote2.m3.1.1.2.cmml" type="integer" xref="footnote2.m3.1.1.2">1</cn><apply id="footnote2.m3.1.1.3.cmml" xref="footnote2.m3.1.1.3"><csymbol cd="ambiguous" id="footnote2.m3.1.1.3.1.cmml" xref="footnote2.m3.1.1.3">superscript</csymbol><ci id="footnote2.m3.1.1.3.2.cmml" xref="footnote2.m3.1.1.3.2">𝑒</ci><apply id="footnote2.m3.1.1.3.3.cmml" xref="footnote2.m3.1.1.3.3"><minus id="footnote2.m3.1.1.3.3.1.cmml" xref="footnote2.m3.1.1.3.3"></minus><cn id="footnote2.m3.1.1.3.3.2.cmml" type="integer" xref="footnote2.m3.1.1.3.3.2">2</cn></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="footnote2.m3.1d">1e^{-2}</annotation><annotation encoding="application/x-llamapun" id="footnote2.m3.1e">1 italic_e start_POSTSUPERSCRIPT - 2 end_POSTSUPERSCRIPT</annotation></semantics></math> learning rate.</span></span></span>. Finally, <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.16.m13.1"><semantics id="S2.SS0.SSS0.Px1.p1.16.m13.1a"><msub id="S2.SS0.SSS0.Px1.p1.16.m13.1.1" xref="S2.SS0.SSS0.Px1.p1.16.m13.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.16.m13.1.1.2" xref="S2.SS0.SSS0.Px1.p1.16.m13.1.1.2.cmml">q</mi><mi id="S2.SS0.SSS0.Px1.p1.16.m13.1.1.3" xref="S2.SS0.SSS0.Px1.p1.16.m13.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.16.m13.1b"><apply id="S2.SS0.SSS0.Px1.p1.16.m13.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.16.m13.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.16.m13.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.16.m13.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.16.m13.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.16.m13.1.1.2">𝑞</ci><ci id="S2.SS0.SSS0.Px1.p1.16.m13.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.16.m13.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.16.m13.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.16.m13.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> was iteratively applied starting at <math alttext="x_{0}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.17.m14.1"><semantics id="S2.SS0.SSS0.Px1.p1.17.m14.1a"><msub id="S2.SS0.SSS0.Px1.p1.17.m14.1.1" xref="S2.SS0.SSS0.Px1.p1.17.m14.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.17.m14.1.1.2" xref="S2.SS0.SSS0.Px1.p1.17.m14.1.1.2.cmml">x</mi><mn id="S2.SS0.SSS0.Px1.p1.17.m14.1.1.3" xref="S2.SS0.SSS0.Px1.p1.17.m14.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.17.m14.1b"><apply id="S2.SS0.SSS0.Px1.p1.17.m14.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.17.m14.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.17.m14.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.17.m14.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.17.m14.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.17.m14.1.1.2">𝑥</ci><cn id="S2.SS0.SSS0.Px1.p1.17.m14.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.17.m14.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.17.m14.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.17.m14.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> five times to produce a sequences of states <math alttext="x^{(1)}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.18.m15.1"><semantics id="S2.SS0.SSS0.Px1.p1.18.m15.1a"><msup id="S2.SS0.SSS0.Px1.p1.18.m15.1.2" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.18.m15.1.2.2" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.2.2.cmml">x</mi><mrow id="S2.SS0.SSS0.Px1.p1.18.m15.1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.2.cmml"><mo id="S2.SS0.SSS0.Px1.p1.18.m15.1.1.1.3.1" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.2.cmml">(</mo><mn id="S2.SS0.SSS0.Px1.p1.18.m15.1.1.1.1" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.1.1.1.cmml">1</mn><mo id="S2.SS0.SSS0.Px1.p1.18.m15.1.1.1.3.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.18.m15.1b"><apply id="S2.SS0.SSS0.Px1.p1.18.m15.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.18.m15.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.2">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.18.m15.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.2.2">𝑥</ci><cn id="S2.SS0.SSS0.Px1.p1.18.m15.1.1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.18.m15.1.1.1.1">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.18.m15.1c">x^{(1)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.18.m15.1d">italic_x start_POSTSUPERSCRIPT ( 1 ) end_POSTSUPERSCRIPT</annotation></semantics></math>, <math alttext="x^{(2)}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.19.m16.1"><semantics id="S2.SS0.SSS0.Px1.p1.19.m16.1a"><msup id="S2.SS0.SSS0.Px1.p1.19.m16.1.2" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.19.m16.1.2.2" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.2.2.cmml">x</mi><mrow id="S2.SS0.SSS0.Px1.p1.19.m16.1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.2.cmml"><mo id="S2.SS0.SSS0.Px1.p1.19.m16.1.1.1.3.1" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.2.cmml">(</mo><mn id="S2.SS0.SSS0.Px1.p1.19.m16.1.1.1.1" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.1.1.1.cmml">2</mn><mo id="S2.SS0.SSS0.Px1.p1.19.m16.1.1.1.3.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.19.m16.1b"><apply id="S2.SS0.SSS0.Px1.p1.19.m16.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.19.m16.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.2">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.19.m16.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.2.2">𝑥</ci><cn id="S2.SS0.SSS0.Px1.p1.19.m16.1.1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.19.m16.1.1.1.1">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.19.m16.1c">x^{(2)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.19.m16.1d">italic_x start_POSTSUPERSCRIPT ( 2 ) end_POSTSUPERSCRIPT</annotation></semantics></math>, …, <math alttext="x^{(5)}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.20.m17.1"><semantics id="S2.SS0.SSS0.Px1.p1.20.m17.1a"><msup id="S2.SS0.SSS0.Px1.p1.20.m17.1.2" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.2.cmml"><mi id="S2.SS0.SSS0.Px1.p1.20.m17.1.2.2" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.2.2.cmml">x</mi><mrow id="S2.SS0.SSS0.Px1.p1.20.m17.1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.2.cmml"><mo id="S2.SS0.SSS0.Px1.p1.20.m17.1.1.1.3.1" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.2.cmml">(</mo><mn id="S2.SS0.SSS0.Px1.p1.20.m17.1.1.1.1" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.1.1.1.cmml">5</mn><mo id="S2.SS0.SSS0.Px1.p1.20.m17.1.1.1.3.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.2.cmml">)</mo></mrow></msup><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.20.m17.1b"><apply id="S2.SS0.SSS0.Px1.p1.20.m17.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.20.m17.1.2.1.cmml" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.2">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.20.m17.1.2.2.cmml" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.2.2">𝑥</ci><cn id="S2.SS0.SSS0.Px1.p1.20.m17.1.1.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.20.m17.1.1.1.1">5</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.20.m17.1c">x^{(5)}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.20.m17.1d">italic_x start_POSTSUPERSCRIPT ( 5 ) end_POSTSUPERSCRIPT</annotation></semantics></math> where <math alttext="x^{(t)}=q_{\theta}(x^{(t-1)})" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.21.m18.3"><semantics id="S2.SS0.SSS0.Px1.p1.21.m18.3a"><mrow id="S2.SS0.SSS0.Px1.p1.21.m18.3.3" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.cmml"><msup id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.2" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.2.cmml">x</mi><mrow id="S2.SS0.SSS0.Px1.p1.21.m18.1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.cmml"><mo id="S2.SS0.SSS0.Px1.p1.21.m18.1.1.1.3.1" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.cmml">(</mo><mi id="S2.SS0.SSS0.Px1.p1.21.m18.1.1.1.1" xref="S2.SS0.SSS0.Px1.p1.21.m18.1.1.1.1.cmml">t</mi><mo id="S2.SS0.SSS0.Px1.p1.21.m18.1.1.1.3.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.cmml">)</mo></mrow></msup><mo id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.2" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.2.cmml">=</mo><mrow id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.cmml"><msub id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.cmml"><mi id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.2" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.2.cmml">q</mi><mi id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.3" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.3.cmml">θ</mi></msub><mo id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.2" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.2.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.cmml">(</mo><msup id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.2" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.2.cmml">x</mi><mrow id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.2" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.cmml">(</mo><mrow id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.2" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.2.cmml">t</mi><mo id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.1" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.1.cmml">−</mo><mn id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.3" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.3.cmml">1</mn></mrow><mo id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.3" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.cmml">)</mo></mrow></msup><mo id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.3" stretchy="false" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.21.m18.3b"><apply id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3"><eq id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.2"></eq><apply id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.3.2">𝑥</ci><ci id="S2.SS0.SSS0.Px1.p1.21.m18.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.1.1.1.1">𝑡</ci></apply><apply id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1"><times id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.2"></times><apply id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.1.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.2.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.2">𝑞</ci><ci id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.3.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.3.3">𝜃</ci></apply><apply id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1">superscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.3.3.1.1.1.1.2">𝑥</ci><apply id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1"><minus id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.1"></minus><ci id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.2">𝑡</ci><cn id="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.21.m18.2.2.1.1.1.3">1</cn></apply></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.21.m18.3c">x^{(t)}=q_{\theta}(x^{(t-1)})</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.21.m18.3d">italic_x start_POSTSUPERSCRIPT ( italic_t ) end_POSTSUPERSCRIPT = italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT ( italic_x start_POSTSUPERSCRIPT ( italic_t - 1 ) end_POSTSUPERSCRIPT )</annotation></semantics></math> and predictions from <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.22.m19.1"><semantics id="S2.SS0.SSS0.Px1.p1.22.m19.1a"><msub id="S2.SS0.SSS0.Px1.p1.22.m19.1.1" xref="S2.SS0.SSS0.Px1.p1.22.m19.1.1.cmml"><mi id="S2.SS0.SSS0.Px1.p1.22.m19.1.1.2" xref="S2.SS0.SSS0.Px1.p1.22.m19.1.1.2.cmml">q</mi><mi id="S2.SS0.SSS0.Px1.p1.22.m19.1.1.3" xref="S2.SS0.SSS0.Px1.p1.22.m19.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.22.m19.1b"><apply id="S2.SS0.SSS0.Px1.p1.22.m19.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.22.m19.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px1.p1.22.m19.1.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.22.m19.1.1">subscript</csymbol><ci id="S2.SS0.SSS0.Px1.p1.22.m19.1.1.2.cmml" xref="S2.SS0.SSS0.Px1.p1.22.m19.1.1.2">𝑞</ci><ci id="S2.SS0.SSS0.Px1.p1.22.m19.1.1.3.cmml" xref="S2.SS0.SSS0.Px1.p1.22.m19.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.22.m19.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.22.m19.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> are obtained deterministically by decoding all <math alttext="L" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.23.m20.1"><semantics id="S2.SS0.SSS0.Px1.p1.23.m20.1a"><mi id="S2.SS0.SSS0.Px1.p1.23.m20.1.1" xref="S2.SS0.SSS0.Px1.p1.23.m20.1.1.cmml">L</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.23.m20.1b"><ci id="S2.SS0.SSS0.Px1.p1.23.m20.1.1.cmml" xref="S2.SS0.SSS0.Px1.p1.23.m20.1.1">𝐿</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.23.m20.1c">L</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.23.m20.1d">italic_L</annotation></semantics></math> positions in parallel. Our model achieved the following sequence of rewards: <math alttext="1" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.24.m21.1"><semantics id="S2.SS0.SSS0.Px1.p1.24.m21.1a"><mn id="S2.SS0.SSS0.Px1.p1.24.m21.1.1" xref="S2.SS0.SSS0.Px1.p1.24.m21.1.1.cmml">1</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.24.m21.1b"><cn id="S2.SS0.SSS0.Px1.p1.24.m21.1.1.cmml" type="integer" xref="S2.SS0.SSS0.Px1.p1.24.m21.1.1">1</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.24.m21.1c">1</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.24.m21.1d">1</annotation></semantics></math>, <math alttext="3.3" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.25.m22.1"><semantics id="S2.SS0.SSS0.Px1.p1.25.m22.1a"><mn id="S2.SS0.SSS0.Px1.p1.25.m22.1.1" xref="S2.SS0.SSS0.Px1.p1.25.m22.1.1.cmml">3.3</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.25.m22.1b"><cn id="S2.SS0.SSS0.Px1.p1.25.m22.1.1.cmml" type="float" xref="S2.SS0.SSS0.Px1.p1.25.m22.1.1">3.3</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.25.m22.1c">3.3</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.25.m22.1d">3.3</annotation></semantics></math>, <math alttext="15.6" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.26.m23.1"><semantics id="S2.SS0.SSS0.Px1.p1.26.m23.1a"><mn id="S2.SS0.SSS0.Px1.p1.26.m23.1.1" xref="S2.SS0.SSS0.Px1.p1.26.m23.1.1.cmml">15.6</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.26.m23.1b"><cn id="S2.SS0.SSS0.Px1.p1.26.m23.1.1.cmml" type="float" xref="S2.SS0.SSS0.Px1.p1.26.m23.1.1">15.6</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.26.m23.1c">15.6</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.26.m23.1d">15.6</annotation></semantics></math>, <math alttext="314.2" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.27.m24.1"><semantics id="S2.SS0.SSS0.Px1.p1.27.m24.1a"><mn id="S2.SS0.SSS0.Px1.p1.27.m24.1.1" xref="S2.SS0.SSS0.Px1.p1.27.m24.1.1.cmml">314.2</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.27.m24.1b"><cn id="S2.SS0.SSS0.Px1.p1.27.m24.1.1.cmml" type="float" xref="S2.SS0.SSS0.Px1.p1.27.m24.1.1">314.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.27.m24.1c">314.2</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.27.m24.1d">314.2</annotation></semantics></math>, <math alttext="314.2" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.28.m25.1"><semantics id="S2.SS0.SSS0.Px1.p1.28.m25.1a"><mn id="S2.SS0.SSS0.Px1.p1.28.m25.1.1" xref="S2.SS0.SSS0.Px1.p1.28.m25.1.1.cmml">314.2</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.28.m25.1b"><cn id="S2.SS0.SSS0.Px1.p1.28.m25.1.1.cmml" type="float" xref="S2.SS0.SSS0.Px1.p1.28.m25.1.1">314.2</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.28.m25.1c">314.2</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.28.m25.1d">314.2</annotation></semantics></math>. Thus, in fewer than five steps, the trained model successfully extrapolates beyond the <math alttext="244.7" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px1.p1.29.m26.1"><semantics id="S2.SS0.SSS0.Px1.p1.29.m26.1a"><mn id="S2.SS0.SSS0.Px1.p1.29.m26.1.1" xref="S2.SS0.SSS0.Px1.p1.29.m26.1.1.cmml">244.7</mn><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px1.p1.29.m26.1b"><cn id="S2.SS0.SSS0.Px1.p1.29.m26.1.1.cmml" type="float" xref="S2.SS0.SSS0.Px1.p1.29.m26.1.1">244.7</cn></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px1.p1.29.m26.1c">244.7</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px1.p1.29.m26.1d">244.7</annotation></semantics></math> state achieved by the MCMC search and achieves the optimum value.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Oracle scoring</h4>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p1.13">To assess the quality of any given sample, we assume access to an oracle function <math alttext="\text{oracle}(x)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS0.SSS0.Px2.p1.1.m1.1a"><mrow id="S2.SS0.SSS0.Px2.p1.1.m1.1.2" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.cmml"><mtext id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.2" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.2a.cmml">oracle</mtext><mo id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.3.2" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.cmml"><mo id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2"><times id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.1"></times><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.2a.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.2"><mtext id="S2.SS0.SSS0.Px2.p1.1.m1.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.2.2">oracle</mtext></ci><ci id="S2.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.1.m1.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.1.m1.1c">\text{oracle}(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.1.m1.1d">oracle ( italic_x )</annotation></semantics></math> which can assign a scalar score to any sample <math alttext="x" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS0.SSS0.Px2.p1.2.m2.1a"><mi id="S2.SS0.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.2.m2.1b"><ci id="S2.SS0.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.2.m2.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.2.m2.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.2.m2.1d">italic_x</annotation></semantics></math>. In the toy example above, the score <math alttext="s(x)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.3.m3.1"><semantics id="S2.SS0.SSS0.Px2.p1.3.m3.1a"><mrow id="S2.SS0.SSS0.Px2.p1.3.m3.1.2" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2.cmml"><mi id="S2.SS0.SSS0.Px2.p1.3.m3.1.2.2" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2.2.cmml">s</mi><mo id="S2.SS0.SSS0.Px2.p1.3.m3.1.2.1" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p1.3.m3.1.2.3.2" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2.cmml"><mo id="S2.SS0.SSS0.Px2.p1.3.m3.1.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p1.3.m3.1.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.3.m3.1b"><apply id="S2.SS0.SSS0.Px2.p1.3.m3.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2"><times id="S2.SS0.SSS0.Px2.p1.3.m3.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2.1"></times><ci id="S2.SS0.SSS0.Px2.p1.3.m3.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.2.2">𝑠</ci><ci id="S2.SS0.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.3.m3.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.3.m3.1c">s(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.3.m3.1d">italic_s ( italic_x )</annotation></semantics></math> is equivalent to an oracle scorer which can calculate the true reward. However, in general, there may not be an efficient way to score sequences. For example, assessing a novel sequence may require conducting physical experiments or running expensive simulations, as in the protein task described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.SS1" title="3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3.1</span></a>). Given a candidate sequence <math alttext="x" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.4.m4.1"><semantics id="S2.SS0.SSS0.Px2.p1.4.m4.1a"><mi id="S2.SS0.SSS0.Px2.p1.4.m4.1.1" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.4.m4.1b"><ci id="S2.SS0.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.4.m4.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.4.m4.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.4.m4.1d">italic_x</annotation></semantics></math>, we assume that <math alttext="\text{oracle}(x)\in\mathcal{Y}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.5.m5.1"><semantics id="S2.SS0.SSS0.Px2.p1.5.m5.1a"><mrow id="S2.SS0.SSS0.Px2.p1.5.m5.1.2" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.cmml"><mrow id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.cmml"><mtext id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.2" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.2a.cmml">oracle</mtext><mo id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.1" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.3.2" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.cmml"><mo id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px2.p1.5.m5.1.1" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.cmml">)</mo></mrow></mrow><mo id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.1" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.1.cmml">∈</mo><mi class="ltx_font_mathcaligraphic" id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.3" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.3.cmml">𝒴</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.5.m5.1b"><apply id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2"><in id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.1"></in><apply id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2"><times id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.1"></times><ci id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.2a.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.2"><mtext id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.2.2">oracle</mtext></ci><ci id="S2.SS0.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.1">𝑥</ci></apply><ci id="S2.SS0.SSS0.Px2.p1.5.m5.1.2.3.cmml" xref="S2.SS0.SSS0.Px2.p1.5.m5.1.2.3">𝒴</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.5.m5.1c">\text{oracle}(x)\in\mathcal{Y}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.5.m5.1d">oracle ( italic_x ) ∈ caligraphic_Y</annotation></semantics></math> may be consulted to assess <math alttext="x" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.6.m6.1"><semantics id="S2.SS0.SSS0.Px2.p1.6.m6.1a"><mi id="S2.SS0.SSS0.Px2.p1.6.m6.1.1" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.6.m6.1b"><ci id="S2.SS0.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.6.m6.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.6.m6.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.6.m6.1d">italic_x</annotation></semantics></math>, but that it is expensive to consult frequently. We instead assume access to a <em class="ltx_emph ltx_font_italic" id="S2.SS0.SSS0.Px2.p1.13.1">guide</em> <math alttext="s(x)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.7.m7.1"><semantics id="S2.SS0.SSS0.Px2.p1.7.m7.1a"><mrow id="S2.SS0.SSS0.Px2.p1.7.m7.1.2" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2.cmml"><mi id="S2.SS0.SSS0.Px2.p1.7.m7.1.2.2" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2.2.cmml">s</mi><mo id="S2.SS0.SSS0.Px2.p1.7.m7.1.2.1" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p1.7.m7.1.2.3.2" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2.cmml"><mo id="S2.SS0.SSS0.Px2.p1.7.m7.1.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px2.p1.7.m7.1.1" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p1.7.m7.1.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.7.m7.1b"><apply id="S2.SS0.SSS0.Px2.p1.7.m7.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2"><times id="S2.SS0.SSS0.Px2.p1.7.m7.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2.1"></times><ci id="S2.SS0.SSS0.Px2.p1.7.m7.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.2.2">𝑠</ci><ci id="S2.SS0.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.7.m7.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.7.m7.1c">s(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.7.m7.1d">italic_s ( italic_x )</annotation></semantics></math> that provides a computationally tractable estimate <math alttext="s(x)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.8.m8.1"><semantics id="S2.SS0.SSS0.Px2.p1.8.m8.1a"><mrow id="S2.SS0.SSS0.Px2.p1.8.m8.1.2" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2.cmml"><mi id="S2.SS0.SSS0.Px2.p1.8.m8.1.2.2" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2.2.cmml">s</mi><mo id="S2.SS0.SSS0.Px2.p1.8.m8.1.2.1" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p1.8.m8.1.2.3.2" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2.cmml"><mo id="S2.SS0.SSS0.Px2.p1.8.m8.1.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px2.p1.8.m8.1.1" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p1.8.m8.1.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.8.m8.1b"><apply id="S2.SS0.SSS0.Px2.p1.8.m8.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2"><times id="S2.SS0.SSS0.Px2.p1.8.m8.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2.1"></times><ci id="S2.SS0.SSS0.Px2.p1.8.m8.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.2.2">𝑠</ci><ci id="S2.SS0.SSS0.Px2.p1.8.m8.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.8.m8.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.8.m8.1c">s(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.8.m8.1d">italic_s ( italic_x )</annotation></semantics></math> of <math alttext="\text{oracle}(x)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.9.m9.1"><semantics id="S2.SS0.SSS0.Px2.p1.9.m9.1a"><mrow id="S2.SS0.SSS0.Px2.p1.9.m9.1.2" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.cmml"><mtext id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.2" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.2a.cmml">oracle</mtext><mo id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.1" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.3.2" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.cmml"><mo id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px2.p1.9.m9.1.1" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.9.m9.1b"><apply id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2"><times id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.1"></times><ci id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.2a.cmml" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.2"><mtext id="S2.SS0.SSS0.Px2.p1.9.m9.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.2.2">oracle</mtext></ci><ci id="S2.SS0.SSS0.Px2.p1.9.m9.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.9.m9.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.9.m9.1c">\text{oracle}(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.9.m9.1d">oracle ( italic_x )</annotation></semantics></math>. For example, <math alttext="s(x)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.10.m10.1"><semantics id="S2.SS0.SSS0.Px2.p1.10.m10.1a"><mrow id="S2.SS0.SSS0.Px2.p1.10.m10.1.2" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2.cmml"><mi id="S2.SS0.SSS0.Px2.p1.10.m10.1.2.2" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2.2.cmml">s</mi><mo id="S2.SS0.SSS0.Px2.p1.10.m10.1.2.1" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p1.10.m10.1.2.3.2" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2.cmml"><mo id="S2.SS0.SSS0.Px2.p1.10.m10.1.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px2.p1.10.m10.1.1" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p1.10.m10.1.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.10.m10.1b"><apply id="S2.SS0.SSS0.Px2.p1.10.m10.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2"><times id="S2.SS0.SSS0.Px2.p1.10.m10.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2.1"></times><ci id="S2.SS0.SSS0.Px2.p1.10.m10.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.2.2">𝑠</ci><ci id="S2.SS0.SSS0.Px2.p1.10.m10.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.10.m10.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.10.m10.1c">s(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.10.m10.1d">italic_s ( italic_x )</annotation></semantics></math> may be a neural network trained to predict properties of <math alttext="x" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.11.m11.1"><semantics id="S2.SS0.SSS0.Px2.p1.11.m11.1a"><mi id="S2.SS0.SSS0.Px2.p1.11.m11.1.1" xref="S2.SS0.SSS0.Px2.p1.11.m11.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.11.m11.1b"><ci id="S2.SS0.SSS0.Px2.p1.11.m11.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.11.m11.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.11.m11.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.11.m11.1d">italic_x</annotation></semantics></math> based on a database of previous experiments with hypothesized sequences <math alttext="x" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.12.m12.1"><semantics id="S2.SS0.SSS0.Px2.p1.12.m12.1a"><mi id="S2.SS0.SSS0.Px2.p1.12.m12.1.1" xref="S2.SS0.SSS0.Px2.p1.12.m12.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.12.m12.1b"><ci id="S2.SS0.SSS0.Px2.p1.12.m12.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.12.m12.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.12.m12.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.12.m12.1d">italic_x</annotation></semantics></math> and measured outcomes <math alttext="\text{oracle}(x)" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p1.13.m13.1"><semantics id="S2.SS0.SSS0.Px2.p1.13.m13.1a"><mrow id="S2.SS0.SSS0.Px2.p1.13.m13.1.2" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.cmml"><mtext id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.2" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.2a.cmml">oracle</mtext><mo id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.1" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.1.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.3.2" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.cmml"><mo id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.3.2.1" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.cmml">(</mo><mi id="S2.SS0.SSS0.Px2.p1.13.m13.1.1" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.1.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.3.2.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p1.13.m13.1b"><apply id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.cmml" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2"><times id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.1"></times><ci id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.2a.cmml" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.2"><mtext id="S2.SS0.SSS0.Px2.p1.13.m13.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.2.2">oracle</mtext></ci><ci id="S2.SS0.SSS0.Px2.p1.13.m13.1.1.cmml" xref="S2.SS0.SSS0.Px2.p1.13.m13.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p1.13.m13.1c">\text{oracle}(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p1.13.m13.1d">oracle ( italic_x )</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S2.SS0.SSS0.Px2.p2">
<p class="ltx_p" id="S2.SS0.SSS0.Px2.p2.2">This guide will only be robust within the range of training values, meaning its ability to guide extrapolation may be limited. For instance, in the sentiment task, the guide is only robust in the training range consisting of ratings from 2 to 4 stars. Despite that, in that task our objective is to generate ratings in the extrapolation range consisting of ratings that are highly negative (1-star) or highly positive (5-star). At test time, we generate <math alttext="x^{\prime}\sim q_{\theta}" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p2.1.m1.1"><semantics id="S2.SS0.SSS0.Px2.p2.1.m1.1a"><mrow id="S2.SS0.SSS0.Px2.p2.1.m1.1.1" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.cmml"><msup id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml"><mi id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.2" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.2.cmml">x</mi><mo id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.3" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.3.cmml">′</mo></msup><mo id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.1" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml">∼</mo><msub id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.2" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.2.cmml">q</mi><mi id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.3" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.cmml">θ</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p2.1.m1.1b"><apply id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="latexml" id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.1">similar-to</csymbol><apply id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2">superscript</csymbol><ci id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.2">𝑥</ci><ci id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.3.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.2.3">′</ci></apply><apply id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3">subscript</csymbol><ci id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.2">𝑞</ci><ci id="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px2.p2.1.m1.1.1.3.3">𝜃</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p2.1.m1.1c">x^{\prime}\sim q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p2.1.m1.1d">italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∼ italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> and evaluate the true performance of the last sequence, <math alttext="\text{oracle}(x^{\prime}_{final})" class="ltx_Math" display="inline" id="S2.SS0.SSS0.Px2.p2.2.m2.1"><semantics id="S2.SS0.SSS0.Px2.p2.2.m2.1a"><mrow id="S2.SS0.SSS0.Px2.p2.2.m2.1.1" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.cmml"><mtext id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3a.cmml">oracle</mtext><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.2" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml">⁢</mo><mrow id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.cmml"><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.2" stretchy="false" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.cmml">(</mo><msubsup id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.cmml"><mi id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.2" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.2.cmml">x</mi><mrow id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.cmml"><mi id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.2" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.2.cmml">f</mi><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.3" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.3.cmml">i</mi><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1a" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.4" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.4.cmml">n</mi><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1b" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.5" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.5.cmml">a</mi><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1c" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1.cmml">⁢</mo><mi id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.6" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.6.cmml">l</mi></mrow><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.3" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.3.cmml">′</mo></msubsup><mo id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.3" stretchy="false" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS0.SSS0.Px2.p2.2.m2.1b"><apply id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1"><times id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.2"></times><ci id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3a.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3"><mtext id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.3">oracle</mtext></ci><apply id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.1.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1">subscript</csymbol><apply id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.1.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1">superscript</csymbol><ci id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.2.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.3.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.2.3">′</ci></apply><apply id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3"><times id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.1"></times><ci id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.2.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.2">𝑓</ci><ci id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.3.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.3">𝑖</ci><ci id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.4.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.4">𝑛</ci><ci id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.5.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.5">𝑎</ci><ci id="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.6.cmml" xref="S2.SS0.SSS0.Px2.p2.2.m2.1.1.1.1.1.3.6">𝑙</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS0.SSS0.Px2.p2.2.m2.1c">\text{oracle}(x^{\prime}_{final})</annotation><annotation encoding="application/x-llamapun" id="S2.SS0.SSS0.Px2.p2.2.m2.1d">oracle ( italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT start_POSTSUBSCRIPT italic_f italic_i italic_n italic_a italic_l end_POSTSUBSCRIPT )</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span>Generating Markov chains</h3>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Surrogate model</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px1.p1.6">As a target for MCMC, we use a surrogate model <math alttext="\ln p(x)=s(x)-\ln Z" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.1.m1.2"><semantics id="S2.SS1.SSS0.Px1.p1.1.m1.2a"><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.2.3" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.cmml"><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.cmml"><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.1.cmml">ln</mi><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2a" lspace="0.167em" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.cmml">⁡</mo><mi id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.2.cmml">p</mi></mrow><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.1.cmml">⁢</mo><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.3.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.cmml"><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.3.2.1" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml">x</mi><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.3.2.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.cmml">)</mo></mrow></mrow><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.1.cmml">=</mo><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.cmml"><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.2.cmml">s</mi><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.1.cmml">⁢</mo><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.3.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.cmml"><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.3.2.1" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.1.m1.2.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.2.cmml">x</mi><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.3.2.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.cmml">)</mo></mrow></mrow><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.1.cmml">−</mo><mrow id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.1" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.1.cmml">ln</mi><mo id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3a" lspace="0.167em" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.cmml">⁡</mo><mi id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.2" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.2.cmml">Z</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.1.m1.2b"><apply id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3"><eq id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.1"></eq><apply id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2"><times id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.1"></times><apply id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2"><ln id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.1"></ln><ci id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.2.2.2">𝑝</ci></apply><ci id="S2.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.1.1">𝑥</ci></apply><apply id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3"><minus id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.1"></minus><apply id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2"><times id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.1"></times><ci id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.2.2">𝑠</ci><ci id="S2.SS1.SSS0.Px1.p1.1.m1.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.2">𝑥</ci></apply><apply id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3"><ln id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.1"></ln><ci id="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.1.m1.2.3.3.3.2">𝑍</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.1.m1.2c">\ln p(x)=s(x)-\ln Z</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.1.m1.2d">roman_ln italic_p ( italic_x ) = italic_s ( italic_x ) - roman_ln italic_Z</annotation></semantics></math>, where <math alttext="s(x)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS1.SSS0.Px1.p1.2.m2.1a"><mrow id="S2.SS1.SSS0.Px1.p1.2.m2.1.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.cmml">s</mi><mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.1.cmml">⁢</mo><mrow id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.2" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.cmml"><mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.2.1" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml">x</mi><mo id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.3.2.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.2.m2.1b"><apply id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2"><times id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.1"></times><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.2.2">𝑠</ci><ci id="S2.SS1.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.2.m2.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.2.m2.1c">s(x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.2.m2.1d">italic_s ( italic_x )</annotation></semantics></math> is a sequence-level score that is efficient to evaluate and <math alttext="Z" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.3.m3.1"><semantics id="S2.SS1.SSS0.Px1.p1.3.m3.1a"><mi id="S2.SS1.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.3.m3.1b"><ci id="S2.SS1.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.3.m3.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.3.m3.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.3.m3.1d">italic_Z</annotation></semantics></math> is the partition function. This defines an energy-based model (EBM), and
multiple scores may be combined using a product-of-experts <math alttext="\ln p(x)=\alpha_{1}s_{1}(x)+\alpha_{2}s_{2}(x)+\ldots-\ln Z" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.4.m4.3"><semantics id="S2.SS1.SSS0.Px1.p1.4.m4.3a"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.cmml"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.cmml"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.1.cmml">ln</mi><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2a" lspace="0.167em" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.cmml">⁡</mo><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.2.cmml">p</mi></mrow><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.1.cmml">⁢</mo><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.cmml"><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.3.2.1" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml">x</mi><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.3.2.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.cmml">)</mo></mrow></mrow><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.1.cmml">=</mo><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.cmml"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.cmml"><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.cmml"><msub id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.2.cmml">α</mi><mn id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.3.cmml">1</mn></msub><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.1.cmml">⁢</mo><msub id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.2.cmml">s</mi><mn id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.3.cmml">1</mn></msub><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.1a" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.1.cmml">⁢</mo><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.4.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.cmml"><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.4.2.1" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.4.m4.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml">x</mi><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.4.2.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.cmml">)</mo></mrow></mrow><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.1.cmml">+</mo><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.cmml"><msub id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.2.cmml">α</mi><mn id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.3.cmml">2</mn></msub><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.1.cmml">⁢</mo><msub id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.2.cmml">s</mi><mn id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.3.cmml">2</mn></msub><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.1a" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.1.cmml">⁢</mo><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.4.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.cmml"><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.4.2.1" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.cmml">(</mo><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.3.cmml">x</mi><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.4.2.2" stretchy="false" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.cmml">)</mo></mrow></mrow><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.1a" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.1.cmml">+</mo><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.4" mathvariant="normal" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.4.cmml">…</mi></mrow><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.1.cmml">−</mo><mrow id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.cmml"><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.1" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.1.cmml">ln</mi><mo id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3a" lspace="0.167em" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.cmml">⁡</mo><mi id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.2" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.2.cmml">Z</mi></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.4.m4.3b"><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4"><eq id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.1"></eq><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2"><times id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.1"></times><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2"><ln id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.1"></ln><ci id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.2.2.2">𝑝</ci></apply><ci id="S2.SS1.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.1.1">𝑥</ci></apply><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3"><minus id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.1"></minus><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2"><plus id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.1"></plus><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2"><times id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.1"></times><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.2">𝛼</ci><cn id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.2.3">1</cn></apply><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3">subscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.2">𝑠</ci><cn id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.2.3.3">1</cn></apply><ci id="S2.SS1.SSS0.Px1.p1.4.m4.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.2.2">𝑥</ci></apply><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3"><times id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.1"></times><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2">subscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.2">𝛼</ci><cn id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.2.3">2</cn></apply><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3">subscript</csymbol><ci id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.2">𝑠</ci><cn id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.3.cmml" type="integer" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.3.3.3">2</cn></apply><ci id="S2.SS1.SSS0.Px1.p1.4.m4.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.3">𝑥</ci></apply><ci id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.4.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.2.4">…</ci></apply><apply id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3"><ln id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.1.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.1"></ln><ci id="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.2.cmml" xref="S2.SS1.SSS0.Px1.p1.4.m4.3.4.3.3.2">𝑍</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.4.m4.3c">\ln p(x)=\alpha_{1}s_{1}(x)+\alpha_{2}s_{2}(x)+\ldots-\ln Z</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.4.m4.3d">roman_ln italic_p ( italic_x ) = italic_α start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ( italic_x ) + italic_α start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ( italic_x ) + … - roman_ln italic_Z</annotation></semantics></math>, weighted with scalar hyperparameter <math alttext="\alpha" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.5.m5.1"><semantics id="S2.SS1.SSS0.Px1.p1.5.m5.1a"><mi id="S2.SS1.SSS0.Px1.p1.5.m5.1.1" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml">α</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.5.m5.1b"><ci id="S2.SS1.SSS0.Px1.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.5.m5.1.1">𝛼</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.5.m5.1c">\alpha</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.5.m5.1d">italic_α</annotation></semantics></math> <cite class="ltx_cite ltx_citemacro_citep">(Mireshghallah et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib31" title="">2022</a>)</cite>. For example, we can have one score measuring the property of interest, while another measures the prior likelihood of the sequence. Note that <math alttext="Z" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px1.p1.6.m6.1"><semantics id="S2.SS1.SSS0.Px1.p1.6.m6.1a"><mi id="S2.SS1.SSS0.Px1.p1.6.m6.1.1" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml">Z</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px1.p1.6.m6.1b"><ci id="S2.SS1.SSS0.Px1.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS0.Px1.p1.6.m6.1.1">𝑍</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px1.p1.6.m6.1c">Z</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px1.p1.6.m6.1d">italic_Z</annotation></semantics></math> involves an intractable sum over sequences, so direct sampling is challenging.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Sampler</h4>
<div class="ltx_para" id="S2.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS1.SSS0.Px2.p1.7">While MCMC is the standard way to draw samples from an EBM, the algorithm suffers from the curse of dimensionality. The sample efficiency of MCMC may be improved with a careful choice of proposal distribution, often requiring careful problem-specific design. Fortunately, language models trained with mask-infilling objectives have been shown to serve as effective proposal distributions <cite class="ltx_cite ltx_citemacro_citep">(Goyal et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib17" title="">2021</a>)</cite>.<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span>See <cite class="ltx_cite ltx_citemacro_citet">Wang &amp; Cho (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib53" title="">2019</a>)</cite> for further context on this approach and <cite class="ltx_cite ltx_citemacro_citet">Hennigen &amp; Kim (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib18" title="">2023</a>)</cite> for some analysis and extensions.</span></span></span>
This allows us to obtain effective proposals using pre-trained language models, which exist for natural language as well as other sequence data such as protein sequences.
Specifically, we use the Metropolis-Hastings (MH) algorithm which uses a proposal distribution <math alttext="q(x^{\prime}\mid x)" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.1.m1.1"><semantics id="S2.SS1.SSS0.Px2.p1.1.m1.1a"><mrow id="S2.SS1.SSS0.Px2.p1.1.m1.1.1" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml">q</mi><mo id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml">⁢</mo><mrow id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml"><mo id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.2" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml">(</mo><mrow id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml"><msup id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.cmml"><mi id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.2" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.2.cmml">x</mi><mo id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.3" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.3.cmml">′</mo></msup><mo id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.1" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.1.cmml">∣</mo><mi id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.3" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.cmml">x</mi></mrow><mo id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.3" stretchy="false" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.1.m1.1b"><apply id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1"><times id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.2"></times><ci id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.3">𝑞</ci><apply id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1"><csymbol cd="latexml" id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.1">conditional</csymbol><apply id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2">superscript</csymbol><ci id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.2">𝑥</ci><ci id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.3.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.2.3">′</ci></apply><ci id="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.1.m1.1.1.1.1.1.3">𝑥</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.1.m1.1c">q(x^{\prime}\mid x)</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.1.m1.1d">italic_q ( italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ∣ italic_x )</annotation></semantics></math> to draw candidate states <math alttext="x^{\prime}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.2.m2.1"><semantics id="S2.SS1.SSS0.Px2.p1.2.m2.1a"><msup id="S2.SS1.SSS0.Px2.p1.2.m2.1.1" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.2" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml">x</mi><mo id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.3" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.2.m2.1b"><apply id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.2">𝑥</ci><ci id="S2.SS1.SSS0.Px2.p1.2.m2.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.2.m2.1c">x^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.2.m2.1d">italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> given the current state <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.3.m3.1"><semantics id="S2.SS1.SSS0.Px2.p1.3.m3.1a"><mi id="S2.SS1.SSS0.Px2.p1.3.m3.1.1" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.3.m3.1b"><ci id="S2.SS1.SSS0.Px2.p1.3.m3.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.3.m3.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.3.m3.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.3.m3.1d">italic_x</annotation></semantics></math>.
These proposals are either accepted, in which case <math alttext="x^{\prime}" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.4.m4.1"><semantics id="S2.SS1.SSS0.Px2.p1.4.m4.1a"><msup id="S2.SS1.SSS0.Px2.p1.4.m4.1.1" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1.cmml"><mi id="S2.SS1.SSS0.Px2.p1.4.m4.1.1.2" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml">x</mi><mo id="S2.SS1.SSS0.Px2.p1.4.m4.1.1.3" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.4.m4.1b"><apply id="S2.SS1.SSS0.Px2.p1.4.m4.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.4.m4.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1">superscript</csymbol><ci id="S2.SS1.SSS0.Px2.p1.4.m4.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1.2">𝑥</ci><ci id="S2.SS1.SSS0.Px2.p1.4.m4.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.4.m4.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.4.m4.1c">x^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.4.m4.1d">italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> is taken as the new state, or rejected, in which case <math alttext="x^{\prime}=x" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.5.m5.1"><semantics id="S2.SS1.SSS0.Px2.p1.5.m5.1a"><mrow id="S2.SS1.SSS0.Px2.p1.5.m5.1.1" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.cmml"><msup id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml"><mi id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.2" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.2.cmml">x</mi><mo id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.3.cmml">′</mo></msup><mo id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.1" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.1.cmml">=</mo><mi id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.3" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml">x</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.5.m5.1b"><apply id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1"><eq id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.1"></eq><apply id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2"><csymbol cd="ambiguous" id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.1.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2">superscript</csymbol><ci id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.2.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.2">𝑥</ci><ci id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.3.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.2.3">′</ci></apply><ci id="S2.SS1.SSS0.Px2.p1.5.m5.1.1.3.cmml" xref="S2.SS1.SSS0.Px2.p1.5.m5.1.1.3">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.5.m5.1c">x^{\prime}=x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.5.m5.1d">italic_x start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT = italic_x</annotation></semantics></math>, according to the standard MH acceptance criterion.
To implement <math alttext="q" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.6.m6.1"><semantics id="S2.SS1.SSS0.Px2.p1.6.m6.1a"><mi id="S2.SS1.SSS0.Px2.p1.6.m6.1.1" xref="S2.SS1.SSS0.Px2.p1.6.m6.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.6.m6.1b"><ci id="S2.SS1.SSS0.Px2.p1.6.m6.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.6.m6.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.6.m6.1c">q</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.6.m6.1d">italic_q</annotation></semantics></math>, we mask a random subset of the current state <math alttext="x" class="ltx_Math" display="inline" id="S2.SS1.SSS0.Px2.p1.7.m7.1"><semantics id="S2.SS1.SSS0.Px2.p1.7.m7.1a"><mi id="S2.SS1.SSS0.Px2.p1.7.m7.1.1" xref="S2.SS1.SSS0.Px2.p1.7.m7.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S2.SS1.SSS0.Px2.p1.7.m7.1b"><ci id="S2.SS1.SSS0.Px2.p1.7.m7.1.1.cmml" xref="S2.SS1.SSS0.Px2.p1.7.m7.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS1.SSS0.Px2.p1.7.m7.1c">x</annotation><annotation encoding="application/x-llamapun" id="S2.SS1.SSS0.Px2.p1.7.m7.1d">italic_x</annotation></semantics></math>, and then infill the masked sequence <cite class="ltx_cite ltx_citemacro_citep">(Devlin et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib9" title="">2019</a>; Lewis et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib26" title="">2020</a>; Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib37" title="">2020</a>)</cite>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span>Training the extrapolative model</h3>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Parametrization</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px1.p1.4">We imbue the extrapolative model with specific inductive biases to encourage extrapolation beyond the training data. Specifically, we allow generation to proceed via multiple intermediate states <math alttext="x_{1},x_{2},\ldots,x_{N}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.1.m1.4"><semantics id="S2.SS2.SSS0.Px1.p1.1.m1.4a"><mrow id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.4.cmml"><msub id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.4" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.4.cmml">,</mo><msub id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.cmml"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.5" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.4.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.1.m1.1.1" mathvariant="normal" xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml">…</mi><mo id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.6" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.4.cmml">,</mo><msub id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.cmml"><mi id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.3" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.3.cmml">N</mi></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.1.m1.4b"><list id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.4.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3"><apply id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px1.p1.1.m1.2.2.1.1.3">1</cn></apply><apply id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.2">𝑥</ci><cn id="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.3.cmml" type="integer" xref="S2.SS2.SSS0.Px1.p1.1.m1.3.3.2.2.3">2</cn></apply><ci id="S2.SS2.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.1.1">…</ci><apply id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.1.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.2">𝑥</ci><ci id="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS2.SSS0.Px1.p1.1.m1.4.4.3.3.3">𝑁</ci></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.1.m1.4c">x_{1},x_{2},\ldots,x_{N}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.1.m1.4d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT</annotation></semantics></math>. The intuition for this strategy, borne out in our experiments (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3" title="3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3</span></a>), is that for extrapolation, it is effective to learn a conditional transformation that makes incremental changes to a state. Unlike transitions in the Markov chains, the model may avail of information from the complete history of previous states <math alttext="x_{1},x_{2},\ldots" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.2.m2.3"><semantics id="S2.SS2.SSS0.Px1.p1.2.m2.3a"><mrow id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.3.cmml"><msub id="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1" xref="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.2" xref="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.3" xref="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.3" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.3.cmml">,</mo><msub id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.cmml"><mi id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.2" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.3" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.3.cmml">2</mn></msub><mo id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.4" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.3.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.2.m2.1.1" mathvariant="normal" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml">…</mi></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.2.m2.3b"><list id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.3.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2"><apply id="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px1.p1.2.m2.2.2.1.1.3">1</cn></apply><apply id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.2">𝑥</ci><cn id="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.3.cmml" type="integer" xref="S2.SS2.SSS0.Px1.p1.2.m2.3.3.2.2.3">2</cn></apply><ci id="S2.SS2.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.2.m2.1.1">…</ci></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.2.m2.3c">x_{1},x_{2},\ldots</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.2.m2.3d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , …</annotation></semantics></math>, <em class="ltx_emph ltx_font_italic" id="S2.SS2.SSS0.Px1.p1.4.1">as well as associated real or predicted scores<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note"><span class="ltx_text ltx_font_upright" id="footnote4.1.1.1">4</span></span><span class="ltx_text ltx_font_upright" id="footnote4.9">We discuss scoring methods further in </span><a class="ltx_ref ltx_refmacro_autoref ltx_font_upright" href="https://arxiv.org/html/2505.20251v1#A1" title="Appendix A Reward choice ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Appendix A</span></a><span class="ltx_text ltx_font_upright" id="footnote4.10">.</span></span></span></span></em> <math alttext="s(x_{1}),s(x_{2}),\ldots,s(x_{n-1})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.3.m3.4"><semantics id="S2.SS2.SSS0.Px1.p1.3.m3.4a"><mrow id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.4.cmml"><mrow id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.3.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.2.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.cmml"><mo id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.2" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.cmml">(</mo><msub id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.3" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.4" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.4.cmml">,</mo><mrow id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.3.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.cmml"><mo id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.2" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.cmml">(</mo><msub id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.3.cmml">2</mn></msub><mo id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.3" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.cmml">)</mo></mrow></mrow><mo id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.5" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.4.cmml">,</mo><mi id="S2.SS2.SSS0.Px1.p1.3.m3.1.1" mathvariant="normal" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml">…</mi><mo id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.6" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.4.cmml">,</mo><mrow id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.3.cmml">s</mi><mo id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.2.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.cmml"><mo id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.2" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.cmml">(</mo><msub id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.2.cmml">x</mi><mrow id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.cmml"><mi id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.2" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.2.cmml">n</mi><mo id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.1" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.1.cmml">−</mo><mn id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.3" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.3.cmml">1</mn></mrow></msub><mo id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.3" stretchy="false" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.3.m3.4b"><list id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.4.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3"><apply id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1"><times id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.2"></times><ci id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.3">𝑠</ci><apply id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px1.p1.3.m3.2.2.1.1.1.1.1.3">1</cn></apply></apply><apply id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2"><times id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.2"></times><ci id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.3.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.3">𝑠</ci><apply id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px1.p1.3.m3.3.3.2.2.1.1.1.3">2</cn></apply></apply><ci id="S2.SS2.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.1.1">…</ci><apply id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3"><times id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.2"></times><ci id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.3.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.3">𝑠</ci><apply id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.2">𝑥</ci><apply id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3"><minus id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.1.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.1"></minus><ci id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.2.cmml" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.2">𝑛</ci><cn id="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.3.cmml" type="integer" xref="S2.SS2.SSS0.Px1.p1.3.m3.4.4.3.3.1.1.1.3.3">1</cn></apply></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.3.m3.4c">s(x_{1}),s(x_{2}),\ldots,s(x_{n-1})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.3.m3.4d">italic_s ( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , italic_s ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , … , italic_s ( italic_x start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT )</annotation></semantics></math> when producing the next state <math alttext="x_{n}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px1.p1.4.m4.1"><semantics id="S2.SS2.SSS0.Px1.p1.4.m4.1a"><msub id="S2.SS2.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.2" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.3" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px1.p1.4.m4.1b"><apply id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.1.cmml" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.2.cmml" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px1.p1.4.m4.1.1.3.cmml" xref="S2.SS2.SSS0.Px1.p1.4.m4.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px1.p1.4.m4.1c">x_{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px1.p1.4.m4.1d">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>. By conditioning on scores, the model has the ability to incorporate these into planning, not unlike the sequence model RL formulations proposed by <cite class="ltx_cite ltx_citemacro_citet">Janner et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib21" title="">2021</a>); Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib7" title="">2024</a>)</cite>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Autoregressive refinement</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p1.1">We create <em class="ltx_emph ltx_font_italic" id="S2.SS2.SSS0.Px2.p1.1.1">training episodes</em> <math alttext="(x_{1},s_{1}),(x_{2},s_{2}),\ldots,(x_{N},s_{N})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p1.1.m1.4"><semantics id="S2.SS2.SSS0.Px2.p1.1.m1.4a"><mrow id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.4.cmml"><mrow id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3.cmml"><mo id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.3" stretchy="false" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3.cmml">(</mo><msub id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.3.cmml">1</mn></msub><mo id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.4" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3.cmml">,</mo><msub id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.cmml"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.2.cmml">s</mi><mn id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.3.cmml">1</mn></msub><mo id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.5" stretchy="false" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3.cmml">)</mo></mrow><mo id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.4" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.4.cmml">,</mo><mrow id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.3.cmml"><mo id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.3" stretchy="false" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.3.cmml">(</mo><msub id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.3.cmml">2</mn></msub><mo id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.4" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.3.cmml">,</mo><msub id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.cmml"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.2.cmml">s</mi><mn id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.3.cmml">2</mn></msub><mo id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.5" stretchy="false" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.3.cmml">)</mo></mrow><mo id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.5" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.4.cmml">,</mo><mi id="S2.SS2.SSS0.Px2.p1.1.m1.1.1" mathvariant="normal" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml">…</mi><mo id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.6" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.4.cmml">,</mo><mrow id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.3.cmml"><mo id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.3" stretchy="false" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.3.cmml">(</mo><msub id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.3.cmml">N</mi></msub><mo id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.4" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.3.cmml">,</mo><msub id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.cmml"><mi id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.2" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.2.cmml">s</mi><mi id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.3" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.3.cmml">N</mi></msub><mo id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.5" stretchy="false" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.3.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p1.1.m1.4b"><list id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.4.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3"><interval closure="open" id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.1.1.3">1</cn></apply><apply id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.2">𝑠</ci><cn id="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p1.1.m1.2.2.1.1.2.2.3">1</cn></apply></interval><interval closure="open" id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.3.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.1.1.3">2</cn></apply><apply id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.2">𝑠</ci><cn id="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p1.1.m1.3.3.2.2.2.2.3">2</cn></apply></interval><ci id="S2.SS2.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.1.1">…</ci><interval closure="open" id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2"><apply id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.1.1.3">𝑁</ci></apply><apply id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.1.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.2">𝑠</ci><ci id="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.3.cmml" xref="S2.SS2.SSS0.Px2.p1.1.m1.4.4.3.3.2.2.3">𝑁</ci></apply></interval></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p1.1.m1.4c">(x_{1},s_{1}),(x_{2},s_{2}),\ldots,(x_{N},s_{N})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p1.1.m1.4d">( italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) , ( italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) , … , ( italic_x start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT , italic_s start_POSTSUBSCRIPT italic_N end_POSTSUBSCRIPT )</annotation></semantics></math> by sub-sampling state sequences from the complete Markov chains<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A3" title="Appendix C Analysis of episode length ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Appendix C</span></a> we discuss the impact of the number of subsampled states, and in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A4" title="Appendix D Ablation of MCMC exploration ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Appendix D</span></a> we discuss the impact of Markov chain length.</span></span></span>. We discuss several strategies for this in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S2.SS3" title="2.3 Creating training episodes ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 2.3</span></a>. The training episodes are encoded as a sequence of tokens:</p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p2">
<p class="ltx_p ltx_align_center" id="S2.SS2.SSS0.Px2.p2.7"><span class="ltx_text" id="S2.SS2.SSS0.Px2.p2.7.7" style="font-size:90%;"><math alttext="x_{0}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.1.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p2.1.1.m1.1a"><msub id="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.2" xref="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.3.cmml">0</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.1.1.m1.1b"><apply id="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p2.1.1.m1.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.1.1.m1.1c">x_{0}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p2.1.1.m1.1d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_typewriter" id="S2.SS2.SSS0.Px2.p2.7.7.1">&lt;seq0&gt;</span> <math alttext="x_{1}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.2.2.m2.1"><semantics id="S2.SS2.SSS0.Px2.p2.2.2.m2.1a"><msub id="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.2.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p2.2.2.m2.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.2.2.m2.1c">x_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p2.2.2.m2.1d">italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_typewriter" id="S2.SS2.SSS0.Px2.p2.7.7.2">&lt;seq1&gt;</span> <math alttext="s_{1}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.3.3.m3.1"><semantics id="S2.SS2.SSS0.Px2.p2.3.3.m3.1a"><msub id="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.2" xref="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.2.cmml">s</mi><mn id="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.3" xref="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.3.3.m3.1b"><apply id="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.2">𝑠</ci><cn id="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p2.3.3.m3.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.3.3.m3.1c">s_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p2.3.3.m3.1d">italic_s start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math> <math alttext="x_{2}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.4.4.m4.1"><semantics id="S2.SS2.SSS0.Px2.p2.4.4.m4.1a"><msub id="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1" xref="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.2" xref="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.3" xref="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.4.4.m4.1b"><apply id="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p2.4.4.m4.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.4.4.m4.1c">x_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p2.4.4.m4.1d">italic_x start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_typewriter" id="S2.SS2.SSS0.Px2.p2.7.7.3">&lt;seq2&gt;</span> <math alttext="s_{2}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.5.5.m5.1"><semantics id="S2.SS2.SSS0.Px2.p2.5.5.m5.1a"><msub id="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1" xref="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.2" xref="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.2.cmml">s</mi><mn id="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.3" xref="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.5.5.m5.1b"><apply id="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.2">𝑠</ci><cn id="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p2.5.5.m5.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.5.5.m5.1c">s_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p2.5.5.m5.1d">italic_s start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math> … <math alttext="x_{n}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.6.6.m6.1"><semantics id="S2.SS2.SSS0.Px2.p2.6.6.m6.1a"><msub id="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1" xref="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.2" xref="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.3" xref="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.6.6.m6.1b"><apply id="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p2.6.6.m6.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.6.6.m6.1c">x_{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p2.6.6.m6.1d">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> <math alttext="s_{n}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p2.7.7.m7.1"><semantics id="S2.SS2.SSS0.Px2.p2.7.7.m7.1a"><msub id="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1" xref="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.2" xref="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.2.cmml">s</mi><mi id="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.3" xref="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p2.7.7.m7.1b"><apply id="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p2.7.7.m7.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p2.7.7.m7.1c">s_{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p2.7.7.m7.1d">italic_s start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> <span class="ltx_text ltx_font_typewriter" id="S2.SS2.SSS0.Px2.p2.7.7.4">&lt;stop&gt;</span></span></p>
</div>
<div class="ltx_para" id="S2.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S2.SS2.SSS0.Px2.p3.7">Above, <span class="ltx_text ltx_font_typewriter" id="S2.SS2.SSS0.Px2.p3.7.1">&lt;</span>seq<math alttext="i" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.1.m1.1"><semantics id="S2.SS2.SSS0.Px2.p3.1.m1.1a"><mi id="S2.SS2.SSS0.Px2.p3.1.m1.1.1" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.1.m1.1b"><ci id="S2.SS2.SSS0.Px2.p3.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.1.m1.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.1.m1.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.1.m1.1d">italic_i</annotation></semantics></math><span class="ltx_text ltx_font_typewriter" id="S2.SS2.SSS0.Px2.p3.7.2">&gt;</span> and <span class="ltx_text ltx_font_typewriter" id="S2.SS2.SSS0.Px2.p3.7.3">&lt;stop&gt;</span> are distinguished symbols encoded either as special vocabulary terms or as strings in a pre-trained model, <math alttext="s_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.2.m2.1"><semantics id="S2.SS2.SSS0.Px2.p3.2.m2.1a"><msub id="S2.SS2.SSS0.Px2.p3.2.m2.1.1" xref="S2.SS2.SSS0.Px2.p3.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.2.m2.1.1.2" xref="S2.SS2.SSS0.Px2.p3.2.m2.1.1.2.cmml">s</mi><mi id="S2.SS2.SSS0.Px2.p3.2.m2.1.1.3" xref="S2.SS2.SSS0.Px2.p3.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.2.m2.1b"><apply id="S2.SS2.SSS0.Px2.p3.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.2.m2.1.1.2">𝑠</ci><ci id="S2.SS2.SSS0.Px2.p3.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.2.m2.1c">s_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.2.m2.1d">italic_s start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are scalar scores, and <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.3.m3.1"><semantics id="S2.SS2.SSS0.Px2.p3.3.m3.1a"><msub id="S2.SS2.SSS0.Px2.p3.3.m3.1.1" xref="S2.SS2.SSS0.Px2.p3.3.m3.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.3.m3.1.1.2" xref="S2.SS2.SSS0.Px2.p3.3.m3.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p3.3.m3.1.1.3" xref="S2.SS2.SSS0.Px2.p3.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.3.m3.1b"><apply id="S2.SS2.SSS0.Px2.p3.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.3.m3.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.3.m3.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.3.m3.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.3.m3.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px2.p3.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.3.m3.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.3.m3.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> are token sequences of possibly variable length. Then <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.4.m4.1"><semantics id="S2.SS2.SSS0.Px2.p3.4.m4.1a"><msub id="S2.SS2.SSS0.Px2.p3.4.m4.1.1" xref="S2.SS2.SSS0.Px2.p3.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.4.m4.1.1.2" xref="S2.SS2.SSS0.Px2.p3.4.m4.1.1.2.cmml">q</mi><mi id="S2.SS2.SSS0.Px2.p3.4.m4.1.1.3" xref="S2.SS2.SSS0.Px2.p3.4.m4.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.4.m4.1b"><apply id="S2.SS2.SSS0.Px2.p3.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.4.m4.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.4.m4.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.4.m4.1.1.2">𝑞</ci><ci id="S2.SS2.SSS0.Px2.p3.4.m4.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.4.m4.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.4.m4.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.4.m4.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is trained using teacher forcing to generate each token of each intermediate state <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.5.m5.1"><semantics id="S2.SS2.SSS0.Px2.p3.5.m5.1a"><msub id="S2.SS2.SSS0.Px2.p3.5.m5.1.1" xref="S2.SS2.SSS0.Px2.p3.5.m5.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.5.m5.1.1.2" xref="S2.SS2.SSS0.Px2.p3.5.m5.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px2.p3.5.m5.1.1.3" xref="S2.SS2.SSS0.Px2.p3.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.5.m5.1b"><apply id="S2.SS2.SSS0.Px2.p3.5.m5.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.5.m5.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.5.m5.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.5.m5.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px2.p3.5.m5.1.1.3.cmml" xref="S2.SS2.SSS0.Px2.p3.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.5.m5.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.5.m5.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> (for <math alttext="i&gt;0" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.6.m6.1"><semantics id="S2.SS2.SSS0.Px2.p3.6.m6.1a"><mrow id="S2.SS2.SSS0.Px2.p3.6.m6.1.1" xref="S2.SS2.SSS0.Px2.p3.6.m6.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.6.m6.1.1.2" xref="S2.SS2.SSS0.Px2.p3.6.m6.1.1.2.cmml">i</mi><mo id="S2.SS2.SSS0.Px2.p3.6.m6.1.1.1" xref="S2.SS2.SSS0.Px2.p3.6.m6.1.1.1.cmml">&gt;</mo><mn id="S2.SS2.SSS0.Px2.p3.6.m6.1.1.3" xref="S2.SS2.SSS0.Px2.p3.6.m6.1.1.3.cmml">0</mn></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.6.m6.1b"><apply id="S2.SS2.SSS0.Px2.p3.6.m6.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.6.m6.1.1"><gt id="S2.SS2.SSS0.Px2.p3.6.m6.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.6.m6.1.1.1"></gt><ci id="S2.SS2.SSS0.Px2.p3.6.m6.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.6.m6.1.1.2">𝑖</ci><cn id="S2.SS2.SSS0.Px2.p3.6.m6.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p3.6.m6.1.1.3">0</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.6.m6.1c">i&gt;0</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.6.m6.1d">italic_i &gt; 0</annotation></semantics></math>) conditioned on all previous states <math alttext="x_{0},x_{1},\ldots,x_{i-1}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px2.p3.7.m7.4"><semantics id="S2.SS2.SSS0.Px2.p3.7.m7.4a"><mrow id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.4.cmml"><msub id="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1" xref="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.cmml"><mi id="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.2" xref="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.3" xref="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.3.cmml">0</mn></msub><mo id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.4" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.4.cmml">,</mo><msub id="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2" xref="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.cmml"><mi id="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.2" xref="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.2.cmml">x</mi><mn id="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.3" xref="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.3.cmml">1</mn></msub><mo id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.5" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.4.cmml">,</mo><mi id="S2.SS2.SSS0.Px2.p3.7.m7.1.1" mathvariant="normal" xref="S2.SS2.SSS0.Px2.p3.7.m7.1.1.cmml">…</mi><mo id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.6" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.4.cmml">,</mo><msub id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.2" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.2.cmml">x</mi><mrow id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.cmml"><mi id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.2" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.2.cmml">i</mi><mo id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.1" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.1.cmml">−</mo><mn id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.3" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.3.cmml">1</mn></mrow></msub></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px2.p3.7.m7.4b"><list id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.4.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3"><apply id="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.2.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.2">𝑥</ci><cn id="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p3.7.m7.2.2.1.1.3">0</cn></apply><apply id="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.1.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.2.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.2">𝑥</ci><cn id="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p3.7.m7.3.3.2.2.3">1</cn></apply><ci id="S2.SS2.SSS0.Px2.p3.7.m7.1.1.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.1.1">…</ci><apply id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3">subscript</csymbol><ci id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.2">𝑥</ci><apply id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3"><minus id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.1.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.1"></minus><ci id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.2.cmml" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.2">𝑖</ci><cn id="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.3.cmml" type="integer" xref="S2.SS2.SSS0.Px2.p3.7.m7.4.4.3.3.3.3">1</cn></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px2.p3.7.m7.4c">x_{0},x_{1},\ldots,x_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px2.p3.7.m7.4d">italic_x start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT , italic_x start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , … , italic_x start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math>. As previously mentioned, the concrete advantage to formulating inference in this way is that revisions can condition on previously generated sequences and energy scores.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Inference</h4>
<div class="ltx_para" id="S2.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS2.SSS0.Px3.p1.5">Since <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S2.SS2.SSS0.Px3.p1.1.m1.1a"><msub id="S2.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S2.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S2.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml">q</mi><mi id="S2.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S2.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S2.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p1.1.m1.1.1.2">𝑞</ci><ci id="S2.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> has a simple autoregressive structure, generating from the model can be done in a variety of ways, including forward sampling and beam search. We note that in principle constrained decoding techniques could be used to enforce adherence to the structure above, but we did not find this necessary in practice. If any intermediate states are added to the sequence (i.e. we use more than the first and best state), after generating each intermediate state <math alttext="x_{i}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="S2.SS2.SSS0.Px3.p1.2.m2.1a"><msub id="S2.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S2.SS2.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p1.2.m2.1.1.2" xref="S2.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p1.2.m2.1.1.3" xref="S2.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p1.2.m2.1b"><apply id="S2.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p1.2.m2.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p1.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p1.2.m2.1c">x_{i}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p1.2.m2.1d">italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, the sequence is either scored using <math alttext="s(x_{i})" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.3.m3.1"><semantics id="S2.SS2.SSS0.Px3.p1.3.m3.1a"><mrow id="S2.SS2.SSS0.Px3.p1.3.m3.1.1" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.3" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml">s</mi><mo id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.2" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml">⁢</mo><mrow id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.cmml"><mo id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.2" stretchy="false" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.cmml">(</mo><msub id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.2" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.3" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.3.cmml">i</mi></msub><mo id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.3" stretchy="false" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p1.3.m3.1b"><apply id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1"><times id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.2"></times><ci id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.3">𝑠</ci><apply id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p1.3.m3.1.1.1.1.1.3">𝑖</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p1.3.m3.1c">s(x_{i})</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p1.3.m3.1d">italic_s ( italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT )</annotation></semantics></math> and the result deterministically appended to the sequence, or <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.4.m4.1"><semantics id="S2.SS2.SSS0.Px3.p1.4.m4.1a"><msub id="S2.SS2.SSS0.Px3.p1.4.m4.1.1" xref="S2.SS2.SSS0.Px3.p1.4.m4.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p1.4.m4.1.1.2" xref="S2.SS2.SSS0.Px3.p1.4.m4.1.1.2.cmml">q</mi><mi id="S2.SS2.SSS0.Px3.p1.4.m4.1.1.3" xref="S2.SS2.SSS0.Px3.p1.4.m4.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p1.4.m4.1b"><apply id="S2.SS2.SSS0.Px3.p1.4.m4.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.4.m4.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p1.4.m4.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.4.m4.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p1.4.m4.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p1.4.m4.1.1.2">𝑞</ci><ci id="S2.SS2.SSS0.Px3.p1.4.m4.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p1.4.m4.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p1.4.m4.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p1.4.m4.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> learns to <em class="ltx_emph ltx_font_italic" id="S2.SS2.SSS0.Px3.p1.5.1">predict</em> the sequence score.<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span>Another possibility is to consult the oracle at <em class="ltx_emph ltx_font_italic" id="footnote6.1">intermediate</em> states of generation, although we do not directly evaluate this version in our experiments, as our setup assumes the necessity of minimizing oracle calls.</span></span></span> When <span class="ltx_text ltx_font_typewriter" id="S2.SS2.SSS0.Px3.p1.5.2">&lt;stop&gt;</span> is generated from the model, the final state <math alttext="x_{n}" class="ltx_Math" display="inline" id="S2.SS2.SSS0.Px3.p1.5.m5.1"><semantics id="S2.SS2.SSS0.Px3.p1.5.m5.1a"><msub id="S2.SS2.SSS0.Px3.p1.5.m5.1.1" xref="S2.SS2.SSS0.Px3.p1.5.m5.1.1.cmml"><mi id="S2.SS2.SSS0.Px3.p1.5.m5.1.1.2" xref="S2.SS2.SSS0.Px3.p1.5.m5.1.1.2.cmml">x</mi><mi id="S2.SS2.SSS0.Px3.p1.5.m5.1.1.3" xref="S2.SS2.SSS0.Px3.p1.5.m5.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS2.SSS0.Px3.p1.5.m5.1b"><apply id="S2.SS2.SSS0.Px3.p1.5.m5.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.5.m5.1.1"><csymbol cd="ambiguous" id="S2.SS2.SSS0.Px3.p1.5.m5.1.1.1.cmml" xref="S2.SS2.SSS0.Px3.p1.5.m5.1.1">subscript</csymbol><ci id="S2.SS2.SSS0.Px3.p1.5.m5.1.1.2.cmml" xref="S2.SS2.SSS0.Px3.p1.5.m5.1.1.2">𝑥</ci><ci id="S2.SS2.SSS0.Px3.p1.5.m5.1.1.3.cmml" xref="S2.SS2.SSS0.Px3.p1.5.m5.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS2.SSS0.Px3.p1.5.m5.1c">x_{n}</annotation><annotation encoding="application/x-llamapun" id="S2.SS2.SSS0.Px3.p1.5.m5.1d">italic_x start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> is taken to be the sample.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.3 </span>Creating training episodes</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">Creating training episodes consisting of the <em class="ltx_emph ltx_font_italic" id="S2.SS3.p1.1.1">entire</em> Markov chain, which could include hundreds of states, is undesirable. Ideally, <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S2.SS3.p1.1.m1.1"><semantics id="S2.SS3.p1.1.m1.1a"><msub id="S2.SS3.p1.1.m1.1.1" xref="S2.SS3.p1.1.m1.1.1.cmml"><mi id="S2.SS3.p1.1.m1.1.1.2" xref="S2.SS3.p1.1.m1.1.1.2.cmml">q</mi><mi id="S2.SS3.p1.1.m1.1.1.3" xref="S2.SS3.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S2.SS3.p1.1.m1.1b"><apply id="S2.SS3.p1.1.m1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.SS3.p1.1.m1.1.1.1.cmml" xref="S2.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.SS3.p1.1.m1.1.1.2.cmml" xref="S2.SS3.p1.1.m1.1.1.2">𝑞</ci><ci id="S2.SS3.p1.1.m1.1.1.3.cmml" xref="S2.SS3.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is computationally efficient at inference time, generating a small number of states before producing the <span class="ltx_text ltx_font_typewriter" id="S2.SS3.p1.1.2">&lt;stop&gt;</span> symbol. As a result, we require relatively short training episodes. Note also that the sampling method might explore high-energy regions of the state space, and it may be sub-optimal to include such exploration in the training episodes; therefore, we ideally want to select state transitions from the complete sample that result in a decreased energy. We examine several strategies for selecting states.</p>
</div>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Uniform thinning</h4>
<div class="ltx_para" id="S2.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px1.p1.5">If the sampling chain tends to monotonically improve the energy, the simple strategy of sub-sampling the states at regular intervals can be expected to result in a state sequence with incremental progress towards a local optimum. In <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS0.Px1.p1.5.1">fixed-length thinning</span>, we choose a number of states <math alttext="n" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.1.m1.1"><semantics id="S2.SS3.SSS0.Px1.p1.1.m1.1a"><mi id="S2.SS3.SSS0.Px1.p1.1.m1.1.1" xref="S2.SS3.SSS0.Px1.p1.1.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px1.p1.1.m1.1b"><ci id="S2.SS3.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S2.SS3.SSS0.Px1.p1.1.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px1.p1.1.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px1.p1.1.m1.1d">italic_n</annotation></semantics></math> and pick states at regular intervals to create our chain of edits. In <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS0.Px1.p1.5.2">variable-length thinning</span>, rather than choosing the number of states <math alttext="n" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.2.m2.1"><semantics id="S2.SS3.SSS0.Px1.p1.2.m2.1a"><mi id="S2.SS3.SSS0.Px1.p1.2.m2.1.1" xref="S2.SS3.SSS0.Px1.p1.2.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px1.p1.2.m2.1b"><ci id="S2.SS3.SSS0.Px1.p1.2.m2.1.1.cmml" xref="S2.SS3.SSS0.Px1.p1.2.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px1.p1.2.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px1.p1.2.m2.1d">italic_n</annotation></semantics></math> independently of the sequence length <math alttext="i" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.3.m3.1"><semantics id="S2.SS3.SSS0.Px1.p1.3.m3.1a"><mi id="S2.SS3.SSS0.Px1.p1.3.m3.1.1" xref="S2.SS3.SSS0.Px1.p1.3.m3.1.1.cmml">i</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px1.p1.3.m3.1b"><ci id="S2.SS3.SSS0.Px1.p1.3.m3.1.1.cmml" xref="S2.SS3.SSS0.Px1.p1.3.m3.1.1">𝑖</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px1.p1.3.m3.1c">i</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px1.p1.3.m3.1d">italic_i</annotation></semantics></math>, we choose a thinning factor <math alttext="k" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px1.p1.4.m4.1"><semantics id="S2.SS3.SSS0.Px1.p1.4.m4.1a"><mi id="S2.SS3.SSS0.Px1.p1.4.m4.1.1" xref="S2.SS3.SSS0.Px1.p1.4.m4.1.1.cmml">k</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px1.p1.4.m4.1b"><ci id="S2.SS3.SSS0.Px1.p1.4.m4.1.1.cmml" xref="S2.SS3.SSS0.Px1.p1.4.m4.1.1">𝑘</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px1.p1.4.m4.1c">k</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px1.p1.4.m4.1d">italic_k</annotation></semantics></math> and calculate <math alttext="n=i//k" class="ltx_math_unparsed" display="inline" id="S2.SS3.SSS0.Px1.p1.5.m5.1"><semantics id="S2.SS3.SSS0.Px1.p1.5.m5.1a"><mrow id="S2.SS3.SSS0.Px1.p1.5.m5.1b"><mi id="S2.SS3.SSS0.Px1.p1.5.m5.1.1">n</mi><mo id="S2.SS3.SSS0.Px1.p1.5.m5.1.2">=</mo><mi id="S2.SS3.SSS0.Px1.p1.5.m5.1.3">i</mi><mo id="S2.SS3.SSS0.Px1.p1.5.m5.1.4" rspace="0em">/</mo><mo id="S2.SS3.SSS0.Px1.p1.5.m5.1.5" lspace="0em">/</mo><mi id="S2.SS3.SSS0.Px1.p1.5.m5.1.6">k</mi></mrow><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px1.p1.5.m5.1c">n=i//k</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px1.p1.5.m5.1d">italic_n = italic_i / / italic_k</annotation></semantics></math>. This dynamically allocates each edit chain a number of states based on the entire edit sequence length.</p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">First and best</h4>
<div class="ltx_para" id="S2.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px2.p1.1">If the task is sufficiently simple, a single step should be adequate to extrapolate. By taking the initial and lowest energy states of the Markov chain, we create single-step training examples.<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span>This can be considered a special case of uniform thinning where the training episode length is two.</span></span></span></p>
</div>
</section>
<section class="ltx_paragraph" id="S2.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Changes in energy</h4>
<div class="ltx_para" id="S2.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S2.SS3.SSS0.Px3.p1.4">Ideally, we would like the states chosen for training episodes to be governed by properties of states in the chain, such as the relative improvements in energy from state to state. A simple way to incorporate this idea into the selection of training episodes is to identify state transitions that most improve the energy. In <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS0.Px3.p1.1.1">fixed-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px3.p1.1.1.m1.1"><semantics id="S2.SS3.SSS0.Px3.p1.1.1.m1.1a"><mi id="S2.SS3.SSS0.Px3.p1.1.1.m1.1.1" mathvariant="normal" xref="S2.SS3.SSS0.Px3.p1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px3.p1.1.1.m1.1b"><ci id="S2.SS3.SSS0.Px3.p1.1.1.m1.1.1.cmml" xref="S2.SS3.SSS0.Px3.p1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px3.p1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px3.p1.1.1.m1.1d">roman_Δ</annotation></semantics></math> energy</span>, we cache the energy for each state while running MCMC, then select the <math alttext="n" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px3.p1.2.m1.1"><semantics id="S2.SS3.SSS0.Px3.p1.2.m1.1a"><mi id="S2.SS3.SSS0.Px3.p1.2.m1.1.1" xref="S2.SS3.SSS0.Px3.p1.2.m1.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px3.p1.2.m1.1b"><ci id="S2.SS3.SSS0.Px3.p1.2.m1.1.1.cmml" xref="S2.SS3.SSS0.Px3.p1.2.m1.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px3.p1.2.m1.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px3.p1.2.m1.1d">italic_n</annotation></semantics></math> states that most improve energy from the previous step to construct our training episode. Rather than selecting <math alttext="n" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px3.p1.3.m2.1"><semantics id="S2.SS3.SSS0.Px3.p1.3.m2.1a"><mi id="S2.SS3.SSS0.Px3.p1.3.m2.1.1" xref="S2.SS3.SSS0.Px3.p1.3.m2.1.1.cmml">n</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px3.p1.3.m2.1b"><ci id="S2.SS3.SSS0.Px3.p1.3.m2.1.1.cmml" xref="S2.SS3.SSS0.Px3.p1.3.m2.1.1">𝑛</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px3.p1.3.m2.1c">n</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px3.p1.3.m2.1d">italic_n</annotation></semantics></math> states, <span class="ltx_text ltx_font_bold" id="S2.SS3.SSS0.Px3.p1.4.2">variable-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S2.SS3.SSS0.Px3.p1.4.2.m1.1"><semantics id="S2.SS3.SSS0.Px3.p1.4.2.m1.1a"><mi id="S2.SS3.SSS0.Px3.p1.4.2.m1.1.1" mathvariant="normal" xref="S2.SS3.SSS0.Px3.p1.4.2.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S2.SS3.SSS0.Px3.p1.4.2.m1.1b"><ci id="S2.SS3.SSS0.Px3.p1.4.2.m1.1.1.cmml" xref="S2.SS3.SSS0.Px3.p1.4.2.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.SS3.SSS0.Px3.p1.4.2.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S2.SS3.SSS0.Px3.p1.4.2.m1.1d">roman_Δ</annotation></semantics></math> energy</span> selects any states which improve energy by a particular threshold, e.g. 10%..</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span>Experiments</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.2">To address whether <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.p1.1.m1.1"><semantics id="S3.p1.1.m1.1a"><msub id="S3.p1.1.m1.1.1" xref="S3.p1.1.m1.1.1.cmml"><mi id="S3.p1.1.m1.1.1.2" xref="S3.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.p1.1.m1.1.1.3" xref="S3.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.1.m1.1b"><apply id="S3.p1.1.m1.1.1.cmml" xref="S3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.p1.1.m1.1.1.1.cmml" xref="S3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.p1.1.m1.1.1.2.cmml" xref="S3.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.p1.1.m1.1.1.3.cmml" xref="S3.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> has the capacity for sample-efficient extrapolation, we apply our method to two tasks from <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite> which require extrapolation: protein engineering and sentiment extrapolation. To demonstrate that <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.p1.2.m2.1"><semantics id="S3.p1.2.m2.1a"><msub id="S3.p1.2.m2.1.1" xref="S3.p1.2.m2.1.1.cmml"><mi id="S3.p1.2.m2.1.1.2" xref="S3.p1.2.m2.1.1.2.cmml">q</mi><mi id="S3.p1.2.m2.1.1.3" xref="S3.p1.2.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p1.2.m2.1b"><apply id="S3.p1.2.m2.1.1.cmml" xref="S3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.p1.2.m2.1.1.1.cmml" xref="S3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.p1.2.m2.1.1.2.cmml" xref="S3.p1.2.m2.1.1.2">𝑞</ci><ci id="S3.p1.2.m2.1.1.3.cmml" xref="S3.p1.2.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p1.2.m2.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.p1.2.m2.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> retains the capacity to “interpolate” (i.e., generalize well in a non-extrapolative task), we evaluate on a complex task solely requiring interpolation, namely text anonymization. In all experiments, to demonstrate method efficiency, we show the number of “iterations” each method takes—we consider “iterations” to loosely correspond to the computational work of passing the sequence through the inference model once. Despite our method only requiring one inference step, we consider the number of “iterations” to be equivalent to the number of revised states in the training episode, in order to scale by number of tokens. In variable-length methods, we report the average number of iterations.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.1 </span>Protein engineering</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">We replicate the ACE2 stability task from <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>. The goal is to generate mutants of the human angiotensin-converting enzyme 2 (ACE2) with higher stability than the wildtype, measured with lower free energy compared to the wildtype (ddG). Lower ddG corresponds to more stable mutants. The protein is represented as a sequence of 83 amino acids, from a vocabulary of 20 amino acids in total. We finetune a ProtBert model <cite class="ltx_cite ltx_citemacro_citep">(Elnaggar et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib12" title="">2020</a>)</cite> to predict ddG from a mutated ACE2 sequence. We use the ACE2 dataset from <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib6" title="">2021</a>)</cite>, restricting the training data to only examples with ddG between -4 and 10. The objective is to generalize to sequences with ddG beyond the training range (i.e. below -4). We describe our experimental procedure in detail in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A2.SS1" title="B.1 Protein engineering ‣ Appendix B Extrapolation experimental details ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ B.1</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">We compare our generated sequences to results from <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite><span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span>In a personal communication, the authors report that their procedure exhibits large variance, and indeed we are unable to reproduce published results using the code released by <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>.</span></span></span>; specifically, we consider their reported scores for masking and infilling, iteratively masking and infilling with ranked outputs (Iterative sampling), Genhance by <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib6" title="">2021</a>)</cite> and Iterative Controllable Extrapolation (ICE) by <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>. In both cases, we report the better-performing variant <span class="ltx_text ltx_font_italic" id="S3.SS1.SSS0.Px1.p1.1.1">with scorer</span>, where at each step the model generates multiple options and chooses the best using the training-time scorer. We also report the scorer-free variant of ICE, which generates a single output at each step, similar to <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px1.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Metrics</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">We evaluate the stability of the generated proteins using FoldX <cite class="ltx_cite ltx_citemacro_citep">(Schymkowitz et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib44" title="">2005</a>)</cite>, which calculates the ddG for each protein. We report the proportion of generated mutants which fall below certain thresholds: -1 and -2.5, which are within the training region, and -5, -6, and -7, which are within the extrapolation region.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.3">Our results with <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS1.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS1.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> trained on training episodes constructed using fixed-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS1.SSS0.Px3.p1.2.m2.1a"><mi id="S3.SS1.SSS0.Px3.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.2.m2.1b"><ci id="S3.SS1.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.2.m2.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.2.m2.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.2.m2.1d">roman_Δ</annotation></semantics></math> energy can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.T1" title="Table 1 ‣ Results ‣ 3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 1</span></a>. Despite the fact that MCMC fails to outperform the baselines taken from <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>, we find that in the extrapolation range <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p1.3.m3.1"><semantics id="S3.SS1.SSS0.Px3.p1.3.m3.1a"><msub id="S3.SS1.SSS0.Px3.p1.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml">q</mi><mi id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p1.3.m3.1b"><apply id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.2">𝑞</ci><ci id="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p1.3.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p1.3.m3.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p1.3.m3.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> significantly outperforms our baselines and MCMC.</p>
</div>
<figure class="ltx_table" id="S3.T1">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T1.7">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T1.6.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T1.6.6.7"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.7.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T1.1.1.1.1">-1<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.1.1.1.1.m1.1"><semantics id="S3.T1.1.1.1.1.m1.1a"><mo id="S3.T1.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T1.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.1.1.1.1.m1.1b"><ci id="S3.T1.1.1.1.1.m1.1.1.cmml" xref="S3.T1.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T1.2.2.2"><span class="ltx_text ltx_font_bold" id="S3.T1.2.2.2.1">-2.5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.2.2.2.1.m1.1"><semantics id="S3.T1.2.2.2.1.m1.1a"><mo id="S3.T1.2.2.2.1.m1.1.1" stretchy="false" xref="S3.T1.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.2.2.2.1.m1.1b"><ci id="S3.T1.2.2.2.1.m1.1.1.cmml" xref="S3.T1.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.3.3.3"><span class="ltx_text ltx_font_bold" id="S3.T1.3.3.3.1">-5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.3.3.3.1.m1.1"><semantics id="S3.T1.3.3.3.1.m1.1a"><mo id="S3.T1.3.3.3.1.m1.1.1" stretchy="false" xref="S3.T1.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.3.3.3.1.m1.1b"><ci id="S3.T1.3.3.3.1.m1.1.1.cmml" xref="S3.T1.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.4.4.4"><span class="ltx_text ltx_font_bold" id="S3.T1.4.4.4.1">-6<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.4.4.4.1.m1.1"><semantics id="S3.T1.4.4.4.1.m1.1a"><mo id="S3.T1.4.4.4.1.m1.1.1" stretchy="false" xref="S3.T1.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.4.4.4.1.m1.1b"><ci id="S3.T1.4.4.4.1.m1.1.1.cmml" xref="S3.T1.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.4.4.4.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="S3.T1.5.5.5"><span class="ltx_text ltx_font_bold" id="S3.T1.5.5.5.1">-7<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T1.5.5.5.1.m1.1"><semantics id="S3.T1.5.5.5.1.m1.1a"><mo id="S3.T1.5.5.5.1.m1.1.1" stretchy="false" xref="S3.T1.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T1.5.5.5.1.m1.1b"><ci id="S3.T1.5.5.5.1.m1.1.1.cmml" xref="S3.T1.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.5.5.5.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T1.6.6.6"><span class="ltx_text ltx_font_bold" id="S3.T1.6.6.6.1">Iterations<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T1.6.6.6.1.m1.1"><semantics id="S3.T1.6.6.6.1.m1.1a"><mo id="S3.T1.6.6.6.1.m1.1.1" stretchy="false" xref="S3.T1.6.6.6.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T1.6.6.6.1.m1.1b"><ci id="S3.T1.6.6.6.1.m1.1.1.cmml" xref="S3.T1.6.6.6.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.6.6.6.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T1.6.6.6.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.8.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.7.8.1.1">Mask/Infill</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.8.1.2">0.033</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.7.8.1.3">0.007</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.8.1.4">0.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.8.1.5">0.000</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.T1.7.8.1.6">0.000</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.8.1.7">1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.9.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.7.9.2.1">Iterative sampling</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.9.2.2">0.998</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.7.9.2.3">0.954</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.9.2.4">0.220</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.9.2.5">0.079</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T1.7.9.2.6">0.001</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.9.2.7">10</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.10.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.7.10.3.1">Genhance w/scorer</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.10.3.2"><span class="ltx_text ltx_font_bold" id="S3.T1.7.10.3.2.1">0.999</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.7.10.3.3">0.978</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.10.3.4">0.159</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.10.3.5">0.040</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T1.7.10.3.6">0.009</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.10.3.7">1</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.11.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.7.11.4.1">ICE scorer-free</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.11.4.2">0.945</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.7.11.4.3">0.598</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.11.4.4">0.062</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.11.4.5">0.017</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T1.7.11.4.6">0.002</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.11.4.7">10</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.12.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T1.7.12.5.1">ICE w/scorer</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.12.5.2">0.998</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T1.7.12.5.3">0.974</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.12.5.4">0.361</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.12.5.5">0.098</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T1.7.12.5.6">0.019</td>
<td class="ltx_td ltx_align_center" id="S3.T1.7.12.5.7">10</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.13.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T1.7.13.6.1">MCMC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.13.6.2"><span class="ltx_text ltx_font_bold" id="S3.T1.7.13.6.2.1">0.999</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T1.7.13.6.3"><span class="ltx_text ltx_font_bold" id="S3.T1.7.13.6.3.1">0.995</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.13.6.4">0.270</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.13.6.5">0.041</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.T1.7.13.6.6">0.005</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T1.7.13.6.7">83</td>
</tr>
<tr class="ltx_tr" id="S3.T1.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T1.7.7.1"><math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.T1.7.7.1.m1.1"><semantics id="S3.T1.7.7.1.m1.1a"><msub id="S3.T1.7.7.1.m1.1.1" xref="S3.T1.7.7.1.m1.1.1.cmml"><mi id="S3.T1.7.7.1.m1.1.1.2" xref="S3.T1.7.7.1.m1.1.1.2.cmml">q</mi><mi id="S3.T1.7.7.1.m1.1.1.3" xref="S3.T1.7.7.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T1.7.7.1.m1.1b"><apply id="S3.T1.7.7.1.m1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.7.7.1.m1.1.1.1.cmml" xref="S3.T1.7.7.1.m1.1.1">subscript</csymbol><ci id="S3.T1.7.7.1.m1.1.1.2.cmml" xref="S3.T1.7.7.1.m1.1.1.2">𝑞</ci><ci id="S3.T1.7.7.1.m1.1.1.3.cmml" xref="S3.T1.7.7.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.7.7.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.7.7.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.7.2">0.972</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T1.7.7.3">0.938</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.7.4"><span class="ltx_text ltx_font_bold" id="S3.T1.7.7.4.1">0.748</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.7.5"><span class="ltx_text ltx_font_bold" id="S3.T1.7.7.5.1">0.616</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S3.T1.7.7.6"><span class="ltx_text ltx_font_bold" id="S3.T1.7.7.6.1">0.464</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T1.7.7.7">3</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 1: </span>Overall ACE2 stability results. Each cell represents the percentage of generated sentences lower than the threshold. Lower ddG is more stable; -1 and -2.5 are in the training range, -5 and below is in the extrapolation range. While MCMC does not approach the success of the baseline, the best variant of <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.T1.10.m1.1"><semantics id="S3.T1.10.m1.1b"><msub id="S3.T1.10.m1.1.1" xref="S3.T1.10.m1.1.1.cmml"><mi id="S3.T1.10.m1.1.1.2" xref="S3.T1.10.m1.1.1.2.cmml">q</mi><mi id="S3.T1.10.m1.1.1.3" xref="S3.T1.10.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T1.10.m1.1c"><apply id="S3.T1.10.m1.1.1.cmml" xref="S3.T1.10.m1.1.1"><csymbol cd="ambiguous" id="S3.T1.10.m1.1.1.1.cmml" xref="S3.T1.10.m1.1.1">subscript</csymbol><ci id="S3.T1.10.m1.1.1.2.cmml" xref="S3.T1.10.m1.1.1.2">𝑞</ci><ci id="S3.T1.10.m1.1.1.3.cmml" xref="S3.T1.10.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.10.m1.1d">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.T1.10.m1.1e">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, trained on training episodes created using fixed-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T1.11.m2.1"><semantics id="S3.T1.11.m2.1b"><mi id="S3.T1.11.m2.1.1" mathvariant="normal" xref="S3.T1.11.m2.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T1.11.m2.1c"><ci id="S3.T1.11.m2.1.1.cmml" xref="S3.T1.11.m2.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T1.11.m2.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T1.11.m2.1e">roman_Δ</annotation></semantics></math> energy to select states, significantly outperforms the baseline.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T2">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T2.12">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T2.5.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T2.5.5.6"><span class="ltx_text ltx_font_bold" id="S3.T2.5.5.6.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S3.T2.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T2.1.1.1.1">Training<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.1.1.1.1.m1.1"><semantics id="S3.T2.1.1.1.1.m1.1a"><mo id="S3.T2.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T2.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.1.1.1.1.m1.1b"><ci id="S3.T2.1.1.1.1.m1.1.1.cmml" xref="S3.T2.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="S3.T2.2.2.2">
<span class="ltx_text ltx_font_bold" id="S3.T2.2.2.2.1">Extrapolation</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T2.2.2.2.m1.1"><semantics id="S3.T2.2.2.2.m1.1a"><mo id="S3.T2.2.2.2.m1.1.1" stretchy="false" xref="S3.T2.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T2.2.2.2.m1.1b"><ci id="S3.T2.2.2.2.m1.1.1.cmml" xref="S3.T2.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.2.2.2.m1.1d">↑</annotation></semantics></math>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.4.4.4">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T2.3.3.3.m1.1"><semantics id="S3.T2.3.3.3.m1.1a"><mi id="S3.T2.3.3.3.m1.1.1" mathvariant="normal" xref="S3.T2.3.3.3.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T2.3.3.3.m1.1b"><ci id="S3.T2.3.3.3.m1.1.1.cmml" xref="S3.T2.3.3.3.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.3.3.3.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T2.3.3.3.m1.1d">roman_Δ</annotation></semantics></math> <span class="ltx_text ltx_font_bold" id="S3.T2.4.4.4.1">Fluency<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.4.4.4.1.m1.1"><semantics id="S3.T2.4.4.4.1.m1.1a"><mo id="S3.T2.4.4.4.1.m1.1.1" stretchy="false" xref="S3.T2.4.4.4.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.4.4.4.1.m1.1b"><ci id="S3.T2.4.4.4.1.m1.1.1.cmml" xref="S3.T2.4.4.4.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.4.4.4.1.m1.1d">↓</annotation></semantics></math></span>
</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T2.5.5.5"><span class="ltx_text ltx_font_bold" id="S3.T2.5.5.5.1">Iterations<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T2.5.5.5.1.m1.1"><semantics id="S3.T2.5.5.5.1.m1.1a"><mo id="S3.T2.5.5.5.1.m1.1.1" stretchy="false" xref="S3.T2.5.5.5.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T2.5.5.5.1.m1.1b"><ci id="S3.T2.5.5.5.1.m1.1.1.cmml" xref="S3.T2.5.5.5.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.5.5.5.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T2.5.5.5.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S3.T2.12.13.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.12.13.1.1">Genhance</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.12.13.1.2">0.908</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.T2.12.13.1.3">0.387</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.12.13.1.4">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.12.13.1.5">1</td>
</tr>
<tr class="ltx_tr" id="S3.T2.12.14.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.12.14.2.1">ICE scorer-free</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.12.14.2.2">0.947</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T2.12.14.2.3">0.376</td>
<td class="ltx_td ltx_align_center" id="S3.T2.12.14.2.4">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.12.14.2.5">10</td>
</tr>
<tr class="ltx_tr" id="S3.T2.12.15.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.12.15.3.1">ICE w/scorer</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.12.15.3.2">0.921</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T2.12.15.3.3">0.610</td>
<td class="ltx_td ltx_align_center" id="S3.T2.12.15.3.4">-</td>
<td class="ltx_td ltx_align_center" id="S3.T2.12.15.3.5">10</td>
</tr>
<tr class="ltx_tr" id="S3.T2.12.16.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T2.12.16.4.1">FUDGE</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T2.12.16.4.2">0.613</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T2.12.16.4.3">0.237</td>
<td class="ltx_td ltx_align_center" id="S3.T2.12.16.4.4">-0.212%</td>
<td class="ltx_td ltx_align_center" id="S3.T2.12.16.4.5">1</td>
</tr>
<tr class="ltx_tr" id="S3.T2.8.8">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T2.8.8.4">MCMC</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T2.6.6.1">0.960<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T2.6.6.1.m1.1"><semantics id="S3.T2.6.6.1.m1.1a"><mo id="S3.T2.6.6.1.m1.1.1" mathsize="50%" xref="S3.T2.6.6.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T2.6.6.1.m1.1b"><csymbol cd="latexml" id="S3.T2.6.6.1.m1.1.1.cmml" xref="S3.T2.6.6.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.6.6.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T2.6.6.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T2.6.6.1.1" style="font-size:50%;">0.004</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.T2.7.7.2">0.809<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T2.7.7.2.m1.1"><semantics id="S3.T2.7.7.2.m1.1a"><mo id="S3.T2.7.7.2.m1.1.1" mathsize="50%" xref="S3.T2.7.7.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T2.7.7.2.m1.1b"><csymbol cd="latexml" id="S3.T2.7.7.2.m1.1.1.cmml" xref="S3.T2.7.7.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.7.7.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T2.7.7.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T2.7.7.2.1" style="font-size:50%;">0.011</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.8.3">0.746%<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T2.8.8.3.m1.1"><semantics id="S3.T2.8.8.3.m1.1a"><mo id="S3.T2.8.8.3.m1.1.1" mathsize="50%" xref="S3.T2.8.8.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T2.8.8.3.m1.1b"><csymbol cd="latexml" id="S3.T2.8.8.3.m1.1.1.cmml" xref="S3.T2.8.8.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.8.8.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T2.8.8.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T2.8.8.3.1" style="font-size:50%;">0.017</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T2.8.8.5">496</td>
</tr>
<tr class="ltx_tr" id="S3.T2.12.12">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T2.9.9.1"><math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.T2.9.9.1.m1.1"><semantics id="S3.T2.9.9.1.m1.1a"><msub id="S3.T2.9.9.1.m1.1.1" xref="S3.T2.9.9.1.m1.1.1.cmml"><mi id="S3.T2.9.9.1.m1.1.1.2" xref="S3.T2.9.9.1.m1.1.1.2.cmml">q</mi><mi id="S3.T2.9.9.1.m1.1.1.3" xref="S3.T2.9.9.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T2.9.9.1.m1.1b"><apply id="S3.T2.9.9.1.m1.1.1.cmml" xref="S3.T2.9.9.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.9.9.1.m1.1.1.1.cmml" xref="S3.T2.9.9.1.m1.1.1">subscript</csymbol><ci id="S3.T2.9.9.1.m1.1.1.2.cmml" xref="S3.T2.9.9.1.m1.1.1.2">𝑞</ci><ci id="S3.T2.9.9.1.m1.1.1.3.cmml" xref="S3.T2.9.9.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.9.9.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.9.9.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T2.10.10.2">0.925<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T2.10.10.2.m1.1"><semantics id="S3.T2.10.10.2.m1.1a"><mo id="S3.T2.10.10.2.m1.1.1" mathsize="50%" xref="S3.T2.10.10.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T2.10.10.2.m1.1b"><csymbol cd="latexml" id="S3.T2.10.10.2.m1.1.1.cmml" xref="S3.T2.10.10.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.10.10.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T2.10.10.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T2.10.10.2.1" style="font-size:50%;">0.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S3.T2.11.11.3">0.734<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T2.11.11.3.m1.1"><semantics id="S3.T2.11.11.3.m1.1a"><mo id="S3.T2.11.11.3.m1.1.1" mathsize="50%" xref="S3.T2.11.11.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T2.11.11.3.m1.1b"><csymbol cd="latexml" id="S3.T2.11.11.3.m1.1.1.cmml" xref="S3.T2.11.11.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.11.11.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T2.11.11.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T2.11.11.3.1" style="font-size:50%;">0.008</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.12.12.4">0.132%<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T2.12.12.4.m1.1"><semantics id="S3.T2.12.12.4.m1.1a"><mo id="S3.T2.12.12.4.m1.1.1" mathsize="50%" xref="S3.T2.12.12.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T2.12.12.4.m1.1b"><csymbol cd="latexml" id="S3.T2.12.12.4.m1.1.1.cmml" xref="S3.T2.12.12.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.12.12.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T2.12.12.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T2.12.12.4.1" style="font-size:50%;">0.015</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T2.12.12.5">1</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 2: </span>Comparing our methods to the <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite> results on the extrapolative sentiment task. We report the proportion of sentences in or beyond the favorable training range (2 stars or fewer for negative sentiment, 4 stars or more for positive sentiment) and a threshold for the extrapolation range (1 star for negative sentiment, 5 stars for positive sentiment). MCMC performs well on those metrics, but notably worsens fluency while requiring nearly 500 iterations. We compare this to <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.T2.15.m1.1"><semantics id="S3.T2.15.m1.1b"><msub id="S3.T2.15.m1.1.1" xref="S3.T2.15.m1.1.1.cmml"><mi id="S3.T2.15.m1.1.1.2" xref="S3.T2.15.m1.1.1.2.cmml">q</mi><mi id="S3.T2.15.m1.1.1.3" xref="S3.T2.15.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T2.15.m1.1c"><apply id="S3.T2.15.m1.1.1.cmml" xref="S3.T2.15.m1.1.1"><csymbol cd="ambiguous" id="S3.T2.15.m1.1.1.1.cmml" xref="S3.T2.15.m1.1.1">subscript</csymbol><ci id="S3.T2.15.m1.1.1.2.cmml" xref="S3.T2.15.m1.1.1.2">𝑞</ci><ci id="S3.T2.15.m1.1.1.3.cmml" xref="S3.T2.15.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.15.m1.1d">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.15.m1.1e">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> trained using first/best training episodes. <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.T2.16.m2.1"><semantics id="S3.T2.16.m2.1b"><msub id="S3.T2.16.m2.1.1" xref="S3.T2.16.m2.1.1.cmml"><mi id="S3.T2.16.m2.1.1.2" xref="S3.T2.16.m2.1.1.2.cmml">q</mi><mi id="S3.T2.16.m2.1.1.3" xref="S3.T2.16.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T2.16.m2.1c"><apply id="S3.T2.16.m2.1.1.cmml" xref="S3.T2.16.m2.1.1"><csymbol cd="ambiguous" id="S3.T2.16.m2.1.1.1.cmml" xref="S3.T2.16.m2.1.1">subscript</csymbol><ci id="S3.T2.16.m2.1.1.2.cmml" xref="S3.T2.16.m2.1.1.2">𝑞</ci><ci id="S3.T2.16.m2.1.1.3.cmml" xref="S3.T2.16.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T2.16.m2.1d">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.T2.16.m2.1e">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> decreases fluency less and requires only a single iteration. We provide 95% confidence intervals over three different test sets. </figcaption>
</figure>
<figure class="ltx_table" id="S3.T3">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S3.T3.4">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T3.3.3">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S3.T3.3.3.4"><span class="ltx_text ltx_font_bold" id="S3.T3.3.3.4.1">Model</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T3.1.1.1.1">EER<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.1.1.1.1.m1.1"><semantics id="S3.T3.1.1.1.1.m1.1a"><mo id="S3.T3.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T3.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.1.1.1.1.m1.1b"><ci id="S3.T3.1.1.1.1.m1.1.1.cmml" xref="S3.T3.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.2.2.2"><span class="ltx_text ltx_font_bold" id="S3.T3.2.2.2.1">SBERT<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T3.2.2.2.1.m1.1"><semantics id="S3.T3.2.2.2.1.m1.1a"><mo id="S3.T3.2.2.2.1.m1.1.1" stretchy="false" xref="S3.T3.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T3.2.2.2.1.m1.1b"><ci id="S3.T3.2.2.2.1.m1.1.1.cmml" xref="S3.T3.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S3.T3.3.3.3"><span class="ltx_text ltx_font_bold" id="S3.T3.3.3.3.1">Iterations<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T3.3.3.3.1.m1.1"><semantics id="S3.T3.3.3.3.1.m1.1a"><mo id="S3.T3.3.3.3.1.m1.1.1" stretchy="false" xref="S3.T3.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T3.3.3.3.1.m1.1b"><ci id="S3.T3.3.3.3.1.m1.1.1.cmml" xref="S3.T3.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T3.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.5.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.4.5.1.1">GPT-3.5</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.5.1.2">0.216</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.5.1.3">0.777</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.5.1.4">1</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.6.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.4.6.2.1">GPT-4</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.2.2">0.238</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.2.3">0.698</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.6.2.4">1</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.7.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.4.7.3.1">DIPPER <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib25" title="">2023</a>)</cite>
</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.3.2">0.206</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.3.3">0.641</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.7.3.4">1</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.8.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T3.4.8.4.1">Round Trip MT</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.4.2">0.110</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.4.3">0.921</td>
<td class="ltx_td ltx_align_center" id="S3.T3.4.8.4.4">1</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.9.5">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T3.4.9.5.1">MCMC</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.9.5.2">0.393</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.9.5.3">0.835</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T3.4.9.5.4">4498</td>
</tr>
<tr class="ltx_tr" id="S3.T3.4.4">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T3.4.4.1"><math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.T3.4.4.1.m1.1"><semantics id="S3.T3.4.4.1.m1.1a"><msub id="S3.T3.4.4.1.m1.1.1" xref="S3.T3.4.4.1.m1.1.1.cmml"><mi id="S3.T3.4.4.1.m1.1.1.2" xref="S3.T3.4.4.1.m1.1.1.2.cmml">q</mi><mi id="S3.T3.4.4.1.m1.1.1.3" xref="S3.T3.4.4.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T3.4.4.1.m1.1b"><apply id="S3.T3.4.4.1.m1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.4.4.1.m1.1.1.1.cmml" xref="S3.T3.4.4.1.m1.1.1">subscript</csymbol><ci id="S3.T3.4.4.1.m1.1.1.2.cmml" xref="S3.T3.4.4.1.m1.1.1.2">𝑞</ci><ci id="S3.T3.4.4.1.m1.1.1.3.cmml" xref="S3.T3.4.4.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.4.4.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.4.4.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.2">0.221</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.3">0.839</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T3.4.4.4">4</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 3: </span>Comparing our methods with anonymization baselines. MCMC achieves improved results over baselines, but takes significantly more iterations than any other method; our best variant of <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.T3.7.m1.1"><semantics id="S3.T3.7.m1.1b"><msub id="S3.T3.7.m1.1.1" xref="S3.T3.7.m1.1.1.cmml"><mi id="S3.T3.7.m1.1.1.2" xref="S3.T3.7.m1.1.1.2.cmml">q</mi><mi id="S3.T3.7.m1.1.1.3" xref="S3.T3.7.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.T3.7.m1.1c"><apply id="S3.T3.7.m1.1.1.cmml" xref="S3.T3.7.m1.1.1"><csymbol cd="ambiguous" id="S3.T3.7.m1.1.1.1.cmml" xref="S3.T3.7.m1.1.1">subscript</csymbol><ci id="S3.T3.7.m1.1.1.2.cmml" xref="S3.T3.7.m1.1.1.2">𝑞</ci><ci id="S3.T3.7.m1.1.1.3.cmml" xref="S3.T3.7.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.7.m1.1d">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.T3.7.m1.1e">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, trained using variable-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T3.8.m2.1"><semantics id="S3.T3.8.m2.1b"><mi id="S3.T3.8.m2.1.1" mathvariant="normal" xref="S3.T3.8.m2.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T3.8.m2.1c"><ci id="S3.T3.8.m2.1.1.cmml" xref="S3.T3.8.m2.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T3.8.m2.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T3.8.m2.1e">roman_Δ</annotation></semantics></math> energy, achieves reasonable performance on both metrics in significantly fewer iterations than MCMC.</figcaption>
</figure>
<figure class="ltx_table" id="S3.T4">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T4.8">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T4.6.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T4.6.6.7"><span class="ltx_text ltx_font_bold" id="S3.T4.6.6.7.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T4.1.1.1.1">-1<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T4.1.1.1.1.m1.1"><semantics id="S3.T4.1.1.1.1.m1.1a"><mo id="S3.T4.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T4.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T4.1.1.1.1.m1.1b"><ci id="S3.T4.1.1.1.1.m1.1.1.cmml" xref="S3.T4.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T4.2.2.2"><span class="ltx_text ltx_font_bold" id="S3.T4.2.2.2.1">-2.5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T4.2.2.2.1.m1.1"><semantics id="S3.T4.2.2.2.1.m1.1a"><mo id="S3.T4.2.2.2.1.m1.1.1" stretchy="false" xref="S3.T4.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T4.2.2.2.1.m1.1b"><ci id="S3.T4.2.2.2.1.m1.1.1.cmml" xref="S3.T4.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.3.3.3"><span class="ltx_text ltx_font_bold" id="S3.T4.3.3.3.1">-5<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T4.3.3.3.1.m1.1"><semantics id="S3.T4.3.3.3.1.m1.1a"><mo id="S3.T4.3.3.3.1.m1.1.1" stretchy="false" xref="S3.T4.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T4.3.3.3.1.m1.1b"><ci id="S3.T4.3.3.3.1.m1.1.1.cmml" xref="S3.T4.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.4.4.4"><span class="ltx_text ltx_font_bold" id="S3.T4.4.4.4.1">-6<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T4.4.4.4.1.m1.1"><semantics id="S3.T4.4.4.4.1.m1.1a"><mo id="S3.T4.4.4.4.1.m1.1.1" stretchy="false" xref="S3.T4.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T4.4.4.4.1.m1.1b"><ci id="S3.T4.4.4.4.1.m1.1.1.cmml" xref="S3.T4.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.4.4.4.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="S3.T4.5.5.5"><span class="ltx_text ltx_font_bold" id="S3.T4.5.5.5.1">-7<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T4.5.5.5.1.m1.1"><semantics id="S3.T4.5.5.5.1.m1.1a"><mo id="S3.T4.5.5.5.1.m1.1.1" stretchy="false" xref="S3.T4.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T4.5.5.5.1.m1.1b"><ci id="S3.T4.5.5.5.1.m1.1.1.cmml" xref="S3.T4.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.5.5.5.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T4.6.6.6"><span class="ltx_text ltx_font_bold" id="S3.T4.6.6.6.1">Iterations<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T4.6.6.6.1.m1.1"><semantics id="S3.T4.6.6.6.1.m1.1a"><mo id="S3.T4.6.6.6.1.m1.1.1" stretchy="false" xref="S3.T4.6.6.6.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T4.6.6.6.1.m1.1b"><ci id="S3.T4.6.6.6.1.m1.1.1.cmml" xref="S3.T4.6.6.6.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.6.6.6.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T4.6.6.6.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T4.8.9.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T4.8.9.1.1">First/Best</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.8.9.1.2"><span class="ltx_text ltx_font_bold" id="S3.T4.8.9.1.2.1">0.978</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T4.8.9.1.3">0.932</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.8.9.1.4">0.609</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.8.9.1.5">0.418</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.T4.8.9.1.6">0.242</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T4.8.9.1.7">1</td>
</tr>
<tr class="ltx_tr" id="S3.T4.8.10.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T4.8.10.2.1">Thinning (fixed-length)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.10.2.2">0.961</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T4.8.10.2.3">0.915</td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.10.2.4">0.715</td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.10.2.5">0.580</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T4.8.10.2.6">0.422</td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.10.2.7">3</td>
</tr>
<tr class="ltx_tr" id="S3.T4.8.11.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T4.8.11.3.1">Thinning (variable-length)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.11.3.2">0.972</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T4.8.11.3.3">0.929</td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.11.3.4">0.714</td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.11.3.5">0.570</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T4.8.11.3.6">0.420</td>
<td class="ltx_td ltx_align_center" id="S3.T4.8.11.3.7">4.890</td>
</tr>
<tr class="ltx_tr" id="S3.T4.7.7">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T4.7.7.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T4.7.7.1.m1.1"><semantics id="S3.T4.7.7.1.m1.1a"><mi id="S3.T4.7.7.1.m1.1.1" mathvariant="normal" xref="S3.T4.7.7.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T4.7.7.1.m1.1b"><ci id="S3.T4.7.7.1.m1.1.1.cmml" xref="S3.T4.7.7.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.7.7.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T4.7.7.1.m1.1d">roman_Δ</annotation></semantics></math> Energy (fixed-length)</td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.2">0.972</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T4.7.7.3"><span class="ltx_text ltx_font_bold" id="S3.T4.7.7.3.1">0.938</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.4"><span class="ltx_text ltx_font_bold" id="S3.T4.7.7.4.1">0.748</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.5"><span class="ltx_text ltx_font_bold" id="S3.T4.7.7.5.1">0.616</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T4.7.7.6"><span class="ltx_text ltx_font_bold" id="S3.T4.7.7.6.1">0.464</span></td>
<td class="ltx_td ltx_align_center" id="S3.T4.7.7.7">3</td>
</tr>
<tr class="ltx_tr" id="S3.T4.8.8">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T4.8.8.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T4.8.8.1.m1.1"><semantics id="S3.T4.8.8.1.m1.1a"><mi id="S3.T4.8.8.1.m1.1.1" mathvariant="normal" xref="S3.T4.8.8.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T4.8.8.1.m1.1b"><ci id="S3.T4.8.8.1.m1.1.1.cmml" xref="S3.T4.8.8.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.8.8.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T4.8.8.1.m1.1d">roman_Δ</annotation></semantics></math> Energy (variable-length)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.8.8.2">0.964</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T4.8.8.3">0.883</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.8.8.4">0.424</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.8.8.5">0.252</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S3.T4.8.8.6">0.133</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T4.8.8.7">3.631</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 4: </span>Varying training episode creation for the ACE2 stability task. We find that fixed-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T4.10.m1.1"><semantics id="S3.T4.10.m1.1b"><mi id="S3.T4.10.m1.1.1" mathvariant="normal" xref="S3.T4.10.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T4.10.m1.1c"><ci id="S3.T4.10.m1.1.1.cmml" xref="S3.T4.10.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T4.10.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T4.10.m1.1e">roman_Δ</annotation></semantics></math> energy outperforms our other training episode creation strategies when extrapolating.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.2 </span>Sentiment extrapolation</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">Given a training dataset of Yelp reviews <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib58" title="">2015</a>)</cite> with sentiment ranging from 2-stars to 4-stars, the goal is to learn to generate reviews that extrapolate beyond the training region to the highly negative (1-star) or highly positive (5-star) reviews. Following <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>, we fit two regression models, a training-time scorer and an oracle scorer used for evaluation. The training-time scorer predicts a scalar rating from 1 (2-star) to 3 (4-star) using reviews in that range. The oracle scorer uses all of the training data and predicts the complete range of ratings given input text. Prior work considers a simple version of this task where success is measured only in proportion of sequences in the extrapolation region. We additionally measure the change in fluency after editing, to prevent our models from greedily optimizing only a single metric at the expense of fluency. Details of our procedure can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A2.SS2" title="B.2 Sentiment ‣ Appendix B Extrapolation experimental details ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ B.2</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">We report results from <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>, namely the ICE and ICE with scorer methods as well as Genhance <cite class="ltx_cite ltx_citemacro_citep">(Chan et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib6" title="">2021</a>)</cite>. ICE with scorer was previously described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.SS1" title="3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3.1</span></a>; without the scorer, the model simply generates a single option for the output sequence. Finally, we report results using FUDGE <cite class="ltx_cite ltx_citemacro_citep">(Yang &amp; Klein, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib55" title="">2021</a>)</cite>, an autoregressive classifier-guided method not specifically designed for extrapolation. We describe our implementation of FUDGE in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A2.SS2" title="B.2 Sentiment ‣ Appendix B Extrapolation experimental details ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ B.2</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Metrics</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">To evaluate sentiment, we use the oracle scorer as described in <cite class="ltx_cite ltx_citemacro_citep">(Padmakumar et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>. When editing in the positive direction, we consider a 4-star review or above to be in the training region, and a 5-star review to be in the extrapolation region; when editing in the negative direction, we consider a 2-star review or below to be in the training region, and a 1-star review to be in the extrapolation region. We also introduce a fluency metric, the median percentage change in perplexity as measured by GPT-2 large <cite class="ltx_cite ltx_citemacro_citep">(Radford et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib36" title="">2019</a>)</cite>. Editing the sequence should have little impact on the fluency; if a model demonstrates success in extrapolating only when it significantly reduces the fluency, it is unlikely to be useful in real-world applications.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1">As the Yelp review dataset does not have a premade validation split <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib58" title="">2015</a>)</cite>, we use the first thousand examples of the test set as a validation set. <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite> report their test results on a random subset of 1831 reviews from the test set, all of which fall in the training range of 2-, 3-, and 4-star reviews. For MCMC and <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px2.p2.1.m1.1"><semantics id="S3.SS2.SSS0.Px2.p2.1.m1.1a"><msub id="S3.SS2.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.2">𝑞</ci><ci id="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px2.p2.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px2.p2.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px2.p2.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, we create three 2000-sentence subsets of the test set and report the average of each of these three runs in our results, finding that there is little variation regardless of the test set. We run FUDGE on one of these 2000-sentence test sets.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.2">We show our results with <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS2.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS2.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> trained on first/best training episodes in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.T2" title="Table 2 ‣ Results ‣ 3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 2</span></a> alongside results from <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>. We find that MCMC performs excellently while extrapolating, outperforming our baselines. Our trained <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS2.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS2.SSS0.Px3.p1.2.m2.1a"><msub id="S3.SS2.SSS0.Px3.p1.2.m2.1.1" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml"><mi id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml">q</mi><mi id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS2.SSS0.Px3.p1.2.m2.1b"><apply id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.1.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.2">𝑞</ci><ci id="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3.cmml" xref="S3.SS2.SSS0.Px3.p1.2.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS2.SSS0.Px3.p1.2.m2.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS2.SSS0.Px3.p1.2.m2.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> outperforms our baselines in extrapolative capacity, and outperforms MCMC in efficiency (as measured by number of iterations) and fluency. Example generations can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A8.SS1" title="H.1 Sentiment ‣ Appendix H Example generations ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ H.1</span></a>.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.3 </span>Anonymization</h3>
<div class="ltx_para" id="S3.SS3.p1">
<p class="ltx_p" id="S3.SS3.p1.2">Writing can exhibit a wide range of stylometric features that can be used to identify the author of a document. In cases where anonymity is desired, there is a need to automatically remove personally-identifying features. Since stylometric features are typically extracted at the document-level <cite class="ltx_cite ltx_citemacro_citep">(Rivera-Soto et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib40" title="">2021</a>)</cite>, it is appealing to tackle this problem using sequence-level objectives. Similar to previous tasks, we first extract training episodes from an MCMC driven sampler. We adapt the style transfer method proposed by <cite class="ltx_cite ltx_citemacro_citet">Khan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib24" title="">2024</a>)</cite> to generate training episodes making one key change: rather than using a specific target style, we parameterize the energy function such that <em class="ltx_emph ltx_font_italic" id="S3.SS3.p1.2.1">any</em> style different from the initial style is desirable. Given some text <math alttext="x" class="ltx_Math" display="inline" id="S3.SS3.p1.1.m1.1"><semantics id="S3.SS3.p1.1.m1.1a"><mi id="S3.SS3.p1.1.m1.1.1" xref="S3.SS3.p1.1.m1.1.1.cmml">x</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.1.m1.1b"><ci id="S3.SS3.p1.1.m1.1.1.cmml" xref="S3.SS3.p1.1.m1.1.1">𝑥</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.1.m1.1c">x</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.1.m1.1d">italic_x</annotation></semantics></math>, the system results in a series of states <math alttext="y_{1},y_{2},...y_{n}" class="ltx_Math" display="inline" id="S3.SS3.p1.2.m2.3"><semantics id="S3.SS3.p1.2.m2.3a"><mrow id="S3.SS3.p1.2.m2.3.3.3" xref="S3.SS3.p1.2.m2.3.3.4.cmml"><msub id="S3.SS3.p1.2.m2.1.1.1.1" xref="S3.SS3.p1.2.m2.1.1.1.1.cmml"><mi id="S3.SS3.p1.2.m2.1.1.1.1.2" xref="S3.SS3.p1.2.m2.1.1.1.1.2.cmml">y</mi><mn id="S3.SS3.p1.2.m2.1.1.1.1.3" xref="S3.SS3.p1.2.m2.1.1.1.1.3.cmml">1</mn></msub><mo id="S3.SS3.p1.2.m2.3.3.3.4" xref="S3.SS3.p1.2.m2.3.3.4.cmml">,</mo><msub id="S3.SS3.p1.2.m2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.cmml"><mi id="S3.SS3.p1.2.m2.2.2.2.2.2" xref="S3.SS3.p1.2.m2.2.2.2.2.2.cmml">y</mi><mn id="S3.SS3.p1.2.m2.2.2.2.2.3" xref="S3.SS3.p1.2.m2.2.2.2.2.3.cmml">2</mn></msub><mo id="S3.SS3.p1.2.m2.3.3.3.5" xref="S3.SS3.p1.2.m2.3.3.4.cmml">,</mo><mrow id="S3.SS3.p1.2.m2.3.3.3.3" xref="S3.SS3.p1.2.m2.3.3.3.3.cmml"><mi id="S3.SS3.p1.2.m2.3.3.3.3.2" mathvariant="normal" xref="S3.SS3.p1.2.m2.3.3.3.3.2.cmml">…</mi><mo id="S3.SS3.p1.2.m2.3.3.3.3.1" xref="S3.SS3.p1.2.m2.3.3.3.3.1.cmml">⁢</mo><msub id="S3.SS3.p1.2.m2.3.3.3.3.3" xref="S3.SS3.p1.2.m2.3.3.3.3.3.cmml"><mi id="S3.SS3.p1.2.m2.3.3.3.3.3.2" xref="S3.SS3.p1.2.m2.3.3.3.3.3.2.cmml">y</mi><mi id="S3.SS3.p1.2.m2.3.3.3.3.3.3" xref="S3.SS3.p1.2.m2.3.3.3.3.3.3.cmml">n</mi></msub></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.SS3.p1.2.m2.3b"><list id="S3.SS3.p1.2.m2.3.3.4.cmml" xref="S3.SS3.p1.2.m2.3.3.3"><apply id="S3.SS3.p1.2.m2.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.1.1.1.1.1.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1">subscript</csymbol><ci id="S3.SS3.p1.2.m2.1.1.1.1.2.cmml" xref="S3.SS3.p1.2.m2.1.1.1.1.2">𝑦</ci><cn id="S3.SS3.p1.2.m2.1.1.1.1.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.1.1.1.1.3">1</cn></apply><apply id="S3.SS3.p1.2.m2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.2.2.2.2.1.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2">subscript</csymbol><ci id="S3.SS3.p1.2.m2.2.2.2.2.2.cmml" xref="S3.SS3.p1.2.m2.2.2.2.2.2">𝑦</ci><cn id="S3.SS3.p1.2.m2.2.2.2.2.3.cmml" type="integer" xref="S3.SS3.p1.2.m2.2.2.2.2.3">2</cn></apply><apply id="S3.SS3.p1.2.m2.3.3.3.3.cmml" xref="S3.SS3.p1.2.m2.3.3.3.3"><times id="S3.SS3.p1.2.m2.3.3.3.3.1.cmml" xref="S3.SS3.p1.2.m2.3.3.3.3.1"></times><ci id="S3.SS3.p1.2.m2.3.3.3.3.2.cmml" xref="S3.SS3.p1.2.m2.3.3.3.3.2">…</ci><apply id="S3.SS3.p1.2.m2.3.3.3.3.3.cmml" xref="S3.SS3.p1.2.m2.3.3.3.3.3"><csymbol cd="ambiguous" id="S3.SS3.p1.2.m2.3.3.3.3.3.1.cmml" xref="S3.SS3.p1.2.m2.3.3.3.3.3">subscript</csymbol><ci id="S3.SS3.p1.2.m2.3.3.3.3.3.2.cmml" xref="S3.SS3.p1.2.m2.3.3.3.3.3.2">𝑦</ci><ci id="S3.SS3.p1.2.m2.3.3.3.3.3.3.cmml" xref="S3.SS3.p1.2.m2.3.3.3.3.3.3">𝑛</ci></apply></apply></list></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.p1.2.m2.3c">y_{1},y_{2},...y_{n}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.p1.2.m2.3d">italic_y start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_y start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … italic_y start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math>, these episodes are then used to train our anonymization system. Details on our adaptation of <cite class="ltx_cite ltx_citemacro_citet">Khan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib24" title="">2024</a>)</cite> can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A7" title="Appendix G Text Anonymization Implementation ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>.</p>
</div>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Baselines</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px1.p1.1">We consider four baseline anonymization systems: GPT3.5, GPT4 <cite class="ltx_cite ltx_citemacro_citep">(OpenAI et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib32" title="">2024</a>)</cite>, DIPPER <cite class="ltx_cite ltx_citemacro_citep">(Krishna et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib25" title="">2023</a>)</cite>, and Round Trip Machine Translation (MT). Implementation details for each system are in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A7.SS1" title="G.1 Baseline Systems ‣ Appendix G Text Anonymization Implementation ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ G.1</span></a>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Metrics</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px2.p1.1">To evaluate the quality of anonymization outputs we consider two metrics measuring author verification: Equal Error Rates (EER), and semantic similarity between original and anonymized text. To compute EER, we replicate the author linking experiment described in <cite class="ltx_cite ltx_citemacro_citet">Khan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib23" title="">2021</a>)</cite>. Our evaluation set consists of 50 authors, each with 16 posts. Given the first 8 <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS0.Px2.p1.1.1">original</em> posts from an author’s history, we attempt to identify the second set of 8 <em class="ltx_emph ltx_font_italic" id="S3.SS3.SSS0.Px2.p1.1.2">anonymized</em> posts as a match, and all other author posts as negatives. We use a pre-trained author embedding <span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/rrivera1849/LUAR-CRUD" title="">https://huggingface.co/rrivera1849/LUAR-CRUD</a></span></span></span> to encode each set of 8 messages into a vector and use cosine similarities between two candidates as a score. If we successfully avoid detection, we expect the EER to rise. We calculate semantic similarity using the publically released <span class="ltx_text ltx_font_typewriter" id="S3.SS3.SSS0.Px2.p1.1.3">all-mpnet-base-v2</span> checkpoint within the sentence transformers library to encode original and anonymized documents. A successful system maintains high semantic similarity.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS3.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Results</h4>
<div class="ltx_para" id="S3.SS3.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS3.SSS0.Px3.p1.2">We find that baseline systems do a poor job at maintaining semantic similarity, or in the case of Round Trip MT, do so at the cost of not circumventing author verification. While the MCMC sampler performs well under both of these metrics, it is costly to run, with an average of 4498 iterations to yield an anonymized sample. Our system, with <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.1.m1.1"><semantics id="S3.SS3.SSS0.Px3.p1.1.m1.1a"><msub id="S3.SS3.SSS0.Px3.p1.1.m1.1.1" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml"><mi id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.1.m1.1b"><apply id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3.cmml" xref="S3.SS3.SSS0.Px3.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> trained on variable-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS3.SSS0.Px3.p1.2.m2.1"><semantics id="S3.SS3.SSS0.Px3.p1.2.m2.1a"><mi id="S3.SS3.SSS0.Px3.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS3.SSS0.Px3.p1.2.m2.1b"><ci id="S3.SS3.SSS0.Px3.p1.2.m2.1.1.cmml" xref="S3.SS3.SSS0.Px3.p1.2.m2.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS3.SSS0.Px3.p1.2.m2.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS3.SSS0.Px3.p1.2.m2.1d">roman_Δ</annotation></semantics></math> energy, returns an anonymized sample with comparatively few in-context iterations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S3.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.4 </span>Analysis of episode creation strategy</h3>
<figure class="ltx_table" id="S3.T5">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T5.21">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T5.4.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T5.4.4.5"><span class="ltx_text ltx_font_bold" id="S3.T5.4.4.5.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T5.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T5.1.1.1.1">Training<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T5.1.1.1.1.m1.1"><semantics id="S3.T5.1.1.1.1.m1.1a"><mo id="S3.T5.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T5.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T5.1.1.1.1.m1.1b"><ci id="S3.T5.1.1.1.1.m1.1.1.cmml" xref="S3.T5.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T5.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="S3.T5.2.2.2">
<span class="ltx_text ltx_font_bold" id="S3.T5.2.2.2.1">Extrapolation</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T5.2.2.2.m1.1"><semantics id="S3.T5.2.2.2.m1.1a"><mo id="S3.T5.2.2.2.m1.1.1" stretchy="false" xref="S3.T5.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T5.2.2.2.m1.1b"><ci id="S3.T5.2.2.2.m1.1.1.cmml" xref="S3.T5.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T5.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.3.3.3"><span class="ltx_text ltx_font_bold" id="S3.T5.3.3.3.1">Fluency<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T5.3.3.3.1.m1.1"><semantics id="S3.T5.3.3.3.1.m1.1a"><mo id="S3.T5.3.3.3.1.m1.1.1" stretchy="false" xref="S3.T5.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T5.3.3.3.1.m1.1b"><ci id="S3.T5.3.3.3.1.m1.1.1.cmml" xref="S3.T5.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T5.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T5.4.4.4"><span class="ltx_text ltx_font_bold" id="S3.T5.4.4.4.1">Iterations<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T5.4.4.4.1.m1.1"><semantics id="S3.T5.4.4.4.1.m1.1a"><mo id="S3.T5.4.4.4.1.m1.1.1" stretchy="false" xref="S3.T5.4.4.4.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T5.4.4.4.1.m1.1b"><ci id="S3.T5.4.4.4.1.m1.1.1.cmml" xref="S3.T5.4.4.4.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.4.4.4.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T5.4.4.4.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T5.7.7">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T5.7.7.4">First/Best</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S3.T5.5.5.1">
<span class="ltx_text ltx_font_bold" id="S3.T5.5.5.1.1">0.925<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.5.5.1.1.m1.1"><semantics id="S3.T5.5.5.1.1.m1.1a"><mo id="S3.T5.5.5.1.1.m1.1.1" mathsize="50%" xref="S3.T5.5.5.1.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.5.5.1.1.m1.1b"><csymbol cd="latexml" id="S3.T5.5.5.1.1.m1.1.1.cmml" xref="S3.T5.5.5.1.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.5.5.1.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.5.5.1.1.m1.1d">±</annotation></semantics></math></span><span class="ltx_text" id="S3.T5.5.5.1.2" style="font-size:50%;">0.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.T5.6.6.2">
<span class="ltx_text ltx_font_bold" id="S3.T5.6.6.2.1">0.734<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.6.6.2.1.m1.1"><semantics id="S3.T5.6.6.2.1.m1.1a"><mo id="S3.T5.6.6.2.1.m1.1.1" mathsize="50%" xref="S3.T5.6.6.2.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.6.6.2.1.m1.1b"><csymbol cd="latexml" id="S3.T5.6.6.2.1.m1.1.1.cmml" xref="S3.T5.6.6.2.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.6.6.2.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.6.6.2.1.m1.1d">±</annotation></semantics></math></span><span class="ltx_text" id="S3.T5.6.6.2.2" style="font-size:50%;">0.008</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.7.7.3">
<span class="ltx_text ltx_font_bold" id="S3.T5.7.7.3.1">0.132%<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.7.7.3.1.m1.1"><semantics id="S3.T5.7.7.3.1.m1.1a"><mo id="S3.T5.7.7.3.1.m1.1.1" mathsize="50%" xref="S3.T5.7.7.3.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.7.7.3.1.m1.1b"><csymbol cd="latexml" id="S3.T5.7.7.3.1.m1.1.1.cmml" xref="S3.T5.7.7.3.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.7.7.3.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.7.7.3.1.m1.1d">±</annotation></semantics></math></span><span class="ltx_text" id="S3.T5.7.7.3.2" style="font-size:50%;">0.015</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T5.7.7.5">1</td>
</tr>
<tr class="ltx_tr" id="S3.T5.10.10">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T5.10.10.4">Thinning (fixed-length)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.8.8.1">0.883<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.8.8.1.m1.1"><semantics id="S3.T5.8.8.1.m1.1a"><mo id="S3.T5.8.8.1.m1.1.1" mathsize="50%" xref="S3.T5.8.8.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.8.8.1.m1.1b"><csymbol cd="latexml" id="S3.T5.8.8.1.m1.1.1.cmml" xref="S3.T5.8.8.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.8.8.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.8.8.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.8.8.1.1" style="font-size:50%;">0.006</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T5.9.9.2">0.642<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.9.9.2.m1.1"><semantics id="S3.T5.9.9.2.m1.1a"><mo id="S3.T5.9.9.2.m1.1.1" mathsize="50%" xref="S3.T5.9.9.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.9.9.2.m1.1b"><csymbol cd="latexml" id="S3.T5.9.9.2.m1.1.1.cmml" xref="S3.T5.9.9.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.9.9.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.9.9.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.9.9.2.1" style="font-size:50%;">0.007</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.10.10.3">0.466%<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.10.10.3.m1.1"><semantics id="S3.T5.10.10.3.m1.1a"><mo id="S3.T5.10.10.3.m1.1.1" mathsize="50%" xref="S3.T5.10.10.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.10.10.3.m1.1b"><csymbol cd="latexml" id="S3.T5.10.10.3.m1.1.1.cmml" xref="S3.T5.10.10.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.10.10.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.10.10.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.10.10.3.1" style="font-size:50%;">0.014</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.10.10.5">4</td>
</tr>
<tr class="ltx_tr" id="S3.T5.13.13">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T5.13.13.4">Thinning (variable-length)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.11.11.1">0.854<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.11.11.1.m1.1"><semantics id="S3.T5.11.11.1.m1.1a"><mo id="S3.T5.11.11.1.m1.1.1" mathsize="50%" xref="S3.T5.11.11.1.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.11.11.1.m1.1b"><csymbol cd="latexml" id="S3.T5.11.11.1.m1.1.1.cmml" xref="S3.T5.11.11.1.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.11.11.1.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.11.11.1.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.11.11.1.1" style="font-size:50%;">0.003</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T5.12.12.2">0.591 <math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.12.12.2.m1.1"><semantics id="S3.T5.12.12.2.m1.1a"><mo id="S3.T5.12.12.2.m1.1.1" mathsize="50%" xref="S3.T5.12.12.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.12.12.2.m1.1b"><csymbol cd="latexml" id="S3.T5.12.12.2.m1.1.1.cmml" xref="S3.T5.12.12.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.12.12.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.12.12.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.12.12.2.1" style="font-size:50%;">0.012</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.13.13.3">0.539%<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.13.13.3.m1.1"><semantics id="S3.T5.13.13.3.m1.1a"><mo id="S3.T5.13.13.3.m1.1.1" mathsize="50%" xref="S3.T5.13.13.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.13.13.3.m1.1b"><csymbol cd="latexml" id="S3.T5.13.13.3.m1.1.1.cmml" xref="S3.T5.13.13.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.13.13.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.13.13.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.13.13.3.1" style="font-size:50%;">0.010</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.13.13.5">3.997</td>
</tr>
<tr class="ltx_tr" id="S3.T5.17.17">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T5.14.14.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T5.14.14.1.m1.1"><semantics id="S3.T5.14.14.1.m1.1a"><mi id="S3.T5.14.14.1.m1.1.1" mathvariant="normal" xref="S3.T5.14.14.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T5.14.14.1.m1.1b"><ci id="S3.T5.14.14.1.m1.1.1.cmml" xref="S3.T5.14.14.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.14.14.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T5.14.14.1.m1.1d">roman_Δ</annotation></semantics></math> Energy (fixed-length)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S3.T5.15.15.2">0.910<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.15.15.2.m1.1"><semantics id="S3.T5.15.15.2.m1.1a"><mo id="S3.T5.15.15.2.m1.1.1" mathsize="50%" xref="S3.T5.15.15.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.15.15.2.m1.1b"><csymbol cd="latexml" id="S3.T5.15.15.2.m1.1.1.cmml" xref="S3.T5.15.15.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.15.15.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.15.15.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.15.15.2.1" style="font-size:50%;">0.005</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T5.16.16.3">0.692<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.16.16.3.m1.1"><semantics id="S3.T5.16.16.3.m1.1a"><mo id="S3.T5.16.16.3.m1.1.1" mathsize="50%" xref="S3.T5.16.16.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.16.16.3.m1.1b"><csymbol cd="latexml" id="S3.T5.16.16.3.m1.1.1.cmml" xref="S3.T5.16.16.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.16.16.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.16.16.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.16.16.3.1" style="font-size:50%;">0.016</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.17.17.4">0.362%<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.17.17.4.m1.1"><semantics id="S3.T5.17.17.4.m1.1a"><mo id="S3.T5.17.17.4.m1.1.1" mathsize="50%" xref="S3.T5.17.17.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.17.17.4.m1.1b"><csymbol cd="latexml" id="S3.T5.17.17.4.m1.1.1.cmml" xref="S3.T5.17.17.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.17.17.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.17.17.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.17.17.4.1" style="font-size:50%;">0.032</span>
</td>
<td class="ltx_td ltx_align_center" id="S3.T5.17.17.5">4</td>
</tr>
<tr class="ltx_tr" id="S3.T5.21.21">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T5.18.18.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T5.18.18.1.m1.1"><semantics id="S3.T5.18.18.1.m1.1a"><mi id="S3.T5.18.18.1.m1.1.1" mathvariant="normal" xref="S3.T5.18.18.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T5.18.18.1.m1.1b"><ci id="S3.T5.18.18.1.m1.1.1.cmml" xref="S3.T5.18.18.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.18.18.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T5.18.18.1.m1.1d">roman_Δ</annotation></semantics></math> Energy (variable-length)</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S3.T5.19.19.2">0.881<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.19.19.2.m1.1"><semantics id="S3.T5.19.19.2.m1.1a"><mo id="S3.T5.19.19.2.m1.1.1" mathsize="50%" xref="S3.T5.19.19.2.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.19.19.2.m1.1b"><csymbol cd="latexml" id="S3.T5.19.19.2.m1.1.1.cmml" xref="S3.T5.19.19.2.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.19.19.2.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.19.19.2.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.19.19.2.1" style="font-size:50%;">0.004</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S3.T5.20.20.3">0.677<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.20.20.3.m1.1"><semantics id="S3.T5.20.20.3.m1.1a"><mo id="S3.T5.20.20.3.m1.1.1" mathsize="50%" xref="S3.T5.20.20.3.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.20.20.3.m1.1b"><csymbol cd="latexml" id="S3.T5.20.20.3.m1.1.1.cmml" xref="S3.T5.20.20.3.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.20.20.3.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.20.20.3.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.20.20.3.1" style="font-size:50%;">
0.006</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.21.21.4">0.396%<math alttext="\pm" class="ltx_Math" display="inline" id="S3.T5.21.21.4.m1.1"><semantics id="S3.T5.21.21.4.m1.1a"><mo id="S3.T5.21.21.4.m1.1.1" mathsize="50%" xref="S3.T5.21.21.4.m1.1.1.cmml">±</mo><annotation-xml encoding="MathML-Content" id="S3.T5.21.21.4.m1.1b"><csymbol cd="latexml" id="S3.T5.21.21.4.m1.1.1.cmml" xref="S3.T5.21.21.4.m1.1.1">plus-or-minus</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S3.T5.21.21.4.m1.1c">\pm</annotation><annotation encoding="application/x-llamapun" id="S3.T5.21.21.4.m1.1d">±</annotation></semantics></math><span class="ltx_text" id="S3.T5.21.21.4.1" style="font-size:50%;">0.028</span>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T5.21.21.5">5.855</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 5: </span>Applying various training episode creation strategies to the sentiment task. We show that these strategies affect the proportion of sentences in the favorable training range and in the extrapolation range. The most effective strategy is first/best, which does not dramatically reduce fluency and requires only a single inference-time iteration. </figcaption>
</figure>
<figure class="ltx_table" id="S3.T6">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S3.T6.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S3.T6.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S3.T6.3.3.4"><span class="ltx_text ltx_font_bold" id="S3.T6.3.3.4.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.1.1.1"><span class="ltx_text ltx_font_bold" id="S3.T6.1.1.1.1">EER<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T6.1.1.1.1.m1.1"><semantics id="S3.T6.1.1.1.1.m1.1a"><mo id="S3.T6.1.1.1.1.m1.1.1" stretchy="false" xref="S3.T6.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T6.1.1.1.1.m1.1b"><ci id="S3.T6.1.1.1.1.m1.1.1.cmml" xref="S3.T6.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T6.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="S3.T6.2.2.2"><span class="ltx_text ltx_font_bold" id="S3.T6.2.2.2.1">SBERT<math alttext="\uparrow" class="ltx_Math" display="inline" id="S3.T6.2.2.2.1.m1.1"><semantics id="S3.T6.2.2.2.1.m1.1a"><mo id="S3.T6.2.2.2.1.m1.1.1" stretchy="false" xref="S3.T6.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="S3.T6.2.2.2.1.m1.1b"><ci id="S3.T6.2.2.2.1.m1.1.1.cmml" xref="S3.T6.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="S3.T6.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S3.T6.3.3.3"><span class="ltx_text ltx_font_bold" id="S3.T6.3.3.3.1">Iterations<math alttext="\downarrow" class="ltx_Math" display="inline" id="S3.T6.3.3.3.1.m1.1"><semantics id="S3.T6.3.3.3.1.m1.1a"><mo id="S3.T6.3.3.3.1.m1.1.1" stretchy="false" xref="S3.T6.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="S3.T6.3.3.3.1.m1.1b"><ci id="S3.T6.3.3.3.1.m1.1.1.cmml" xref="S3.T6.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="S3.T6.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S3.T6.5.6.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S3.T6.5.6.1.1">First/Best</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.5.6.1.2">0.132</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="S3.T6.5.6.1.3"><span class="ltx_text ltx_font_bold" id="S3.T6.5.6.1.3.1">0.923</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S3.T6.5.6.1.4">1</td>
</tr>
<tr class="ltx_tr" id="S3.T6.5.7.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.5.7.2.1">Thinning (fixed-length)</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.7.2.2">0.209</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T6.5.7.2.3">0.810</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.7.2.4">4</td>
</tr>
<tr class="ltx_tr" id="S3.T6.5.8.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.5.8.3.1">Thinning (variable-length)</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.8.3.2">0.202</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T6.5.8.3.3">0.809</td>
<td class="ltx_td ltx_align_center" id="S3.T6.5.8.3.4">12.75</td>
</tr>
<tr class="ltx_tr" id="S3.T6.4.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="S3.T6.4.4.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T6.4.4.1.m1.1"><semantics id="S3.T6.4.4.1.m1.1a"><mi id="S3.T6.4.4.1.m1.1.1" mathvariant="normal" xref="S3.T6.4.4.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T6.4.4.1.m1.1b"><ci id="S3.T6.4.4.1.m1.1.1.cmml" xref="S3.T6.4.4.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.4.4.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T6.4.4.1.m1.1d">roman_Δ</annotation></semantics></math> Energy (fixed-length)</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.2">0.192</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="S3.T6.4.4.3">0.840</td>
<td class="ltx_td ltx_align_center" id="S3.T6.4.4.4">4</td>
</tr>
<tr class="ltx_tr" id="S3.T6.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S3.T6.5.5.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T6.5.5.1.m1.1"><semantics id="S3.T6.5.5.1.m1.1a"><mi id="S3.T6.5.5.1.m1.1.1" mathvariant="normal" xref="S3.T6.5.5.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T6.5.5.1.m1.1b"><ci id="S3.T6.5.5.1.m1.1.1.cmml" xref="S3.T6.5.5.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.5.5.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T6.5.5.1.m1.1d">roman_Δ</annotation></semantics></math> Energy (variable-length)</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.5.5.2"><span class="ltx_text ltx_font_bold" id="S3.T6.5.5.2.1">0.221</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="S3.T6.5.5.3">0.839</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S3.T6.5.5.4">12.75</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 6: </span>Anonymization results with our proposed episode strategies. <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.T6.7.m1.1"><semantics id="S3.T6.7.m1.1b"><mi id="S3.T6.7.m1.1.1" mathvariant="normal" xref="S3.T6.7.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.T6.7.m1.1c"><ci id="S3.T6.7.m1.1.1.cmml" xref="S3.T6.7.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.T6.7.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.T6.7.m1.1e">roman_Δ</annotation></semantics></math> energy strategies tend to have higher SBERT scores than thinning strategies, with little to no tradeoff on EER. </figcaption>
</figure>
<div class="ltx_para" id="S3.SS4.p1">
<p class="ltx_p" id="S3.SS4.p1.8">Tables <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.T4" title="Table 4 ‣ Results ‣ 3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">4</span></a>, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.T5" title="Table 5 ‣ 3.4 Analysis of episode creation strategy ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">5</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#S3.T6" title="Table 6 ‣ 3.4 Analysis of episode creation strategy ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">6</span></a> show the effects of different methods of creating training episodes to train <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS4.p1.1.m1.1"><semantics id="S3.SS4.p1.1.m1.1a"><msub id="S3.SS4.p1.1.m1.1.1" xref="S3.SS4.p1.1.m1.1.1.cmml"><mi id="S3.SS4.p1.1.m1.1.1.2" xref="S3.SS4.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS4.p1.1.m1.1.1.3" xref="S3.SS4.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.1.m1.1b"><apply id="S3.SS4.p1.1.m1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS4.p1.1.m1.1.1.1.cmml" xref="S3.SS4.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS4.p1.1.m1.1.1.2.cmml" xref="S3.SS4.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.SS4.p1.1.m1.1.1.3.cmml" xref="S3.SS4.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> as described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S2.SS3" title="2.3 Creating training episodes ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 2.3</span></a>. In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.T4" title="Table 4 ‣ Results ‣ 3.1 Protein engineering ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 4</span></a>, we find that selecting states using <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS4.p1.2.m2.1"><semantics id="S3.SS4.p1.2.m2.1a"><mi id="S3.SS4.p1.2.m2.1.1" mathvariant="normal" xref="S3.SS4.p1.2.m2.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.2.m2.1b"><ci id="S3.SS4.p1.2.m2.1.1.cmml" xref="S3.SS4.p1.2.m2.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.2.m2.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.2.m2.1d">roman_Δ</annotation></semantics></math> energy (fixed-length) outperforms both naive thinning methods by several points. However, <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS4.p1.3.m3.1"><semantics id="S3.SS4.p1.3.m3.1a"><mi id="S3.SS4.p1.3.m3.1.1" mathvariant="normal" xref="S3.SS4.p1.3.m3.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.3.m3.1b"><ci id="S3.SS4.p1.3.m3.1.1.cmml" xref="S3.SS4.p1.3.m3.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.3.m3.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.3.m3.1d">roman_Δ</annotation></semantics></math> energy (variable-length) underperforms significantly. This weakness is not found in the results for sentiment (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.T5" title="Table 5 ‣ 3.4 Analysis of episode creation strategy ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 5</span></a>) or anonymization (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.T6" title="Table 6 ‣ 3.4 Analysis of episode creation strategy ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 6</span></a>), where variable-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS4.p1.4.m4.1"><semantics id="S3.SS4.p1.4.m4.1a"><mi id="S3.SS4.p1.4.m4.1.1" mathvariant="normal" xref="S3.SS4.p1.4.m4.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.4.m4.1b"><ci id="S3.SS4.p1.4.m4.1.1.cmml" xref="S3.SS4.p1.4.m4.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.4.m4.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.4.m4.1d">roman_Δ</annotation></semantics></math> energy performs comparatively to fixed-length <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS4.p1.5.m5.1"><semantics id="S3.SS4.p1.5.m5.1a"><mi id="S3.SS4.p1.5.m5.1.1" mathvariant="normal" xref="S3.SS4.p1.5.m5.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.5.m5.1b"><ci id="S3.SS4.p1.5.m5.1.1.cmml" xref="S3.SS4.p1.5.m5.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.5.m5.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.5.m5.1d">roman_Δ</annotation></semantics></math> energy. In sentiment, it’s clear that <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS4.p1.6.m6.1"><semantics id="S3.SS4.p1.6.m6.1a"><mi id="S3.SS4.p1.6.m6.1.1" mathvariant="normal" xref="S3.SS4.p1.6.m6.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.6.m6.1b"><ci id="S3.SS4.p1.6.m6.1.1.cmml" xref="S3.SS4.p1.6.m6.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.6.m6.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.6.m6.1d">roman_Δ</annotation></semantics></math> energy methods of selecting training episodes have advantages over thinning in the extrapolation range.This pattern is echoed in our interpolation task, anonymization: <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS4.p1.7.m7.1"><semantics id="S3.SS4.p1.7.m7.1a"><mi id="S3.SS4.p1.7.m7.1.1" mathvariant="normal" xref="S3.SS4.p1.7.m7.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.7.m7.1b"><ci id="S3.SS4.p1.7.m7.1.1.cmml" xref="S3.SS4.p1.7.m7.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.7.m7.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.7.m7.1d">roman_Δ</annotation></semantics></math> energy methods and thinning methods both achieve similar EER, as all data is within the training range. However, <math alttext="\Delta" class="ltx_Math" display="inline" id="S3.SS4.p1.8.m8.1"><semantics id="S3.SS4.p1.8.m8.1a"><mi id="S3.SS4.p1.8.m8.1.1" mathvariant="normal" xref="S3.SS4.p1.8.m8.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="S3.SS4.p1.8.m8.1b"><ci id="S3.SS4.p1.8.m8.1.1.cmml" xref="S3.SS4.p1.8.m8.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS4.p1.8.m8.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="S3.SS4.p1.8.m8.1d">roman_Δ</annotation></semantics></math> energy methods preserve more semantic features of the text compared to uniform thinning, similarly to the fluency results in sentiment. This may indicate that thinning methods tend to change more elements of the text that are irrelevant to the target score. These results suggest that in cases when the model cannot learn a transformation in a single step—our “first/best” variant—choosing states using their change in energy is likely to result in the best outcome.</p>
</div>
</section>
<section class="ltx_subsection" id="S3.SS5">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">3.5 </span>Approximating <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS5.1.m1.1"><semantics id="S3.SS5.1.m1.1b"><msub id="S3.SS5.1.m1.1.1" xref="S3.SS5.1.m1.1.1.cmml"><mi id="S3.SS5.1.m1.1.1.2" xref="S3.SS5.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS5.1.m1.1.1.3" xref="S3.SS5.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.1.m1.1c"><apply id="S3.SS5.1.m1.1.1.cmml" xref="S3.SS5.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.1.m1.1.1.1.cmml" xref="S3.SS5.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.1.m1.1.1.2.cmml" xref="S3.SS5.1.m1.1.1.2">𝑞</ci><ci id="S3.SS5.1.m1.1.1.3.cmml" xref="S3.SS5.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.1.m1.1d">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.1.m1.1e">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> through further MCMC exploration</h3>
<div class="ltx_para" id="S3.SS5.p1">
<p class="ltx_p" id="S3.SS5.p1.3">In the case of the protein engineering task, we find that <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS5.p1.1.m1.1"><semantics id="S3.SS5.p1.1.m1.1a"><msub id="S3.SS5.p1.1.m1.1.1" xref="S3.SS5.p1.1.m1.1.1.cmml"><mi id="S3.SS5.p1.1.m1.1.1.2" xref="S3.SS5.p1.1.m1.1.1.2.cmml">q</mi><mi id="S3.SS5.p1.1.m1.1.1.3" xref="S3.SS5.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.1.m1.1b"><apply id="S3.SS5.p1.1.m1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.1.m1.1.1.1.cmml" xref="S3.SS5.p1.1.m1.1.1">subscript</csymbol><ci id="S3.SS5.p1.1.m1.1.1.2.cmml" xref="S3.SS5.p1.1.m1.1.1.2">𝑞</ci><ci id="S3.SS5.p1.1.m1.1.1.3.cmml" xref="S3.SS5.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> significantly outperforms MCMC in the extrapolation range. As the protein synthesis task involves starting from the wildtype each time, we investigate whether we can achieve similar performance to <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS5.p1.2.m2.1"><semantics id="S3.SS5.p1.2.m2.1a"><msub id="S3.SS5.p1.2.m2.1.1" xref="S3.SS5.p1.2.m2.1.1.cmml"><mi id="S3.SS5.p1.2.m2.1.1.2" xref="S3.SS5.p1.2.m2.1.1.2.cmml">q</mi><mi id="S3.SS5.p1.2.m2.1.1.3" xref="S3.SS5.p1.2.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.2.m2.1b"><apply id="S3.SS5.p1.2.m2.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.2.m2.1.1.1.cmml" xref="S3.SS5.p1.2.m2.1.1">subscript</csymbol><ci id="S3.SS5.p1.2.m2.1.1.2.cmml" xref="S3.SS5.p1.2.m2.1.1.2">𝑞</ci><ci id="S3.SS5.p1.2.m2.1.1.3.cmml" xref="S3.SS5.p1.2.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.2.m2.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.2.m2.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> by simply running further steps of MCMC. We run the model for ten epochs<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>83 steps, the sequence length</span></span></span> and report the results in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.F2" title="Figure 2 ‣ 3.5 Approximating 𝑞_𝜃 through further MCMC exploration ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Figure 2</span></a>. We find that further MCMC does not begin to approach the performance of <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.SS5.p1.3.m3.1"><semantics id="S3.SS5.p1.3.m3.1a"><msub id="S3.SS5.p1.3.m3.1.1" xref="S3.SS5.p1.3.m3.1.1.cmml"><mi id="S3.SS5.p1.3.m3.1.1.2" xref="S3.SS5.p1.3.m3.1.1.2.cmml">q</mi><mi id="S3.SS5.p1.3.m3.1.1.3" xref="S3.SS5.p1.3.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS5.p1.3.m3.1b"><apply id="S3.SS5.p1.3.m3.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS5.p1.3.m3.1.1.1.cmml" xref="S3.SS5.p1.3.m3.1.1">subscript</csymbol><ci id="S3.SS5.p1.3.m3.1.1.2.cmml" xref="S3.SS5.p1.3.m3.1.1.2">𝑞</ci><ci id="S3.SS5.p1.3.m3.1.1.3.cmml" xref="S3.SS5.p1.3.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS5.p1.3.m3.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.SS5.p1.3.m3.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, demonstrating that our model may have a generalization benefit that cannot be replicated by further MCMC sampling.</p>
</div>
<figure class="ltx_figure" id="S3.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="219" id="S3.F2.g1" src="x2.png" width="294"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>In the protein engineering task, comparing MCMC performance (solid line) over ten epochs, or 830 steps, compared to the performance of <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.F2.3.m1.1"><semantics id="S3.F2.3.m1.1b"><msub id="S3.F2.3.m1.1.1" xref="S3.F2.3.m1.1.1.cmml"><mi id="S3.F2.3.m1.1.1.2" xref="S3.F2.3.m1.1.1.2.cmml">q</mi><mi id="S3.F2.3.m1.1.1.3" xref="S3.F2.3.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.3.m1.1c"><apply id="S3.F2.3.m1.1.1.cmml" xref="S3.F2.3.m1.1.1"><csymbol cd="ambiguous" id="S3.F2.3.m1.1.1.1.cmml" xref="S3.F2.3.m1.1.1">subscript</csymbol><ci id="S3.F2.3.m1.1.1.2.cmml" xref="S3.F2.3.m1.1.1.2">𝑞</ci><ci id="S3.F2.3.m1.1.1.3.cmml" xref="S3.F2.3.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.3.m1.1d">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.3.m1.1e">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> (dotted line) trained on MCMC data generated on one epoch, or 83 steps. We find that MCMC does not approach the performance of <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S3.F2.4.m2.1"><semantics id="S3.F2.4.m2.1b"><msub id="S3.F2.4.m2.1.1" xref="S3.F2.4.m2.1.1.cmml"><mi id="S3.F2.4.m2.1.1.2" xref="S3.F2.4.m2.1.1.2.cmml">q</mi><mi id="S3.F2.4.m2.1.1.3" xref="S3.F2.4.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S3.F2.4.m2.1c"><apply id="S3.F2.4.m2.1.1.cmml" xref="S3.F2.4.m2.1.1"><csymbol cd="ambiguous" id="S3.F2.4.m2.1.1.1.cmml" xref="S3.F2.4.m2.1.1">subscript</csymbol><ci id="S3.F2.4.m2.1.1.2.cmml" xref="S3.F2.4.m2.1.1.2">𝑞</ci><ci id="S3.F2.4.m2.1.1.3.cmml" xref="S3.F2.4.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.F2.4.m2.1d">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S3.F2.4.m2.1e">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> and does not notably improve after even two epochs.</figcaption>
</figure>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span>Related Work</h2>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Controllable generation</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px1.p1.1">Autoregressive decoding is a favored strategy in controllable text generation. Prior to the advent of instruction-tuned LLMs, a discriminator model was often used to guide decoding <cite class="ltx_cite ltx_citemacro_citep">(Dathathri et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib8" title="">2020</a>; Yang &amp; Klein, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib55" title="">2021</a>)</cite>. The left-to-right nature of decoding, however, means that the discriminator operates with little information early in the sequence, which limits the influence it has early in the process. Our approach addresses this shortcoming by following a <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px1.p1.1.1">sequence-level</span> text generation objective, providing a notion of control that depends on the <span class="ltx_text ltx_font_italic" id="S4.SS0.SSS0.Px1.p1.1.2">entire</span> sequence and can therefore incorporate sequence-level scores as feedback in the generative process.
Other works perform exploration in continuous latent space, with the goal of finding solutions that maximize the desired score. To that end, variational autoencoders have been used in several domains for controllable generation <cite class="ltx_cite ltx_citemacro_citep">(Sevgen et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib45" title="">2023</a>; Wang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib54" title="">2019</a>)</cite>. Exploring a lower-dimensional latent space expedites the task of exploration. However, VAEs are challenged by the fact that output samples have higher variance than input sequences <cite class="ltx_cite ltx_citemacro_citep">(Bredell et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib4" title="">2023</a>)</cite>. Apart from VAEs, <cite class="ltx_cite ltx_citemacro_citet">Chan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib6" title="">2021</a>)</cite> perturb representations of a sequence in a learned latent space to generate sequences that score well on sequence-level metrics;<cite class="ltx_cite ltx_citemacro_citet">Tagasovska et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib49" title="">2024</a>)</cite> performs discriminator-free controllable generation using pairwise “matching” and optimization in latent space. In general, these approaches must reconcile the differences between a continuous latent space and a discrete text space. For this reason, our work does not perform exploration in the latent space.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Editing models</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px2.p1.1">Incremental edits offer models multiple chances to explore the sequence space, increasing the likelihood that they find more optimal solutions.
These edits may be token-level changes <cite class="ltx_cite ltx_citemacro_citep">(Reid &amp; Neubig, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib38" title="">2022</a>; Malmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib30" title="">2019</a>; Kasner &amp; Dušek, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib22" title="">2020</a>; Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib57" title="">2020b</a>)</cite>, alterations to short subsequences <cite class="ltx_cite ltx_citemacro_citep">(Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib43" title="">2023</a>)</cite>, or even rewrites of the entire sequence <cite class="ltx_cite ltx_citemacro_citep">(Agrawal &amp; Carpuat, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib1" title="">2022</a>; Shu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib46" title="">2024</a>)</cite>.
A challenge for constructing editing models is the need for supervised training data. Many editing models are trained on sequences of edits from Wikipedia pages <cite class="ltx_cite ltx_citemacro_citep">(Schick et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib43" title="">2023</a>; Malmi et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib30" title="">2019</a>; Reid &amp; Neubig, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib38" title="">2022</a>)</cite>, as it is an easily accessible repository of edited text. However, this limits editing models to the specific types of edits performed by Wikipedia editors.
To avoid this limitation, <cite class="ltx_cite ltx_citemacro_citet">Zhang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib57" title="">2020b</a>)</cite> use an MCTS approach that instead guides the edits with a variety of constraints. Our approach has the same advantages and also offers a means to drastically speed up inference by learning <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S4.SS0.SSS0.Px2.p1.1.m1.1"><semantics id="S4.SS0.SSS0.Px2.p1.1.m1.1a"><msub id="S4.SS0.SSS0.Px2.p1.1.m1.1.1" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml"><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml">q</mi><mi id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.3" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S4.SS0.SSS0.Px2.p1.1.m1.1b"><apply id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.1.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1">subscript</csymbol><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.2">𝑞</ci><ci id="S4.SS0.SSS0.Px2.p1.1.m1.1.1.3.cmml" xref="S4.SS0.SSS0.Px2.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S4.SS0.SSS0.Px2.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S4.SS0.SSS0.Px2.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Reinforcement Learning</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px3.p1.1">Reinforcement learning (RL) is effective at learning a policy to maximize its reward; however, the formulation of the reward function impacts the success of the policy, as policies may overfit to a proxy reward function rather than satisfying the underlying objective <cite class="ltx_cite ltx_citemacro_citep">(Gao et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib14" title="">2023</a>)</cite>. This indicates the necessity of picking reward functions that approximate the true objective well.
Our work bears many conceptual similarities to RL, notably the use of explicit score modeling in our learned extrapolation model <cite class="ltx_cite ltx_citemacro_citep">(Janner et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib21" title="">2021</a>; Chen et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib7" title="">2024</a>)</cite>. A related approach is <cite class="ltx_cite ltx_citemacro_citet">Jain et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib20" title="">2022</a>)</cite>, who use a diversity-promoting RL objective to learn a policy without MCMC while preserving adequate exploration.
However, our approach is considerably simpler than RL to apply, as our policy is fit using standard supervised learning, and therefore, is straightforward to apply in settings involving large language models.</p>
</div>
</section>
<section class="ltx_paragraph" id="S4.SS0.SSS0.Px4">
<h4 class="ltx_title ltx_title_paragraph">Inference-time scaling</h4>
<div class="ltx_para" id="S4.SS0.SSS0.Px4.p1">
<p class="ltx_p" id="S4.SS0.SSS0.Px4.p1.1">Prior works have found that applying more compute during test time (i.e., via more expensive process reward models or search algorithms) can improve the performance of language models in a variety of settings <cite class="ltx_cite ltx_citemacro_citep">(Snell et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib47" title="">2025</a>)</cite>.
Monte Carlo methods have been proposed as an efficient way to search for optimal solutions during inference <cite class="ltx_cite ltx_citemacro_citep">(Puri et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib35" title="">2025</a>)</cite>.
Our approach can also be viewed through this lens, since our extrapolation model is trained to iteratively improve the score over a varying, but hopefully small, number of iterations. Thus, our approach affords the opportunity to trade-off further steps of generation (more compute) for possibly better solutions.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span>Conclusion</h2>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Main findings</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px1.p1.1">Can pre-trained language models be leveraged to learn a sample-efficient extrapolation model? Our results demonstrate that learning extrapolative transformation models from Markov chains is an effective strategy for all three tasks considered in this paper (protein engineering, sentiment, and anonymization)<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A5" title="Appendix E Diversity of generations ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Appendix E</span></a>, we demonstrate that outputs are diverse as well as high-scoring.</span></span></span>. We outperform baseline methods in dramatically fewer steps than MCMC. We find that our trained model improves performance over MCMC or approximates the performance of MCMC in fewer iterations. We additionally find that in cases when <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="S5.SS0.SSS0.Px1.p1.1.m1.1"><semantics id="S5.SS0.SSS0.Px1.p1.1.m1.1a"><msub id="S5.SS0.SSS0.Px1.p1.1.m1.1.1" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml"><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml">q</mi><mi id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="S5.SS0.SSS0.Px1.p1.1.m1.1b"><apply id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.1.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1">subscript</csymbol><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.2">𝑞</ci><ci id="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3.cmml" xref="S5.SS0.SSS0.Px1.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S5.SS0.SSS0.Px1.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="S5.SS0.SSS0.Px1.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> outperforms MCMC, we are unable to replicate this performance with further MCMC sampling (see <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.SS5" title="3.5 Approximating 𝑞_𝜃 through further MCMC exploration ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3.5</span></a>). Examining strategies for constructing training episode in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.SS4" title="3.4 Analysis of episode creation strategy ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3.4</span></a>, we find that using information from changes in energy increases the fine-tuned model’s performance.</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Limitations</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px2.p1.1">Due to the fact that our extrapolation tasks require methods to have not been explicitly tuned on data in the extrapolation range, we are unable to compare to many state-of-the-art baselines, such as prompting.<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>For instance, it would be trivially easy to generate a highly positive review by prompting an instruction-tuned LLM, as they have almost certainly been trained on that task.</span></span></span> While we compare to these methods in our interpolation task (<a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S3.SS3" title="3.3 Anonymization ‣ 3 Experiments ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 3.3</span></a>), for our extrapolation tasks we compare to baselines explicitly designed for extrapolation which control training conditions. Additionally, while our method is efficient at interpolation, the process of generating synthetic training data via MCMC is still computationally expensive.
Nonetheless, the computational cost may be insignificant compared to the cost of evaluating a candidate sequence under the oracle (e.g., conducting a physical experiment in biological sequence design tasks).</p>
</div>
</section>
<section class="ltx_paragraph" id="S5.SS0.SSS0.Px3">
<h4 class="ltx_title ltx_title_paragraph">Future work</h4>
<div class="ltx_para" id="S5.SS0.SSS0.Px3.p1">
<p class="ltx_p" id="S5.SS0.SSS0.Px3.p1.1">We are excited about the prospect of applying the proposed approach to other sequence-level extrapolation problems, such as molecule design, where pre-trained sequence models are available. For the tasks we consider in this paper, we expect that better pre-trained models, for example trained on more data or that have more parameters, will result in improved performance. Another interesting avenue for future work would be to perform further on-policy fine-tuning of our policy after initializing it using the proposed approach, which we expect could further improve performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="Sx1">
<h2 class="ltx_title ltx_title_section">Impact Statement</h2>
<div class="ltx_para" id="Sx1.p1">
<p class="ltx_p" id="Sx1.p1.1">This paper presents work whose goal is to advance the field of
Machine Learning. There are many potential societal consequences
of our work, none which we feel must be specifically highlighted here.</p>
</div>
</section>
<section class="ltx_section" id="Sx2">
<h2 class="ltx_title ltx_title_section">Acknowledgements</h2>
<div class="ltx_para" id="Sx2.p1">
<p class="ltx_p" id="Sx2.p1.1">This work was supported
by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research
Projects Activity (IARPA), via the HIATUS Program under contract D2022-2205150003. The
views and conclusions contained herein are those of the authors and should not be interpreted as
necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or
the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for
governmental purposes notwithstanding any copyright annotation therein.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Agrawal &amp; Carpuat (2022)</span>
<span class="ltx_bibblock">
Agrawal, S. and Carpuat, M.

</span>
<span class="ltx_bibblock">An imitation learning curriculum for text editing with non-autoregressive models.

</span>
<span class="ltx_bibblock">In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  7550–7563, Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.acl-long.520</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.520/" title="">https://aclanthology.org/2022.acl-long.520/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Andrews &amp; Bishop (2019)</span>
<span class="ltx_bibblock">
Andrews, N. and Bishop, M.

</span>
<span class="ltx_bibblock">Learning invariant representations of social media users.

</span>
<span class="ltx_bibblock">In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pp.  1684–1695, Hong Kong, China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D19-1178</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D19-1178" title="">https://aclanthology.org/D19-1178</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bavarian et al. (2022)</span>
<span class="ltx_bibblock">
Bavarian, M., Jun, H., Tezak, N., Schulman, J., McLeavey, C., Tworek, J., and Chen, M.

</span>
<span class="ltx_bibblock">Efficient training of language models to fill in the middle, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2207.14255" title="">https://arxiv.org/abs/2207.14255</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Bredell et al. (2023)</span>
<span class="ltx_bibblock">
Bredell, G., Flouris, K., Chaitanya, K., Erdil, E., and Konukoglu, E.

</span>
<span class="ltx_bibblock">Explicitly minimizing the blur error of variational autoencoders.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=9krnQ-ue9M" title="">https://openreview.net/forum?id=9krnQ-ue9M</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chakrabarty et al. (2024)</span>
<span class="ltx_bibblock">
Chakrabarty, T., Laban, P., Agarwal, D., Muresan, S., and Wu, C.-S.

</span>
<span class="ltx_bibblock">Art or artifice? large language models and the false promise of creativity.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">Proceedings of the CHI Conference on Human Factors in Computing Systems</em>, pp.  1–34, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chan et al. (2021)</span>
<span class="ltx_bibblock">
Chan, A., Madani, A., Krause, B., and Naik, N.

</span>
<span class="ltx_bibblock">Deep extrapolation for attribute-enhanced generation.

</span>
<span class="ltx_bibblock">In Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">Advances in Neural Information Processing Systems</em>, 2021.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=NCDMYD2y5kK" title="">https://openreview.net/forum?id=NCDMYD2y5kK</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Chen et al. (2024)</span>
<span class="ltx_bibblock">
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I.

</span>
<span class="ltx_bibblock">Decision transformer: reinforcement learning via sequence modeling.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">Proceedings of the 35th International Conference on Neural Information Processing Systems</em>, NIPS ’21, Red Hook, NY, USA, 2024. Curran Associates Inc.

</span>
<span class="ltx_bibblock">ISBN 9781713845393.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dathathri et al. (2020)</span>
<span class="ltx_bibblock">
Dathathri, S., Madotto, A., Lan, J., Hung, J., Frank, E., Molino, P., Yosinski, J., and Liu, R.

</span>
<span class="ltx_bibblock">Plug and play language models: A simple approach to controlled text generation.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">International Conference on Learning Representations</em>, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=H1edEyBKDS" title="">https://openreview.net/forum?id=H1edEyBKDS</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Devlin et al. (2019)</span>
<span class="ltx_bibblock">
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K.

</span>
<span class="ltx_bibblock">BERT: Pre-training of deep bidirectional transformers for language understanding.

</span>
<span class="ltx_bibblock">In Burstein, J., Doran, C., and Solorio, T. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.  4171–4186, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1423</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1423" title="">https://aclanthology.org/N19-1423</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dubey et al. (2024)</span>
<span class="ltx_bibblock">
Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., Goyal, A., Hartshorn, A., Yang, A., Mitra, A., Sravankumar, A., Korenev, A., Hinsvark, A., Rao, A., Zhang, A., Rodriguez, A., Gregerson, A., Spataru, A., Rozière, B., Biron, B., Tang, B., Chern, B., Caucheteux, C., Nayak, C., Bi, C., Marra, C., McConnell, C., Keller, C., Touret, C., Wu, C., Wong, C., Ferrer, C. C., Nikolaidis, C., Allonsius, D., Song, D., Pintz, D., Livshits, D., Esiobu, D., Choudhary, D., Mahajan, D., Garcia-Olano, D., Perino, D., Hupkes, D., Lakomkin, E., AlBadawy, E., Lobanova, E., Dinan, E., Smith, E. M., Radenovic, F., Zhang, F., Synnaeve, G., Lee, G., Anderson, G. L., Nail, G., Mialon, G., Pang, G., Cucurell, G., Nguyen, H., Korevaar, H., Xu, H., Touvron, H., Zarov, I., Ibarra, I. A., Kloumann, I. M., Misra, I., Evtimov, I., Copet, J., Lee, J., Geffert, J., Vranes, J., Park, J., Mahadeokar, J., Shah, J., van der Linde, J., Billock, J., Hong, J., Lee, J., Fu, J., Chi,
J., Huang, J., Liu, J., Wang, J., Yu, J., Bitton, J., Spisak, J., Park, J., Rocca, J., Johnstun, J., Saxe, J., Jia, J., Alwala, K. V., Upasani, K., Plawiak, K., Li, K., Heafield, K., Stone, K., and et al.

</span>
<span class="ltx_bibblock">The Llama 3 herd of models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">CoRR</em>, abs/2407.21783, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://doi.org/10.48550/arXiv.2407.21783" title="">https://doi.org/10.48550/arXiv.2407.21783</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Dziri et al. (2023)</span>
<span class="ltx_bibblock">
Dziri, N., Lu, X., Sclar, M., Li, X. L., Jiang, L., Lin, B. Y., Welleck, S., West, P., Bhagavatula, C., Bras, R. L., Hwang, J. D., Sanyal, S., Ren, X., Ettinger, A., Harchaoui, Z., and Choi, Y.

</span>
<span class="ltx_bibblock">Faith and fate: Limits of transformers on compositionality.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=Fkckkr3ya8" title="">https://openreview.net/forum?id=Fkckkr3ya8</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Elnaggar et al. (2020)</span>
<span class="ltx_bibblock">
Elnaggar, A., Heinzinger, M., Dallago, C., Rehawi, G., Wang, Y., Jones, L., Gibbs, T., Feher, T., Angerer, C., Steinegger, M., Bhowmik, D., and Rost, B.

</span>
<span class="ltx_bibblock">ProtTrans: Towards cracking the language of life’s code through self-supervised deep learning and high performance computing.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">CoRR</em>, abs/2007.06225, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2007.06225" title="">https://arxiv.org/abs/2007.06225</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Fu et al. (2023)</span>
<span class="ltx_bibblock">
Fu, N., Wei, L., Song, Y., Li, Q., Xin, R., Omee, S. S., Dong, R., Siriwardane, E. M. D., and Hu, J.

</span>
<span class="ltx_bibblock">Material transformers: deep learning language models for generative materials design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">Machine Learning: Science and Technology</em>, 4(1):015001, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2023)</span>
<span class="ltx_bibblock">
Gao, L., Schulman, J., and Hilton, J.

</span>
<span class="ltx_bibblock">Scaling laws for reward model overoptimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">Proceedings of the 40th International Conference on Machine Learning</em>, ICML’23. JMLR.org, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gao et al. (2022)</span>
<span class="ltx_bibblock">
Gao, W., Fu, T., Sun, J., and Coley, C. W.

</span>
<span class="ltx_bibblock">Sample efficiency matters: A benchmark for practical molecular optimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=yCZRdI0Y7G" title="">https://openreview.net/forum?id=yCZRdI0Y7G</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Gómez-Rodríguez &amp; Williams (2023)</span>
<span class="ltx_bibblock">
Gómez-Rodríguez, C. and Williams, P.

</span>
<span class="ltx_bibblock">A confederacy of models: a comprehensive evaluation of LLMs on creative writing.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">The 2023 Conference on Empirical Methods in Natural Language Processing</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Goyal et al. (2021)</span>
<span class="ltx_bibblock">
Goyal, K., Dyer, C., and Berg-Kirkpatrick, T.

</span>
<span class="ltx_bibblock">Exposing the implicit energy networks behind masked language models via Metropolis–Hastings.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">International Conference on Learning Representations</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hennigen &amp; Kim (2023)</span>
<span class="ltx_bibblock">
Hennigen, L. T. and Kim, Y.

</span>
<span class="ltx_bibblock">Deriving language models from masked language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)</em>, pp.  1149–1159, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Hu et al. (2022)</span>
<span class="ltx_bibblock">
Hu, E. J., yelong shen, Wallis, P., Allen-Zhu, Z., Li, Y., Wang, S., Wang, L., and Chen, W.

</span>
<span class="ltx_bibblock">LoRA: Low-rank adaptation of large language models.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">International Conference on Learning Representations</em>, 2022.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=nZeVKeeFYf9" title="">https://openreview.net/forum?id=nZeVKeeFYf9</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Jain et al. (2022)</span>
<span class="ltx_bibblock">
Jain, M., Bengio, E., Hernandez-Garcia, A., Rector-Brooks, J., Dossou, B. F., Ekbote, C. A., Fu, J., Zhang, T., Kilgour, M., Zhang, D., et al.

</span>
<span class="ltx_bibblock">Biological sequence design with gflownets.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">International Conference on Machine Learning</em>, pp.  9786–9801. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Janner et al. (2021)</span>
<span class="ltx_bibblock">
Janner, M., Li, Q., and Levine, S.

</span>
<span class="ltx_bibblock">Offline reinforcement learning as one big sequence modeling problem.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib21.1.1">Advances in neural information processing systems</em>, 34:1273–1286, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Kasner &amp; Dušek (2020)</span>
<span class="ltx_bibblock">
Kasner, Z. and Dušek, O.

</span>
<span class="ltx_bibblock">Data-to-text generation with iterative text editing.

</span>
<span class="ltx_bibblock">In Davis, B., Graham, Y., Kelleher, J., and Sripada, Y. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">Proceedings of the 13th International Conference on Natural Language Generation</em>, pp.  60–67, Dublin, Ireland, December 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.inlg-1.9</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.inlg-1.9/" title="">https://aclanthology.org/2020.inlg-1.9/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. (2021)</span>
<span class="ltx_bibblock">
Khan, A., Fleming, E., Schofield, N., Bishop, M., and Andrews, N.

</span>
<span class="ltx_bibblock">A deep metric learning approach to account linking.

</span>
<span class="ltx_bibblock">In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pp.  5275–5287, Online, June 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.naacl-main.415</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.415" title="">https://aclanthology.org/2021.naacl-main.415</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Khan et al. (2024)</span>
<span class="ltx_bibblock">
Khan, A., Wang, A., Hager, S., and Andrews, N.

</span>
<span class="ltx_bibblock">Learning to generate text in arbitrary writing styles, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Krishna et al. (2023)</span>
<span class="ltx_bibblock">
Krishna, K., Song, Y., Karpinska, M., Wieting, J. F., and Iyyer, M.

</span>
<span class="ltx_bibblock">Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">Thirty-seventh Conference on Neural Information Processing Systems</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=WbFhFvjjKj" title="">https://openreview.net/forum?id=WbFhFvjjKj</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lewis et al. (2020)</span>
<span class="ltx_bibblock">
Lewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L.

</span>
<span class="ltx_bibblock">BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension.

</span>
<span class="ltx_bibblock">In Jurafsky, D., Chai, J., Schluter, N., and Tetreault, J. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics</em>, pp.  7871–7880, Online, July 2020. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.acl-main.703</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.acl-main.703/" title="">https://aclanthology.org/2020.acl-main.703/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Li et al. (2017)</span>
<span class="ltx_bibblock">
Li, Y., Turner, R. E., and Liu, Q.

</span>
<span class="ltx_bibblock">Approximate inference with amortised MCMC.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:1702.08343</em>, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Liu et al. (2020)</span>
<span class="ltx_bibblock">
Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Lewis, M., Zettlemoyer, L., and Stoyanov, V.

</span>
<span class="ltx_bibblock">Ro{bert}a: A robustly optimized {bert} pretraining approach, 2020.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=SyxS0T4tvS" title="">https://openreview.net/forum?id=SyxS0T4tvS</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Lu et al. (2025)</span>
<span class="ltx_bibblock">
Lu, Y., Wang, D., Li, T., Jiang, D., Khudanpur, S., Jiang, M., and Khashabi, D.

</span>
<span class="ltx_bibblock">Benchmarking language model creativity: A case study on code generation.

</span>
<span class="ltx_bibblock">In Chiruzzo, L., Ritter, A., and Wang, L. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, pp.  2776–2794, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">ISBN 979-8-89176-189-6.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2025.naacl-long.141/" title="">https://aclanthology.org/2025.naacl-long.141/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Malmi et al. (2019)</span>
<span class="ltx_bibblock">
Malmi, E., Krause, S., Rothe, S., Mirylenka, D., and Severyn, A.

</span>
<span class="ltx_bibblock">Encode, tag, realize: High-precision text editing.

</span>
<span class="ltx_bibblock">In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pp.  5054–5065, Hong Kong, China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D19-1510</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D19-1510/" title="">https://aclanthology.org/D19-1510/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Mireshghallah et al. (2022)</span>
<span class="ltx_bibblock">
Mireshghallah, F., Goyal, K., and Berg-Kirkpatrick, T.

</span>
<span class="ltx_bibblock">Mix and match: Learning-free controllable text generation using energy language models.

</span>
<span class="ltx_bibblock">In Muresan, S., Nakov, P., and Villavicencio, A. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, pp.  401–415, Dublin, Ireland, May 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.acl-long.31</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.acl-long.31" title="">https://aclanthology.org/2022.acl-long.31</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">OpenAI et al. (2024)</span>
<span class="ltx_bibblock">
OpenAI, Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., Avila, R., Babuschkin, I., Balaji, S., Balcom, V., Baltescu, P., Bao, H., Bavarian, M., Belgum, J., Bello, I., Berdine, J., Bernadett-Shapiro, G., Berner, C., Bogdonoff, L., Boiko, O., Boyd, M., Brakman, A.-L., Brockman, G., Brooks, T., Brundage, M., Button, K., Cai, T., Campbell, R., Cann, A., Carey, B., Carlson, C., Carmichael, R., Chan, B., Chang, C., Chantzis, F., Chen, D., Chen, S., Chen, R., Chen, J., Chen, M., Chess, B., Cho, C., Chu, C., Chung, H. W., Cummings, D., Currier, J., Dai, Y., Decareaux, C., Degry, T., Deutsch, N., Deville, D., Dhar, A., Dohan, D., Dowling, S., Dunning, S., Ecoffet, A., Eleti, A., Eloundou, T., Farhi, D., Fedus, L., Felix, N., Fishman, S. P., Forte, J., Fulford, I., Gao, L., Georges, E., Gibson, C., Goel, V., Gogineni, T., Goh, G., Gontijo-Lopes, R., Gordon, J., Grafstein, M., Gray, S., Greene, R., Gross, J., Gu, S. S., Guo, Y., Hallacy,
C., Han, J., Harris, J., He, Y., Heaton, M., Heidecke, J., Hesse, C., Hickey, A., Hickey, W., Hoeschele, P., Houghton, B., Hsu, K., Hu, S., Hu, X., Huizinga, J., Jain, S., Jain, S., Jang, J., Jiang, A., Jiang, R., Jin, H., Jin, D., Jomoto, S., Jonn, B., Jun, H., Kaftan, T., Łukasz Kaiser, Kamali, A., Kanitscheider, I., Keskar, N. S., Khan, T., Kilpatrick, L., Kim, J. W., Kim, C., Kim, Y., Kirchner, J. H., Kiros, J., Knight, M., Kokotajlo, D., Łukasz Kondraciuk, Kondrich, A., Konstantinidis, A., Kosic, K., Krueger, G., Kuo, V., Lampe, M., Lan, I., Lee, T., Leike, J., Leung, J., Levy, D., Li, C. M., Lim, R., Lin, M., Lin, S., Litwin, M., Lopez, T., Lowe, R., Lue, P., Makanju, A., Malfacini, K., Manning, S., Markov, T., Markovski, Y., Martin, B., Mayer, K., Mayne, A., McGrew, B., McKinney, S. M., McLeavey, C., McMillan, P., McNeil, J., Medina, D., Mehta, A., Menick, J., Metz, L., Mishchenko, A., Mishkin, P., Monaco, V., Morikawa, E., Mossing, D., Mu, T., Murati, M., Murk, O., Mély, D., Nair, A., Nakano, R.,
Nayak, R., Neelakantan, A., Ngo, R., Noh, H., Ouyang, L., O’Keefe, C., Pachocki, J., Paino, A., Palermo, J., Pantuliano, A., Parascandolo, G., Parish, J., Parparita, E., Passos, A., Pavlov, M., Peng, A., Perelman, A., de Avila Belbute Peres, F., Petrov, M., de Oliveira Pinto, H. P., Michael, Pokorny, Pokrass, M., Pong, V. H., Powell, T., Power, A., Power, B., Proehl, E., Puri, R., Radford, A., Rae, J., Ramesh, A., Raymond, C., Real, F., Rimbach, K., Ross, C., Rotsted, B., Roussez, H., Ryder, N., Saltarelli, M., Sanders, T., Santurkar, S., Sastry, G., Schmidt, H., Schnurr, D., Schulman, J., Selsam, D., Sheppard, K., Sherbakov, T., Shieh, J., Shoker, S., Shyam, P., Sidor, S., Sigler, E., Simens, M., Sitkin, J., Slama, K., Sohl, I., Sokolowsky, B., Song, Y., Staudacher, N., Such, F. P., Summers, N., Sutskever, I., Tang, J., Tezak, N., Thompson, M. B., Tillet, P., Tootoonchian, A., Tseng, E., Tuggle, P., Turley, N., Tworek, J., Uribe, J. F. C., Vallone, A., Vijayvergiya, A., Voss, C., Wainwright, C., Wang,
J. J., Wang, A., Wang, B., Ward, J., Wei, J., Weinmann, C., Welihinda, A., Welinder, P., Weng, J., Weng, L., Wiethoff, M., Willner, D., Winter, C., Wolrich, S., Wong, H., Workman, L., Wu, S., Wu, J., Wu, M., Xiao, K., Xu, T., Yoo, S., Yu, K., Yuan, Q., Zaremba, W., Zellers, R., Zhang, C., Zhang, M., Zhao, S., Zheng, T., Zhuang, J., Zhuk, W., and Zoph, B.

</span>
<span class="ltx_bibblock">GPT-4 technical report, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2303.08774" title="">https://arxiv.org/abs/2303.08774</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Padmakumar et al. (2023)</span>
<span class="ltx_bibblock">
Padmakumar, V., Pang, R. Y., He, H., and Parikh, A. P.

</span>
<span class="ltx_bibblock">Extrapolative controlled sequence generation via iterative refinement.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">International Conference on Machine Learning</em>, pp.  26792–26808. PMLR, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Papineni et al. (2002)</span>
<span class="ltx_bibblock">
Papineni, K., Roukos, S., Ward, T., and Zhu, W.-J.

</span>
<span class="ltx_bibblock">Bleu: a method for automatic evaluation of machine translation.

</span>
<span class="ltx_bibblock">In Isabelle, P., Charniak, E., and Lin, D. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics</em>, pp.  311–318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.3115/1073083.1073135</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/P02-1040/" title="">https://aclanthology.org/P02-1040/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Puri et al. (2025)</span>
<span class="ltx_bibblock">
Puri, I., Sudalairaj, S., Xu, G., Xu, K., and Srivastava, A.

</span>
<span class="ltx_bibblock">A probabilistic inference approach to inference-time scaling of llms using particle-based monte carlo methods, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2502.01618" title="">https://arxiv.org/abs/2502.01618</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Radford et al. (2019)</span>
<span class="ltx_bibblock">
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I.

</span>
<span class="ltx_bibblock">Language models are unsupervised multitask learners.

</span>
<span class="ltx_bibblock">2019.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://api.semanticscholar.org/CorpusID:160025533" title="">https://api.semanticscholar.org/CorpusID:160025533</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Raffel et al. (2020)</span>
<span class="ltx_bibblock">
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J.

</span>
<span class="ltx_bibblock">Exploring the limits of transfer learning with a unified text-to-text transformer.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">J. Mach. Learn. Res.</em>, 21(1), January 2020.

</span>
<span class="ltx_bibblock">ISSN 1532-4435.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reid &amp; Neubig (2022)</span>
<span class="ltx_bibblock">
Reid, M. and Neubig, G.

</span>
<span class="ltx_bibblock">Learning to model editing processes.

</span>
<span class="ltx_bibblock">In Goldberg, Y., Kozareva, Z., and Zhang, Y. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Findings of the Association for Computational Linguistics: EMNLP 2022</em>, pp.  3822–3832, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2022.findings-emnlp.280</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2022.findings-emnlp.280/" title="">https://aclanthology.org/2022.findings-emnlp.280/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Reimers &amp; Gurevych (2019)</span>
<span class="ltx_bibblock">
Reimers, N. and Gurevych, I.

</span>
<span class="ltx_bibblock">Sentence-BERT: Sentence embeddings using Siamese BERT-networks.

</span>
<span class="ltx_bibblock">In Inui, K., Jiang, J., Ng, V., and Wan, X. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em>, pp.  3982–3992, Hong Kong, China, November 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/D19-1410</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/D19-1410" title="">https://aclanthology.org/D19-1410</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Rivera-Soto et al. (2021)</span>
<span class="ltx_bibblock">
Rivera-Soto, R. A., Miano, O. E., Ordonez, J., Chen, B. Y., Khan, A., Bishop, M., and Andrews, N.

</span>
<span class="ltx_bibblock">Learning universal authorship representations.

</span>
<span class="ltx_bibblock">In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing</em>, pp.  913–919, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.emnlp-main.70</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.emnlp-main.70" title="">https://aclanthology.org/2021.emnlp-main.70</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Romera-Paredes et al. (2024)</span>
<span class="ltx_bibblock">
Romera-Paredes, B., Barekatain, M., Novikov, A., Balog, M., Kumar, M. P., Dupont, E., Ruiz, F. J., Ellenberg, J. S., Wang, P., Fawzi, O., et al.

</span>
<span class="ltx_bibblock">Mathematical discoveries from program search with large language models.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">Nature</em>, 625(7995):468–475, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schaeffer et al. (2024)</span>
<span class="ltx_bibblock">
Schaeffer, R., Miranda, B., and Koyejo, S.

</span>
<span class="ltx_bibblock">Are emergent abilities of large language models a mirage?

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">Advances in Neural Information Processing Systems</em>, 36, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schick et al. (2023)</span>
<span class="ltx_bibblock">
Schick, T., Yu, J. A., Jiang, Z., Petroni, F., Lewis, P., Izacard, G., You, Q., Nalmpantis, C., Grave, E., and Riedel, S.

</span>
<span class="ltx_bibblock">PEER: A collaborative language model.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=KbYevcLjnc" title="">https://openreview.net/forum?id=KbYevcLjnc</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Schymkowitz et al. (2005)</span>
<span class="ltx_bibblock">
Schymkowitz, J., Ferkinghoff-Borg, J., Stricher, F., Nys, R., Rousseau, F., and Serrano, L.

</span>
<span class="ltx_bibblock">The FoldX web server: An online force field.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">Nucleic acids research</em>, 33:W382–8, 08 2005.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1093/nar/gki387</span>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Sevgen et al. (2023)</span>
<span class="ltx_bibblock">
Sevgen, E., Moller, J., Lange, A., Parker, J., Quigley, S., Mayer, J., Srivastava, P., Gayatri, S., Hosfield, D., Korshunova, M., Livne, M., Gill, M., Ranganathan, R., Costa, A. B., and Ferguson, A. L.

</span>
<span class="ltx_bibblock">ProT-VAE: Protein transformer variational autoencoder for functional protein design.

</span>
<span class="ltx_bibblock"><em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">bioRxiv</em>, 2023.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.1101/2023.01.23.525232</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://www.biorxiv.org/content/early/2023/01/24/2023.01.23.525232" title="">https://www.biorxiv.org/content/early/2023/01/24/2023.01.23.525232</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Shu et al. (2024)</span>
<span class="ltx_bibblock">
Shu, L., Luo, L., Hoskere, J., Zhu, Y., Tong, S., Chen, J., and Meng, L.

</span>
<span class="ltx_bibblock">RewriteLM: An instruction-tuned large languagemodel for text rewriting.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">Proceedings of the AAAI Conference on Artificial Intelligence, 38(17), 18970-18980</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://ojs.aaai.org/index.php/AAAI/article/view/29863/31505" title="">https://ojs.aaai.org/index.php/AAAI/article/view/29863/31505</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Snell et al. (2025)</span>
<span class="ltx_bibblock">
Snell, C. V., Lee, J., Xu, K., and Kumar, A.

</span>
<span class="ltx_bibblock">Scaling LLM test-time compute optimally can be more effective than scaling parameters for reasoning.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">The Thirteenth International Conference on Learning Representations</em>, 2025.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=4FWAwZtd2n" title="">https://openreview.net/forum?id=4FWAwZtd2n</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Swanson et al. (2021)</span>
<span class="ltx_bibblock">
Swanson, B., Mathewson, K., Pietrzak, B., Chen, S., and Dinalescu, M.

</span>
<span class="ltx_bibblock">Story centaur: Large language model few shot learning as a creative writing tool.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations</em>, pp.  244–256, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tagasovska et al. (2024)</span>
<span class="ltx_bibblock">
Tagasovska, N., Gligorijevic, V., Cho, K., and Loukas, A.

</span>
<span class="ltx_bibblock">Implicitly guided design with PropEn: Match your data to follow the gradient.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">The Thirty-eighth Annual Conference on Neural Information Processing Systems</em>, 2024.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=dhFHO90INk" title="">https://openreview.net/forum?id=dhFHO90INk</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tang et al. (2020)</span>
<span class="ltx_bibblock">
Tang, Y., Tran, C., Li, X., Chen, P.-J., Goyal, N., Chaudhary, V., Gu, J., and Fan, A.

</span>
<span class="ltx_bibblock">Multilingual translation with extensible multilingual pretraining and finetuning.

</span>
<span class="ltx_bibblock">2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Tay et al. (2023)</span>
<span class="ltx_bibblock">
Tay, Y., Dehghani, M., Tran, V. Q., Garcia, X., Wei, J., Wang, X., Chung, H. W., Bahri, D., Schuster, T., Zheng, S., Zhou, D., Houlsby, N., and Metzler, D.

</span>
<span class="ltx_bibblock">UL2: Unifying language learning paradigms.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">The Eleventh International Conference on Learning Representations</em>, 2023.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://openreview.net/forum?id=6ruVLB727MC" title="">https://openreview.net/forum?id=6ruVLB727MC</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Trabucco et al. (2022)</span>
<span class="ltx_bibblock">
Trabucco, B., Geng, X., Kumar, A., and Levine, S.

</span>
<span class="ltx_bibblock">Design-bench: Benchmarks for data-driven offline model-based optimization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">International Conference on Machine Learning</em>, pp.  21658–21676. PMLR, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang &amp; Cho (2019)</span>
<span class="ltx_bibblock">
Wang, A. and Cho, K.

</span>
<span class="ltx_bibblock">BERT has a mouth, and it must speak: BERT as a Markov random field language model.

</span>
<span class="ltx_bibblock">In Bosselut, A., Celikyilmaz, A., Ghazvininejad, M., Iyer, S., Khandelwal, U., Rashkin, H., and Wolf, T. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">Proceedings of the Workshop on Methods for Optimizing and Evaluating Neural Language Generation</em>, pp.  30–36, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/W19-2304</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/W19-2304/" title="">https://aclanthology.org/W19-2304/</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Wang et al. (2019)</span>
<span class="ltx_bibblock">
Wang, W., Gan, Z., Xu, H., Zhang, R., Wang, G., Shen, D., Chen, C., and Carin, L.

</span>
<span class="ltx_bibblock">Topic-guided variational auto-encoder for text generation.

</span>
<span class="ltx_bibblock">In Burstein, J., Doran, C., and Solorio, T. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)</em>, pp.  166–177, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/N19-1015</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/N19-1015" title="">https://aclanthology.org/N19-1015</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Yang &amp; Klein (2021)</span>
<span class="ltx_bibblock">
Yang, K. and Klein, D.

</span>
<span class="ltx_bibblock">FUDGE: Controlled text generation with future discriminators.

</span>
<span class="ltx_bibblock">In Toutanova, K., Rumshisky, A., Zettlemoyer, L., Hakkani-Tur, D., Beltagy, I., Bethard, S., Cotterell, R., Chakraborty, T., and Zhou, Y. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies</em>, pp.  3511–3535, Online, June 2021. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2021.naacl-main.276</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2021.naacl-main.276" title="">https://aclanthology.org/2021.naacl-main.276</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020a)</span>
<span class="ltx_bibblock">
Zhang, J., Zhao, Y., Saleh, M., and Liu, P. J.

</span>
<span class="ltx_bibblock">PEGASUS: pre-training with extracted gap-sentences for abstractive summarization.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">Proceedings of the 37th International Conference on Machine Learning</em>, ICML’20. JMLR.org, 2020a.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2020b)</span>
<span class="ltx_bibblock">
Zhang, M., Jiang, N., Li, L., and Xue, Y.

</span>
<span class="ltx_bibblock">Language generation via combinatorial constraint satisfaction: A tree search enhanced Monte-Carlo approach.

</span>
<span class="ltx_bibblock">In Cohn, T., He, Y., and Liu, Y. (eds.), <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">Findings of the Association for Computational Linguistics: EMNLP 2020</em>, pp.  1286–1298, Online, November 2020b. Association for Computational Linguistics.

</span>
<span class="ltx_bibblock">doi: <span class="ltx_ref ltx_nolink ltx_Url ltx_ref_self">10.18653/v1/2020.findings-emnlp.115</span>.

</span>
<span class="ltx_bibblock">URL <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://aclanthology.org/2020.findings-emnlp.115" title="">https://aclanthology.org/2020.findings-emnlp.115</a>.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_role_refnum ltx_tag_bibitem">Zhang et al. (2015)</span>
<span class="ltx_bibblock">
Zhang, X., Zhao, J., and LeCun, Y.

</span>
<span class="ltx_bibblock">Character-level convolutional networks for text classification.

</span>
<span class="ltx_bibblock">In <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1</em>, NIPS’15, pp.  649–657, Cambridge, MA, USA, 2015. MIT Press.

</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="A1">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix A </span>Reward choice</h2>
<div class="ltx_para" id="A1.p1">
<p class="ltx_p" id="A1.p1.5">We predicate our method on the assumption that there is an energy function <math alttext="s" class="ltx_Math" display="inline" id="A1.p1.1.m1.1"><semantics id="A1.p1.1.m1.1a"><mi id="A1.p1.1.m1.1.1" xref="A1.p1.1.m1.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="A1.p1.1.m1.1b"><ci id="A1.p1.1.m1.1.1.cmml" xref="A1.p1.1.m1.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.1.m1.1c">s</annotation><annotation encoding="application/x-llamapun" id="A1.p1.1.m1.1d">italic_s</annotation></semantics></math> that can guide the edit sequence. In the case where <math alttext="s" class="ltx_Math" display="inline" id="A1.p1.2.m2.1"><semantics id="A1.p1.2.m2.1a"><mi id="A1.p1.2.m2.1.1" xref="A1.p1.2.m2.1.1.cmml">s</mi><annotation-xml encoding="MathML-Content" id="A1.p1.2.m2.1b"><ci id="A1.p1.2.m2.1.1.cmml" xref="A1.p1.2.m2.1.1">𝑠</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.2.m2.1c">s</annotation><annotation encoding="application/x-llamapun" id="A1.p1.2.m2.1d">italic_s</annotation></semantics></math> is slow or otherwise difficult to compute at inference time, we consider an alternative inspired by <cite class="ltx_cite ltx_citemacro_citet">Chen et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib7" title="">2024</a>)</cite>. They conceptualize <span class="ltx_text ltx_font_italic" id="A1.p1.5.1">returns-to-go</span>, where the model predicts the outcomes/rewards of its actions rather than directly being fed the reward. In our case, we allow <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A1.p1.3.m3.1"><semantics id="A1.p1.3.m3.1a"><msub id="A1.p1.3.m3.1.1" xref="A1.p1.3.m3.1.1.cmml"><mi id="A1.p1.3.m3.1.1.2" xref="A1.p1.3.m3.1.1.2.cmml">q</mi><mi id="A1.p1.3.m3.1.1.3" xref="A1.p1.3.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A1.p1.3.m3.1b"><apply id="A1.p1.3.m3.1.1.cmml" xref="A1.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A1.p1.3.m3.1.1.1.cmml" xref="A1.p1.3.m3.1.1">subscript</csymbol><ci id="A1.p1.3.m3.1.1.2.cmml" xref="A1.p1.3.m3.1.1.2">𝑞</ci><ci id="A1.p1.3.m3.1.1.3.cmml" xref="A1.p1.3.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.3.m3.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.3.m3.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> to predict <math alttext="s(x)" class="ltx_Math" display="inline" id="A1.p1.4.m4.1"><semantics id="A1.p1.4.m4.1a"><mrow id="A1.p1.4.m4.1.2" xref="A1.p1.4.m4.1.2.cmml"><mi id="A1.p1.4.m4.1.2.2" xref="A1.p1.4.m4.1.2.2.cmml">s</mi><mo id="A1.p1.4.m4.1.2.1" xref="A1.p1.4.m4.1.2.1.cmml">⁢</mo><mrow id="A1.p1.4.m4.1.2.3.2" xref="A1.p1.4.m4.1.2.cmml"><mo id="A1.p1.4.m4.1.2.3.2.1" stretchy="false" xref="A1.p1.4.m4.1.2.cmml">(</mo><mi id="A1.p1.4.m4.1.1" xref="A1.p1.4.m4.1.1.cmml">x</mi><mo id="A1.p1.4.m4.1.2.3.2.2" stretchy="false" xref="A1.p1.4.m4.1.2.cmml">)</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="A1.p1.4.m4.1b"><apply id="A1.p1.4.m4.1.2.cmml" xref="A1.p1.4.m4.1.2"><times id="A1.p1.4.m4.1.2.1.cmml" xref="A1.p1.4.m4.1.2.1"></times><ci id="A1.p1.4.m4.1.2.2.cmml" xref="A1.p1.4.m4.1.2.2">𝑠</ci><ci id="A1.p1.4.m4.1.1.cmml" xref="A1.p1.4.m4.1.1">𝑥</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.4.m4.1c">s(x)</annotation><annotation encoding="application/x-llamapun" id="A1.p1.4.m4.1d">italic_s ( italic_x )</annotation></semantics></math>, rather than using the real output of the scoring function. As an ablation, we also examine the effects of using no reward whatsoever– can <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A1.p1.5.m5.1"><semantics id="A1.p1.5.m5.1a"><msub id="A1.p1.5.m5.1.1" xref="A1.p1.5.m5.1.1.cmml"><mi id="A1.p1.5.m5.1.1.2" xref="A1.p1.5.m5.1.1.2.cmml">q</mi><mi id="A1.p1.5.m5.1.1.3" xref="A1.p1.5.m5.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A1.p1.5.m5.1b"><apply id="A1.p1.5.m5.1.1.cmml" xref="A1.p1.5.m5.1.1"><csymbol cd="ambiguous" id="A1.p1.5.m5.1.1.1.cmml" xref="A1.p1.5.m5.1.1">subscript</csymbol><ci id="A1.p1.5.m5.1.1.2.cmml" xref="A1.p1.5.m5.1.1.2">𝑞</ci><ci id="A1.p1.5.m5.1.1.3.cmml" xref="A1.p1.5.m5.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A1.p1.5.m5.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A1.p1.5.m5.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> achieve similar success using only the implicit reward derived from the sequence?</p>
</div>
<div class="ltx_para" id="A1.p2">
<p class="ltx_p" id="A1.p2.1">Analyzing the results shown in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A1.T7" title="Table 7 ‣ Appendix A Reward choice ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 7</span></a>, <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A1.T8" title="Table 8 ‣ Appendix A Reward choice ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 8</span></a> and <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A1.T9" title="Table 9 ‣ Appendix A Reward choice ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 9</span></a>, we find that it is not uniformly beneficial to use the energy function at each step. Using a predicted reward or no reward benefits efficiency, as interrupting generation to run the proxy function is no longer necessary. Based on these results, in our main-text experiments, we choose to predict the energy.</p>
</div>
<figure class="ltx_table" id="A1.T7">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T7.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T7.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T7.3.3.4"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.4.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="A1.T7.3.3.5"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.5.1">Reward</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T7.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T7.1.1.1.1">EER<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T7.1.1.1.1.m1.1"><semantics id="A1.T7.1.1.1.1.m1.1a"><mo id="A1.T7.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T7.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T7.1.1.1.1.m1.1b"><ci id="A1.T7.1.1.1.1.m1.1.1.cmml" xref="A1.T7.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T7.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="A1.T7.2.2.2">
<span class="ltx_text ltx_font_bold" id="A1.T7.2.2.2.1">SBERT</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T7.2.2.2.m1.1"><semantics id="A1.T7.2.2.2.m1.1a"><mo id="A1.T7.2.2.2.m1.1.1" stretchy="false" xref="A1.T7.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T7.2.2.2.m1.1b"><ci id="A1.T7.2.2.2.m1.1.1.cmml" xref="A1.T7.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T7.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T7.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T7.3.3.3.1">Iterations<math alttext="\downarrow" class="ltx_Math" display="inline" id="A1.T7.3.3.3.1.m1.1"><semantics id="A1.T7.3.3.3.1.m1.1a"><mo id="A1.T7.3.3.3.1.m1.1.1" stretchy="false" xref="A1.T7.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A1.T7.3.3.3.1.m1.1b"><ci id="A1.T7.3.3.3.1.m1.1.1.cmml" xref="A1.T7.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T7.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T7.5.6.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T7.5.6.1.1" rowspan="3"><span class="ltx_text" id="A1.T7.5.6.1.1.1">Thinning (fixed-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="A1.T7.5.6.1.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T7.5.6.1.3">0.198</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="A1.T7.5.6.1.4">0.809</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T7.5.6.1.5">4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.7.2">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.7.2.1">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.5.7.2.2">0.179</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.7.2.3">0.689</td>
<td class="ltx_td ltx_align_center" id="A1.T7.5.7.2.4">4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.8.3">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.8.3.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.5.8.3.2"><span class="ltx_text ltx_font_bold" id="A1.T7.5.8.3.2.1">0.209</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.8.3.3"><span class="ltx_text ltx_font_bold" id="A1.T7.5.8.3.3.1">0.810</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.5.8.3.4">4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.9.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.5.9.4.1" rowspan="3"><span class="ltx_text" id="A1.T7.5.9.4.1.1">Thinning (variable-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T7.5.9.4.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.5.9.4.3"><span class="ltx_text ltx_font_bold" id="A1.T7.5.9.4.3.1">0.202</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T7.5.9.4.4">0.809</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.5.9.4.5">4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.10.5">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.10.5.1">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.5.10.5.2">0.176</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.10.5.3">0.767</td>
<td class="ltx_td ltx_align_center" id="A1.T7.5.10.5.4">10</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.11.6">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.11.6.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.5.11.6.2">0.198</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.11.6.3"><span class="ltx_text ltx_font_bold" id="A1.T7.5.11.6.3.1">0.813</span></td>
<td class="ltx_td ltx_align_center" id="A1.T7.5.11.6.4">10</td>
</tr>
<tr class="ltx_tr" id="A1.T7.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T7.4.4.1" rowspan="3"><span class="ltx_text" id="A1.T7.4.4.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T7.4.4.1.1.m1.1"><semantics id="A1.T7.4.4.1.1.m1.1a"><mi id="A1.T7.4.4.1.1.m1.1.1" mathvariant="normal" xref="A1.T7.4.4.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.4.4.1.1.m1.1b"><ci id="A1.T7.4.4.1.1.m1.1.1.cmml" xref="A1.T7.4.4.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.4.4.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T7.4.4.1.1.m1.1d">roman_Δ</annotation></semantics></math> Energy(fixed-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T7.4.4.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.4.4.3">0.192</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T7.4.4.4"><span class="ltx_text ltx_font_bold" id="A1.T7.4.4.4.1">0.840</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.4.4.5">4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.12.7">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.12.7.1">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.5.12.7.2">0.180</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.12.7.3">0.723</td>
<td class="ltx_td ltx_align_center" id="A1.T7.5.12.7.4">4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.13.8">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.13.8.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.5.13.8.2"><span class="ltx_text ltx_font_bold" id="A1.T7.5.13.8.2.1">0.202</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.13.8.3">0.810</td>
<td class="ltx_td ltx_align_center" id="A1.T7.5.13.8.4">4</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A1.T7.5.5.1" rowspan="3"><span class="ltx_text" id="A1.T7.5.5.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T7.5.5.1.1.m1.1"><semantics id="A1.T7.5.5.1.1.m1.1a"><mi id="A1.T7.5.5.1.1.m1.1.1" mathvariant="normal" xref="A1.T7.5.5.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T7.5.5.1.1.m1.1b"><ci id="A1.T7.5.5.1.1.m1.1.1.cmml" xref="A1.T7.5.5.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T7.5.5.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T7.5.5.1.1.m1.1d">roman_Δ</annotation></semantics></math> Energy(variable-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T7.5.5.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T7.5.5.3">0.212</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T7.5.5.4">0.809</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T7.5.5.5">10</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.14.9">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.14.9.1">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T7.5.14.9.2">0.179</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T7.5.14.9.3">0.693</td>
<td class="ltx_td ltx_align_center" id="A1.T7.5.14.9.4">10</td>
</tr>
<tr class="ltx_tr" id="A1.T7.5.15.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="A1.T7.5.15.10.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T7.5.15.10.2"><span class="ltx_text ltx_font_bold" id="A1.T7.5.15.10.2.1">0.221</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="A1.T7.5.15.10.3"><span class="ltx_text ltx_font_bold" id="A1.T7.5.15.10.3.1">0.839</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T7.5.15.10.4">10</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 7: </span>Comparing varying reward types on the anonymization task.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T8">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T8.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T8.3.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T8.3.3.4"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.4.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="A1.T8.3.3.5"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.5.1">Reward</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T8.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T8.1.1.1.1">Training<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T8.1.1.1.1.m1.1"><semantics id="A1.T8.1.1.1.1.m1.1a"><mo id="A1.T8.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T8.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T8.1.1.1.1.m1.1b"><ci id="A1.T8.1.1.1.1.m1.1.1.cmml" xref="A1.T8.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T8.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="A1.T8.2.2.2">
<span class="ltx_text ltx_font_bold" id="A1.T8.2.2.2.1">Extrapolation</span> <math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T8.2.2.2.m1.1"><semantics id="A1.T8.2.2.2.m1.1a"><mo id="A1.T8.2.2.2.m1.1.1" stretchy="false" xref="A1.T8.2.2.2.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T8.2.2.2.m1.1b"><ci id="A1.T8.2.2.2.m1.1.1.cmml" xref="A1.T8.2.2.2.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.2.2.2.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T8.2.2.2.m1.1d">↑</annotation></semantics></math>
</th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T8.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T8.3.3.3.1">Fluency<math alttext="\downarrow" class="ltx_Math" display="inline" id="A1.T8.3.3.3.1.m1.1"><semantics id="A1.T8.3.3.3.1.m1.1a"><mo id="A1.T8.3.3.3.1.m1.1.1" stretchy="false" xref="A1.T8.3.3.3.1.m1.1.1.cmml">↓</mo><annotation-xml encoding="MathML-Content" id="A1.T8.3.3.3.1.m1.1b"><ci id="A1.T8.3.3.3.1.m1.1.1.cmml" xref="A1.T8.3.3.3.1.m1.1.1">↓</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.3.3.3.1.m1.1c">\downarrow</annotation><annotation encoding="application/x-llamapun" id="A1.T8.3.3.3.1.m1.1d">↓</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T8.5.6.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T8.5.6.1.1" rowspan="3"><span class="ltx_text" id="A1.T8.5.6.1.1.1">Thinning (fixed-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="A1.T8.5.6.1.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T8.5.6.1.3">0.870</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="A1.T8.5.6.1.4">0.634</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T8.5.6.1.5">
<span class="ltx_text ltx_font_bold" id="A1.T8.5.6.1.5.1">0.466</span>%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.7.2">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.7.2.1">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.5.7.2.2">0.856</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.7.2.3"><span class="ltx_text ltx_font_bold" id="A1.T8.5.7.2.3.1">0.671</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.5.7.2.4">0.927%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.8.3">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.8.3.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.5.8.3.2"><span class="ltx_text ltx_font_bold" id="A1.T8.5.8.3.2.1">0.883</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.8.3.3">0.642</td>
<td class="ltx_td ltx_align_center" id="A1.T8.5.8.3.4"><span class="ltx_text ltx_font_bold" id="A1.T8.5.8.3.4.1">0.466%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.9.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.5.9.4.1" rowspan="3"><span class="ltx_text" id="A1.T8.5.9.4.1.1">Thinning (variable-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T8.5.9.4.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.5.9.4.3">0.834</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T8.5.9.4.4">0.572</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.5.9.4.5"><span class="ltx_text ltx_font_bold" id="A1.T8.5.9.4.5.1">0.522%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.10.5">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.10.5.1">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.5.10.5.2">0.820</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.10.5.3"><span class="ltx_text ltx_font_bold" id="A1.T8.5.10.5.3.1">0.610</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.5.10.5.4">1.071%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.11.6">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.11.6.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.5.11.6.2"><span class="ltx_text ltx_font_bold" id="A1.T8.5.11.6.2.1">0.854</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.11.6.3">0.591</td>
<td class="ltx_td ltx_align_center" id="A1.T8.5.11.6.4">0.539%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.4.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T8.4.4.1" rowspan="3"><span class="ltx_text" id="A1.T8.4.4.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T8.4.4.1.1.m1.1"><semantics id="A1.T8.4.4.1.1.m1.1a"><mi id="A1.T8.4.4.1.1.m1.1.1" mathvariant="normal" xref="A1.T8.4.4.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T8.4.4.1.1.m1.1b"><ci id="A1.T8.4.4.1.1.m1.1.1.cmml" xref="A1.T8.4.4.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.4.4.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T8.4.4.1.1.m1.1d">roman_Δ</annotation></semantics></math> Energy(fixed-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T8.4.4.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.4.4.3">0.905</td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T8.4.4.4">0.683</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.4.4.5">0.375%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.12.7">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.12.7.1">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.5.12.7.2">0.890</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.12.7.3">0.679</td>
<td class="ltx_td ltx_align_center" id="A1.T8.5.12.7.4">0.778%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.13.8">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.13.8.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.5.13.8.2"><span class="ltx_text ltx_font_bold" id="A1.T8.5.13.8.2.1">0.910</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.13.8.3"><span class="ltx_text ltx_font_bold" id="A1.T8.5.13.8.3.1">0.692</span></td>
<td class="ltx_td ltx_align_center" id="A1.T8.5.13.8.4"><span class="ltx_text ltx_font_bold" id="A1.T8.5.13.8.4.1">0.362%</span></td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.5">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A1.T8.5.5.1" rowspan="3"><span class="ltx_text" id="A1.T8.5.5.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T8.5.5.1.1.m1.1"><semantics id="A1.T8.5.5.1.1.m1.1a"><mi id="A1.T8.5.5.1.1.m1.1.1" mathvariant="normal" xref="A1.T8.5.5.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T8.5.5.1.1.m1.1b"><ci id="A1.T8.5.5.1.1.m1.1.1.cmml" xref="A1.T8.5.5.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T8.5.5.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T8.5.5.1.1.m1.1d">roman_Δ</annotation></semantics></math> Energy(variable-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T8.5.5.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T8.5.5.3"><span class="ltx_text ltx_font_bold" id="A1.T8.5.5.3.1">0.887</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T8.5.5.4"><span class="ltx_text ltx_font_bold" id="A1.T8.5.5.4.1">0.681</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T8.5.5.5">0.454%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.14.9">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.14.9.1">Real</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T8.5.14.9.2">0.706</td>
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T8.5.14.9.3">0.474</td>
<td class="ltx_td ltx_align_center" id="A1.T8.5.14.9.4">0.972%</td>
</tr>
<tr class="ltx_tr" id="A1.T8.5.15.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="A1.T8.5.15.10.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T8.5.15.10.2">0.881</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="A1.T8.5.15.10.3">0.677</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T8.5.15.10.4"><span class="ltx_text ltx_font_bold" id="A1.T8.5.15.10.4.1">0.410%</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 8: </span>Comparing varying reward types on the sentiment task.</figcaption>
</figure>
<figure class="ltx_table" id="A1.T9">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A1.T9.7">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A1.T9.5.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="A1.T9.5.5.6"><span class="ltx_text ltx_font_bold" id="A1.T9.5.5.6.1">Model</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_rr ltx_border_tt" id="A1.T9.5.5.7"><span class="ltx_text ltx_font_bold" id="A1.T9.5.5.7.1">Reward</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.1.1.1"><span class="ltx_text ltx_font_bold" id="A1.T9.1.1.1.1">-1<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T9.1.1.1.1.m1.1"><semantics id="A1.T9.1.1.1.1.m1.1a"><mo id="A1.T9.1.1.1.1.m1.1.1" stretchy="false" xref="A1.T9.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T9.1.1.1.1.m1.1b"><ci id="A1.T9.1.1.1.1.m1.1.1.cmml" xref="A1.T9.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T9.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A1.T9.2.2.2"><span class="ltx_text ltx_font_bold" id="A1.T9.2.2.2.1">-2.5<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T9.2.2.2.1.m1.1"><semantics id="A1.T9.2.2.2.1.m1.1a"><mo id="A1.T9.2.2.2.1.m1.1.1" stretchy="false" xref="A1.T9.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T9.2.2.2.1.m1.1b"><ci id="A1.T9.2.2.2.1.m1.1.1.cmml" xref="A1.T9.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T9.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.3.3.3"><span class="ltx_text ltx_font_bold" id="A1.T9.3.3.3.1">-5<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T9.3.3.3.1.m1.1"><semantics id="A1.T9.3.3.3.1.m1.1a"><mo id="A1.T9.3.3.3.1.m1.1.1" stretchy="false" xref="A1.T9.3.3.3.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T9.3.3.3.1.m1.1b"><ci id="A1.T9.3.3.3.1.m1.1.1.cmml" xref="A1.T9.3.3.3.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.3.3.3.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T9.3.3.3.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.4.4.4"><span class="ltx_text ltx_font_bold" id="A1.T9.4.4.4.1">-6<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T9.4.4.4.1.m1.1"><semantics id="A1.T9.4.4.4.1.m1.1a"><mo id="A1.T9.4.4.4.1.m1.1.1" stretchy="false" xref="A1.T9.4.4.4.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T9.4.4.4.1.m1.1b"><ci id="A1.T9.4.4.4.1.m1.1.1.cmml" xref="A1.T9.4.4.4.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.4.4.4.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T9.4.4.4.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A1.T9.5.5.5"><span class="ltx_text ltx_font_bold" id="A1.T9.5.5.5.1">-7<math alttext="\uparrow" class="ltx_Math" display="inline" id="A1.T9.5.5.5.1.m1.1"><semantics id="A1.T9.5.5.5.1.m1.1a"><mo id="A1.T9.5.5.5.1.m1.1.1" stretchy="false" xref="A1.T9.5.5.5.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A1.T9.5.5.5.1.m1.1b"><ci id="A1.T9.5.5.5.1.m1.1.1.cmml" xref="A1.T9.5.5.5.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.5.5.5.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A1.T9.5.5.5.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A1.T9.7.8.1">
<td class="ltx_td ltx_align_left ltx_border_tt" id="A1.T9.7.8.1.1" rowspan="3"><span class="ltx_text" id="A1.T9.7.8.1.1.1">Thinning (fixed-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_tt" id="A1.T9.7.8.1.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.7.8.1.3"><span class="ltx_text ltx_font_bold" id="A1.T9.7.8.1.3.1">0.979</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="A1.T9.7.8.1.4"><span class="ltx_text ltx_font_bold" id="A1.T9.7.8.1.4.1">0.951</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.7.8.1.5"><span class="ltx_text ltx_font_bold" id="A1.T9.7.8.1.5.1">0.786</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.7.8.1.6"><span class="ltx_text ltx_font_bold" id="A1.T9.7.8.1.6.1">0.658</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A1.T9.7.8.1.7"><span class="ltx_text ltx_font_bold" id="A1.T9.7.8.1.7.1">0.502</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.9.2">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T9.7.9.2.1">Real</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.9.2.2">0.959</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.7.9.2.3">0.908</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.9.2.4">0.698</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.9.2.5">0.551</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.9.2.6">0.390</td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.10.3">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T9.7.10.3.1">Predicted</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.10.3.2">0.961</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.7.10.3.3">0.915</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.10.3.4">0.715</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.10.3.5">0.580</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.10.3.6">0.422</td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.11.4">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.7.11.4.1" rowspan="3"><span class="ltx_text" id="A1.T9.7.11.4.1.1">Thinning (variable-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T9.7.11.4.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.7.11.4.3">0.968</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.7.11.4.4">0.897</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.7.11.4.5">0.478</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.7.11.4.6">0.274</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.7.11.4.7">0.128</td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.12.5">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T9.7.12.5.1">Real</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.12.5.2"><span class="ltx_text ltx_font_bold" id="A1.T9.7.12.5.2.1">0.980</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.7.12.5.3"><span class="ltx_text ltx_font_bold" id="A1.T9.7.12.5.3.1">0.953</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.12.5.4">0.663</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.12.5.5">0.507</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.12.5.6">0.379</td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.13.6">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T9.7.13.6.1">Predicted</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.13.6.2">0.972</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.7.13.6.3">0.929</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.13.6.4"><span class="ltx_text ltx_font_bold" id="A1.T9.7.13.6.4.1">0.714</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.13.6.5"><span class="ltx_text ltx_font_bold" id="A1.T9.7.13.6.5.1">0.570</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.13.6.6"><span class="ltx_text ltx_font_bold" id="A1.T9.7.13.6.6.1">0.420</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.6.6">
<td class="ltx_td ltx_align_left ltx_border_t" id="A1.T9.6.6.1" rowspan="3"><span class="ltx_text" id="A1.T9.6.6.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T9.6.6.1.1.m1.1"><semantics id="A1.T9.6.6.1.1.m1.1a"><mi id="A1.T9.6.6.1.1.m1.1.1" mathvariant="normal" xref="A1.T9.6.6.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T9.6.6.1.1.m1.1b"><ci id="A1.T9.6.6.1.1.m1.1.1.cmml" xref="A1.T9.6.6.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.6.6.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T9.6.6.1.1.m1.1d">roman_Δ</annotation></semantics></math> Energy(fixed-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T9.6.6.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.6.6.3"><span class="ltx_text ltx_font_bold" id="A1.T9.6.6.3.1">0.978</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.6.6.4"><span class="ltx_text ltx_font_bold" id="A1.T9.6.6.4.1">0.949</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.6.6.5"><span class="ltx_text ltx_font_bold" id="A1.T9.6.6.5.1">0.785</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.6.6.6"><span class="ltx_text ltx_font_bold" id="A1.T9.6.6.6.1">0.651</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.6.6.7"><span class="ltx_text ltx_font_bold" id="A1.T9.6.6.7.1">0.493</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.14.7">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T9.7.14.7.1">Real</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.14.7.2">0.970</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.7.14.7.3">0.932</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.14.7.4">0.745</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.14.7.5">0.605</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.14.7.6">0.443</td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.15.8">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T9.7.15.8.1">Predicted</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.15.8.2">0.972</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.7.15.8.3">0.938</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.15.8.4">0.748</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.15.8.5">0.616</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.15.8.6">0.464</td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_t" id="A1.T9.7.7.1" rowspan="3"><span class="ltx_text" id="A1.T9.7.7.1.1"><math alttext="\Delta" class="ltx_Math" display="inline" id="A1.T9.7.7.1.1.m1.1"><semantics id="A1.T9.7.7.1.1.m1.1a"><mi id="A1.T9.7.7.1.1.m1.1.1" mathvariant="normal" xref="A1.T9.7.7.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A1.T9.7.7.1.1.m1.1b"><ci id="A1.T9.7.7.1.1.m1.1.1.cmml" xref="A1.T9.7.7.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A1.T9.7.7.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A1.T9.7.7.1.1.m1.1d">roman_Δ</annotation></semantics></math> Energy(variable-length)</span></td>
<td class="ltx_td ltx_align_center ltx_border_rr ltx_border_t" id="A1.T9.7.7.2">None</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.7.7.3">0.964</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="A1.T9.7.7.4">0.886</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.7.7.5">0.463</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.7.7.6">0.276</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A1.T9.7.7.7">0.145</td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.16.9">
<td class="ltx_td ltx_align_center ltx_border_rr" id="A1.T9.7.16.9.1">Real</td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.16.9.2"><span class="ltx_text ltx_font_bold" id="A1.T9.7.16.9.2.1">0.970</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="A1.T9.7.16.9.3"><span class="ltx_text ltx_font_bold" id="A1.T9.7.16.9.3.1">0.929</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.16.9.4"><span class="ltx_text ltx_font_bold" id="A1.T9.7.16.9.4.1">0.566</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.16.9.5"><span class="ltx_text ltx_font_bold" id="A1.T9.7.16.9.5.1">0.362</span></td>
<td class="ltx_td ltx_align_center" id="A1.T9.7.16.9.6"><span class="ltx_text ltx_font_bold" id="A1.T9.7.16.9.6.1">0.205</span></td>
</tr>
<tr class="ltx_tr" id="A1.T9.7.17.10">
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_rr" id="A1.T9.7.17.10.1">Predicted</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.7.17.10.2">0.964</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="A1.T9.7.17.10.3">0.883</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.7.17.10.4">0.424</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.7.17.10.5">0.252</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A1.T9.7.17.10.6">0.133</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 9: </span>Comparing the effects of varying reward type on the ACE2 protein engineering task. </figcaption>
</figure>
</section>
<section class="ltx_appendix" id="A2">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix B </span>Extrapolation experimental details</h2>
<section class="ltx_subsection" id="A2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.1 </span>Protein engineering</h3>
<div class="ltx_para" id="A2.SS1.p1">
<p class="ltx_p" id="A2.SS1.p1.1">Starting from wildtype ACE2, we iteratively sample for 83 steps, using the trained ddG scorer and Hamming distance as our experts in the product of experts energy function. We use the pre-trained Prot-T5-XL model from <cite class="ltx_cite ltx_citemacro_citep">(Elnaggar et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib12" title="">2020</a>)</cite> as our proposal distribution, and following the experimental procedure of <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>, we restrict the sampler from resampling a constant span of 8 tokens (NTNITEEN) to prevent too much divergence from the wildtype sequence.</p>
</div>
<div class="ltx_para" id="A2.SS1.p2">
<p class="ltx_p" id="A2.SS1.p2.1">To train <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A2.SS1.p2.1.m1.1"><semantics id="A2.SS1.p2.1.m1.1a"><msub id="A2.SS1.p2.1.m1.1.1" xref="A2.SS1.p2.1.m1.1.1.cmml"><mi id="A2.SS1.p2.1.m1.1.1.2" xref="A2.SS1.p2.1.m1.1.1.2.cmml">q</mi><mi id="A2.SS1.p2.1.m1.1.1.3" xref="A2.SS1.p2.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p2.1.m1.1b"><apply id="A2.SS1.p2.1.m1.1.1.cmml" xref="A2.SS1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p2.1.m1.1.1.1.cmml" xref="A2.SS1.p2.1.m1.1.1">subscript</csymbol><ci id="A2.SS1.p2.1.m1.1.1.2.cmml" xref="A2.SS1.p2.1.m1.1.1.2">𝑞</ci><ci id="A2.SS1.p2.1.m1.1.1.3.cmml" xref="A2.SS1.p2.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p2.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p2.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, we finetune Prot-T5-XL using low rank adaptation (LoRA)<cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib19" title="">2022</a>)</cite>. Further details can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A6" title="Appendix F Hyperparameters ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Appendix F</span></a>. At inference time, we prompt with the wildtype sequence and sample 10,000 mutants.</p>
</div>
<div class="ltx_para" id="A2.SS1.p3">
<p class="ltx_p" id="A2.SS1.p3.1">One challenge of this task is the lack of separate test/validation splits, as the protein always mutates from the wildtype sequence. We take several measures to attempt to avoid overfitting. Most obviously, we minimize hyperparameter tuning, and when it is absolutely necessary to choose a hyperparameter(e.g. selecting appropriate weights for the EBM) we start from a mutant variety of ACE2. When training <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A2.SS1.p3.1.m1.1"><semantics id="A2.SS1.p3.1.m1.1a"><msub id="A2.SS1.p3.1.m1.1.1" xref="A2.SS1.p3.1.m1.1.1.cmml"><mi id="A2.SS1.p3.1.m1.1.1.2" xref="A2.SS1.p3.1.m1.1.1.2.cmml">q</mi><mi id="A2.SS1.p3.1.m1.1.1.3" xref="A2.SS1.p3.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS1.p3.1.m1.1b"><apply id="A2.SS1.p3.1.m1.1.1.cmml" xref="A2.SS1.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS1.p3.1.m1.1.1.1.cmml" xref="A2.SS1.p3.1.m1.1.1">subscript</csymbol><ci id="A2.SS1.p3.1.m1.1.1.2.cmml" xref="A2.SS1.p3.1.m1.1.1.2">𝑞</ci><ci id="A2.SS1.p3.1.m1.1.1.3.cmml" xref="A2.SS1.p3.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS1.p3.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A2.SS1.p3.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>, we also limit the length of variable-length training episodes to 10. We emphasize, however, that overfitting to the training data would tend to be <span class="ltx_text ltx_font_italic" id="A2.SS1.p3.1.1">disadvantageous</span> to the model, as overfitting to training data would necessarily fail to extrapolate beyond the training range.</p>
</div>
</section>
<section class="ltx_subsection" id="A2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">B.2 </span>Sentiment</h3>
<div class="ltx_para" id="A2.SS2.p1">
<p class="ltx_p" id="A2.SS2.p1.1">In our energy function, the first term is the training-time scorer proposed by <cite class="ltx_cite ltx_citemacro_citet">Padmakumar et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib33" title="">2023</a>)</cite>, which incentivizes sentiment control. The second is a Hamming distance term, which incentivizes semantic closeness to the original document. We use this EBM and sample 66,163 sentences <span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>For computational efficiency, we run MCMC only on sentences with length of 64 tokens or fewer.</span></span></span> using a pretrained T5-3B model <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib37" title="">2020</a>)</cite> as our proposal distribution for both conversion to positive sentiment and negative sentiment, giving us a combined training dataset of 132,326 markov chains. We finetune T5-base <cite class="ltx_cite ltx_citemacro_citep">(Raffel et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib37" title="">2020</a>)</cite> on these chains to train <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A2.SS2.p1.1.m1.1"><semantics id="A2.SS2.p1.1.m1.1a"><msub id="A2.SS2.p1.1.m1.1.1" xref="A2.SS2.p1.1.m1.1.1.cmml"><mi id="A2.SS2.p1.1.m1.1.1.2" xref="A2.SS2.p1.1.m1.1.1.2.cmml">q</mi><mi id="A2.SS2.p1.1.m1.1.1.3" xref="A2.SS2.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A2.SS2.p1.1.m1.1b"><apply id="A2.SS2.p1.1.m1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A2.SS2.p1.1.m1.1.1.1.cmml" xref="A2.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="A2.SS2.p1.1.m1.1.1.2.cmml" xref="A2.SS2.p1.1.m1.1.1.2">𝑞</ci><ci id="A2.SS2.p1.1.m1.1.1.3.cmml" xref="A2.SS2.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A2.SS2.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A2.SS2.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>; we add a prefix <span class="ltx_text ltx_font_typewriter" id="A2.SS2.p1.1.1">"Make this {positive, negative}: "</span> to cue the direction of edits, rather than training two separate models. Hyperparameters can be found in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A6" title="Appendix F Hyperparameters ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Appendix F</span></a>.</p>
</div>
<div class="ltx_para" id="A2.SS2.p2">
<p class="ltx_p" id="A2.SS2.p2.1">We also implement a popular controllable generation method, FUDGE <cite class="ltx_cite ltx_citemacro_citep">(Yang &amp; Klein, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib55" title="">2021</a>)</cite>, as for the sentiment control task. To train the forward looking model, we fine-tune RoBERTa <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib28" title="">2020</a>)</cite> on the three classes in our training regime (2, 3, 4 star reviews) for 5000 total steps. Instead of running FUDGE with a decoder only model, we use PEGASUS <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib56" title="">2020a</a>)</cite>, a sequence to sequence paraphraser of similar size to the models used in our other approaches. At inference time in our evaluations, we supply the PEGASUS paraphraser with FUDGE with control codes for 2 and 4 star reviews, and measure how well the approach is able to generate 1 and 5 star reviews.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A3">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix C </span>Analysis of episode length</h2>
<div class="ltx_para" id="A3.p1">
<p class="ltx_p" id="A3.p1.1">In complex tasks such as protein synthesis and anonymization, we find a noticeable benefit to using multiple edit steps rather than taking only the first and best states. We show the results of training anonymization models with on various episode lengths in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A3.T10" title="Table 10 ‣ Appendix C Analysis of episode length ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 10</span></a>. We find that training the model on longer episodes consistently decreases semantic similarity for anonymization. Training on longer episodes improves EER up to a certain point, after which the SBERT decreases without consistent improvement in EER. We select our number of states based on this point.</p>
</div>
<figure class="ltx_table" id="A3.T10">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A3.T10.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A3.T10.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A3.T10.2.2.3"><span class="ltx_text ltx_font_bold" id="A3.T10.2.2.3.1">Episode length</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.1.1.1"><span class="ltx_text ltx_font_bold" id="A3.T10.1.1.1.1">EER<math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T10.1.1.1.1.m1.1"><semantics id="A3.T10.1.1.1.1.m1.1a"><mo id="A3.T10.1.1.1.1.m1.1.1" stretchy="false" xref="A3.T10.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T10.1.1.1.1.m1.1b"><ci id="A3.T10.1.1.1.1.m1.1.1.cmml" xref="A3.T10.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A3.T10.2.2.2"><span class="ltx_text ltx_font_bold" id="A3.T10.2.2.2.1">SBERT<math alttext="\uparrow" class="ltx_Math" display="inline" id="A3.T10.2.2.2.1.m1.1"><semantics id="A3.T10.2.2.2.1.m1.1a"><mo id="A3.T10.2.2.2.1.m1.1.1" stretchy="false" xref="A3.T10.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A3.T10.2.2.2.1.m1.1b"><ci id="A3.T10.2.2.2.1.m1.1.1.cmml" xref="A3.T10.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A3.T10.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A3.T10.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A3.T10.2.3.1.1">2 (First/Best)</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.2.3.1.2">0.132</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A3.T10.2.3.1.3"><span class="ltx_text ltx_font_bold" id="A3.T10.2.3.1.3.1">0.923</span></td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T10.2.4.2.1">3</th>
<td class="ltx_td ltx_align_center" id="A3.T10.2.4.2.2">0.161</td>
<td class="ltx_td ltx_align_center" id="A3.T10.2.4.2.3">0.857</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T10.2.5.3.1">4</th>
<td class="ltx_td ltx_align_center" id="A3.T10.2.5.3.2">0.150</td>
<td class="ltx_td ltx_align_center" id="A3.T10.2.5.3.3">0.827</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.6.4">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T10.2.6.4.1">5</th>
<td class="ltx_td ltx_align_center" id="A3.T10.2.6.4.2"><span class="ltx_text ltx_font_bold" id="A3.T10.2.6.4.2.1">0.224</span></td>
<td class="ltx_td ltx_align_center" id="A3.T10.2.6.4.3">0.835</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.7.5">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T10.2.7.5.1">6</th>
<td class="ltx_td ltx_align_center" id="A3.T10.2.7.5.2">0.187</td>
<td class="ltx_td ltx_align_center" id="A3.T10.2.7.5.3">0.776</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.8.6">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A3.T10.2.8.6.1">7</th>
<td class="ltx_td ltx_align_center" id="A3.T10.2.8.6.2">0.198</td>
<td class="ltx_td ltx_align_center" id="A3.T10.2.8.6.3">0.762</td>
</tr>
<tr class="ltx_tr" id="A3.T10.2.9.7">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A3.T10.2.9.7.1">8</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.2.9.7.2">0.200</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A3.T10.2.9.7.3">0.745</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 10: </span>Anonymization results with our proposed episode strategies. <math alttext="\Delta" class="ltx_Math" display="inline" id="A3.T10.4.m1.1"><semantics id="A3.T10.4.m1.1b"><mi id="A3.T10.4.m1.1.1" mathvariant="normal" xref="A3.T10.4.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A3.T10.4.m1.1c"><ci id="A3.T10.4.m1.1.1.cmml" xref="A3.T10.4.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A3.T10.4.m1.1d">\Delta</annotation><annotation encoding="application/x-llamapun" id="A3.T10.4.m1.1e">roman_Δ</annotation></semantics></math> energy strategies tend to have higher SBERT scores than thinning strategies, with little to no tradeoff on EER. </figcaption>
</figure>
<div class="ltx_para" id="A3.p2">
<p class="ltx_p" id="A3.p2.1">In the case of sentiment, there is no clear benefit to adding states: the task is sufficiently simple that a single edit is the most effective way to extrapolate, hence why we consider first/best to be the best method in this case.</p>
</div>
</section>
<section class="ltx_appendix" id="A4">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix D </span>Ablation of MCMC exploration</h2>
<div class="ltx_para" id="A4.p1">
<p class="ltx_p" id="A4.p1.1">As <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A4.p1.1.m1.1"><semantics id="A4.p1.1.m1.1a"><msub id="A4.p1.1.m1.1.1" xref="A4.p1.1.m1.1.1.cmml"><mi id="A4.p1.1.m1.1.1.2" xref="A4.p1.1.m1.1.1.2.cmml">q</mi><mi id="A4.p1.1.m1.1.1.3" xref="A4.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p1.1.m1.1b"><apply id="A4.p1.1.m1.1.1.cmml" xref="A4.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A4.p1.1.m1.1.1.1.cmml" xref="A4.p1.1.m1.1.1">subscript</csymbol><ci id="A4.p1.1.m1.1.1.2.cmml" xref="A4.p1.1.m1.1.1.2">𝑞</ci><ci id="A4.p1.1.m1.1.1.3.cmml" xref="A4.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A4.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> is trained on the Markov chains created through MCMC, we investigate how allowing fewer steps of MCMC (and therefore less opportunity for exploration) impacts our results.</p>
</div>
<div class="ltx_para" id="A4.p2">
<p class="ltx_p" id="A4.p2.1">For our anonymization task, we run for an average of 4,498 MCMC steps. In <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A4.T11" title="Table 11 ‣ Appendix D Ablation of MCMC exploration ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 11</span></a> we report results compared to two models trained on shorter subsets of the Markov chains, not using any steps after 25% or 50% of the chain length to construct the training episode. We use the <math alttext="\Delta" class="ltx_Math" display="inline" id="A4.p2.1.m1.1"><semantics id="A4.p2.1.m1.1a"><mi id="A4.p2.1.m1.1.1" mathvariant="normal" xref="A4.p2.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A4.p2.1.m1.1b"><ci id="A4.p2.1.m1.1.1.cmml" xref="A4.p2.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.p2.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A4.p2.1.m1.1d">roman_Δ</annotation></semantics></math> energy (fixed-length) method of selecting episodes from these chains.</p>
</div>
<figure class="ltx_table" id="A4.T11">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A4.T11.2">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A4.T11.2.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_r ltx_border_tt" id="A4.T11.2.2.3"><span class="ltx_text ltx_font_bold" id="A4.T11.2.2.3.1">MCMC steps</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.1.1.1"><span class="ltx_text ltx_font_bold" id="A4.T11.1.1.1.1">EER<math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T11.1.1.1.1.m1.1"><semantics id="A4.T11.1.1.1.1.m1.1a"><mo id="A4.T11.1.1.1.1.m1.1.1" stretchy="false" xref="A4.T11.1.1.1.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T11.1.1.1.1.m1.1b"><ci id="A4.T11.1.1.1.1.m1.1.1.cmml" xref="A4.T11.1.1.1.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T11.1.1.1.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A4.T11.1.1.1.1.m1.1d">↑</annotation></semantics></math></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A4.T11.2.2.2"><span class="ltx_text ltx_font_bold" id="A4.T11.2.2.2.1">SBERT<math alttext="\uparrow" class="ltx_Math" display="inline" id="A4.T11.2.2.2.1.m1.1"><semantics id="A4.T11.2.2.2.1.m1.1a"><mo id="A4.T11.2.2.2.1.m1.1.1" stretchy="false" xref="A4.T11.2.2.2.1.m1.1.1.cmml">↑</mo><annotation-xml encoding="MathML-Content" id="A4.T11.2.2.2.1.m1.1b"><ci id="A4.T11.2.2.2.1.m1.1.1.cmml" xref="A4.T11.2.2.2.1.m1.1.1">↑</ci></annotation-xml><annotation encoding="application/x-tex" id="A4.T11.2.2.2.1.m1.1c">\uparrow</annotation><annotation encoding="application/x-llamapun" id="A4.T11.2.2.2.1.m1.1d">↑</annotation></semantics></math></span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A4.T11.2.3.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r ltx_border_t" id="A4.T11.2.3.1.1">25%</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.2.3.1.2">0.200</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="A4.T11.2.3.1.3">0.701</td>
</tr>
<tr class="ltx_tr" id="A4.T11.2.4.2">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_r" id="A4.T11.2.4.2.1">50%</th>
<td class="ltx_td ltx_align_center" id="A4.T11.2.4.2.2">0.188</td>
<td class="ltx_td ltx_align_center" id="A4.T11.2.4.2.3">0.781</td>
</tr>
<tr class="ltx_tr" id="A4.T11.2.5.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb ltx_border_r" id="A4.T11.2.5.3.1">100%</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.2.5.3.2"><span class="ltx_text ltx_font_bold" id="A4.T11.2.5.3.2.1">0.224</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A4.T11.2.5.3.3"><span class="ltx_text ltx_font_bold" id="A4.T11.2.5.3.3.1">0.835</span></td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 11: </span>Anonymization results with our proposed episode strategies on shorter chains. We find that reducing the number of MCMC steps significantly reduces the score.</figcaption>
</figure>
<div class="ltx_para" id="A4.p3">
<p class="ltx_p" id="A4.p3.1">We find that our best model with both metrics is trained on the full chain. We find that, while EER tends to improve, we see the greatest improvement in the semantic similarity metric, with the model trained on the full chains achieving an improvement of 0.134 over the model trained on 25%-length chains. This demonstrates that improved Markov chains correspond to improved performance from <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A4.p3.1.m1.1"><semantics id="A4.p3.1.m1.1a"><msub id="A4.p3.1.m1.1.1" xref="A4.p3.1.m1.1.1.cmml"><mi id="A4.p3.1.m1.1.1.2" xref="A4.p3.1.m1.1.1.2.cmml">q</mi><mi id="A4.p3.1.m1.1.1.3" xref="A4.p3.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A4.p3.1.m1.1b"><apply id="A4.p3.1.m1.1.1.cmml" xref="A4.p3.1.m1.1.1"><csymbol cd="ambiguous" id="A4.p3.1.m1.1.1.1.cmml" xref="A4.p3.1.m1.1.1">subscript</csymbol><ci id="A4.p3.1.m1.1.1.2.cmml" xref="A4.p3.1.m1.1.1.2">𝑞</ci><ci id="A4.p3.1.m1.1.1.3.cmml" xref="A4.p3.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A4.p3.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A4.p3.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>; this may additionally mean that improvements to the sampling procedure, such as annealing, may lead to improved performance with models trained on that data.</p>
</div>
</section>
<section class="ltx_appendix" id="A5">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix E </span>Diversity of generations</h2>
<div class="ltx_para" id="A5.p1">
<p class="ltx_p" id="A5.p1.1">In theory, a model can trivially achieve extrapolation by producing a single output in the extrapolation range in response to any input. We here consider the diversity of the outputs. For our protein task, we count the number of unique outputs, and find that 100% of the 10k proteins generated using were unique. For sentiment and anonymization, we run corpus BLEU score <cite class="ltx_cite ltx_citemacro_citep">(Papineni et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib34" title="">2002</a>)</cite> between all pairs of generated sentences, finding that we achieve only 1.39 BLEU for sentiment and 0.03 BLEU for anonymization, meaning there is an extremely low amount of token overlap between generated sentences.</p>
</div>
</section>
<section class="ltx_appendix" id="A6">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix F </span>Hyperparameters</h2>
<div class="ltx_para" id="A6.p1">
<p class="ltx_p" id="A6.p1.3"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A6.T12" title="Table 12 ‣ Appendix F Hyperparameters ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 12</span></a> shows the hyperparameters used in our framework. <span class="ltx_text ltx_font_italic" id="A6.p1.3.1">MCMC sampling epochs</span> refers to the number of iterations: we consider that MCMC has run for one epoch when it has run for as many iterations as tokens in the sentence. <span class="ltx_text ltx_font_italic" id="A6.p1.3.2">Fixed-length length</span> refers to the number of selected states in a training episode when using our two fixed-length methods. <math alttext="\Delta" class="ltx_Math" display="inline" id="A6.p1.1.m1.1"><semantics id="A6.p1.1.m1.1a"><mi id="A6.p1.1.m1.1.1" mathvariant="normal" xref="A6.p1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A6.p1.1.m1.1b"><ci id="A6.p1.1.m1.1.1.cmml" xref="A6.p1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A6.p1.1.m1.1d">roman_Δ</annotation></semantics></math><span class="ltx_text ltx_font_italic" id="A6.p1.3.3"> energy (variable-length) threshold</span> and <span class="ltx_text ltx_font_italic" id="A6.p1.3.4">thinning factor(variable-length)</span> refer to the hyperparameters used to determine sequence length for the variable-length training episodes, as described in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S2.SS3" title="2.3 Creating training episodes ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 2.3</span></a>. <span class="ltx_text ltx_font_italic" id="A6.p1.3.5">LoRA rank</span> and <span class="ltx_text ltx_font_italic" id="A6.p1.3.6">learning rate</span> are the hyperparameters used while training <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A6.p1.2.m2.1"><semantics id="A6.p1.2.m2.1a"><msub id="A6.p1.2.m2.1.1" xref="A6.p1.2.m2.1.1.cmml"><mi id="A6.p1.2.m2.1.1.2" xref="A6.p1.2.m2.1.1.2.cmml">q</mi><mi id="A6.p1.2.m2.1.1.3" xref="A6.p1.2.m2.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A6.p1.2.m2.1b"><apply id="A6.p1.2.m2.1.1.cmml" xref="A6.p1.2.m2.1.1"><csymbol cd="ambiguous" id="A6.p1.2.m2.1.1.1.cmml" xref="A6.p1.2.m2.1.1">subscript</csymbol><ci id="A6.p1.2.m2.1.1.2.cmml" xref="A6.p1.2.m2.1.1.2">𝑞</ci><ci id="A6.p1.2.m2.1.1.3.cmml" xref="A6.p1.2.m2.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.2.m2.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A6.p1.2.m2.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>; as sentiment did not use LoRA, we do not report LoRA rank. <span class="ltx_text ltx_font_italic" id="A6.p1.3.7">Decoding temperature</span> and <span class="ltx_text ltx_font_italic" id="A6.p1.3.8">Decoding top k</span> refer to the hyperparameters used while generating using <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A6.p1.3.m3.1"><semantics id="A6.p1.3.m3.1a"><msub id="A6.p1.3.m3.1.1" xref="A6.p1.3.m3.1.1.cmml"><mi id="A6.p1.3.m3.1.1.2" xref="A6.p1.3.m3.1.1.2.cmml">q</mi><mi id="A6.p1.3.m3.1.1.3" xref="A6.p1.3.m3.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A6.p1.3.m3.1b"><apply id="A6.p1.3.m3.1.1.cmml" xref="A6.p1.3.m3.1.1"><csymbol cd="ambiguous" id="A6.p1.3.m3.1.1.1.cmml" xref="A6.p1.3.m3.1.1">subscript</csymbol><ci id="A6.p1.3.m3.1.1.2.cmml" xref="A6.p1.3.m3.1.1.2">𝑞</ci><ci id="A6.p1.3.m3.1.1.3.cmml" xref="A6.p1.3.m3.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.p1.3.m3.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A6.p1.3.m3.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>. Detailed implementation details for sentiment and protein engineering tasks are reported in the main text, and the details of the energy function used during MCMC are reported below; detailed implementation details for anonymization are reported in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A7" title="Appendix G Text Anonymization Implementation ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Appendix G</span></a>.</p>
</div>
<figure class="ltx_table" id="A6.T12">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A6.T12.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A6.T12.1.2.1">
<th class="ltx_td ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="A6.T12.1.2.1.1"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A6.T12.1.2.1.2"><span class="ltx_text ltx_font_bold" id="A6.T12.1.2.1.2.1">Protein engineering</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A6.T12.1.2.1.3"><span class="ltx_text ltx_font_bold" id="A6.T12.1.2.1.3.1">Sentiment</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="A6.T12.1.2.1.4"><span class="ltx_text ltx_font_bold" id="A6.T12.1.2.1.4.1">Anonymization</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A6.T12.1.3.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="A6.T12.1.3.1.1">MCMC sampling epochs</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.3.1.2">1</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.3.1.3">8</td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="A6.T12.1.3.1.4">40</td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.4.2">
<td class="ltx_td ltx_align_left ltx_border_r" id="A6.T12.1.4.2.1">Fixed-length length</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.4.2.2">4</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.4.2.3">5</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.4.2.4">5</td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.1">
<td class="ltx_td ltx_align_left ltx_border_r" id="A6.T12.1.1.1">
<math alttext="\Delta" class="ltx_Math" display="inline" id="A6.T12.1.1.1.m1.1"><semantics id="A6.T12.1.1.1.m1.1a"><mi id="A6.T12.1.1.1.m1.1.1" mathvariant="normal" xref="A6.T12.1.1.1.m1.1.1.cmml">Δ</mi><annotation-xml encoding="MathML-Content" id="A6.T12.1.1.1.m1.1b"><ci id="A6.T12.1.1.1.m1.1.1.cmml" xref="A6.T12.1.1.1.m1.1.1">Δ</ci></annotation-xml><annotation encoding="application/x-tex" id="A6.T12.1.1.1.m1.1c">\Delta</annotation><annotation encoding="application/x-llamapun" id="A6.T12.1.1.1.m1.1d">roman_Δ</annotation></semantics></math> energy (variable-length) threshold</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.2">20%</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.3">2%</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.1.4">1%</td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.5.3">
<td class="ltx_td ltx_align_left ltx_border_r" id="A6.T12.1.5.3.1">Thinning factor(variable-length)</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.5.3.2">2</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.5.3.3">100</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.5.3.4">3</td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.6.4">
<td class="ltx_td ltx_align_left ltx_border_r" id="A6.T12.1.6.4.1">LoRA rank</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.6.4.2">16</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.6.4.3">-</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.6.4.4">16</td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.7.5">
<td class="ltx_td ltx_align_left ltx_border_r" id="A6.T12.1.7.5.1">Learning rate</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.7.5.2">2E-4</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.7.5.3">1E-4</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.7.5.4">5E-5</td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.8.6">
<td class="ltx_td ltx_align_left ltx_border_r" id="A6.T12.1.8.6.1">Decoding temperature</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.8.6.2">1.5</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.8.6.3">1.1</td>
<td class="ltx_td ltx_align_center" id="A6.T12.1.8.6.4">1.1</td>
</tr>
<tr class="ltx_tr" id="A6.T12.1.9.7">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="A6.T12.1.9.7.1">Decoding top k</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.9.7.2">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.9.7.3">16</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="A6.T12.1.9.7.4">50</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 12: </span>Hyperparameters</figcaption>
</figure>
<section class="ltx_paragraph" id="A6.SS0.SSS0.Px1">
<h4 class="ltx_title ltx_title_paragraph">Protein engineering energy function</h4>
<div class="ltx_para" id="A6.SS0.SSS0.Px1.p1">
<p class="ltx_p" id="A6.SS0.SSS0.Px1.p1.1">In our energy function, we use a weight of 500 on the training scorer term (ddG) and a weight of 10 on the Hamming distance term. In other words:</p>
<table class="ltx_equation ltx_eqn_table" id="A6.E1">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s(x)=500*s_{\text{ddg}}(x)+10*s_{\text{hamming}}(x)" class="ltx_Math" display="block" id="A6.E1.m1.3"><semantics id="A6.E1.m1.3a"><mrow id="A6.E1.m1.3.4" xref="A6.E1.m1.3.4.cmml"><mrow id="A6.E1.m1.3.4.2" xref="A6.E1.m1.3.4.2.cmml"><mi id="A6.E1.m1.3.4.2.2" xref="A6.E1.m1.3.4.2.2.cmml">s</mi><mo id="A6.E1.m1.3.4.2.1" xref="A6.E1.m1.3.4.2.1.cmml">⁢</mo><mrow id="A6.E1.m1.3.4.2.3.2" xref="A6.E1.m1.3.4.2.cmml"><mo id="A6.E1.m1.3.4.2.3.2.1" stretchy="false" xref="A6.E1.m1.3.4.2.cmml">(</mo><mi id="A6.E1.m1.1.1" xref="A6.E1.m1.1.1.cmml">x</mi><mo id="A6.E1.m1.3.4.2.3.2.2" stretchy="false" xref="A6.E1.m1.3.4.2.cmml">)</mo></mrow></mrow><mo id="A6.E1.m1.3.4.1" xref="A6.E1.m1.3.4.1.cmml">=</mo><mrow id="A6.E1.m1.3.4.3" xref="A6.E1.m1.3.4.3.cmml"><mrow id="A6.E1.m1.3.4.3.2" xref="A6.E1.m1.3.4.3.2.cmml"><mrow id="A6.E1.m1.3.4.3.2.2" xref="A6.E1.m1.3.4.3.2.2.cmml"><mn id="A6.E1.m1.3.4.3.2.2.2" xref="A6.E1.m1.3.4.3.2.2.2.cmml">500</mn><mo id="A6.E1.m1.3.4.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="A6.E1.m1.3.4.3.2.2.1.cmml">∗</mo><msub id="A6.E1.m1.3.4.3.2.2.3" xref="A6.E1.m1.3.4.3.2.2.3.cmml"><mi id="A6.E1.m1.3.4.3.2.2.3.2" xref="A6.E1.m1.3.4.3.2.2.3.2.cmml">s</mi><mtext id="A6.E1.m1.3.4.3.2.2.3.3" xref="A6.E1.m1.3.4.3.2.2.3.3a.cmml">ddg</mtext></msub></mrow><mo id="A6.E1.m1.3.4.3.2.1" xref="A6.E1.m1.3.4.3.2.1.cmml">⁢</mo><mrow id="A6.E1.m1.3.4.3.2.3.2" xref="A6.E1.m1.3.4.3.2.cmml"><mo id="A6.E1.m1.3.4.3.2.3.2.1" stretchy="false" xref="A6.E1.m1.3.4.3.2.cmml">(</mo><mi id="A6.E1.m1.2.2" xref="A6.E1.m1.2.2.cmml">x</mi><mo id="A6.E1.m1.3.4.3.2.3.2.2" stretchy="false" xref="A6.E1.m1.3.4.3.2.cmml">)</mo></mrow></mrow><mo id="A6.E1.m1.3.4.3.1" xref="A6.E1.m1.3.4.3.1.cmml">+</mo><mrow id="A6.E1.m1.3.4.3.3" xref="A6.E1.m1.3.4.3.3.cmml"><mrow id="A6.E1.m1.3.4.3.3.2" xref="A6.E1.m1.3.4.3.3.2.cmml"><mn id="A6.E1.m1.3.4.3.3.2.2" xref="A6.E1.m1.3.4.3.3.2.2.cmml">10</mn><mo id="A6.E1.m1.3.4.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="A6.E1.m1.3.4.3.3.2.1.cmml">∗</mo><msub id="A6.E1.m1.3.4.3.3.2.3" xref="A6.E1.m1.3.4.3.3.2.3.cmml"><mi id="A6.E1.m1.3.4.3.3.2.3.2" xref="A6.E1.m1.3.4.3.3.2.3.2.cmml">s</mi><mtext id="A6.E1.m1.3.4.3.3.2.3.3" xref="A6.E1.m1.3.4.3.3.2.3.3a.cmml">hamming</mtext></msub></mrow><mo id="A6.E1.m1.3.4.3.3.1" xref="A6.E1.m1.3.4.3.3.1.cmml">⁢</mo><mrow id="A6.E1.m1.3.4.3.3.3.2" xref="A6.E1.m1.3.4.3.3.cmml"><mo id="A6.E1.m1.3.4.3.3.3.2.1" stretchy="false" xref="A6.E1.m1.3.4.3.3.cmml">(</mo><mi id="A6.E1.m1.3.3" xref="A6.E1.m1.3.3.cmml">x</mi><mo id="A6.E1.m1.3.4.3.3.3.2.2" stretchy="false" xref="A6.E1.m1.3.4.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.E1.m1.3b"><apply id="A6.E1.m1.3.4.cmml" xref="A6.E1.m1.3.4"><eq id="A6.E1.m1.3.4.1.cmml" xref="A6.E1.m1.3.4.1"></eq><apply id="A6.E1.m1.3.4.2.cmml" xref="A6.E1.m1.3.4.2"><times id="A6.E1.m1.3.4.2.1.cmml" xref="A6.E1.m1.3.4.2.1"></times><ci id="A6.E1.m1.3.4.2.2.cmml" xref="A6.E1.m1.3.4.2.2">𝑠</ci><ci id="A6.E1.m1.1.1.cmml" xref="A6.E1.m1.1.1">𝑥</ci></apply><apply id="A6.E1.m1.3.4.3.cmml" xref="A6.E1.m1.3.4.3"><plus id="A6.E1.m1.3.4.3.1.cmml" xref="A6.E1.m1.3.4.3.1"></plus><apply id="A6.E1.m1.3.4.3.2.cmml" xref="A6.E1.m1.3.4.3.2"><times id="A6.E1.m1.3.4.3.2.1.cmml" xref="A6.E1.m1.3.4.3.2.1"></times><apply id="A6.E1.m1.3.4.3.2.2.cmml" xref="A6.E1.m1.3.4.3.2.2"><times id="A6.E1.m1.3.4.3.2.2.1.cmml" xref="A6.E1.m1.3.4.3.2.2.1"></times><cn id="A6.E1.m1.3.4.3.2.2.2.cmml" type="integer" xref="A6.E1.m1.3.4.3.2.2.2">500</cn><apply id="A6.E1.m1.3.4.3.2.2.3.cmml" xref="A6.E1.m1.3.4.3.2.2.3"><csymbol cd="ambiguous" id="A6.E1.m1.3.4.3.2.2.3.1.cmml" xref="A6.E1.m1.3.4.3.2.2.3">subscript</csymbol><ci id="A6.E1.m1.3.4.3.2.2.3.2.cmml" xref="A6.E1.m1.3.4.3.2.2.3.2">𝑠</ci><ci id="A6.E1.m1.3.4.3.2.2.3.3a.cmml" xref="A6.E1.m1.3.4.3.2.2.3.3"><mtext id="A6.E1.m1.3.4.3.2.2.3.3.cmml" mathsize="70%" xref="A6.E1.m1.3.4.3.2.2.3.3">ddg</mtext></ci></apply></apply><ci id="A6.E1.m1.2.2.cmml" xref="A6.E1.m1.2.2">𝑥</ci></apply><apply id="A6.E1.m1.3.4.3.3.cmml" xref="A6.E1.m1.3.4.3.3"><times id="A6.E1.m1.3.4.3.3.1.cmml" xref="A6.E1.m1.3.4.3.3.1"></times><apply id="A6.E1.m1.3.4.3.3.2.cmml" xref="A6.E1.m1.3.4.3.3.2"><times id="A6.E1.m1.3.4.3.3.2.1.cmml" xref="A6.E1.m1.3.4.3.3.2.1"></times><cn id="A6.E1.m1.3.4.3.3.2.2.cmml" type="integer" xref="A6.E1.m1.3.4.3.3.2.2">10</cn><apply id="A6.E1.m1.3.4.3.3.2.3.cmml" xref="A6.E1.m1.3.4.3.3.2.3"><csymbol cd="ambiguous" id="A6.E1.m1.3.4.3.3.2.3.1.cmml" xref="A6.E1.m1.3.4.3.3.2.3">subscript</csymbol><ci id="A6.E1.m1.3.4.3.3.2.3.2.cmml" xref="A6.E1.m1.3.4.3.3.2.3.2">𝑠</ci><ci id="A6.E1.m1.3.4.3.3.2.3.3a.cmml" xref="A6.E1.m1.3.4.3.3.2.3.3"><mtext id="A6.E1.m1.3.4.3.3.2.3.3.cmml" mathsize="70%" xref="A6.E1.m1.3.4.3.3.2.3.3">hamming</mtext></ci></apply></apply><ci id="A6.E1.m1.3.3.cmml" xref="A6.E1.m1.3.3">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.E1.m1.3c">s(x)=500*s_{\text{ddg}}(x)+10*s_{\text{hamming}}(x)</annotation><annotation encoding="application/x-llamapun" id="A6.E1.m1.3d">italic_s ( italic_x ) = 500 ∗ italic_s start_POSTSUBSCRIPT ddg end_POSTSUBSCRIPT ( italic_x ) + 10 ∗ italic_s start_POSTSUBSCRIPT hamming end_POSTSUBSCRIPT ( italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_paragraph" id="A6.SS0.SSS0.Px2">
<h4 class="ltx_title ltx_title_paragraph">Sentiment energy function</h4>
<div class="ltx_para" id="A6.SS0.SSS0.Px2.p1">
<p class="ltx_p" id="A6.SS0.SSS0.Px2.p1.1">In our energy function, we use a weight of 1E5 on the training scorer term (sentiment) and a weight of 100 on the Hamming distance term. In other words:</p>
<table class="ltx_equation ltx_eqn_table" id="A6.E2">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s(x)=1\text{E}5*s_{\text{sentiment}}(x)+100*s_{\text{hamming}}(x)" class="ltx_Math" display="block" id="A6.E2.m1.3"><semantics id="A6.E2.m1.3a"><mrow id="A6.E2.m1.3.4" xref="A6.E2.m1.3.4.cmml"><mrow id="A6.E2.m1.3.4.2" xref="A6.E2.m1.3.4.2.cmml"><mi id="A6.E2.m1.3.4.2.2" xref="A6.E2.m1.3.4.2.2.cmml">s</mi><mo id="A6.E2.m1.3.4.2.1" xref="A6.E2.m1.3.4.2.1.cmml">⁢</mo><mrow id="A6.E2.m1.3.4.2.3.2" xref="A6.E2.m1.3.4.2.cmml"><mo id="A6.E2.m1.3.4.2.3.2.1" stretchy="false" xref="A6.E2.m1.3.4.2.cmml">(</mo><mi id="A6.E2.m1.1.1" xref="A6.E2.m1.1.1.cmml">x</mi><mo id="A6.E2.m1.3.4.2.3.2.2" stretchy="false" xref="A6.E2.m1.3.4.2.cmml">)</mo></mrow></mrow><mo id="A6.E2.m1.3.4.1" xref="A6.E2.m1.3.4.1.cmml">=</mo><mrow id="A6.E2.m1.3.4.3" xref="A6.E2.m1.3.4.3.cmml"><mrow id="A6.E2.m1.3.4.3.2" xref="A6.E2.m1.3.4.3.2.cmml"><mrow id="A6.E2.m1.3.4.3.2.2" xref="A6.E2.m1.3.4.3.2.2.cmml"><mrow id="A6.E2.m1.3.4.3.2.2.2" xref="A6.E2.m1.3.4.3.2.2.2.cmml"><mn id="A6.E2.m1.3.4.3.2.2.2.2" xref="A6.E2.m1.3.4.3.2.2.2.2.cmml">1</mn><mo id="A6.E2.m1.3.4.3.2.2.2.1" xref="A6.E2.m1.3.4.3.2.2.2.1.cmml">⁢</mo><mtext id="A6.E2.m1.3.4.3.2.2.2.3" xref="A6.E2.m1.3.4.3.2.2.2.3a.cmml">E</mtext><mo id="A6.E2.m1.3.4.3.2.2.2.1a" xref="A6.E2.m1.3.4.3.2.2.2.1.cmml">⁢</mo><mn id="A6.E2.m1.3.4.3.2.2.2.4" xref="A6.E2.m1.3.4.3.2.2.2.4.cmml">5</mn></mrow><mo id="A6.E2.m1.3.4.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="A6.E2.m1.3.4.3.2.2.1.cmml">∗</mo><msub id="A6.E2.m1.3.4.3.2.2.3" xref="A6.E2.m1.3.4.3.2.2.3.cmml"><mi id="A6.E2.m1.3.4.3.2.2.3.2" xref="A6.E2.m1.3.4.3.2.2.3.2.cmml">s</mi><mtext id="A6.E2.m1.3.4.3.2.2.3.3" xref="A6.E2.m1.3.4.3.2.2.3.3a.cmml">sentiment</mtext></msub></mrow><mo id="A6.E2.m1.3.4.3.2.1" xref="A6.E2.m1.3.4.3.2.1.cmml">⁢</mo><mrow id="A6.E2.m1.3.4.3.2.3.2" xref="A6.E2.m1.3.4.3.2.cmml"><mo id="A6.E2.m1.3.4.3.2.3.2.1" stretchy="false" xref="A6.E2.m1.3.4.3.2.cmml">(</mo><mi id="A6.E2.m1.2.2" xref="A6.E2.m1.2.2.cmml">x</mi><mo id="A6.E2.m1.3.4.3.2.3.2.2" stretchy="false" xref="A6.E2.m1.3.4.3.2.cmml">)</mo></mrow></mrow><mo id="A6.E2.m1.3.4.3.1" xref="A6.E2.m1.3.4.3.1.cmml">+</mo><mrow id="A6.E2.m1.3.4.3.3" xref="A6.E2.m1.3.4.3.3.cmml"><mrow id="A6.E2.m1.3.4.3.3.2" xref="A6.E2.m1.3.4.3.3.2.cmml"><mn id="A6.E2.m1.3.4.3.3.2.2" xref="A6.E2.m1.3.4.3.3.2.2.cmml">100</mn><mo id="A6.E2.m1.3.4.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="A6.E2.m1.3.4.3.3.2.1.cmml">∗</mo><msub id="A6.E2.m1.3.4.3.3.2.3" xref="A6.E2.m1.3.4.3.3.2.3.cmml"><mi id="A6.E2.m1.3.4.3.3.2.3.2" xref="A6.E2.m1.3.4.3.3.2.3.2.cmml">s</mi><mtext id="A6.E2.m1.3.4.3.3.2.3.3" xref="A6.E2.m1.3.4.3.3.2.3.3a.cmml">hamming</mtext></msub></mrow><mo id="A6.E2.m1.3.4.3.3.1" xref="A6.E2.m1.3.4.3.3.1.cmml">⁢</mo><mrow id="A6.E2.m1.3.4.3.3.3.2" xref="A6.E2.m1.3.4.3.3.cmml"><mo id="A6.E2.m1.3.4.3.3.3.2.1" stretchy="false" xref="A6.E2.m1.3.4.3.3.cmml">(</mo><mi id="A6.E2.m1.3.3" xref="A6.E2.m1.3.3.cmml">x</mi><mo id="A6.E2.m1.3.4.3.3.3.2.2" stretchy="false" xref="A6.E2.m1.3.4.3.3.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A6.E2.m1.3b"><apply id="A6.E2.m1.3.4.cmml" xref="A6.E2.m1.3.4"><eq id="A6.E2.m1.3.4.1.cmml" xref="A6.E2.m1.3.4.1"></eq><apply id="A6.E2.m1.3.4.2.cmml" xref="A6.E2.m1.3.4.2"><times id="A6.E2.m1.3.4.2.1.cmml" xref="A6.E2.m1.3.4.2.1"></times><ci id="A6.E2.m1.3.4.2.2.cmml" xref="A6.E2.m1.3.4.2.2">𝑠</ci><ci id="A6.E2.m1.1.1.cmml" xref="A6.E2.m1.1.1">𝑥</ci></apply><apply id="A6.E2.m1.3.4.3.cmml" xref="A6.E2.m1.3.4.3"><plus id="A6.E2.m1.3.4.3.1.cmml" xref="A6.E2.m1.3.4.3.1"></plus><apply id="A6.E2.m1.3.4.3.2.cmml" xref="A6.E2.m1.3.4.3.2"><times id="A6.E2.m1.3.4.3.2.1.cmml" xref="A6.E2.m1.3.4.3.2.1"></times><apply id="A6.E2.m1.3.4.3.2.2.cmml" xref="A6.E2.m1.3.4.3.2.2"><times id="A6.E2.m1.3.4.3.2.2.1.cmml" xref="A6.E2.m1.3.4.3.2.2.1"></times><apply id="A6.E2.m1.3.4.3.2.2.2.cmml" xref="A6.E2.m1.3.4.3.2.2.2"><times id="A6.E2.m1.3.4.3.2.2.2.1.cmml" xref="A6.E2.m1.3.4.3.2.2.2.1"></times><cn id="A6.E2.m1.3.4.3.2.2.2.2.cmml" type="integer" xref="A6.E2.m1.3.4.3.2.2.2.2">1</cn><ci id="A6.E2.m1.3.4.3.2.2.2.3a.cmml" xref="A6.E2.m1.3.4.3.2.2.2.3"><mtext id="A6.E2.m1.3.4.3.2.2.2.3.cmml" xref="A6.E2.m1.3.4.3.2.2.2.3">E</mtext></ci><cn id="A6.E2.m1.3.4.3.2.2.2.4.cmml" type="integer" xref="A6.E2.m1.3.4.3.2.2.2.4">5</cn></apply><apply id="A6.E2.m1.3.4.3.2.2.3.cmml" xref="A6.E2.m1.3.4.3.2.2.3"><csymbol cd="ambiguous" id="A6.E2.m1.3.4.3.2.2.3.1.cmml" xref="A6.E2.m1.3.4.3.2.2.3">subscript</csymbol><ci id="A6.E2.m1.3.4.3.2.2.3.2.cmml" xref="A6.E2.m1.3.4.3.2.2.3.2">𝑠</ci><ci id="A6.E2.m1.3.4.3.2.2.3.3a.cmml" xref="A6.E2.m1.3.4.3.2.2.3.3"><mtext id="A6.E2.m1.3.4.3.2.2.3.3.cmml" mathsize="70%" xref="A6.E2.m1.3.4.3.2.2.3.3">sentiment</mtext></ci></apply></apply><ci id="A6.E2.m1.2.2.cmml" xref="A6.E2.m1.2.2">𝑥</ci></apply><apply id="A6.E2.m1.3.4.3.3.cmml" xref="A6.E2.m1.3.4.3.3"><times id="A6.E2.m1.3.4.3.3.1.cmml" xref="A6.E2.m1.3.4.3.3.1"></times><apply id="A6.E2.m1.3.4.3.3.2.cmml" xref="A6.E2.m1.3.4.3.3.2"><times id="A6.E2.m1.3.4.3.3.2.1.cmml" xref="A6.E2.m1.3.4.3.3.2.1"></times><cn id="A6.E2.m1.3.4.3.3.2.2.cmml" type="integer" xref="A6.E2.m1.3.4.3.3.2.2">100</cn><apply id="A6.E2.m1.3.4.3.3.2.3.cmml" xref="A6.E2.m1.3.4.3.3.2.3"><csymbol cd="ambiguous" id="A6.E2.m1.3.4.3.3.2.3.1.cmml" xref="A6.E2.m1.3.4.3.3.2.3">subscript</csymbol><ci id="A6.E2.m1.3.4.3.3.2.3.2.cmml" xref="A6.E2.m1.3.4.3.3.2.3.2">𝑠</ci><ci id="A6.E2.m1.3.4.3.3.2.3.3a.cmml" xref="A6.E2.m1.3.4.3.3.2.3.3"><mtext id="A6.E2.m1.3.4.3.3.2.3.3.cmml" mathsize="70%" xref="A6.E2.m1.3.4.3.3.2.3.3">hamming</mtext></ci></apply></apply><ci id="A6.E2.m1.3.3.cmml" xref="A6.E2.m1.3.3">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A6.E2.m1.3c">s(x)=1\text{E}5*s_{\text{sentiment}}(x)+100*s_{\text{hamming}}(x)</annotation><annotation encoding="application/x-llamapun" id="A6.E2.m1.3d">italic_s ( italic_x ) = 1 E 5 ∗ italic_s start_POSTSUBSCRIPT sentiment end_POSTSUBSCRIPT ( italic_x ) + 100 ∗ italic_s start_POSTSUBSCRIPT hamming end_POSTSUBSCRIPT ( italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
</section>
</section>
<section class="ltx_appendix" id="A7">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix G </span>Text Anonymization Implementation</h2>
<section class="ltx_subsection" id="A7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.1 </span>Baseline Systems</h3>
<div class="ltx_para" id="A7.SS1.p1">
<p class="ltx_p" id="A7.SS1.p1.1">GPT3.5 and 4 use the following prompt to anonymize text: 
<br class="ltx_break"/>
<br class="ltx_break"/><span class="ltx_text ltx_font_typewriter" id="A7.SS1.p1.1.1">‘‘You are a helpful assistant who follows instructions and is helping anonymize text. Re-write the following reddit post to anonymize the author, remove all stylistic info that can be used to identify the author: &lt;input_text&gt;</span>”
<br class="ltx_break"/>
<br class="ltx_break"/>Based on optimal validation performance, we ran DIPPER with a lexical diversity of 60, order diversity of 40, and temperature of 0.75 <span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>We used the released checkpoint here: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://huggingface.co/kalpeshk2011/dipper-paraphraser-xxl" title="">https://huggingface.co/kalpeshk2011/dipper-paraphraser-xxl</a></span></span></span>. For the round trip machine translation system, we use the many to many model proposed by <cite class="ltx_cite ltx_citemacro_citet">Tang et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib50" title="">2020</a>)</cite>. We translate the initial text from English to German, and then back to English to obtain a paraphrase.</p>
</div>
</section>
<section class="ltx_subsection" id="A7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.2 </span>Data</h3>
<div class="ltx_para" id="A7.SS2.p1">
<p class="ltx_p" id="A7.SS2.p1.1">We sample training and evaluation data from the Reddit IUR dataset proposed by <cite class="ltx_cite ltx_citemacro_citet">Andrews &amp; Bishop (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib2" title="">2019</a>)</cite>. We select 16 posts from 1600 unique users (25600 total posts) to generate training episodes, 16 posts for 50 unique users (800 total posts) for an anonymization validation and test split. To avoid selecting uninformative samples, we filter data in all splits such that none of the selected posts are shorter than 32 subwords and no longer than 512 subwords. We use the <span class="ltx_text ltx_font_typewriter" id="A7.SS2.p1.1.1">RoBERTa-base</span> model tokenizer to count subwords <cite class="ltx_cite ltx_citemacro_citep">(Liu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib28" title="">2020</a>)</cite>.</p>
</div>
<div class="ltx_para" id="A7.SS2.p2">
<p class="ltx_p" id="A7.SS2.p2.1">To generate training episodes, we largely follow the approach proposed by <cite class="ltx_cite ltx_citemacro_citet">Khan et al. (<a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib24" title="">2024</a>)</cite>, using four experts to parameterize an energy function. OPT-1.3B is used to capture fluency <cite class="ltx_cite ltx_citemacro_citep">(Zhang et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib57" title="">2020b</a>)</cite>, hamming distance is used to discourage excessive edits, LUAR is used to measure stylistic similarity <cite class="ltx_cite ltx_citemacro_citep">(Rivera-Soto et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib40" title="">2021</a>)</cite>, and SBERT is used to measure semantic retention <span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>Note the SBERT checkpoint used here is different than the one used in our evaluations.</span></span></span><cite class="ltx_cite ltx_citemacro_citep">(Reimers &amp; Gurevych, <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib39" title="">2019</a>)</cite>. The weights associated with each expert are 10, 1, 1E7, 5E5 respectively. In other words:</p>
</div>
<div class="ltx_para" id="A7.SS2.p3">
<table class="ltx_equation ltx_eqn_table" id="A7.E3">
<tbody><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_eqn_cell ltx_align_center"><math alttext="s(x)=10*s_{\text{fluency}}(x)+1*s_{\text{hamming}}(x)+1\text{E}7*s_{\text{LUAR%
}}(x)+5\text{E}5*s_{\text{SBERT}}(x)" class="ltx_Math" display="block" id="A7.E3.m1.5"><semantics id="A7.E3.m1.5a"><mrow id="A7.E3.m1.5.6" xref="A7.E3.m1.5.6.cmml"><mrow id="A7.E3.m1.5.6.2" xref="A7.E3.m1.5.6.2.cmml"><mi id="A7.E3.m1.5.6.2.2" xref="A7.E3.m1.5.6.2.2.cmml">s</mi><mo id="A7.E3.m1.5.6.2.1" xref="A7.E3.m1.5.6.2.1.cmml">⁢</mo><mrow id="A7.E3.m1.5.6.2.3.2" xref="A7.E3.m1.5.6.2.cmml"><mo id="A7.E3.m1.5.6.2.3.2.1" stretchy="false" xref="A7.E3.m1.5.6.2.cmml">(</mo><mi id="A7.E3.m1.1.1" xref="A7.E3.m1.1.1.cmml">x</mi><mo id="A7.E3.m1.5.6.2.3.2.2" stretchy="false" xref="A7.E3.m1.5.6.2.cmml">)</mo></mrow></mrow><mo id="A7.E3.m1.5.6.1" xref="A7.E3.m1.5.6.1.cmml">=</mo><mrow id="A7.E3.m1.5.6.3" xref="A7.E3.m1.5.6.3.cmml"><mrow id="A7.E3.m1.5.6.3.2" xref="A7.E3.m1.5.6.3.2.cmml"><mrow id="A7.E3.m1.5.6.3.2.2" xref="A7.E3.m1.5.6.3.2.2.cmml"><mn id="A7.E3.m1.5.6.3.2.2.2" xref="A7.E3.m1.5.6.3.2.2.2.cmml">10</mn><mo id="A7.E3.m1.5.6.3.2.2.1" lspace="0.222em" rspace="0.222em" xref="A7.E3.m1.5.6.3.2.2.1.cmml">∗</mo><msub id="A7.E3.m1.5.6.3.2.2.3" xref="A7.E3.m1.5.6.3.2.2.3.cmml"><mi id="A7.E3.m1.5.6.3.2.2.3.2" xref="A7.E3.m1.5.6.3.2.2.3.2.cmml">s</mi><mtext id="A7.E3.m1.5.6.3.2.2.3.3" xref="A7.E3.m1.5.6.3.2.2.3.3a.cmml">fluency</mtext></msub></mrow><mo id="A7.E3.m1.5.6.3.2.1" xref="A7.E3.m1.5.6.3.2.1.cmml">⁢</mo><mrow id="A7.E3.m1.5.6.3.2.3.2" xref="A7.E3.m1.5.6.3.2.cmml"><mo id="A7.E3.m1.5.6.3.2.3.2.1" stretchy="false" xref="A7.E3.m1.5.6.3.2.cmml">(</mo><mi id="A7.E3.m1.2.2" xref="A7.E3.m1.2.2.cmml">x</mi><mo id="A7.E3.m1.5.6.3.2.3.2.2" stretchy="false" xref="A7.E3.m1.5.6.3.2.cmml">)</mo></mrow></mrow><mo id="A7.E3.m1.5.6.3.1" xref="A7.E3.m1.5.6.3.1.cmml">+</mo><mrow id="A7.E3.m1.5.6.3.3" xref="A7.E3.m1.5.6.3.3.cmml"><mrow id="A7.E3.m1.5.6.3.3.2" xref="A7.E3.m1.5.6.3.3.2.cmml"><mn id="A7.E3.m1.5.6.3.3.2.2" xref="A7.E3.m1.5.6.3.3.2.2.cmml">1</mn><mo id="A7.E3.m1.5.6.3.3.2.1" lspace="0.222em" rspace="0.222em" xref="A7.E3.m1.5.6.3.3.2.1.cmml">∗</mo><msub id="A7.E3.m1.5.6.3.3.2.3" xref="A7.E3.m1.5.6.3.3.2.3.cmml"><mi id="A7.E3.m1.5.6.3.3.2.3.2" xref="A7.E3.m1.5.6.3.3.2.3.2.cmml">s</mi><mtext id="A7.E3.m1.5.6.3.3.2.3.3" xref="A7.E3.m1.5.6.3.3.2.3.3a.cmml">hamming</mtext></msub></mrow><mo id="A7.E3.m1.5.6.3.3.1" xref="A7.E3.m1.5.6.3.3.1.cmml">⁢</mo><mrow id="A7.E3.m1.5.6.3.3.3.2" xref="A7.E3.m1.5.6.3.3.cmml"><mo id="A7.E3.m1.5.6.3.3.3.2.1" stretchy="false" xref="A7.E3.m1.5.6.3.3.cmml">(</mo><mi id="A7.E3.m1.3.3" xref="A7.E3.m1.3.3.cmml">x</mi><mo id="A7.E3.m1.5.6.3.3.3.2.2" stretchy="false" xref="A7.E3.m1.5.6.3.3.cmml">)</mo></mrow></mrow><mo id="A7.E3.m1.5.6.3.1a" xref="A7.E3.m1.5.6.3.1.cmml">+</mo><mrow id="A7.E3.m1.5.6.3.4" xref="A7.E3.m1.5.6.3.4.cmml"><mrow id="A7.E3.m1.5.6.3.4.2" xref="A7.E3.m1.5.6.3.4.2.cmml"><mrow id="A7.E3.m1.5.6.3.4.2.2" xref="A7.E3.m1.5.6.3.4.2.2.cmml"><mn id="A7.E3.m1.5.6.3.4.2.2.2" xref="A7.E3.m1.5.6.3.4.2.2.2.cmml">1</mn><mo id="A7.E3.m1.5.6.3.4.2.2.1" xref="A7.E3.m1.5.6.3.4.2.2.1.cmml">⁢</mo><mtext id="A7.E3.m1.5.6.3.4.2.2.3" xref="A7.E3.m1.5.6.3.4.2.2.3a.cmml">E</mtext><mo id="A7.E3.m1.5.6.3.4.2.2.1a" xref="A7.E3.m1.5.6.3.4.2.2.1.cmml">⁢</mo><mn id="A7.E3.m1.5.6.3.4.2.2.4" xref="A7.E3.m1.5.6.3.4.2.2.4.cmml">7</mn></mrow><mo id="A7.E3.m1.5.6.3.4.2.1" lspace="0.222em" rspace="0.222em" xref="A7.E3.m1.5.6.3.4.2.1.cmml">∗</mo><msub id="A7.E3.m1.5.6.3.4.2.3" xref="A7.E3.m1.5.6.3.4.2.3.cmml"><mi id="A7.E3.m1.5.6.3.4.2.3.2" xref="A7.E3.m1.5.6.3.4.2.3.2.cmml">s</mi><mtext id="A7.E3.m1.5.6.3.4.2.3.3" xref="A7.E3.m1.5.6.3.4.2.3.3a.cmml">LUAR</mtext></msub></mrow><mo id="A7.E3.m1.5.6.3.4.1" xref="A7.E3.m1.5.6.3.4.1.cmml">⁢</mo><mrow id="A7.E3.m1.5.6.3.4.3.2" xref="A7.E3.m1.5.6.3.4.cmml"><mo id="A7.E3.m1.5.6.3.4.3.2.1" stretchy="false" xref="A7.E3.m1.5.6.3.4.cmml">(</mo><mi id="A7.E3.m1.4.4" xref="A7.E3.m1.4.4.cmml">x</mi><mo id="A7.E3.m1.5.6.3.4.3.2.2" stretchy="false" xref="A7.E3.m1.5.6.3.4.cmml">)</mo></mrow></mrow><mo id="A7.E3.m1.5.6.3.1b" xref="A7.E3.m1.5.6.3.1.cmml">+</mo><mrow id="A7.E3.m1.5.6.3.5" xref="A7.E3.m1.5.6.3.5.cmml"><mrow id="A7.E3.m1.5.6.3.5.2" xref="A7.E3.m1.5.6.3.5.2.cmml"><mrow id="A7.E3.m1.5.6.3.5.2.2" xref="A7.E3.m1.5.6.3.5.2.2.cmml"><mn id="A7.E3.m1.5.6.3.5.2.2.2" xref="A7.E3.m1.5.6.3.5.2.2.2.cmml">5</mn><mo id="A7.E3.m1.5.6.3.5.2.2.1" xref="A7.E3.m1.5.6.3.5.2.2.1.cmml">⁢</mo><mtext id="A7.E3.m1.5.6.3.5.2.2.3" xref="A7.E3.m1.5.6.3.5.2.2.3a.cmml">E</mtext><mo id="A7.E3.m1.5.6.3.5.2.2.1a" xref="A7.E3.m1.5.6.3.5.2.2.1.cmml">⁢</mo><mn id="A7.E3.m1.5.6.3.5.2.2.4" xref="A7.E3.m1.5.6.3.5.2.2.4.cmml">5</mn></mrow><mo id="A7.E3.m1.5.6.3.5.2.1" lspace="0.222em" rspace="0.222em" xref="A7.E3.m1.5.6.3.5.2.1.cmml">∗</mo><msub id="A7.E3.m1.5.6.3.5.2.3" xref="A7.E3.m1.5.6.3.5.2.3.cmml"><mi id="A7.E3.m1.5.6.3.5.2.3.2" xref="A7.E3.m1.5.6.3.5.2.3.2.cmml">s</mi><mtext id="A7.E3.m1.5.6.3.5.2.3.3" xref="A7.E3.m1.5.6.3.5.2.3.3a.cmml">SBERT</mtext></msub></mrow><mo id="A7.E3.m1.5.6.3.5.1" xref="A7.E3.m1.5.6.3.5.1.cmml">⁢</mo><mrow id="A7.E3.m1.5.6.3.5.3.2" xref="A7.E3.m1.5.6.3.5.cmml"><mo id="A7.E3.m1.5.6.3.5.3.2.1" stretchy="false" xref="A7.E3.m1.5.6.3.5.cmml">(</mo><mi id="A7.E3.m1.5.5" xref="A7.E3.m1.5.5.cmml">x</mi><mo id="A7.E3.m1.5.6.3.5.3.2.2" stretchy="false" xref="A7.E3.m1.5.6.3.5.cmml">)</mo></mrow></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="A7.E3.m1.5b"><apply id="A7.E3.m1.5.6.cmml" xref="A7.E3.m1.5.6"><eq id="A7.E3.m1.5.6.1.cmml" xref="A7.E3.m1.5.6.1"></eq><apply id="A7.E3.m1.5.6.2.cmml" xref="A7.E3.m1.5.6.2"><times id="A7.E3.m1.5.6.2.1.cmml" xref="A7.E3.m1.5.6.2.1"></times><ci id="A7.E3.m1.5.6.2.2.cmml" xref="A7.E3.m1.5.6.2.2">𝑠</ci><ci id="A7.E3.m1.1.1.cmml" xref="A7.E3.m1.1.1">𝑥</ci></apply><apply id="A7.E3.m1.5.6.3.cmml" xref="A7.E3.m1.5.6.3"><plus id="A7.E3.m1.5.6.3.1.cmml" xref="A7.E3.m1.5.6.3.1"></plus><apply id="A7.E3.m1.5.6.3.2.cmml" xref="A7.E3.m1.5.6.3.2"><times id="A7.E3.m1.5.6.3.2.1.cmml" xref="A7.E3.m1.5.6.3.2.1"></times><apply id="A7.E3.m1.5.6.3.2.2.cmml" xref="A7.E3.m1.5.6.3.2.2"><times id="A7.E3.m1.5.6.3.2.2.1.cmml" xref="A7.E3.m1.5.6.3.2.2.1"></times><cn id="A7.E3.m1.5.6.3.2.2.2.cmml" type="integer" xref="A7.E3.m1.5.6.3.2.2.2">10</cn><apply id="A7.E3.m1.5.6.3.2.2.3.cmml" xref="A7.E3.m1.5.6.3.2.2.3"><csymbol cd="ambiguous" id="A7.E3.m1.5.6.3.2.2.3.1.cmml" xref="A7.E3.m1.5.6.3.2.2.3">subscript</csymbol><ci id="A7.E3.m1.5.6.3.2.2.3.2.cmml" xref="A7.E3.m1.5.6.3.2.2.3.2">𝑠</ci><ci id="A7.E3.m1.5.6.3.2.2.3.3a.cmml" xref="A7.E3.m1.5.6.3.2.2.3.3"><mtext id="A7.E3.m1.5.6.3.2.2.3.3.cmml" mathsize="70%" xref="A7.E3.m1.5.6.3.2.2.3.3">fluency</mtext></ci></apply></apply><ci id="A7.E3.m1.2.2.cmml" xref="A7.E3.m1.2.2">𝑥</ci></apply><apply id="A7.E3.m1.5.6.3.3.cmml" xref="A7.E3.m1.5.6.3.3"><times id="A7.E3.m1.5.6.3.3.1.cmml" xref="A7.E3.m1.5.6.3.3.1"></times><apply id="A7.E3.m1.5.6.3.3.2.cmml" xref="A7.E3.m1.5.6.3.3.2"><times id="A7.E3.m1.5.6.3.3.2.1.cmml" xref="A7.E3.m1.5.6.3.3.2.1"></times><cn id="A7.E3.m1.5.6.3.3.2.2.cmml" type="integer" xref="A7.E3.m1.5.6.3.3.2.2">1</cn><apply id="A7.E3.m1.5.6.3.3.2.3.cmml" xref="A7.E3.m1.5.6.3.3.2.3"><csymbol cd="ambiguous" id="A7.E3.m1.5.6.3.3.2.3.1.cmml" xref="A7.E3.m1.5.6.3.3.2.3">subscript</csymbol><ci id="A7.E3.m1.5.6.3.3.2.3.2.cmml" xref="A7.E3.m1.5.6.3.3.2.3.2">𝑠</ci><ci id="A7.E3.m1.5.6.3.3.2.3.3a.cmml" xref="A7.E3.m1.5.6.3.3.2.3.3"><mtext id="A7.E3.m1.5.6.3.3.2.3.3.cmml" mathsize="70%" xref="A7.E3.m1.5.6.3.3.2.3.3">hamming</mtext></ci></apply></apply><ci id="A7.E3.m1.3.3.cmml" xref="A7.E3.m1.3.3">𝑥</ci></apply><apply id="A7.E3.m1.5.6.3.4.cmml" xref="A7.E3.m1.5.6.3.4"><times id="A7.E3.m1.5.6.3.4.1.cmml" xref="A7.E3.m1.5.6.3.4.1"></times><apply id="A7.E3.m1.5.6.3.4.2.cmml" xref="A7.E3.m1.5.6.3.4.2"><times id="A7.E3.m1.5.6.3.4.2.1.cmml" xref="A7.E3.m1.5.6.3.4.2.1"></times><apply id="A7.E3.m1.5.6.3.4.2.2.cmml" xref="A7.E3.m1.5.6.3.4.2.2"><times id="A7.E3.m1.5.6.3.4.2.2.1.cmml" xref="A7.E3.m1.5.6.3.4.2.2.1"></times><cn id="A7.E3.m1.5.6.3.4.2.2.2.cmml" type="integer" xref="A7.E3.m1.5.6.3.4.2.2.2">1</cn><ci id="A7.E3.m1.5.6.3.4.2.2.3a.cmml" xref="A7.E3.m1.5.6.3.4.2.2.3"><mtext id="A7.E3.m1.5.6.3.4.2.2.3.cmml" xref="A7.E3.m1.5.6.3.4.2.2.3">E</mtext></ci><cn id="A7.E3.m1.5.6.3.4.2.2.4.cmml" type="integer" xref="A7.E3.m1.5.6.3.4.2.2.4">7</cn></apply><apply id="A7.E3.m1.5.6.3.4.2.3.cmml" xref="A7.E3.m1.5.6.3.4.2.3"><csymbol cd="ambiguous" id="A7.E3.m1.5.6.3.4.2.3.1.cmml" xref="A7.E3.m1.5.6.3.4.2.3">subscript</csymbol><ci id="A7.E3.m1.5.6.3.4.2.3.2.cmml" xref="A7.E3.m1.5.6.3.4.2.3.2">𝑠</ci><ci id="A7.E3.m1.5.6.3.4.2.3.3a.cmml" xref="A7.E3.m1.5.6.3.4.2.3.3"><mtext id="A7.E3.m1.5.6.3.4.2.3.3.cmml" mathsize="70%" xref="A7.E3.m1.5.6.3.4.2.3.3">LUAR</mtext></ci></apply></apply><ci id="A7.E3.m1.4.4.cmml" xref="A7.E3.m1.4.4">𝑥</ci></apply><apply id="A7.E3.m1.5.6.3.5.cmml" xref="A7.E3.m1.5.6.3.5"><times id="A7.E3.m1.5.6.3.5.1.cmml" xref="A7.E3.m1.5.6.3.5.1"></times><apply id="A7.E3.m1.5.6.3.5.2.cmml" xref="A7.E3.m1.5.6.3.5.2"><times id="A7.E3.m1.5.6.3.5.2.1.cmml" xref="A7.E3.m1.5.6.3.5.2.1"></times><apply id="A7.E3.m1.5.6.3.5.2.2.cmml" xref="A7.E3.m1.5.6.3.5.2.2"><times id="A7.E3.m1.5.6.3.5.2.2.1.cmml" xref="A7.E3.m1.5.6.3.5.2.2.1"></times><cn id="A7.E3.m1.5.6.3.5.2.2.2.cmml" type="integer" xref="A7.E3.m1.5.6.3.5.2.2.2">5</cn><ci id="A7.E3.m1.5.6.3.5.2.2.3a.cmml" xref="A7.E3.m1.5.6.3.5.2.2.3"><mtext id="A7.E3.m1.5.6.3.5.2.2.3.cmml" xref="A7.E3.m1.5.6.3.5.2.2.3">E</mtext></ci><cn id="A7.E3.m1.5.6.3.5.2.2.4.cmml" type="integer" xref="A7.E3.m1.5.6.3.5.2.2.4">5</cn></apply><apply id="A7.E3.m1.5.6.3.5.2.3.cmml" xref="A7.E3.m1.5.6.3.5.2.3"><csymbol cd="ambiguous" id="A7.E3.m1.5.6.3.5.2.3.1.cmml" xref="A7.E3.m1.5.6.3.5.2.3">subscript</csymbol><ci id="A7.E3.m1.5.6.3.5.2.3.2.cmml" xref="A7.E3.m1.5.6.3.5.2.3.2">𝑠</ci><ci id="A7.E3.m1.5.6.3.5.2.3.3a.cmml" xref="A7.E3.m1.5.6.3.5.2.3.3"><mtext id="A7.E3.m1.5.6.3.5.2.3.3.cmml" mathsize="70%" xref="A7.E3.m1.5.6.3.5.2.3.3">SBERT</mtext></ci></apply></apply><ci id="A7.E3.m1.5.5.cmml" xref="A7.E3.m1.5.5">𝑥</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.E3.m1.5c">s(x)=10*s_{\text{fluency}}(x)+1*s_{\text{hamming}}(x)+1\text{E}7*s_{\text{LUAR%
}}(x)+5\text{E}5*s_{\text{SBERT}}(x)</annotation><annotation encoding="application/x-llamapun" id="A7.E3.m1.5d">italic_s ( italic_x ) = 10 ∗ italic_s start_POSTSUBSCRIPT fluency end_POSTSUBSCRIPT ( italic_x ) + 1 ∗ italic_s start_POSTSUBSCRIPT hamming end_POSTSUBSCRIPT ( italic_x ) + 1 E 7 ∗ italic_s start_POSTSUBSCRIPT LUAR end_POSTSUBSCRIPT ( italic_x ) + 5 E 5 ∗ italic_s start_POSTSUBSCRIPT SBERT end_POSTSUBSCRIPT ( italic_x )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
</section>
<section class="ltx_subsection" id="A7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">G.3 </span><math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A7.SS3.1.m1.1"><semantics id="A7.SS3.1.m1.1b"><msub id="A7.SS3.1.m1.1.1" xref="A7.SS3.1.m1.1.1.cmml"><mi id="A7.SS3.1.m1.1.1.2" xref="A7.SS3.1.m1.1.1.2.cmml">q</mi><mi id="A7.SS3.1.m1.1.1.3" xref="A7.SS3.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A7.SS3.1.m1.1c"><apply id="A7.SS3.1.m1.1.1.cmml" xref="A7.SS3.1.m1.1.1"><csymbol cd="ambiguous" id="A7.SS3.1.m1.1.1.1.cmml" xref="A7.SS3.1.m1.1.1">subscript</csymbol><ci id="A7.SS3.1.m1.1.1.2.cmml" xref="A7.SS3.1.m1.1.1.2">𝑞</ci><ci id="A7.SS3.1.m1.1.1.3.cmml" xref="A7.SS3.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.1.m1.1d">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A7.SS3.1.m1.1e">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> and Inference</h3>
<div class="ltx_para" id="A7.SS3.p1">
<p class="ltx_p" id="A7.SS3.p1.1">We learn <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A7.SS3.p1.1.m1.1"><semantics id="A7.SS3.p1.1.m1.1a"><msub id="A7.SS3.p1.1.m1.1.1" xref="A7.SS3.p1.1.m1.1.1.cmml"><mi id="A7.SS3.p1.1.m1.1.1.2" xref="A7.SS3.p1.1.m1.1.1.2.cmml">q</mi><mi id="A7.SS3.p1.1.m1.1.1.3" xref="A7.SS3.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A7.SS3.p1.1.m1.1b"><apply id="A7.SS3.p1.1.m1.1.1.cmml" xref="A7.SS3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A7.SS3.p1.1.m1.1.1.1.cmml" xref="A7.SS3.p1.1.m1.1.1">subscript</csymbol><ci id="A7.SS3.p1.1.m1.1.1.2.cmml" xref="A7.SS3.p1.1.m1.1.1.2">𝑞</ci><ci id="A7.SS3.p1.1.m1.1.1.3.cmml" xref="A7.SS3.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A7.SS3.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A7.SS3.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math> with Llama3.1-8B using supervised finetuning and the extracted training episodes <cite class="ltx_cite ltx_citemacro_citep">(Dubey et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib10" title="">2024</a>)</cite>. We finetune using LoRA <cite class="ltx_cite ltx_citemacro_citep">(Hu et al., <a class="ltx_ref" href="https://arxiv.org/html/2505.20251v1#bib.bib19" title="">2022</a>)</cite>, with a rank of 16 and scaling factor of 32. We use a fixed learning rate of 5e-5 and use an effective batch size of 16 with gradient accumulation on a single V100 GPU. During training, a sequence of states is sampled from a given chain using one of the strategies outlined in <a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#S2.SS3" title="2.3 Creating training episodes ‣ 2 Proposed Method ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">§ 2.3</span></a>. Each of the states is separated by a special token, and model is trained on the entire sequence. An example of a sample is as follows: <span class="ltx_text ltx_font_typewriter" id="A7.SS3.p1.1.1">&lt;bos&gt;[SEQ0] State 1 [SEQ1]...&lt;eos&gt;</span>.
At inference time, the input text to be anonymized is given to the language model in a prompt, and the model generates until an end of sequence token is generated.</p>
</div>
</section>
</section>
<section class="ltx_appendix" id="A8">
<h2 class="ltx_title ltx_title_appendix">
<span class="ltx_tag ltx_tag_appendix">Appendix H </span>Example generations</h2>
<section class="ltx_subsection" id="A8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.1 </span>Sentiment</h3>
<div class="ltx_para" id="A8.SS1.p1">
<p class="ltx_p" id="A8.SS1.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A8.T13" title="Table 13 ‣ H.1 Sentiment ‣ Appendix H Example generations ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 13</span></a> shows 5 randomly selected positive and negative examples from <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A8.SS1.p1.1.m1.1"><semantics id="A8.SS1.p1.1.m1.1a"><msub id="A8.SS1.p1.1.m1.1.1" xref="A8.SS1.p1.1.m1.1.1.cmml"><mi id="A8.SS1.p1.1.m1.1.1.2" xref="A8.SS1.p1.1.m1.1.1.2.cmml">q</mi><mi id="A8.SS1.p1.1.m1.1.1.3" xref="A8.SS1.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A8.SS1.p1.1.m1.1b"><apply id="A8.SS1.p1.1.m1.1.1.cmml" xref="A8.SS1.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A8.SS1.p1.1.m1.1.1.1.cmml" xref="A8.SS1.p1.1.m1.1.1">subscript</csymbol><ci id="A8.SS1.p1.1.m1.1.1.2.cmml" xref="A8.SS1.p1.1.m1.1.1.2">𝑞</ci><ci id="A8.SS1.p1.1.m1.1.1.3.cmml" xref="A8.SS1.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.SS1.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A8.SS1.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="A8.T13">
<table class="ltx_tabular ltx_centering ltx_align_middle" id="A8.T13.1">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A8.T13.1.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r" id="A8.T13.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.1.2.1">
<span class="ltx_p" id="A8.T13.1.1.2.1.1" style="width:195.1pt;"><span class="ltx_text ltx_font_bold" id="A8.T13.1.1.2.1.1.1">Original sentence</span></span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top" id="A8.T13.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.1.1.1">
<span class="ltx_p" id="A8.T13.1.1.1.1.1" style="width:195.1pt;"><math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A8.T13.1.1.1.1.1.m1.1"><semantics id="A8.T13.1.1.1.1.1.m1.1a"><msub id="A8.T13.1.1.1.1.1.m1.1.1" xref="A8.T13.1.1.1.1.1.m1.1.1.cmml"><mi id="A8.T13.1.1.1.1.1.m1.1.1.2" xref="A8.T13.1.1.1.1.1.m1.1.1.2.cmml">q</mi><mi id="A8.T13.1.1.1.1.1.m1.1.1.3" xref="A8.T13.1.1.1.1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A8.T13.1.1.1.1.1.m1.1b"><apply id="A8.T13.1.1.1.1.1.m1.1.1.cmml" xref="A8.T13.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A8.T13.1.1.1.1.1.m1.1.1.1.cmml" xref="A8.T13.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="A8.T13.1.1.1.1.1.m1.1.1.2.cmml" xref="A8.T13.1.1.1.1.1.m1.1.1.2">𝑞</ci><ci id="A8.T13.1.1.1.1.1.m1.1.1.3.cmml" xref="A8.T13.1.1.1.1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.T13.1.1.1.1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A8.T13.1.1.1.1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A8.T13.1.1.1.1.1.1"> modified sentence</span></span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.2.1">
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2" id="A8.T13.1.2.1.1"><span class="ltx_text ltx_font_bold" id="A8.T13.1.2.1.1.1">Positive</span></td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.3.2.1.1">
<span class="ltx_p" id="A8.T13.1.3.2.1.1.1" style="width:195.1pt;">“By far one of the best buffets in las Vegas!”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.3.2.2.1">
<span class="ltx_p" id="A8.T13.1.3.2.2.1.1" style="width:195.1pt;">“By far one of the most amazing food restaurants in Las Vegas!”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.4.3.1.1">
<span class="ltx_p" id="A8.T13.1.4.3.1.1.1" style="width:195.1pt;">“This is a good local bar. The wings were average and they had a good beer special with 3 dollar Coronas on Wednesday.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.4.3.2.1">
<span class="ltx_p" id="A8.T13.1.4.3.2.1.1" style="width:195.1pt;">“This is a really amazing club! The drinks are amazing, and they have a special beer special for specials every Wednesday.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.5.4.1.1">
<span class="ltx_p" id="A8.T13.1.5.4.1.1.1" style="width:195.1pt;">“Great park with nice amenities. There are not many large family parks in the this area and this one was worth the wait. It has everything you would want in a park and more.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.5.4.2.1">
<span class="ltx_p" id="A8.T13.1.5.4.2.1.1" style="width:195.1pt;">“Great park with amazing amenities. There are not very many family parks in the area but this one is the best! It has everything you could want in a park and more.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.6.5.1.1">
<span class="ltx_p" id="A8.T13.1.6.5.1.1.1" style="width:195.1pt;">“Good enough job. Better than Danny’s. They are relentless on the up sales though. I dropped a car off to have it detailed and the guy had the stones to call me 15 minutes later trying to up sale me.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.6.5.2.1">
<span class="ltx_p" id="A8.T13.1.6.5.2.1.1" style="width:195.1pt;">“Good job! This is my favorite shop for car! Very knowledgeable and a lot more efficient at the down sales too!! Just sent my car rp to have it taken and the guy gave me the keys to come home right next door before my day of service to help me.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.7.6">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.7.6.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.7.6.1.1">
<span class="ltx_p" id="A8.T13.1.7.6.1.1.1" style="width:195.1pt;">“Great sushi and service. The 5th star just being held back because of the high prices. Incredible ambiance and easy to make reservations online.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.7.6.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.7.6.2.1">
<span class="ltx_p" id="A8.T13.1.7.6.2.1.1" style="width:195.1pt;">“Great sushi and service! The 5 star just being held back because of the high prices. Incredible ambiance and easy to make reservations online.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.8.7">
<td class="ltx_td ltx_align_center ltx_align_top ltx_border_tt" colspan="2" id="A8.T13.1.8.7.1"><span class="ltx_text ltx_font_bold" id="A8.T13.1.8.7.1.1">Negative</span></td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.9.8">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.9.8.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.9.8.1.1">
<span class="ltx_p" id="A8.T13.1.9.8.1.1.1" style="width:195.1pt;">“By far one of the best buffets in las Vegas!”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.9.8.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.9.8.2.1">
<span class="ltx_p" id="A8.T13.1.9.8.2.1.1" style="width:195.1pt;">“”By the way, this restaurant is absolutely disgusting. Best to get out of Vegas!”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.10.9">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.10.9.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.10.9.1.1">
<span class="ltx_p" id="A8.T13.1.10.9.1.1.1" style="width:195.1pt;">“This is a good local bar. The wings were average and they had a good beer special with 3 dollar Coronas on Wednesday.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.10.9.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.10.9.2.1">
<span class="ltx_p" id="A8.T13.1.10.9.2.1.1" style="width:195.1pt;">“This place is a total disappointment The food was horrible and there were two people in the room that were extremely cold with no water, we left on a Wednesday.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.11.10">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.11.10.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.11.10.1.1">
<span class="ltx_p" id="A8.T13.1.11.10.1.1.1" style="width:195.1pt;">“Great park with nice amenities. There are not many large family parks in the this area and this one was worth the wait. It has everything you would want in a park and more.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.11.10.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.11.10.2.1">
<span class="ltx_p" id="A8.T13.1.11.10.2.1.1" style="width:195.1pt;">“Great location, the food wasn’t good. There are more parks in this area and there was one that wasn’t worth it. This one is a disaster. It’s the worst we have ever seen in an experience a restaurant more.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.12.11">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T13.1.12.11.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.12.11.1.1">
<span class="ltx_p" id="A8.T13.1.12.11.1.1.1" style="width:195.1pt;">“Good enough job. Better than Danny’s. They are relentless on the up sales though. I dropped a car off to have it detailed and the guy had the stones to call me 15 minutes later trying to up sale me.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T13.1.12.11.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.12.11.2.1">
<span class="ltx_p" id="A8.T13.1.12.11.2.1.1" style="width:195.1pt;">“Good enough. They have a better sham than it. They are horrible, ill trained and inexperienced and will NEVER work. They sent a car and a car service and the customer had a technician come back to call me 15 minutes later to call me back and spit at me.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T13.1.13.12">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_r ltx_border_t" id="A8.T13.1.13.12.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.13.12.1.1">
<span class="ltx_p" id="A8.T13.1.13.12.1.1.1" style="width:195.1pt;">“Great sushi and service. The 5th star just being held back because of the high prices. Incredible ambiance and easy to make reservations online.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_b ltx_border_t" id="A8.T13.1.13.12.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T13.1.13.12.2.1">
<span class="ltx_p" id="A8.T13.1.13.12.2.1.1" style="width:195.1pt;">“Great atmosphere, but bad food, a very poor place, the food was poor and the prices were high for a very, mediocre meal with very bad service,. Book ahead online.”</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 13: </span>Randomly selected generated sentences for sentiment task.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="A8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">H.2 </span>Anonymization</h3>
<div class="ltx_para" id="A8.SS2.p1">
<p class="ltx_p" id="A8.SS2.p1.1"><a class="ltx_ref ltx_refmacro_autoref" href="https://arxiv.org/html/2505.20251v1#A8.T14" title="Table 14 ‣ H.2 Anonymization ‣ Appendix H Example generations ‣ Learning Extrapolative Sequence Transformations from Markov Chains"><span class="ltx_text ltx_ref_tag">Table 14</span></a> shows 5 randomly selected examples from <math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A8.SS2.p1.1.m1.1"><semantics id="A8.SS2.p1.1.m1.1a"><msub id="A8.SS2.p1.1.m1.1.1" xref="A8.SS2.p1.1.m1.1.1.cmml"><mi id="A8.SS2.p1.1.m1.1.1.2" xref="A8.SS2.p1.1.m1.1.1.2.cmml">q</mi><mi id="A8.SS2.p1.1.m1.1.1.3" xref="A8.SS2.p1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A8.SS2.p1.1.m1.1b"><apply id="A8.SS2.p1.1.m1.1.1.cmml" xref="A8.SS2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="A8.SS2.p1.1.m1.1.1.1.cmml" xref="A8.SS2.p1.1.m1.1.1">subscript</csymbol><ci id="A8.SS2.p1.1.m1.1.1.2.cmml" xref="A8.SS2.p1.1.m1.1.1.2">𝑞</ci><ci id="A8.SS2.p1.1.m1.1.1.3.cmml" xref="A8.SS2.p1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.SS2.p1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A8.SS2.p1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<figure class="ltx_table" id="A8.T14">
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="A8.T14.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="A8.T14.1.1">
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column ltx_border_r" id="A8.T14.1.1.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.1.2.1">
<span class="ltx_p" id="A8.T14.1.1.2.1.1" style="width:195.1pt;"><span class="ltx_text ltx_font_bold" id="A8.T14.1.1.2.1.1.1">Original sentence</span></span>
</span>
</th>
<th class="ltx_td ltx_align_justify ltx_align_top ltx_th ltx_th_column" id="A8.T14.1.1.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.1.1.1">
<span class="ltx_p" id="A8.T14.1.1.1.1.1" style="width:195.1pt;"><math alttext="q_{\theta}" class="ltx_Math" display="inline" id="A8.T14.1.1.1.1.1.m1.1"><semantics id="A8.T14.1.1.1.1.1.m1.1a"><msub id="A8.T14.1.1.1.1.1.m1.1.1" xref="A8.T14.1.1.1.1.1.m1.1.1.cmml"><mi id="A8.T14.1.1.1.1.1.m1.1.1.2" xref="A8.T14.1.1.1.1.1.m1.1.1.2.cmml">q</mi><mi id="A8.T14.1.1.1.1.1.m1.1.1.3" xref="A8.T14.1.1.1.1.1.m1.1.1.3.cmml">θ</mi></msub><annotation-xml encoding="MathML-Content" id="A8.T14.1.1.1.1.1.m1.1b"><apply id="A8.T14.1.1.1.1.1.m1.1.1.cmml" xref="A8.T14.1.1.1.1.1.m1.1.1"><csymbol cd="ambiguous" id="A8.T14.1.1.1.1.1.m1.1.1.1.cmml" xref="A8.T14.1.1.1.1.1.m1.1.1">subscript</csymbol><ci id="A8.T14.1.1.1.1.1.m1.1.1.2.cmml" xref="A8.T14.1.1.1.1.1.m1.1.1.2">𝑞</ci><ci id="A8.T14.1.1.1.1.1.m1.1.1.3.cmml" xref="A8.T14.1.1.1.1.1.m1.1.1.3">𝜃</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="A8.T14.1.1.1.1.1.m1.1c">q_{\theta}</annotation><annotation encoding="application/x-llamapun" id="A8.T14.1.1.1.1.1.m1.1d">italic_q start_POSTSUBSCRIPT italic_θ end_POSTSUBSCRIPT</annotation></semantics></math><span class="ltx_text ltx_font_bold" id="A8.T14.1.1.1.1.1.1"> modified sentence</span></span>
</span>
</th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="A8.T14.1.2.1">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_tt" id="A8.T14.1.2.1.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.2.1.1.1">
<span class="ltx_p" id="A8.T14.1.2.1.1.1.1" style="width:195.1pt;">“I had it reset as well once or twice. Initially i thought i read it wrong but i guess it was the bug. I hope Trion finds some way not to ban accidedntal events.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_tt" id="A8.T14.1.2.1.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.2.1.2.1">
<span class="ltx_p" id="A8.T14.1.2.1.2.1.1" style="width:195.1pt;">“had it happen to me just once, and maybe two or so times as well. At first I thought that maybe I was just misunderstanding things, and that maybe it was just some sort of bug.. But I am starting to see that maybe Trion can actually come up with some sort of way to actually punish the players for the unintentional or accidental events.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T14.1.3.2">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T14.1.3.2.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.3.2.1.1">
<span class="ltx_p" id="A8.T14.1.3.2.1.1.1" style="width:195.1pt;">“This is the only known species of spider that will release insects from its web if they are not properly accessorized. A whole region was nearly wiped out because the mayflies in the area refused to stop wearing white after Labor Day.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T14.1.3.2.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.3.2.2.1">
<span class="ltx_p" id="A8.T14.1.3.2.2.1.1" style="width:195.1pt;">“This is the one species of spider, that release insects into its web, when they’re not correctly accessorised. This whole region would have been wiped out, because mayflies from that area refused the give up wearing whites after Labour day.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T14.1.4.3">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T14.1.4.3.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.4.3.1.1">
<span class="ltx_p" id="A8.T14.1.4.3.1.1.1" style="width:195.1pt;">“That’s not a euphemism. He’s really got ‘North American Morals’ tattooed along the side. But when he’s not rock-hard with freedom, it just says ‘NorM”’</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T14.1.4.3.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.4.3.2.1">
<span class="ltx_p" id="A8.T14.1.4.3.2.1.1" style="width:195.1pt;">“That is more than a tattoo of word; it a euphemized word. He has a tattoo word, North Americas Freedoms, at his side. When he is hard or full of freedoms it reads North M”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T14.1.5.4">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T14.1.5.4.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.5.4.1.1">
<span class="ltx_p" id="A8.T14.1.5.4.1.1.1" style="width:195.1pt;">“Well said. Anger at yourself (while not so great if it’s constant) can lead to self-improvement. It can be the extra kick that you need to stay motivated.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T14.1.5.4.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.5.4.2.1">
<span class="ltx_p" id="A8.T14.1.5.4.2.1.1" style="width:195.1pt;">“Well said! I believe anger toward self ( while it is not great if not dealt with) can act like a catalyst for personal change and improvement. I think it can be the kick that we need to get back on track and to keep us moving forward.”</span>
</span>
</td>
</tr>
<tr class="ltx_tr" id="A8.T14.1.6.5">
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_r ltx_border_t" id="A8.T14.1.6.5.1">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.6.5.1.1">
<span class="ltx_p" id="A8.T14.1.6.5.1.1.1" style="width:195.1pt;">“I totally agree with you, but I don’t think it will change. Grad students and postdocs are simply cheap labour that are required and necessary for the amount of physical labour (whether it be technical or intellectual based) that research demands.”</span>
</span>
</td>
<td class="ltx_td ltx_align_justify ltx_align_top ltx_border_t" id="A8.T14.1.6.5.2">
<span class="ltx_inline-block ltx_align_top" id="A8.T14.1.6.5.2.1">
<span class="ltx_p" id="A8.T14.1.6.5.2.1.1" style="width:195.1pt;">“totally agree. I don’t know if it will. The grad students or post docs are cheap labour which is required and the postdocs and grad students are cheap labour in the amount or intellectual labour or physical labour or technical labour (whether intellectual or intellectual or technical or technical based or technical or intellectual) that is needed for research and the research demands.”</span>
</span>
</td>
</tr>
</tbody>
</table>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">Table 14: </span>Randomly selected generated sentences for anonymization task.</figcaption>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 26 17:24:12 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>

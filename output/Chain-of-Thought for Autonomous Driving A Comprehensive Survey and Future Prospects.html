<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects</title>
<!--Generated on Mon May 26 17:02:56 2025 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Chain-of-Thought,  Autonomous Driving,  Large Language Models,  Reasoning,  Self-evolution,  Reflection
" lang="en" name="keywords"/>
<base href="/html/2505.20223v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S1" title="In Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">I </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S2" title="In Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">II </span><span class="ltx_text ltx_font_smallcaps">Foundations of Autonomous Driving CoT</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S2.SS1" title="In II Foundations of Autonomous Driving CoT ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-A</span> </span><span class="ltx_text ltx_font_italic">Development of Autonomous Driving</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S2.SS2" title="In II Foundations of Autonomous Driving CoT ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-B</span> </span><span class="ltx_text ltx_font_italic">LLMs for Autonomous Driving</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S2.SS3" title="In II Foundations of Autonomous Driving CoT ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">II-C</span> </span><span class="ltx_text ltx_font_italic">Theoretical Foundations of CoT</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3" title="In Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">III </span><span class="ltx_text ltx_font_smallcaps">CoT Methods for Autonomous Driving</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS1" title="In III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-A</span> </span><span class="ltx_text ltx_font_italic">Pipeline Paradigms</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS1.SSS0.Px1" title="In III-A Pipeline Paradigms ‣ III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title">Modular Driving CoT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS1.SSS0.Px2" title="In III-A Pipeline Paradigms ‣ III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title">Logical Driving CoT</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS1.SSS0.Px3" title="In III-A Pipeline Paradigms ‣ III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title">Reflective Driving CoT</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS2" title="In III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">III-B</span> </span><span class="ltx_text ltx_font_italic">Task-Oriented Domains</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS2.SSS0.Px1" title="In III-B Task-Oriented Domains ‣ III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title">Perception and Understanding</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS2.SSS0.Px2" title="In III-B Task-Oriented Domains ‣ III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title">Prediction and Planning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS2.SSS0.Px3" title="In III-B Task-Oriented Domains ‣ III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title">Decision-Making and Control</span></a></li>
<li class="ltx_tocentry ltx_tocentry_paragraph"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S3.SS2.SSS0.Px4" title="In III-B Task-Oriented Domains ‣ III CoT Methods for Autonomous Driving ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title">End-to-End Autonomous Driving</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S4" title="In Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">IV </span><span class="ltx_text ltx_font_smallcaps">Datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S4.SS1" title="In IV Datasets ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-A</span> </span><span class="ltx_text ltx_font_italic">Cognition-augmented autonomous driving datasets</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S4.SS2" title="In IV Datasets ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">IV-B</span> </span><span class="ltx_text ltx_font_italic">Evaluation Metrics</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S5" title="In Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">V </span><span class="ltx_text ltx_font_smallcaps">Discussion</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S6" title="In Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VI </span><span class="ltx_text ltx_font_smallcaps">Challenges and Future Directions</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S6.SS1" title="In VI Challenges and Future Directions ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-A</span> </span><span class="ltx_text ltx_font_italic">Challenges for CoT in autonomous driving</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S6.SS2" title="In VI Challenges and Future Directions ‣ Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref"><span class="ltx_text">VI-B</span> </span><span class="ltx_text ltx_font_italic">Future Directions</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#S7" title="In Chain-of-Thought for Autonomous Driving: A Comprehensive Survey and Future Prospects"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">VII </span><span class="ltx_text ltx_font_smallcaps">Conclusion</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">Chain-of-Thought for Autonomous Driving:
<br class="ltx_break"/>A Comprehensive Survey and Future Prospects</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Yixin Cui, Haotian Lin, Shuo Yang, Yixiao Wang
<br class="ltx_break"/>Yanjun Huang, and Hong Chen
</span><span class="ltx_author_notes">Manuscript received 20 January 2025.
This work was supported by the National Key Research and Development Program of China under Grant 2022YFB2502900 and the National Natural Science Foundation of China, Joint Fund for Innovative Enterprise Development (U23B2061). (Corresponding author: Yanjun Huang.)Yixin Cui, Shuo Yang and Yixiao Wang are with the School of Automotive Studies, Tongji University, Shanghai 201804, China (e-mail: 2411448@tongji.edu.cn; 2111550@tongji.edu.cn; 2353147@tongji.edu.cn).Haotian Lin is with the School of Physics Science and Engineering, Tongji University, Shanghai 200092, China (e-mail: 2250120@tongji.edu.cn).Yanjun Huang is with the School of Automotive Studies, Tongji University, Shanghai 201804, China, and also with the Frontiers Science Center for Intelligent Autonomous Systems, Shanghai 200120, China (e-mail: yanjun_huang@tongji.edu.cn).Hong Chen is with the College of Electronics and Information Engineering and the Clean Energy Automotive Engineering Center, Tongji University, Shanghai 201804, China (e-mail: chenhong2019@tongji.edu.cn).</span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id1.id1">The rapid evolution of large language models in natural language processing has substantially elevated their semantic understanding and logical reasoning capabilities. Such proficiencies have been leveraged in autonomous driving systems, contributing to significant improvements in system performance. Models such as OpenAI o1 and DeepSeek-R1, leverage Chain-of-Thought (CoT) reasoning, an advanced cognitive method that simulates human thinking processes, demonstrating remarkable reasoning capabilities in complex tasks. By structuring complex driving scenarios within a systematic reasoning framework, this approach has emerged as a prominent research focus in autonomous driving, substantially improving the system’s ability to handle challenging cases. This paper investigates how CoT methods improve the reasoning abilities of autonomous driving models. Based on a comprehensive literature review, we present a systematic analysis of the motivations, methodologies, challenges, and future research directions of CoT in autonomous driving. Furthermore, we propose the insight of combining CoT with self-learning to facilitate self-evolution in driving systems. To ensure the relevance and timeliness of this study, we have compiled a dynamic repository of literature and open-source projects, diligently updated to incorporate forefront developments. The repository is publicly available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/cuiyx1720/Awesome-CoT4AD" title="">https://github.com/cuiyx1720/Awesome-CoT4AD</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Chain-of-Thought, Autonomous Driving, Large Language Models, Reasoning, Self-evolution, Reflection

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">I </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">The advent of large language models (LLMs) marks a significant breakthrough in the field of artificial intelligence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib1" title="">1</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib2" title="">2</a>]</cite>. Through pre-training on massive datasets, LLMs demonstrate exceptional capabilities in semantic comprehension, inductive reasoning, and knowledge generalization <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib3" title="">3</a>]</cite>. Models like OpenAI’s ChatGPT and Meta’s Llama have overcome the limitations of traditional language models in terms of generation quality, logical coherence, and domain adaptability, thanks to their innovative architectures and optimized training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib4" title="">4</a>]</cite>. These advancements have not only driven transformative changes in natural language processing but also served as a cornerstone for the development of artificial general intelligence (AGI), heralding vast prospects for the application of intelligent technologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib5" title="">5</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">With the development of electronic systems and artificial intelligence, autonomous driving has emerged as a pivotal component of intelligent transportation systems, exhibiting substantial progression <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib6" title="">6</a>]</cite>. LLMs, with their powerful language comprehension capabilities, provide innovative pathways to address the key challenges confronted by autonomous driving systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib7" title="">7</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib8" title="">8</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib9" title="">9</a>]</cite>. Particularly in dynamic and complex traffic scenarios, LLMs demonstrate significant improvements in tackling limitations such as insufficient environmental understanding and poor algorithmic interpretability in existing systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib10" title="">10</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib11" title="">11</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">While LLMs excel in real-time responsiveness, their limitations in deep reasoning constrain performance on complex tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib12" title="">12</a>]</cite>. Chain-of-Thought (CoT) addresses this by simulating human cognition through sequential reasoning steps, significantly enhancing model capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib13" title="">13</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib14" title="">14</a>]</cite>. Widely recognized in both academic and industrial circles, recent reasoning large models leveraging CoT, such as OpenAI o1 and DeepSeek-R1, have demonstrated expert-level performance in mathematics and programming, substantially outperforming other LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib15" title="">15</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib16" title="">16</a>]</cite>. Researchers are increasingly exploring CoT applications in autonomous driving, embodied AI, medicine, and finance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib17" title="">17</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib18" title="">18</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib19" title="">19</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib20" title="">20</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="872" id="S1.F1.g1" src="x1.png" width="885"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 1: </span>It presents an overview of Chain-of-Thought for autonomous driving, outlining the motivations, framework, methodological paradigms, key challenges, and supporting datasets involved in this emerging research direction.</figcaption>
</figure>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">In the realm of autonomous driving, Chain-of-Thought technology, owing to its high alignment with human driving cognition, holds great potential to drive innovative breakthroughs in autonomous systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib18" title="">18</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib21" title="">21</a>]</cite> . By simulating human drivers’ decision-making processes through structured reasoning mechanisms, this technology significantly enhances a system’s capability to handle complex scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib22" title="">22</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib23" title="">23</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib24" title="">24</a>]</cite>. At its core, CoT decomposes driving tasks into interpretable, multi-step reasoning processes, enabling the system to make rational decisions through logical reasoning, just like an experienced human driver <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">Although some review studies have explored the applications of LLMs, vision-language models (VLMs), and multimodal large models (MLMs) in autonomous driving, with some research touching upon chain-of-thought techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib9" title="">9</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib26" title="">26</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib27" title="">27</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib28" title="">28</a>]</cite>. To the best of our knowledge, none have focused specifically on how CoT technology advances autonomous driving. Therefore, there is a notable absence of a comprehensive review that consolidates and analyzes the application of CoT methods in autonomous driving. Most current reviews on CoT primarily analyze its underlying technical principles within large models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib29" title="">29</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib30" title="">30</a>]</cite>. In contrast, this work provides a structured synthesis of the cutting-edge advancements of CoT technology in the field of autonomous driving. Beyond methodical categorization and dataset analysis, it critically delves into existing technical bottlenecks and proposes forward-looking research directions. Through this systematic perspective, the aim is to provide readers with pioneering insights into how CoT is transforming autonomous driving.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">The main contributions of this work can be summarized as follows:</p>
<ol class="ltx_enumerate" id="S1.I1">
<li class="ltx_item" id="S1.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">1.</span>
<div class="ltx_para" id="S1.I1.i1.p1">
<p class="ltx_p" id="S1.I1.i1.p1.1">We present a comprehensive review of chain-of-thought reasoning in autonomous driving and categorize existing research along two dimensions.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">2.</span>
<div class="ltx_para" id="S1.I1.i2.p1">
<p class="ltx_p" id="S1.I1.i2.p1.1">We conduct an exhaustive evaluation of existing autonomous driving reasoning datasets used for CoT, providing a comparative analysis of their characteristics and applicability.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">3.</span>
<div class="ltx_para" id="S1.I1.i3.p1">
<p class="ltx_p" id="S1.I1.i3.p1.1">We identify and analyze promising applications of CoT methods in autonomous driving, while proposing concrete directions for future research.</p>
</div>
</li>
<li class="ltx_item" id="S1.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">4.</span>
<div class="ltx_para" id="S1.I1.i4.p1">
<p class="ltx_p" id="S1.I1.i4.p1.1">We provide an in-depth discussion of the core advantages, critical challenges, and research gaps in this field.</p>
</div>
</li>
</ol>
</div>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">The structure of this paper is as follows: Section II presents the background and foundational concepts of the study; Section III categorizes the Chain-of-Thought methods applied to autonomous driving; Section IV introduces the relevant datasets for driving-related CoT; Section V discusses the application methods and current trends; and Section VI summarizes the future challenges and directions for the application of driving CoT.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">II </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Foundations of Autonomous Driving CoT</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section delves into the relevant background, exploring the foundational concepts in the following areas: autonomous driving (II-A), the application of large language models in autonomous driving (II-B), and Chain-of-Thought (II-C).</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS1.5.1.1">II-A</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS1.6.2">Development of Autonomous Driving</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The evolution of autonomous driving is primarily manifested in the continuous innovation of system architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib31" title="">31</a>]</cite>. In the early stages of development, modular autonomous driving predominated, decomposing critical modules such as perception, prediction, decision-making, planning, and control into discrete modules that are connected sequentially <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib32" title="">32</a>]</cite>. This architecture demonstrated considerable advantages during development and debugging. However, inherent limitations exist in terms of the inter-module information transfer accuracy and collaborative optimization efficiency. With the significant progress in deep learning, end-to-end methods gradually became the focal point of research <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib11" title="">11</a>]</cite>. The fundamental principle of this paradigm is to directly extract features from raw sensor data and generate vehicle control actions or trajectories based on these features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib33" title="">33</a>]</cite>. It not only substantially simplifies the complexity of the system architecture but also improves overall performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib34" title="">34</a>]</cite>. Despite its promise, the end-to-end paradigm still faces many challenges, such as limited interpretability and difficulties in handling long-tail scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib35" title="">35</a>]</cite>. Addressing these issues constitutes a fundamental requirement for the next generation of autonomous driving.</p>
</div>
<div class="ltx_para" id="S2.SS1.p2">
<p class="ltx_p" id="S2.SS1.p2.1">From the perspective of technological paradigm evolution, autonomous driving has advanced through three distinct stages: rule-driven, data-driven, and knowledge-driven <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib36" title="">36</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS1.p3">
<p class="ltx_p" id="S2.SS1.p3.1">In the rule-driven paradigm, which relies on manually created logical rules to operate <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib37" title="">37</a>]</cite>. While it offers strong interpretability, it demonstrates significant limitations when handling complex and dynamic traffic scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib38" title="">38</a>]</cite>. Subsequently, data-driven methods used large-scale driving datasets and deep neural networks to simulate human driving behavior, adopting end-to-end large models through imitation learning to improve system adaptability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib39" title="">39</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib40" title="">40</a>]</cite>. However, this paradigm introduces challenges such as data dependency, poor generalization, and reduced interpretability.</p>
</div>
<div class="ltx_para" id="S2.SS1.p4">
<p class="ltx_p" id="S2.SS1.p4.1">In recent years, knowledge-driven autonomous driving has gradually become a promising direction, effectively combining the advantages of rule-driven and data-driven approaches and incorporating insights into human cognitive processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib41" title="">41</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib42" title="">42</a>]</cite>. Knowledge, in this context, represents a highly abstract and systematic representation of human understanding regarding driving scenarios and decision-making, encapsulating distilled experience and reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib36" title="">36</a>]</cite>. The knowledge-driven paradigm first constructs an abstract “knowledge representation space” to encode high-level driving concepts, causal relationships, and traffic rules <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib43" title="">43</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib44" title="">44</a>]</cite>. This paradigm integrates cutting-edge technologies such as LLMs, world models, and self-learning methods <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib41" title="">41</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib45" title="">45</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib46" title="">46</a>]</cite>. By combining these elements, it equips autonomous systems with advanced reasoning and continual learning capabilities. Specific cognitive techniques, including chain of thought, reflection mechanisms, and curiosity exploration, play a key role in driving this knowledge-enhanced autonomy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib47" title="">47</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib48" title="">48</a>]</cite>. The knowledge-driven paradigm establishes a new theoretical foundation and methodological framework for addressing issues like corner cases, while paving the way for the development of higher-level autonomous systems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib49" title="">49</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S2.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="255" id="S2.F2.g1" src="x2.png" width="886"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 2: </span>It compares different variants of thought reasoning paradigms, from basic input-output to advanced forms like chain, tree, and graph of thought, highlighting the evolution of thought processes for complex problem-solving.</figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS2.5.1.1">II-B</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS2.6.2">LLMs for Autonomous Driving</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">Transformer-based LLMs have ushered in a revolutionary era in artificial intelligence <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib1" title="">1</a>]</cite>. From early models like GPT-3 to current models such as GPT-4o, LLaMA 4, Deepseek-v3, and Gemini 2.5, LLMs exhibit unparalleled prowess in language understanding, knowledge acquisition, and logical reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib4" title="">4</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib50" title="">50</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib51" title="">51</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib52" title="">52</a>]</cite>. These advancements have profoundly impacted natural language processing (NLP) and computer vision (CV), and have opened up new transformative opportunities for autonomous driving technologies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib20" title="">20</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib53" title="">53</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">The initial exploratory research on applying LLMs to autonomous driving primarily focused on converting sensor data and traffic rules into linguistic representations, leveraging LLMs for decision generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib22" title="">22</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib41" title="">41</a>]</cite>. It showcased LLMs’ potential for static scene understanding and basic decision-making. However, when confronted with complex dynamic environments, the models still face challenges in generalization capability and real-time performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib27" title="">27</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS2.p3">
<p class="ltx_p" id="S2.SS2.p3.1">The emergence of multimodal large language models (MLLMs), particularly vision language models (VLMs) and vision language action models (VLAs), has enabled deep fusion of heterogeneous sensor data including cameras, LiDAR, and HD maps <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib54" title="">54</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib55" title="">55</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib56" title="">56</a>]</cite>. This breakthrough significantly enhances the system’s capability for comprehensive, multi-dimensional environmental perception and integrated reasoning. Concurrently, as MLLMs’ integration with autonomous driving systems deepens, the end-to-end paradigm further improves autonomous driving systems’ overall performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib57" title="">57</a>]</cite>. The next generation of reasoning LLMs will exhibit significantly improved intelligence, adaptability, and generalization performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib58" title="">58</a>]</cite>.</p>
</div>
</section>
<section class="ltx_subsection" id="S2.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S2.SS3.5.1.1">II-C</span> </span><span class="ltx_text ltx_font_italic" id="S2.SS3.6.2">Theoretical Foundations of CoT</span>
</h3>
<div class="ltx_para" id="S2.SS3.p1">
<p class="ltx_p" id="S2.SS3.p1.1">LLMs have demonstrated remarkable emergent capabilities in contextual learning and complex reasoning tasks. Among these, the Chain-of-Thought reasoning technique has garnered significant attention due to its human-like cognitive characteristics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib13" title="">13</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib59" title="">59</a>]</cite>. It enables large models to solve novel problems through step-by-step reasoning processes, demonstrating capabilities that approximate human cognitive patterns <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib44" title="">44</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p2">
<p class="ltx_p" id="S2.SS3.p2.1">From a methodological perspective, Chain-of-Thought represents a structured multi-step reasoning paradigm that differs fundamentally from traditional intuitive Input-Output response mechanisms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib60" title="">60</a>]</cite>. It marks a cognitive advancement in AI interaction, shifting from mere result output to comprehension of the process. By establishing sequential reasoning processes, CoT allows models to perform multi-step derivations when addressing complex problems, ultimately generating well-justified explanations or decisions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib61" title="">61</a>]</cite>. One distinctive feature of this approach is its recursive nature, where each reasoning step depends on the outcome of the preceding step, thereby forming a coherent logical chain.</p>
</div>
<div class="ltx_para" id="S2.SS3.p3">
<p class="ltx_p" id="S2.SS3.p3.1">While a standardized formulation of CoT reasoning has yet to be established, this study adopts a formalism similar to state transition to characterize the reasoning process. The recursive decomposition of each reasoning step in the thought chain is represented as the process of thought transition. Taking the most classic chain of thought as an example, the reasoning process is represented as a chain of thought transitions, formally described as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S7.EGx1">
<tbody id="S2.E1"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle C:=(P\xrightarrow{T_{1}}S_{1})\odot(S_{1}\xrightarrow{T_{2}}S_{2%
})\odot\cdots\odot(S_{n-1}\xrightarrow{T_{n}}R)" class="ltx_Math" display="inline" id="S2.E1.m1.3"><semantics id="S2.E1.m1.3a"><mrow id="S2.E1.m1.3.3" xref="S2.E1.m1.3.3.cmml"><mi id="S2.E1.m1.3.3.5" xref="S2.E1.m1.3.3.5.cmml">C</mi><mo id="S2.E1.m1.3.3.4" lspace="0.278em" rspace="0.278em" xref="S2.E1.m1.3.3.4.cmml">:=</mo><mrow id="S2.E1.m1.3.3.3" xref="S2.E1.m1.3.3.3.cmml"><mrow id="S2.E1.m1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S2.E1.m1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.2.cmml">P</mi><mover accent="true" id="S2.E1.m1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.cmml"><mo id="S2.E1.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.1.2.cmml">→</mo><msub id="S2.E1.m1.1.1.1.1.1.1.1.1" xref="S2.E1.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.1.1.2" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml">T</mi><mn id="S2.E1.m1.1.1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml">1</mn></msub></mover><msub id="S2.E1.m1.1.1.1.1.1.1.3" xref="S2.E1.m1.1.1.1.1.1.1.3.cmml"><mi id="S2.E1.m1.1.1.1.1.1.1.3.2" xref="S2.E1.m1.1.1.1.1.1.1.3.2.cmml">S</mi><mn id="S2.E1.m1.1.1.1.1.1.1.3.3" xref="S2.E1.m1.1.1.1.1.1.1.3.3.cmml">1</mn></msub></mrow><mo id="S2.E1.m1.1.1.1.1.1.3" rspace="0.055em" stretchy="false" xref="S2.E1.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S2.E1.m1.3.3.3.4" rspace="0.222em" xref="S2.E1.m1.3.3.3.4.cmml">⊙</mo><mrow id="S2.E1.m1.2.2.2.2.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><mo id="S2.E1.m1.2.2.2.2.1.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S2.E1.m1.2.2.2.2.1.1" xref="S2.E1.m1.2.2.2.2.1.1.cmml"><msub id="S2.E1.m1.2.2.2.2.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.2.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.2.2" xref="S2.E1.m1.2.2.2.2.1.1.2.2.cmml">S</mi><mn id="S2.E1.m1.2.2.2.2.1.1.2.3" xref="S2.E1.m1.2.2.2.2.1.1.2.3.cmml">1</mn></msub><mover accent="true" id="S2.E1.m1.2.2.2.2.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.cmml"><mo id="S2.E1.m1.2.2.2.2.1.1.1.2" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.1.2.cmml">→</mo><msub id="S2.E1.m1.2.2.2.2.1.1.1.1" xref="S2.E1.m1.2.2.2.2.1.1.1.1.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.1.1.2" xref="S2.E1.m1.2.2.2.2.1.1.1.1.2.cmml">T</mi><mn id="S2.E1.m1.2.2.2.2.1.1.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.1.1.3.cmml">2</mn></msub></mover><msub id="S2.E1.m1.2.2.2.2.1.1.3" xref="S2.E1.m1.2.2.2.2.1.1.3.cmml"><mi id="S2.E1.m1.2.2.2.2.1.1.3.2" xref="S2.E1.m1.2.2.2.2.1.1.3.2.cmml">S</mi><mn id="S2.E1.m1.2.2.2.2.1.1.3.3" xref="S2.E1.m1.2.2.2.2.1.1.3.3.cmml">2</mn></msub></mrow><mo id="S2.E1.m1.2.2.2.2.1.3" rspace="0.055em" stretchy="false" xref="S2.E1.m1.2.2.2.2.1.1.cmml">)</mo></mrow><mo id="S2.E1.m1.3.3.3.4a" rspace="0.222em" xref="S2.E1.m1.3.3.3.4.cmml">⊙</mo><mi id="S2.E1.m1.3.3.3.5" mathvariant="normal" xref="S2.E1.m1.3.3.3.5.cmml">⋯</mi><mo id="S2.E1.m1.3.3.3.4b" lspace="0.222em" rspace="0.222em" xref="S2.E1.m1.3.3.3.4.cmml">⊙</mo><mrow id="S2.E1.m1.3.3.3.3.1" xref="S2.E1.m1.3.3.3.3.1.1.cmml"><mo id="S2.E1.m1.3.3.3.3.1.2" stretchy="false" xref="S2.E1.m1.3.3.3.3.1.1.cmml">(</mo><mrow id="S2.E1.m1.3.3.3.3.1.1" xref="S2.E1.m1.3.3.3.3.1.1.cmml"><msub id="S2.E1.m1.3.3.3.3.1.1.2" xref="S2.E1.m1.3.3.3.3.1.1.2.cmml"><mi id="S2.E1.m1.3.3.3.3.1.1.2.2" xref="S2.E1.m1.3.3.3.3.1.1.2.2.cmml">S</mi><mrow id="S2.E1.m1.3.3.3.3.1.1.2.3" xref="S2.E1.m1.3.3.3.3.1.1.2.3.cmml"><mi id="S2.E1.m1.3.3.3.3.1.1.2.3.2" xref="S2.E1.m1.3.3.3.3.1.1.2.3.2.cmml">n</mi><mo id="S2.E1.m1.3.3.3.3.1.1.2.3.1" xref="S2.E1.m1.3.3.3.3.1.1.2.3.1.cmml">−</mo><mn id="S2.E1.m1.3.3.3.3.1.1.2.3.3" xref="S2.E1.m1.3.3.3.3.1.1.2.3.3.cmml">1</mn></mrow></msub><mover accent="true" id="S2.E1.m1.3.3.3.3.1.1.1" xref="S2.E1.m1.3.3.3.3.1.1.1.cmml"><mo id="S2.E1.m1.3.3.3.3.1.1.1.2" stretchy="false" xref="S2.E1.m1.3.3.3.3.1.1.1.2.cmml">→</mo><msub id="S2.E1.m1.3.3.3.3.1.1.1.1" xref="S2.E1.m1.3.3.3.3.1.1.1.1.cmml"><mi id="S2.E1.m1.3.3.3.3.1.1.1.1.2" xref="S2.E1.m1.3.3.3.3.1.1.1.1.2.cmml">T</mi><mi id="S2.E1.m1.3.3.3.3.1.1.1.1.3" xref="S2.E1.m1.3.3.3.3.1.1.1.1.3.cmml">n</mi></msub></mover><mi id="S2.E1.m1.3.3.3.3.1.1.3" xref="S2.E1.m1.3.3.3.3.1.1.3.cmml">R</mi></mrow><mo id="S2.E1.m1.3.3.3.3.1.3" stretchy="false" xref="S2.E1.m1.3.3.3.3.1.1.cmml">)</mo></mrow></mrow></mrow><annotation-xml encoding="MathML-Content" id="S2.E1.m1.3b"><apply id="S2.E1.m1.3.3.cmml" xref="S2.E1.m1.3.3"><csymbol cd="latexml" id="S2.E1.m1.3.3.4.cmml" xref="S2.E1.m1.3.3.4">assign</csymbol><ci id="S2.E1.m1.3.3.5.cmml" xref="S2.E1.m1.3.3.5">𝐶</ci><apply id="S2.E1.m1.3.3.3.cmml" xref="S2.E1.m1.3.3.3"><csymbol cd="latexml" id="S2.E1.m1.3.3.3.4.cmml" xref="S2.E1.m1.3.3.3.4">direct-product</csymbol><apply id="S2.E1.m1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1"><apply id="S2.E1.m1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1"><apply id="S2.E1.m1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.1.2">𝑇</ci><cn id="S2.E1.m1.1.1.1.1.1.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.1.1.1.1.3">1</cn></apply><ci id="S2.E1.m1.1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.1.2">→</ci></apply><ci id="S2.E1.m1.1.1.1.1.1.1.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.2">𝑃</ci><apply id="S2.E1.m1.1.1.1.1.1.1.3.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.1.1.1.1.1.1.3.1.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S2.E1.m1.1.1.1.1.1.1.3.2.cmml" xref="S2.E1.m1.1.1.1.1.1.1.3.2">𝑆</ci><cn id="S2.E1.m1.1.1.1.1.1.1.3.3.cmml" type="integer" xref="S2.E1.m1.1.1.1.1.1.1.3.3">1</cn></apply></apply><apply id="S2.E1.m1.2.2.2.2.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1"><apply id="S2.E1.m1.2.2.2.2.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1"><apply id="S2.E1.m1.2.2.2.2.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.1.1.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.1.2">𝑇</ci><cn id="S2.E1.m1.2.2.2.2.1.1.1.1.3.cmml" type="integer" xref="S2.E1.m1.2.2.2.2.1.1.1.1.3">2</cn></apply><ci id="S2.E1.m1.2.2.2.2.1.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.1.2">→</ci></apply><apply id="S2.E1.m1.2.2.2.2.1.1.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.2.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.2.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.2.2">𝑆</ci><cn id="S2.E1.m1.2.2.2.2.1.1.2.3.cmml" type="integer" xref="S2.E1.m1.2.2.2.2.1.1.2.3">1</cn></apply><apply id="S2.E1.m1.2.2.2.2.1.1.3.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S2.E1.m1.2.2.2.2.1.1.3.1.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S2.E1.m1.2.2.2.2.1.1.3.2.cmml" xref="S2.E1.m1.2.2.2.2.1.1.3.2">𝑆</ci><cn id="S2.E1.m1.2.2.2.2.1.1.3.3.cmml" type="integer" xref="S2.E1.m1.2.2.2.2.1.1.3.3">2</cn></apply></apply><ci id="S2.E1.m1.3.3.3.5.cmml" xref="S2.E1.m1.3.3.3.5">⋯</ci><apply id="S2.E1.m1.3.3.3.3.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1"><apply id="S2.E1.m1.3.3.3.3.1.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1"><apply id="S2.E1.m1.3.3.3.3.1.1.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.1.1.1.1.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.1.1.1.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1.2">𝑇</ci><ci id="S2.E1.m1.3.3.3.3.1.1.1.1.3.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.1.3">𝑛</ci></apply><ci id="S2.E1.m1.3.3.3.3.1.1.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.1.2">→</ci></apply><apply id="S2.E1.m1.3.3.3.3.1.1.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.2"><csymbol cd="ambiguous" id="S2.E1.m1.3.3.3.3.1.1.2.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.2">subscript</csymbol><ci id="S2.E1.m1.3.3.3.3.1.1.2.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.2.2">𝑆</ci><apply id="S2.E1.m1.3.3.3.3.1.1.2.3.cmml" xref="S2.E1.m1.3.3.3.3.1.1.2.3"><minus id="S2.E1.m1.3.3.3.3.1.1.2.3.1.cmml" xref="S2.E1.m1.3.3.3.3.1.1.2.3.1"></minus><ci id="S2.E1.m1.3.3.3.3.1.1.2.3.2.cmml" xref="S2.E1.m1.3.3.3.3.1.1.2.3.2">𝑛</ci><cn id="S2.E1.m1.3.3.3.3.1.1.2.3.3.cmml" type="integer" xref="S2.E1.m1.3.3.3.3.1.1.2.3.3">1</cn></apply></apply><ci id="S2.E1.m1.3.3.3.3.1.1.3.cmml" xref="S2.E1.m1.3.3.3.3.1.1.3">𝑅</ci></apply></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.E1.m1.3c">\displaystyle C:=(P\xrightarrow{T_{1}}S_{1})\odot(S_{1}\xrightarrow{T_{2}}S_{2%
})\odot\cdots\odot(S_{n-1}\xrightarrow{T_{n}}R)</annotation><annotation encoding="application/x-llamapun" id="S2.E1.m1.3d">italic_C := ( italic_P start_ARROW start_OVERACCENT italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT ) ⊙ ( italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT ) ⊙ ⋯ ⊙ ( italic_S start_POSTSUBSCRIPT italic_n - 1 end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW italic_R )</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(1)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S2.SS3.p4">
<ul class="ltx_itemize" id="S2.I1">
<li class="ltx_item" id="S2.I1.i1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i1.p1">
<p class="ltx_p" id="S2.I1.i1.p1.2"><math alttext="P" class="ltx_Math" display="inline" id="S2.I1.i1.p1.1.m1.1"><semantics id="S2.I1.i1.p1.1.m1.1a"><mi id="S2.I1.i1.p1.1.m1.1.1" xref="S2.I1.i1.p1.1.m1.1.1.cmml">P</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.1.m1.1b"><ci id="S2.I1.i1.p1.1.m1.1.1.cmml" xref="S2.I1.i1.p1.1.m1.1.1">𝑃</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.1.m1.1c">P</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.1.m1.1d">italic_P</annotation></semantics></math> is the initial task. <math alttext="R" class="ltx_Math" display="inline" id="S2.I1.i1.p1.2.m2.1"><semantics id="S2.I1.i1.p1.2.m2.1a"><mi id="S2.I1.i1.p1.2.m2.1.1" xref="S2.I1.i1.p1.2.m2.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S2.I1.i1.p1.2.m2.1b"><ci id="S2.I1.i1.p1.2.m2.1.1.cmml" xref="S2.I1.i1.p1.2.m2.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i1.p1.2.m2.1c">R</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i1.p1.2.m2.1d">italic_R</annotation></semantics></math> is the final result.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i2.p1">
<p class="ltx_p" id="S2.I1.i2.p1.3"><math alttext="T_{1}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.1.m1.1"><semantics id="S2.I1.i2.p1.1.m1.1a"><msub id="S2.I1.i2.p1.1.m1.1.1" xref="S2.I1.i2.p1.1.m1.1.1.cmml"><mi id="S2.I1.i2.p1.1.m1.1.1.2" xref="S2.I1.i2.p1.1.m1.1.1.2.cmml">T</mi><mn id="S2.I1.i2.p1.1.m1.1.1.3" xref="S2.I1.i2.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.1.m1.1b"><apply id="S2.I1.i2.p1.1.m1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.1.m1.1.1.1.cmml" xref="S2.I1.i2.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.1.m1.1.1.2.cmml" xref="S2.I1.i2.p1.1.m1.1.1.2">𝑇</ci><cn id="S2.I1.i2.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.I1.i2.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.1.m1.1c">T_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.1.m1.1d">italic_T start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="T_{2}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.2.m2.1"><semantics id="S2.I1.i2.p1.2.m2.1a"><msub id="S2.I1.i2.p1.2.m2.1.1" xref="S2.I1.i2.p1.2.m2.1.1.cmml"><mi id="S2.I1.i2.p1.2.m2.1.1.2" xref="S2.I1.i2.p1.2.m2.1.1.2.cmml">T</mi><mn id="S2.I1.i2.p1.2.m2.1.1.3" xref="S2.I1.i2.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.2.m2.1b"><apply id="S2.I1.i2.p1.2.m2.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.2.m2.1.1.1.cmml" xref="S2.I1.i2.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.2.m2.1.1.2.cmml" xref="S2.I1.i2.p1.2.m2.1.1.2">𝑇</ci><cn id="S2.I1.i2.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.I1.i2.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.2.m2.1c">T_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.2.m2.1d">italic_T start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, …, <math alttext="T_{n}" class="ltx_Math" display="inline" id="S2.I1.i2.p1.3.m3.1"><semantics id="S2.I1.i2.p1.3.m3.1a"><msub id="S2.I1.i2.p1.3.m3.1.1" xref="S2.I1.i2.p1.3.m3.1.1.cmml"><mi id="S2.I1.i2.p1.3.m3.1.1.2" xref="S2.I1.i2.p1.3.m3.1.1.2.cmml">T</mi><mi id="S2.I1.i2.p1.3.m3.1.1.3" xref="S2.I1.i2.p1.3.m3.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i2.p1.3.m3.1b"><apply id="S2.I1.i2.p1.3.m3.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i2.p1.3.m3.1.1.1.cmml" xref="S2.I1.i2.p1.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i2.p1.3.m3.1.1.2.cmml" xref="S2.I1.i2.p1.3.m3.1.1.2">𝑇</ci><ci id="S2.I1.i2.p1.3.m3.1.1.3.cmml" xref="S2.I1.i2.p1.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i2.p1.3.m3.1c">T_{n}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i2.p1.3.m3.1d">italic_T start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> are the reasoning steps, i.e., the reasoning processes in the chain.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i3.p1">
<p class="ltx_p" id="S2.I1.i3.p1.3"><math alttext="S_{1}" class="ltx_Math" display="inline" id="S2.I1.i3.p1.1.m1.1"><semantics id="S2.I1.i3.p1.1.m1.1a"><msub id="S2.I1.i3.p1.1.m1.1.1" xref="S2.I1.i3.p1.1.m1.1.1.cmml"><mi id="S2.I1.i3.p1.1.m1.1.1.2" xref="S2.I1.i3.p1.1.m1.1.1.2.cmml">S</mi><mn id="S2.I1.i3.p1.1.m1.1.1.3" xref="S2.I1.i3.p1.1.m1.1.1.3.cmml">1</mn></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.1.m1.1b"><apply id="S2.I1.i3.p1.1.m1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.1.m1.1.1.1.cmml" xref="S2.I1.i3.p1.1.m1.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.1.m1.1.1.2.cmml" xref="S2.I1.i3.p1.1.m1.1.1.2">𝑆</ci><cn id="S2.I1.i3.p1.1.m1.1.1.3.cmml" type="integer" xref="S2.I1.i3.p1.1.m1.1.1.3">1</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.1.m1.1c">S_{1}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.1.m1.1d">italic_S start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT</annotation></semantics></math>, <math alttext="S_{2}" class="ltx_Math" display="inline" id="S2.I1.i3.p1.2.m2.1"><semantics id="S2.I1.i3.p1.2.m2.1a"><msub id="S2.I1.i3.p1.2.m2.1.1" xref="S2.I1.i3.p1.2.m2.1.1.cmml"><mi id="S2.I1.i3.p1.2.m2.1.1.2" xref="S2.I1.i3.p1.2.m2.1.1.2.cmml">S</mi><mn id="S2.I1.i3.p1.2.m2.1.1.3" xref="S2.I1.i3.p1.2.m2.1.1.3.cmml">2</mn></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.2.m2.1b"><apply id="S2.I1.i3.p1.2.m2.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.2.m2.1.1.1.cmml" xref="S2.I1.i3.p1.2.m2.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.2.m2.1.1.2.cmml" xref="S2.I1.i3.p1.2.m2.1.1.2">𝑆</ci><cn id="S2.I1.i3.p1.2.m2.1.1.3.cmml" type="integer" xref="S2.I1.i3.p1.2.m2.1.1.3">2</cn></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.2.m2.1c">S_{2}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.2.m2.1d">italic_S start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT</annotation></semantics></math>, …, <math alttext="S_{n}" class="ltx_Math" display="inline" id="S2.I1.i3.p1.3.m3.1"><semantics id="S2.I1.i3.p1.3.m3.1a"><msub id="S2.I1.i3.p1.3.m3.1.1" xref="S2.I1.i3.p1.3.m3.1.1.cmml"><mi id="S2.I1.i3.p1.3.m3.1.1.2" xref="S2.I1.i3.p1.3.m3.1.1.2.cmml">S</mi><mi id="S2.I1.i3.p1.3.m3.1.1.3" xref="S2.I1.i3.p1.3.m3.1.1.3.cmml">n</mi></msub><annotation-xml encoding="MathML-Content" id="S2.I1.i3.p1.3.m3.1b"><apply id="S2.I1.i3.p1.3.m3.1.1.cmml" xref="S2.I1.i3.p1.3.m3.1.1"><csymbol cd="ambiguous" id="S2.I1.i3.p1.3.m3.1.1.1.cmml" xref="S2.I1.i3.p1.3.m3.1.1">subscript</csymbol><ci id="S2.I1.i3.p1.3.m3.1.1.2.cmml" xref="S2.I1.i3.p1.3.m3.1.1.2">𝑆</ci><ci id="S2.I1.i3.p1.3.m3.1.1.3.cmml" xref="S2.I1.i3.p1.3.m3.1.1.3">𝑛</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i3.p1.3.m3.1c">S_{n}</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i3.p1.3.m3.1d">italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT</annotation></semantics></math> are the intermediate states obtained after each reasoning step.</p>
</div>
</li>
<li class="ltx_item" id="S2.I1.i4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">•</span>
<div class="ltx_para" id="S2.I1.i4.p1">
<p class="ltx_p" id="S2.I1.i4.p1.1"><math alttext="\odot" class="ltx_Math" display="inline" id="S2.I1.i4.p1.1.m1.1"><semantics id="S2.I1.i4.p1.1.m1.1a"><mo id="S2.I1.i4.p1.1.m1.1.1" xref="S2.I1.i4.p1.1.m1.1.1.cmml">⊙</mo><annotation-xml encoding="MathML-Content" id="S2.I1.i4.p1.1.m1.1b"><csymbol cd="latexml" id="S2.I1.i4.p1.1.m1.1.1.cmml" xref="S2.I1.i4.p1.1.m1.1.1">direct-product</csymbol></annotation-xml><annotation encoding="application/x-tex" id="S2.I1.i4.p1.1.m1.1c">\odot</annotation><annotation encoding="application/x-llamapun" id="S2.I1.i4.p1.1.m1.1d">⊙</annotation></semantics></math> denotes the symbol between reasoning tasks, indicating that each reasoning step depends on the result of the previous one.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="S2.SS3.p5">
<p class="ltx_p" id="S2.SS3.p5.1">The development of Chain-of-Thought technology has undergone several key stages. Initially proposed by DeepMind, the CoT guides LLMs to progressively demonstrate their reasoning processes, forming essential intermediate concepts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib13" title="">13</a>]</cite>. The original research employed few-shot CoT, which provides manually designed examples containing both problems and their corresponding reasoning chains in the prompts to instruct models in generating similar reasoning processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib2" title="">2</a>]</cite>. Subsequent research found that even simple prompts like ”Let’s think step by step” could effectively stimulate models’ reasoning capabilities, leading to the emergence of Zero-shot CoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib62" title="">62</a>]</cite>. This approach significantly reduces the manual effort required for designing reasoning chains while improving scalability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib63" title="">63</a>]</cite>. Recognizing potential logical errors in reasoning chains generated solely through Zero-shot CoT, researchers proposed Auto-CoT, which automatically constructs reasoning demonstrations for large models through clustering and example sampling. Supervised CoT provides models with explicit reasoning steps to enhance their structured reasoning capabilities through supervision <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib64" title="">64</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p6">
<p class="ltx_p" id="S2.SS3.p6.1">Recent extensions to CoT methods have yielded several innovative variants optimized for different aspects of complex reasoning. These include Self-consistency with CoT, which improves robustness by integrating multiple reasoning paths <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib65" title="">65</a>]</cite>; Tree of Thought (ToT), which supports branching exploration <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib66" title="">66</a>]</cite>; and the Graph of Thought (GoT) framework that enables graph-structured reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib67" title="">67</a>]</cite>. Each advancement contributes to expanding the application scope and effectiveness of reasoning-capable large language models. Notably, Visual CoT employs textual representations of visual content as an intermediary between visual inputs and reasoning processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib68" title="">68</a>]</cite>. CoT applications in visual domains show particular promise for multimodal reasoning tasks, such as critical challenges in autonomous driving scenarios that require integrating visual perception with high-level reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib69" title="">69</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S2.SS3.p7">
<p class="ltx_p" id="S2.SS3.p7.1">This section aims to provide a concise background introduction to Chain-of-Thought and summarize its subsequent developments, offering readers a clear framework for understanding these key concepts and their applications in autonomous driving.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE I: </span>Comparative Analysis of Autonomous Driving CoT Models</figcaption><div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S2.T1.13"><span class="ltx_text ltx_font_bold" id="S2.T1.13.1" style="font-size:80%;">Abbreviations:<span class="ltx_text ltx_font_medium" id="S2.T1.13.1.1"> SF / MF = Single-frame / Multi-frame, SV / MV = Single-view / Multi-view, 2DKS = 2D position / Kinematic state,</span></span></p>
</div>
<div class="ltx_flex_break"></div>
<div class="ltx_flex_cell ltx_flex_size_1">
<p class="ltx_p ltx_figure_panel ltx_align_center" id="S2.T1.12"><span class="ltx_text" id="S2.T1.12.13" style="font-size:80%;">TI = Task instruction, EH = Ego history, Mem = Memory</span>
<span class="ltx_tabular ltx_guessed_headers ltx_align_middle" id="S2.T1.12.12">
<span class="ltx_thead">
<span class="ltx_tr" id="S2.T1.12.12.13.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.12.12.13.1.1" style="padding:3pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.12.12.13.1.1.1">Name</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S2.T1.12.12.13.1.2" style="padding:3pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.12.12.13.1.2.1">Year</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.12.12.13.1.3" style="padding:3pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.12.12.13.1.3.1">Task</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.12.12.13.1.4" style="padding:3pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.12.12.13.1.4.1">Pipeline</span></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_tt" id="S2.T1.12.12.13.1.5" style="padding:3pt 4.0pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.12.12.13.1.5.1">Chain-of-thought cognitive process</span></span></span>
</span>
<span class="ltx_tbody">
<span class="ltx_tr" id="S2.T1.1.1.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.1.1.2" style="padding:3pt 4.0pt;">Agent-Driver<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib70" title="">70</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S2.T1.1.1.1.3" style="padding:3pt 4.0pt;">2023</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.1.4" style="padding:3pt 4.0pt;">Perception</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.1.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left ltx_border_t" id="S2.T1.1.1.1.1" style="padding:3pt 4.0pt;">SF-MV → Key object detection → High-level intent → Trajectory <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.1.1.1.1.m1.1"><semantics id="S2.T1.1.1.1.1.m1.1a"><mo id="S2.T1.1.1.1.1.m1.1.1" xref="S2.T1.1.1.1.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.1.1.1.1.m1.1b"><ci id="S2.T1.1.1.1.1.m1.1.1.cmml" xref="S2.T1.1.1.1.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.1.1.1.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.1.1.1.1.m1.1d">↺</annotation></semantics></math> Refine, Mem</span></span>
<span class="ltx_tr" id="S2.T1.12.12.14.1">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.14.1.1" style="padding:3pt 4.0pt;">Dolphins<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib71" title="">71</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.14.1.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.14.1.3" style="padding:3pt 4.0pt;">Perception</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.14.1.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.14.1.5" style="padding:3pt 4.0pt;">MF-SV, Question → GCoT-fine-tuning → Perception answer</span></span>
<span class="ltx_tr" id="S2.T1.12.12.15.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.15.2.1" style="padding:3pt 4.0pt;">RIV-CoT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib72" title="">72</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.15.2.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.15.2.3" style="padding:3pt 4.0pt;">Perception</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.15.2.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.15.2.5" style="padding:3pt 4.0pt;">SF-SV, Question → Bounding box → Image crop → Perception answer</span></span>
<span class="ltx_tr" id="S2.T1.12.12.16.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.16.3.1" style="padding:3pt 4.0pt;">DriveAgent<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib73" title="">73</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.16.3.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.16.3.3" style="padding:3pt 4.0pt;">Perception</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.16.3.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.16.3.5" style="padding:3pt 4.0pt;">MF-SV, LiDAR → Description → Vehicle reasoning → Scene analysis → Perception</span></span>
<span class="ltx_tr" id="S2.T1.12.12.17.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.17.4.1" style="padding:3pt 4.0pt;">AgentThink<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib74" title="">74</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.17.4.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.17.4.3" style="padding:3pt 4.0pt;">Perception</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.17.4.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.17.4.5" style="padding:3pt 4.0pt;">SF-MV, Question → Tool use → Uncertainty flag → Perception answer</span></span>
<span class="ltx_tr" id="S2.T1.12.12.18.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.18.5.1" style="padding:3pt 4.0pt;">Reason2Drive<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib25" title="">25</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.18.5.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.18.5.3" style="padding:3pt 4.0pt;">Prediction</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.18.5.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.18.5.5" style="padding:3pt 4.0pt;">MF-SV, Question → Perception answer → Prediction answer → Motion visualization</span></span>
<span class="ltx_tr" id="S2.T1.12.12.19.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.19.6.1" style="padding:3pt 4.0pt;">Motion-LLaVA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib75" title="">75</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.19.6.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.19.6.3" style="padding:3pt 4.0pt;">Prediction</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.19.6.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.19.6.5" style="padding:3pt 4.0pt;">2DKS, Question → Aggregated in-context reasoning → Prediction answer</span></span>
<span class="ltx_tr" id="S2.T1.12.12.20.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.20.7.1" style="padding:3pt 4.0pt;">LC-LLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib76" title="">76</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.20.7.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.20.7.3" style="padding:3pt 4.0pt;">Prediction</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.20.7.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.20.7.5" style="padding:3pt 4.0pt;">2DKS, EH → Feature detection → Intention → Motion prediction, Explanation</span></span>
<span class="ltx_tr" id="S2.T1.12.12.21.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.21.8.1" style="padding:3pt 4.0pt;">CoT-Drive<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib77" title="">77</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.21.8.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.21.8.3" style="padding:3pt 4.0pt;">Prediction</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.21.8.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.21.8.5" style="padding:3pt 4.0pt;">2DKS → Background → Interaction analysis → Risk assessment → Motion prediction</span></span>
<span class="ltx_tr" id="S2.T1.12.12.22.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.22.9.1" style="padding:3pt 4.0pt;">SenseRAG<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib78" title="">78</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.22.9.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.22.9.3" style="padding:3pt 4.0pt;">Prediction</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.22.9.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.22.9.5" style="padding:3pt 4.0pt;">SF-MV, LiDAR, Structured data → Data injection → RAG → Motion Prediction</span></span>
<span class="ltx_tr" id="S2.T1.12.12.23.10">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.23.10.1" style="padding:3pt 4.0pt;">GPT-Driver<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib79" title="">79</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.23.10.2" style="padding:3pt 4.0pt;">2023</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.23.10.3" style="padding:3pt 4.0pt;">Planning</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.23.10.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.23.10.5" style="padding:3pt 4.0pt;">2DKS, EH, TI → Key object detection → Interaction prediction → Trajectory</span></span>
<span class="ltx_tr" id="S2.T1.2.2.2">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.2.2" style="padding:3pt 4.0pt;">PlanAgent<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib80" title="">80</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.2.2.2.3" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.2.2.2.4" style="padding:3pt 4.0pt;">Planning</span>
<span class="ltx_td ltx_align_left" id="S2.T1.2.2.2.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.2.2.2.1" style="padding:3pt 4.0pt;">MF-MV, 2DKS → Global, local info → Scene description → Planning code <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.2.2.2.1.m1.1"><semantics id="S2.T1.2.2.2.1.m1.1a"><mo id="S2.T1.2.2.2.1.m1.1.1" xref="S2.T1.2.2.2.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.2.2.2.1.m1.1b"><ci id="S2.T1.2.2.2.1.m1.1.1.cmml" xref="S2.T1.2.2.2.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.2.2.2.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.2.2.2.1.m1.1d">↺</annotation></semantics></math> Refine</span></span>
<span class="ltx_tr" id="S2.T1.12.12.24.11">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.24.11.1" style="padding:3pt 4.0pt;">DriveVLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib24" title="">24</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.24.11.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.24.11.3" style="padding:3pt 4.0pt;">Planning</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.24.11.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.24.11.5" style="padding:3pt 4.0pt;">MF-MV → Scene description → Scene Analysis → Hierarchical Planning → Trajectory</span></span>
<span class="ltx_tr" id="S2.T1.12.12.25.12">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.25.12.1" style="padding:3pt 4.0pt;">RDA-Driver<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib81" title="">81</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.25.12.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.25.12.3" style="padding:3pt 4.0pt;">Planning</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.25.12.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.25.12.5" style="padding:3pt 4.0pt;">SF-MV, EH → Key object detection and prediction → High-level intent → Trajectory</span></span>
<span class="ltx_tr" id="S2.T1.12.12.26.13">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.26.13.1" style="padding:3pt 4.0pt;">AlphaDrive<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib57" title="">57</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.26.13.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.26.13.3" style="padding:3pt 4.0pt;">Planning</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.26.13.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.26.13.5" style="padding:3pt 4.0pt;">MF-SV → Key object detection → High-level intent</span></span>
<span class="ltx_tr" id="S2.T1.12.12.27.14">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.27.14.1" style="padding:3pt 4.0pt;">CALMM-Drive<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib82" title="">82</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.27.14.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.27.14.3" style="padding:3pt 4.0pt;">Planning</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.27.14.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.27.14.5" style="padding:3pt 4.0pt;">BEV, EH, TI → Top-K action, Confidence → Trajectory → Hierarchical refinement</span></span>
<span class="ltx_tr" id="S2.T1.12.12.28.15">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.28.15.1" style="padding:3pt 4.0pt;">LanguageMPC<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib83" title="">83</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.28.15.2" style="padding:3pt 4.0pt;">2023</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.28.15.3" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.28.15.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.28.15.5" style="padding:3pt 4.0pt;">2DKS, EH, TI → Vehicle detection → Situational awareness → Meta action</span></span>
<span class="ltx_tr" id="S2.T1.3.3.3">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.3.3.3.2" style="padding:3pt 4.0pt;">DiLu<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib41" title="">41</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.3.3.3.3" style="padding:3pt 4.0pt;">2023</span>
<span class="ltx_td ltx_align_left" id="S2.T1.3.3.3.4" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.3.3.3.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.3.3.3.1" style="padding:3pt 4.0pt;">2DKS → Scene description → Meta action <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.3.3.3.1.m1.1"><semantics id="S2.T1.3.3.3.1.m1.1a"><mo id="S2.T1.3.3.3.1.m1.1.1" xref="S2.T1.3.3.3.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.3.3.3.1.m1.1b"><ci id="S2.T1.3.3.3.1.m1.1.1.cmml" xref="S2.T1.3.3.3.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.3.3.3.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.3.3.3.1.m1.1d">↺</annotation></semantics></math> Refine, Mem</span></span>
<span class="ltx_tr" id="S2.T1.12.12.29.16">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.29.16.1" style="padding:3pt 4.0pt;">DriveMLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib84" title="">84</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.29.16.2" style="padding:3pt 4.0pt;">2023</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.29.16.3" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.29.16.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.29.16.5" style="padding:3pt 4.0pt;">MF-MV, TI, LiDAR → Linguistic description → Speed-path decision</span></span>
<span class="ltx_tr" id="S2.T1.4.4.4">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.4.4.4.2" style="padding:3pt 4.0pt;">Receive-Reason-React<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib85" title="">85</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.4.4.4.3" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.4.4.4.4" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.4.4.4.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.4.4.4.1" style="padding:3pt 4.0pt;">2DKS, TI, In-cabin info → Scene description → Explanation → Meta action <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.4.4.4.1.m1.1"><semantics id="S2.T1.4.4.4.1.m1.1a"><mo id="S2.T1.4.4.4.1.m1.1.1" xref="S2.T1.4.4.4.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.4.4.4.1.m1.1b"><ci id="S2.T1.4.4.4.1.m1.1.1.cmml" xref="S2.T1.4.4.4.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.4.4.4.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.4.4.4.1.m1.1d">↺</annotation></semantics></math> Mem</span></span>
<span class="ltx_tr" id="S2.T1.5.5.5">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.5.5.5.2" style="padding:3pt 4.0pt;">SafeDrive<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib86" title="">86</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.5.5.5.3" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.5.5.5.4" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.5.5.5.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.5.5.5.1" style="padding:3pt 4.0pt;">2DKS, TI → Risk evaluation → Key object detection → Meta action <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.5.5.5.1.m1.1"><semantics id="S2.T1.5.5.5.1.m1.1a"><mo id="S2.T1.5.5.5.1.m1.1.1" xref="S2.T1.5.5.5.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.5.5.5.1.m1.1b"><ci id="S2.T1.5.5.5.1.m1.1.1.cmml" xref="S2.T1.5.5.5.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.5.5.5.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.5.5.5.1.m1.1d">↺</annotation></semantics></math> Refine, Mem</span></span>
<span class="ltx_tr" id="S2.T1.6.6.6">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.6.6.6.2" style="padding:3pt 4.0pt;">KOMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib87" title="">87</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.6.6.6.3" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.6.6.6.4" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.6.6.6.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.6.6.6.1" style="padding:3pt 4.0pt;">2DKS → Scene description → Goal → Planning → Meta action <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.6.6.6.1.m1.1"><semantics id="S2.T1.6.6.6.1.m1.1a"><mo id="S2.T1.6.6.6.1.m1.1.1" xref="S2.T1.6.6.6.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.6.6.6.1.m1.1b"><ci id="S2.T1.6.6.6.1.m1.1.1.cmml" xref="S2.T1.6.6.6.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.6.6.6.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.6.6.6.1.m1.1d">↺</annotation></semantics></math> Refine, Mem</span></span>
<span class="ltx_tr" id="S2.T1.7.7.7">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.7.7.7.2" style="padding:3pt 4.0pt;">LeapAD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib49" title="">49</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.7.7.7.3" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.7.7.7.4" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.7.7.7.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.7.7.7.1" style="padding:3pt 4.0pt;">SF-MV → Scene description → Dual-process <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.7.7.7.1.m1.1"><semantics id="S2.T1.7.7.7.1.m1.1a"><mo id="S2.T1.7.7.7.1.m1.1.1" xref="S2.T1.7.7.7.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.7.7.7.1.m1.1b"><ci id="S2.T1.7.7.7.1.m1.1.1.cmml" xref="S2.T1.7.7.7.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.7.7.7.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.7.7.7.1.m1.1d">↺</annotation></semantics></math> Refine, Mem</span></span>
<span class="ltx_tr" id="S2.T1.8.8.8">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.8.8.8.2" style="padding:3pt 4.0pt;">LeapVAD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib88" title="">88</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.8.8.8.3" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.8.8.8.4" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.8.8.8.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.8.8.8.1" style="padding:3pt 4.0pt;">MF-MV → Scene description, ACC-ACT similarity → Dual-process <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.8.8.8.1.m1.1"><semantics id="S2.T1.8.8.8.1.m1.1a"><mo id="S2.T1.8.8.8.1.m1.1.1" xref="S2.T1.8.8.8.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.8.8.8.1.m1.1b"><ci id="S2.T1.8.8.8.1.m1.1.1.cmml" xref="S2.T1.8.8.8.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.8.8.8.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.8.8.8.1.m1.1d">↺</annotation></semantics></math> Refine, Mem</span></span>
<span class="ltx_tr" id="S2.T1.9.9.9">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.9.9.9.2" style="padding:3pt 4.0pt;">CoDrivingLLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib89" title="">89</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.9.9.9.3" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.9.9.9.4" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.9.9.9.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.9.9.9.1" style="padding:3pt 4.0pt;">2DKS → State perception → Intent sharing → Negotiation → Meta action <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.9.9.9.1.m1.1"><semantics id="S2.T1.9.9.9.1.m1.1a"><mo id="S2.T1.9.9.9.1.m1.1.1" xref="S2.T1.9.9.9.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.9.9.9.1.m1.1b"><ci id="S2.T1.9.9.9.1.m1.1.1.cmml" xref="S2.T1.9.9.9.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.9.9.9.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.9.9.9.1.m1.1d">↺</annotation></semantics></math> Mem</span></span>
<span class="ltx_tr" id="S2.T1.10.10.10">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.10.10.10.2" style="padding:3pt 4.0pt;">Actor-Reasoner<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib90" title="">90</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.10.10.10.3" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.10.10.10.4" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.10.10.10.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.10.10.10.1" style="padding:3pt 4.0pt;">2DKS, TI, Mem database → Intent prediction → Driving style → Meta action <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.10.10.10.1.m1.1"><semantics id="S2.T1.10.10.10.1.m1.1a"><mo id="S2.T1.10.10.10.1.m1.1.1" xref="S2.T1.10.10.10.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.10.10.10.1.m1.1b"><ci id="S2.T1.10.10.10.1.m1.1.1.cmml" xref="S2.T1.10.10.10.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.10.10.10.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.10.10.10.1.m1.1d">↺</annotation></semantics></math> Mem</span></span>
<span class="ltx_tr" id="S2.T1.12.12.30.17">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.30.17.1" style="padding:3pt 4.0pt;">CoT-VLM4Tar<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib91" title="">91</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.30.17.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.30.17.3" style="padding:3pt 4.0pt;">Decision</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.30.17.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.30.17.5" style="padding:3pt 4.0pt;">SF-SV → Situation classification → Scene Analysis → High-level intent → Meta action</span></span>
<span class="ltx_tr" id="S2.T1.12.12.31.18">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.31.18.1" style="padding:3pt 4.0pt;">LLM-Driver<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib92" title="">92</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.31.18.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.31.18.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.31.18.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.31.18.5" style="padding:3pt 4.0pt;">2DKS → Scene vectors grounding → Prediction answer, Control</span></span>
<span class="ltx_tr" id="S2.T1.11.11.11">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.11.11.11.2" style="padding:3pt 4.0pt;">PKRD-CoT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib93" title="">93</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.11.11.11.3" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.11.11.11.4" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.11.11.11.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.11.11.11.1" style="padding:3pt 4.0pt;">SF-MV → Scene description → Object detection → High-level intent <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.11.11.11.1.m1.1"><semantics id="S2.T1.11.11.11.1.m1.1a"><mo id="S2.T1.11.11.11.1.m1.1.1" xref="S2.T1.11.11.11.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.11.11.11.1.m1.1b"><ci id="S2.T1.11.11.11.1.m1.1.1.cmml" xref="S2.T1.11.11.11.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.11.11.11.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.11.11.11.1.m1.1d">↺</annotation></semantics></math> Mem</span></span>
<span class="ltx_tr" id="S2.T1.12.12.32.19">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.32.19.1" style="padding:3pt 4.0pt;">LMDrive<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib94" title="">94</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.32.19.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.32.19.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.32.19.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.32.19.5" style="padding:3pt 4.0pt;">MF-MV, TI, LiDAR → Feature detection → Trajectory → PID control</span></span>
<span class="ltx_tr" id="S2.T1.12.12.33.20">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.33.20.1" style="padding:3pt 4.0pt;">WiseAD<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib42" title="">42</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.33.20.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.33.20.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.33.20.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.33.20.5" style="padding:3pt 4.0pt;">MF-SV, TI, Question → Answer (Scene description, Risk analysis…) + Trajectory</span></span>
<span class="ltx_tr" id="S2.T1.12.12.34.21">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.34.21.1" style="padding:3pt 4.0pt;">DriveCoT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib18" title="">18</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.34.21.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.34.21.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.34.21.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.34.21.5" style="padding:3pt 4.0pt;">MF-MV → Multidimensional prediction → Logical decision → Meta action</span></span>
<span class="ltx_tr" id="S2.T1.12.12.35.22">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.35.22.1" style="padding:3pt 4.0pt;">Senna<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib95" title="">95</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.35.22.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.35.22.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.35.22.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.35.22.5" style="padding:3pt 4.0pt;">SF-MV, TI → Meta-action → Perception → Motion prediction → Trajectory</span></span>
<span class="ltx_tr" id="S2.T1.12.12.36.23">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.36.23.1" style="padding:3pt 4.0pt;">EMMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib96" title="">96</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.36.23.2" style="padding:3pt 4.0pt;">2024</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.36.23.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.36.23.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.36.23.5" style="padding:3pt 4.0pt;">SF-MV, EH, TI → Scene description → Key object description → Meta action</span></span>
<span class="ltx_tr" id="S2.T1.12.12.37.24">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.37.24.1" style="padding:3pt 4.0pt;">OpenEMMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib97" title="">97</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.37.24.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.37.24.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.37.24.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.37.24.5" style="padding:3pt 4.0pt;">SF-SV, EH → Key object, Scene description, High-level intent → Trajectory</span></span>
<span class="ltx_tr" id="S2.T1.12.12.38.25">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.38.25.1" style="padding:3pt 4.0pt;">LightEMMA<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib98" title="">98</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.38.25.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.38.25.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.38.25.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.38.25.5" style="padding:3pt 4.0pt;">SF-SV, EH → Scene description → High-level intent → Trajectory</span></span>
<span class="ltx_tr" id="S2.T1.12.12.12">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.12.2" style="padding:3pt 4.0pt;">ORION<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib99" title="">99</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.12.3" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.12.4" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.12.5" style="padding:3pt 4.0pt;">Reflective</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.12.1" style="padding:3pt 4.0pt;">SF-MV, TI → Feature extraction → Scene analysis → Meta action → Trajectory <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S2.T1.12.12.12.1.m1.1"><semantics id="S2.T1.12.12.12.1.m1.1a"><mo id="S2.T1.12.12.12.1.m1.1.1" xref="S2.T1.12.12.12.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S2.T1.12.12.12.1.m1.1b"><ci id="S2.T1.12.12.12.1.m1.1.1.cmml" xref="S2.T1.12.12.12.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S2.T1.12.12.12.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S2.T1.12.12.12.1.m1.1d">↺</annotation></semantics></math> Mem</span></span>
<span class="ltx_tr" id="S2.T1.12.12.39.26">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.39.26.1" style="padding:3pt 4.0pt;">DriveLM<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib100" title="">100</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.39.26.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.39.26.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.39.26.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.39.26.5" style="padding:3pt 4.0pt;">SF-SV, Question → Perception → Prediction → High-level intent → Trajectory</span></span>
<span class="ltx_tr" id="S2.T1.12.12.40.27">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.40.27.1" style="padding:3pt 4.0pt;">See2DriveX<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib101" title="">101</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.40.27.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.40.27.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.40.27.4" style="padding:3pt 4.0pt;">Modular</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.40.27.5" style="padding:3pt 4.0pt;">MF-MV, 2DKS, TI, BEV → Scene description → Meta action → Trajectory → Control</span></span>
<span class="ltx_tr" id="S2.T1.12.12.41.28">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.41.28.1" style="padding:3pt 4.0pt;">PRIMEDrive-CoT<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib102" title="">102</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.41.28.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.41.28.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.41.28.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.41.28.5" style="padding:3pt 4.0pt;">SF-MV, LiDAR → Uncertainty/Risk → Interaction → Logical decision → Meta action</span></span>
<span class="ltx_tr" id="S2.T1.12.12.42.29">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.42.29.1" style="padding:3pt 4.0pt;">LangCoop<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib103" title="">103</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S2.T1.12.12.42.29.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.42.29.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.42.29.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left" id="S2.T1.12.12.42.29.5" style="padding:3pt 4.0pt;">SF-SV → Scene description → High-level intent → LangPack integration → Control</span></span>
<span class="ltx_tr" id="S2.T1.12.12.43.30">
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.12.12.43.30.1" style="padding:3pt 4.0pt;">X-Driver<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib104" title="">104</a>]</cite></span>
<span class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S2.T1.12.12.43.30.2" style="padding:3pt 4.0pt;">2025</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.12.12.43.30.3" style="padding:3pt 4.0pt;">E2E</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.12.12.43.30.4" style="padding:3pt 4.0pt;">Logical</span>
<span class="ltx_td ltx_align_left ltx_border_bb" id="S2.T1.12.12.43.30.5" style="padding:3pt 4.0pt;">SF-SV, TI → Object detection, Traffic sign, Lane info → Trajectory</span></span>
</span>
</span></p>
</div>
</div>
</figure>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">III </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">CoT Methods for Autonomous Driving</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">The Chain-of-Thought method is gradually being applied in autonomous driving to enhance the system’s reasoning ability and decision-making level in complex environments. This section systematically classifies existing methods from two dimensions: structural characteristics(III-A) and task domain(III-B). The task domain perspective focuses on the specific applications of CoT, while the structural characteristics perspective reveals the differences in CoT’s structural design. This dual classification framework helps to comprehensively review the current research outcomes.</p>
</div>
<section class="ltx_subsection" id="S3.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS1.5.1.1">III-A</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS1.6.2">Pipeline Paradigms</span>
</h3>
<div class="ltx_para" id="S3.SS1.p1">
<p class="ltx_p" id="S3.SS1.p1.1">This paper will conduct a thorough analysis of autonomous driving CoT’s structural characteristics from the pipeline paradigm perspective.</p>
</div>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Modular Driving CoT</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p1.1">The Modular Driving CoT pipeline adheres to the hierarchical architecture of autonomous driving, decomposing the driving task into multiple independent submodules like perception, decision-making, planning, and control. Each submodule is responsible for specific reasoning tasks and sequentially transmits its outputs downstream, thereby achieving driving functionality in complex scenarios. Through hierarchical propagation, raw sensor data is progressively transformed into final driving commands. This modular design not only enhances the system’s interpretability but also enables independent optimization of individual modules, allowing flexible adaptation to diverse scenario requirements.
The mathematical formulation of the Modular Driving CoT pipeline is as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S7.EGx2">
<tbody id="S3.E2"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle C_{m}:=\bigodot_{i=1}^{n}\bigl{(}S_{i-1}\xrightarrow{T_{i}}S_{i}%
\bigr{)}\;\to\;R" class="ltx_Math" display="inline" id="S3.E2.m1.1"><semantics id="S3.E2.m1.1a"><mrow id="S3.E2.m1.1.1" xref="S3.E2.m1.1.1.cmml"><msub id="S3.E2.m1.1.1.3" xref="S3.E2.m1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.3.2" xref="S3.E2.m1.1.1.3.2.cmml">C</mi><mi id="S3.E2.m1.1.1.3.3" xref="S3.E2.m1.1.1.3.3.cmml">m</mi></msub><mo id="S3.E2.m1.1.1.4" lspace="0.278em" rspace="0.278em" xref="S3.E2.m1.1.1.4.cmml">:=</mo><mrow id="S3.E2.m1.1.1.1" xref="S3.E2.m1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E2.m1.1.1.1.2" xref="S3.E2.m1.1.1.1.2.cmml"><munderover id="S3.E2.m1.1.1.1.2a" xref="S3.E2.m1.1.1.1.2.cmml"><mo id="S3.E2.m1.1.1.1.2.2.2" movablelimits="false" xref="S3.E2.m1.1.1.1.2.2.2.cmml">⨀</mo><mrow id="S3.E2.m1.1.1.1.2.2.3" xref="S3.E2.m1.1.1.1.2.2.3.cmml"><mi id="S3.E2.m1.1.1.1.2.2.3.2" xref="S3.E2.m1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.2.2.3.1" xref="S3.E2.m1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E2.m1.1.1.1.2.2.3.3" xref="S3.E2.m1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E2.m1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.2.3.cmml">n</mi></munderover></mstyle><mrow id="S3.E2.m1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E2.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E2.m1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.cmml"><msub id="S3.E2.m1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.2.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.2" xref="S3.E2.m1.1.1.1.1.1.1.2.2.cmml">S</mi><mrow id="S3.E2.m1.1.1.1.1.1.1.2.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.2.3.2" xref="S3.E2.m1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E2.m1.1.1.1.1.1.1.2.3.1" xref="S3.E2.m1.1.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S3.E2.m1.1.1.1.1.1.1.2.3.3" xref="S3.E2.m1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mover accent="true" id="S3.E2.m1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.cmml"><mo id="S3.E2.m1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E2.m1.1.1.1.1.1.1.1.2.cmml">→</mo><msub id="S3.E2.m1.1.1.1.1.1.1.1.1" xref="S3.E2.m1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.2" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.E2.m1.1.1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub></mover><msub id="S3.E2.m1.1.1.1.1.1.1.3" xref="S3.E2.m1.1.1.1.1.1.1.3.cmml"><mi id="S3.E2.m1.1.1.1.1.1.1.3.2" xref="S3.E2.m1.1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.E2.m1.1.1.1.1.1.1.3.3" xref="S3.E2.m1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E2.m1.1.1.1.1.1.3" maxsize="120%" minsize="120%" rspace="0.280em" xref="S3.E2.m1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E2.m1.1.1.5" rspace="0.558em" stretchy="false" xref="S3.E2.m1.1.1.5.cmml">→</mo><mi id="S3.E2.m1.1.1.6" xref="S3.E2.m1.1.1.6.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E2.m1.1b"><apply id="S3.E2.m1.1.1.cmml" xref="S3.E2.m1.1.1"><and id="S3.E2.m1.1.1a.cmml" xref="S3.E2.m1.1.1"></and><apply id="S3.E2.m1.1.1b.cmml" xref="S3.E2.m1.1.1"><csymbol cd="latexml" id="S3.E2.m1.1.1.4.cmml" xref="S3.E2.m1.1.1.4">assign</csymbol><apply id="S3.E2.m1.1.1.3.cmml" xref="S3.E2.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.3.2">𝐶</ci><ci id="S3.E2.m1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.3.3">𝑚</ci></apply><apply id="S3.E2.m1.1.1.1.cmml" xref="S3.E2.m1.1.1.1"><apply id="S3.E2.m1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.2">superscript</csymbol><apply id="S3.E2.m1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.2.2.1.cmml" xref="S3.E2.m1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.2.2.2.cmml" xref="S3.E2.m1.1.1.1.2.2.2">⨀</ci><apply id="S3.E2.m1.1.1.1.2.2.3.cmml" xref="S3.E2.m1.1.1.1.2.2.3"><eq id="S3.E2.m1.1.1.1.2.2.3.1.cmml" xref="S3.E2.m1.1.1.1.2.2.3.1"></eq><ci id="S3.E2.m1.1.1.1.2.2.3.2.cmml" xref="S3.E2.m1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.E2.m1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E2.m1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E2.m1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.2.3">𝑛</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1"><apply id="S3.E2.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.E2.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S3.E2.m1.1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.1.2">→</ci></apply><apply id="S3.E2.m1.1.1.1.1.1.1.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.2">𝑆</ci><apply id="S3.E2.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3"><minus id="S3.E2.m1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3.1"></minus><ci id="S3.E2.m1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.2.3.2">𝑖</ci><cn id="S3.E2.m1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S3.E2.m1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E2.m1.1.1.1.1.1.1.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E2.m1.1.1.1.1.1.1.3.1.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E2.m1.1.1.1.1.1.1.3.2.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.2">𝑆</ci><ci id="S3.E2.m1.1.1.1.1.1.1.3.3.cmml" xref="S3.E2.m1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.E2.m1.1.1c.cmml" xref="S3.E2.m1.1.1"><ci id="S3.E2.m1.1.1.5.cmml" xref="S3.E2.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2505.20223v1#S3.E2.m1.1.1.1.cmml" id="S3.E2.m1.1.1d.cmml" xref="S3.E2.m1.1.1"></share><ci id="S3.E2.m1.1.1.6.cmml" xref="S3.E2.m1.1.1.6">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E2.m1.1c">\displaystyle C_{m}:=\bigodot_{i=1}^{n}\bigl{(}S_{i-1}\xrightarrow{T_{i}}S_{i}%
\bigr{)}\;\to\;R</annotation><annotation encoding="application/x-llamapun" id="S3.E2.m1.1d">italic_C start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT := ⨀ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) → italic_R</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(2)</span></td>
</tr></tbody>
</table>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="343" id="S3.F3.g1" src="x3.png" width="443"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 3: </span>As a representative of Modular Driving CoT, DriveLM decomposes driving tasks into subtasks including perception, prediction, and planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib100" title="">100</a>]</cite>. CoT provides explicit reasoning processes for decision-making.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p2.6">The first module takes perceptual information as input, where <math alttext="T_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.1.m1.1"><semantics id="S3.SS1.SSS0.Px1.p2.1.m1.1a"><msub id="S3.SS1.SSS0.Px1.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml">T</mi><mi id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.2">𝑇</ci><ci id="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.1.m1.1c">T_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.1.m1.1d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> denotes the thought transition process of the i-th submodule,<math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.2.m2.1"><semantics id="S3.SS1.SSS0.Px1.p2.2.m2.1a"><msub id="S3.SS1.SSS0.Px1.p2.2.m2.1.1" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.cmml">S</mi><mi id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.2.m2.1b"><apply id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.2">𝑆</ci><ci id="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.2.m2.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.2.m2.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> represents the intermediate state of the i-th module. Each module processes the input state <math alttext="S_{i-1}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.3.m3.1"><semantics id="S3.SS1.SSS0.Px1.p2.3.m3.1a"><msub id="S3.SS1.SSS0.Px1.p2.3.m3.1.1" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml">S</mi><mrow id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml"><mi id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.2" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.cmml">i</mi><mo id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.1" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.1.cmml">−</mo><mn id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.3" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.3.cmml">1</mn></mrow></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.3.m3.1b"><apply id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.2">𝑆</ci><apply id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3"><minus id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.1.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.1"></minus><ci id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.2.cmml" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.2">𝑖</ci><cn id="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.3.cmml" type="integer" xref="S3.SS1.SSS0.Px1.p2.3.m3.1.1.3.3">1</cn></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.3.m3.1c">S_{i-1}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.3.m3.1d">italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT</annotation></semantics></math> via <math alttext="T_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.4.m4.1"><semantics id="S3.SS1.SSS0.Px1.p2.4.m4.1a"><msub id="S3.SS1.SSS0.Px1.p2.4.m4.1.1" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.2" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1.2.cmml">T</mi><mi id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.3" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.4.m4.1b"><apply id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1.2">𝑇</ci><ci id="S3.SS1.SSS0.Px1.p2.4.m4.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.4.m4.1c">T_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.4.m4.1d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and outputs <math alttext="S_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.5.m5.1"><semantics id="S3.SS1.SSS0.Px1.p2.5.m5.1a"><msub id="S3.SS1.SSS0.Px1.p2.5.m5.1.1" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.cmml"><mi id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.2" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.2.cmml">S</mi><mi id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.3" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.5.m5.1b"><apply id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.2.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.2">𝑆</ci><ci id="S3.SS1.SSS0.Px1.p2.5.m5.1.1.3.cmml" xref="S3.SS1.SSS0.Px1.p2.5.m5.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.5.m5.1c">S_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.5.m5.1d">italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>, ultimately generating the driving output <math alttext="R" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px1.p2.6.m6.1"><semantics id="S3.SS1.SSS0.Px1.p2.6.m6.1a"><mi id="S3.SS1.SSS0.Px1.p2.6.m6.1.1" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1.cmml">R</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px1.p2.6.m6.1b"><ci id="S3.SS1.SSS0.Px1.p2.6.m6.1.1.cmml" xref="S3.SS1.SSS0.Px1.p2.6.m6.1.1">𝑅</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px1.p2.6.m6.1c">R</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px1.p2.6.m6.1d">italic_R</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px1.p3">
<p class="ltx_p" id="S3.SS1.SSS0.Px1.p3.1">The pipeline adopts a modular design, enabling each module of the thought chain to independently process relevant data while coordinating through a unified system. This modular pipeline effectively reduces system coupling, enhancing operational transparency and traceability while facilitating development and debugging. However, as each module pursues distinct optimization objectives, joint training struggles to achieve global optimization. Additionally, multi-stage information transmission tends to accumulate latency and errors.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Logical Driving CoT</h4>
<figure class="ltx_figure" id="S3.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="370" id="S3.F4.g1" src="x4.png" width="886"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 4: </span>PRIMEDrive-CoT evaluates collision probability, occlusion conditions, and unknown object risks through a hierarchical triple-risk verification process, constructing a reasoning framework based on Logical Driving CoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib102" title="">102</a>]</cite>.</figcaption>
</figure>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p1.1">Unlike Modular Driving CoT which strictly follows autonomous driving’s hierarchical architecture, Logical Driving CoT adopts a fundamentally different approach during the Problem Decomposition phase by breaking down driving tasks into multiple logically interconnected sub-problems rather than modular components <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib18" title="">18</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib91" title="">91</a>]</cite>. In the subsequent Subproblem Solving phase, the Logical Driving CoT methodology places greater emphasis on constraining the reasoning process through rigorous logical judgments. It introduces a more sophisticated logical constraint mechanism where thought processes and logical chains collaboratively govern the system’s operations, ultimately generating driving decisions through a series of logical steps. The pipeline constructs multi-branch reasoning chains that perform cognitive reasoning, conditional judgments, or cost evaluations at each logical node to optimize candidate strategies. The Logical Driving CoT can be formally expressed as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S7.EGx3">
<tbody id="S3.E3"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle C_{l}:=\Bigl{(}\bigodot_{i=1}^{n}\bigl{(}S_{i-1}\xrightarrow[%
\phi_{i}]{T_{i}}S_{i}\bigr{)}\Bigr{)}\;\to\;R" class="ltx_Math" display="inline" id="S3.E3.m1.1"><semantics id="S3.E3.m1.1a"><mrow id="S3.E3.m1.1.1" xref="S3.E3.m1.1.1.cmml"><msub id="S3.E3.m1.1.1.3" xref="S3.E3.m1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.3.2" xref="S3.E3.m1.1.1.3.2.cmml">C</mi><mi id="S3.E3.m1.1.1.3.3" xref="S3.E3.m1.1.1.3.3.cmml">l</mi></msub><mo id="S3.E3.m1.1.1.4" lspace="0.278em" rspace="0.278em" xref="S3.E3.m1.1.1.4.cmml">:=</mo><mrow id="S3.E3.m1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.2" maxsize="160%" minsize="160%" xref="S3.E3.m1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E3.m1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.2.cmml"><munderover id="S3.E3.m1.1.1.1.1.1.2a" xref="S3.E3.m1.1.1.1.1.1.2.cmml"><mo id="S3.E3.m1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E3.m1.1.1.1.1.1.2.2.2.cmml">⨀</mo><mrow id="S3.E3.m1.1.1.1.1.1.2.2.3" xref="S3.E3.m1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.2.2.3.2" xref="S3.E3.m1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E3.m1.1.1.1.1.1.2.2.3.1" xref="S3.E3.m1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E3.m1.1.1.1.1.1.2.2.3.3" xref="S3.E3.m1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E3.m1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.2.3.cmml">n</mi></munderover></mstyle><mrow id="S3.E3.m1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml">S</mi><mrow id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><munderover accent="true" accentunder="true" id="S3.E3.m1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.2" stretchy="false" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">→</mo><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2" mathsize="142%" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">ϕ</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3" mathsize="140%" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.2.cmml">T</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.3.cmml">i</mi></msub></munderover><msub id="S3.E3.m1.1.1.1.1.1.1.1.1.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E3.m1.1.1.1.1.1.1.1.3" maxsize="120%" minsize="120%" xref="S3.E3.m1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E3.m1.1.1.1.1.3" maxsize="160%" minsize="160%" rspace="0.280em" xref="S3.E3.m1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E3.m1.1.1.5" rspace="0.558em" stretchy="false" xref="S3.E3.m1.1.1.5.cmml">→</mo><mi id="S3.E3.m1.1.1.6" xref="S3.E3.m1.1.1.6.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E3.m1.1b"><apply id="S3.E3.m1.1.1.cmml" xref="S3.E3.m1.1.1"><and id="S3.E3.m1.1.1a.cmml" xref="S3.E3.m1.1.1"></and><apply id="S3.E3.m1.1.1b.cmml" xref="S3.E3.m1.1.1"><csymbol cd="latexml" id="S3.E3.m1.1.1.4.cmml" xref="S3.E3.m1.1.1.4">assign</csymbol><apply id="S3.E3.m1.1.1.3.cmml" xref="S3.E3.m1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.3.2">𝐶</ci><ci id="S3.E3.m1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.3.3">𝑙</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E3.m1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.2.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.2.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.2">⨀</ci><apply id="S3.E3.m1.1.1.1.1.1.2.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.3"><eq id="S3.E3.m1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E3.m1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.E3.m1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E3.m1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.2">italic-ϕ</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1"><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.2">𝑇</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.1.3">𝑖</ci></apply><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.1.2.2">→</ci></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.2">𝑆</ci><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3"><minus id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.1"></minus><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.2">𝑖</ci><cn id="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S3.E3.m1.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E3.m1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E3.m1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.2">𝑆</ci><ci id="S3.E3.m1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E3.m1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply></apply><apply id="S3.E3.m1.1.1c.cmml" xref="S3.E3.m1.1.1"><ci id="S3.E3.m1.1.1.5.cmml" xref="S3.E3.m1.1.1.5">→</ci><share href="https://arxiv.org/html/2505.20223v1#S3.E3.m1.1.1.1.cmml" id="S3.E3.m1.1.1d.cmml" xref="S3.E3.m1.1.1"></share><ci id="S3.E3.m1.1.1.6.cmml" xref="S3.E3.m1.1.1.6">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E3.m1.1c">\displaystyle C_{l}:=\Bigl{(}\bigodot_{i=1}^{n}\bigl{(}S_{i-1}\xrightarrow[%
\phi_{i}]{T_{i}}S_{i}\bigr{)}\Bigr{)}\;\to\;R</annotation><annotation encoding="application/x-llamapun" id="S3.E3.m1.1d">italic_C start_POSTSUBSCRIPT italic_l end_POSTSUBSCRIPT := ( ⨀ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT start_ARROW start_UNDERACCENT italic_ϕ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_UNDERACCENT start_ARROW start_OVERACCENT italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW end_ARROW italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) → italic_R</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(3)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p2.3"><math alttext="\phi_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.1.m1.1"><semantics id="S3.SS1.SSS0.Px2.p2.1.m1.1a"><msub id="S3.SS1.SSS0.Px2.p2.1.m1.1.1" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml">ϕ</mi><mi id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p2.1.m1.1b"><apply id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.2">italic-ϕ</ci><ci id="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p2.1.m1.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p2.1.m1.1c">\phi_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p2.1.m1.1d">italic_ϕ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> denotes a logical operator that constrains the reasoning process. The solution to each sub-problem must simultaneously satisfy both the thought transition information <math alttext="T_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.2.m2.1"><semantics id="S3.SS1.SSS0.Px2.p2.2.m2.1a"><msub id="S3.SS1.SSS0.Px2.p2.2.m2.1.1" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p2.2.m2.1.1.2" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1.2.cmml">T</mi><mi id="S3.SS1.SSS0.Px2.p2.2.m2.1.1.3" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p2.2.m2.1b"><apply id="S3.SS1.SSS0.Px2.p2.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p2.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p2.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1.2">𝑇</ci><ci id="S3.SS1.SSS0.Px2.p2.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p2.2.m2.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p2.2.m2.1c">T_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p2.2.m2.1d">italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> and the logical constraint <math alttext="\phi_{i}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px2.p2.3.m3.1"><semantics id="S3.SS1.SSS0.Px2.p2.3.m3.1a"><msub id="S3.SS1.SSS0.Px2.p2.3.m3.1.1" xref="S3.SS1.SSS0.Px2.p2.3.m3.1.1.cmml"><mi id="S3.SS1.SSS0.Px2.p2.3.m3.1.1.2" xref="S3.SS1.SSS0.Px2.p2.3.m3.1.1.2.cmml">ϕ</mi><mi id="S3.SS1.SSS0.Px2.p2.3.m3.1.1.3" xref="S3.SS1.SSS0.Px2.p2.3.m3.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px2.p2.3.m3.1b"><apply id="S3.SS1.SSS0.Px2.p2.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.3.m3.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px2.p2.3.m3.1.1.1.cmml" xref="S3.SS1.SSS0.Px2.p2.3.m3.1.1">subscript</csymbol><ci id="S3.SS1.SSS0.Px2.p2.3.m3.1.1.2.cmml" xref="S3.SS1.SSS0.Px2.p2.3.m3.1.1.2">italic-ϕ</ci><ci id="S3.SS1.SSS0.Px2.p2.3.m3.1.1.3.cmml" xref="S3.SS1.SSS0.Px2.p2.3.m3.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px2.p2.3.m3.1c">\phi_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px2.p2.3.m3.1d">italic_ϕ start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math>.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px2.p3">
<p class="ltx_p" id="S3.SS1.SSS0.Px2.p3.1">The pipeline also incorporates subtasks like collision-risk estimation and stop sign checks, utilizing Least-to-Most Prompting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib105" title="">105</a>]</cite>. This strategy guides the model through progressively complex questions, reflecting the step-by-step reasoning process humans use. Taking DriveCoT as a case in point, the system first detects obstacles ahead and analyzes their motion states and relative positions using real-time perception data. Through this logical reasoning chain, it then determines appropriate strategies like deceleration, stopping, or evasive maneuvers. This logic-driven reasoning approach significantly enhances decision-making accuracy and stability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib102" title="">102</a>]</cite>. Particularly in high-risk scenarios, it effectively reduces the likelihood of misjudgments and decision latency.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS1.SSS0.Px3">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Reflective Driving CoT</h4>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p1.1">The Reflective Driving CoT builds upon the previous two types of CoT reasoning pipelines by introducing the reflective update mechanism. By evaluating the discrepancy between execution results and pre-stored experiences, it triggers self-reflection at the chain’s terminal, enabling system self-iteration, error correction, and memory bank updates.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p2.1">This pipeline transforms the unidirectional thought chain into a bidirectional cognitive loop, equipping the system with causal reasoning and knowledge transfer capabilities. Consequently, in out-of-distribution (OOD) scenarios, the system can generate adaptive strategies through logical inference, effectively handling complex and dynamic driving environments. The formulation is as follows:</p>
<table class="ltx_equationgroup ltx_eqn_align ltx_eqn_table" id="S7.EGx4">
<tbody id="S3.E4"><tr class="ltx_equation ltx_eqn_row ltx_align_baseline">
<td class="ltx_eqn_cell ltx_eqn_center_padleft"></td>
<td class="ltx_td ltx_align_right ltx_eqn_cell"><math alttext="\displaystyle C_{r}:=\Bigl{(}\bigodot_{i=1}^{n}\bigl{(}S_{i-1}\xrightarrow{T_{%
i}}S_{i}\bigr{)}\Bigr{)}\;\odot\;\Bigl{(}S_{n}\circlearrowleft S_{0}^{\prime}%
\Bigr{)}\;\to\;R" class="ltx_Math" display="inline" id="S3.E4.m1.2"><semantics id="S3.E4.m1.2a"><mrow id="S3.E4.m1.2.2" xref="S3.E4.m1.2.2.cmml"><msub id="S3.E4.m1.2.2.4" xref="S3.E4.m1.2.2.4.cmml"><mi id="S3.E4.m1.2.2.4.2" xref="S3.E4.m1.2.2.4.2.cmml">C</mi><mi id="S3.E4.m1.2.2.4.3" xref="S3.E4.m1.2.2.4.3.cmml">r</mi></msub><mo id="S3.E4.m1.2.2.5" lspace="0.278em" rspace="0.278em" xref="S3.E4.m1.2.2.5.cmml">:=</mo><mrow id="S3.E4.m1.2.2.2" xref="S3.E4.m1.2.2.2.cmml"><mrow id="S3.E4.m1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.2" maxsize="160%" minsize="160%" xref="S3.E4.m1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.cmml"><mstyle displaystyle="true" id="S3.E4.m1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml"><munderover id="S3.E4.m1.1.1.1.1.1.1.2a" xref="S3.E4.m1.1.1.1.1.1.1.2.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.2.2.2" movablelimits="false" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2.cmml">⨀</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.2.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.2.2.3.2" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.2.cmml">i</mi><mo id="S3.E4.m1.1.1.1.1.1.1.2.2.3.1" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.1.cmml">=</mo><mn id="S3.E4.m1.1.1.1.1.1.1.2.2.3.3" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.3.cmml">1</mn></mrow><mi id="S3.E4.m1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.2.3.cmml">n</mi></munderover></mstyle><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.2" maxsize="120%" minsize="120%" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">(</mo><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml"><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml">S</mi><mrow id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml">i</mi><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml">−</mo><mn id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml">1</mn></mrow></msub><mover accent="true" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml"><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2" stretchy="false" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml">→</mo><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml">T</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml">i</mi></msub></mover><msub id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml"><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.cmml">S</mi><mi id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml">i</mi></msub></mrow><mo id="S3.E4.m1.1.1.1.1.1.1.1.1.3" maxsize="120%" minsize="120%" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.1.1.1.1.1.3" maxsize="160%" minsize="160%" rspace="0.335em" xref="S3.E4.m1.1.1.1.1.1.1.cmml">)</mo></mrow><mo id="S3.E4.m1.2.2.2.3" rspace="0.502em" xref="S3.E4.m1.2.2.2.3.cmml">⊙</mo><mrow id="S3.E4.m1.2.2.2.2.1" xref="S3.E4.m1.2.2.2.2.1.1.cmml"><mo id="S3.E4.m1.2.2.2.2.1.2" maxsize="160%" minsize="160%" xref="S3.E4.m1.2.2.2.2.1.1.cmml">(</mo><mrow id="S3.E4.m1.2.2.2.2.1.1" xref="S3.E4.m1.2.2.2.2.1.1.cmml"><msub id="S3.E4.m1.2.2.2.2.1.1.2" xref="S3.E4.m1.2.2.2.2.1.1.2.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.2.2" xref="S3.E4.m1.2.2.2.2.1.1.2.2.cmml">S</mi><mi id="S3.E4.m1.2.2.2.2.1.1.2.3" xref="S3.E4.m1.2.2.2.2.1.1.2.3.cmml">n</mi></msub><mo id="S3.E4.m1.2.2.2.2.1.1.1" xref="S3.E4.m1.2.2.2.2.1.1.1.cmml">↺</mo><msubsup id="S3.E4.m1.2.2.2.2.1.1.3" xref="S3.E4.m1.2.2.2.2.1.1.3.cmml"><mi id="S3.E4.m1.2.2.2.2.1.1.3.2.2" xref="S3.E4.m1.2.2.2.2.1.1.3.2.2.cmml">S</mi><mn id="S3.E4.m1.2.2.2.2.1.1.3.2.3" xref="S3.E4.m1.2.2.2.2.1.1.3.2.3.cmml">0</mn><mo id="S3.E4.m1.2.2.2.2.1.1.3.3" xref="S3.E4.m1.2.2.2.2.1.1.3.3.cmml">′</mo></msubsup></mrow><mo id="S3.E4.m1.2.2.2.2.1.3" maxsize="160%" minsize="160%" rspace="0.280em" xref="S3.E4.m1.2.2.2.2.1.1.cmml">)</mo></mrow></mrow><mo id="S3.E4.m1.2.2.6" rspace="0.558em" stretchy="false" xref="S3.E4.m1.2.2.6.cmml">→</mo><mi id="S3.E4.m1.2.2.7" xref="S3.E4.m1.2.2.7.cmml">R</mi></mrow><annotation-xml encoding="MathML-Content" id="S3.E4.m1.2b"><apply id="S3.E4.m1.2.2.cmml" xref="S3.E4.m1.2.2"><and id="S3.E4.m1.2.2a.cmml" xref="S3.E4.m1.2.2"></and><apply id="S3.E4.m1.2.2b.cmml" xref="S3.E4.m1.2.2"><csymbol cd="latexml" id="S3.E4.m1.2.2.5.cmml" xref="S3.E4.m1.2.2.5">assign</csymbol><apply id="S3.E4.m1.2.2.4.cmml" xref="S3.E4.m1.2.2.4"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.4.1.cmml" xref="S3.E4.m1.2.2.4">subscript</csymbol><ci id="S3.E4.m1.2.2.4.2.cmml" xref="S3.E4.m1.2.2.4.2">𝐶</ci><ci id="S3.E4.m1.2.2.4.3.cmml" xref="S3.E4.m1.2.2.4.3">𝑟</ci></apply><apply id="S3.E4.m1.2.2.2.cmml" xref="S3.E4.m1.2.2.2"><csymbol cd="latexml" id="S3.E4.m1.2.2.2.3.cmml" xref="S3.E4.m1.2.2.2.3">direct-product</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2">superscript</csymbol><apply id="S3.E4.m1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.2.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.2">⨀</ci><apply id="S3.E4.m1.1.1.1.1.1.1.2.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3"><eq id="S3.E4.m1.1.1.1.1.1.1.2.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.1"></eq><ci id="S3.E4.m1.1.1.1.1.1.1.2.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.2">𝑖</ci><cn id="S3.E4.m1.1.1.1.1.1.1.2.2.3.3.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.1.1.2.2.3.3">1</cn></apply></apply><ci id="S3.E4.m1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.2.3">𝑛</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1"><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.2">𝑇</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.1.3">𝑖</ci></apply><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.1.2">→</ci></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.2">𝑆</ci><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3"><minus id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.1"></minus><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.2">𝑖</ci><cn id="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3.cmml" type="integer" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.2.3.3">1</cn></apply></apply><apply id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.1.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3">subscript</csymbol><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.2">𝑆</ci><ci id="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3.cmml" xref="S3.E4.m1.1.1.1.1.1.1.1.1.1.3.3">𝑖</ci></apply></apply></apply><apply id="S3.E4.m1.2.2.2.2.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1"><ci id="S3.E4.m1.2.2.2.2.1.1.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.1">↺</ci><apply id="S3.E4.m1.2.2.2.2.1.1.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.2.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2.2">𝑆</ci><ci id="S3.E4.m1.2.2.2.2.1.1.2.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.2.3">𝑛</ci></apply><apply id="S3.E4.m1.2.2.2.2.1.1.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.3.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3">superscript</csymbol><apply id="S3.E4.m1.2.2.2.2.1.1.3.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3"><csymbol cd="ambiguous" id="S3.E4.m1.2.2.2.2.1.1.3.2.1.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3">subscript</csymbol><ci id="S3.E4.m1.2.2.2.2.1.1.3.2.2.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3.2.2">𝑆</ci><cn id="S3.E4.m1.2.2.2.2.1.1.3.2.3.cmml" type="integer" xref="S3.E4.m1.2.2.2.2.1.1.3.2.3">0</cn></apply><ci id="S3.E4.m1.2.2.2.2.1.1.3.3.cmml" xref="S3.E4.m1.2.2.2.2.1.1.3.3">′</ci></apply></apply></apply></apply><apply id="S3.E4.m1.2.2c.cmml" xref="S3.E4.m1.2.2"><ci id="S3.E4.m1.2.2.6.cmml" xref="S3.E4.m1.2.2.6">→</ci><share href="https://arxiv.org/html/2505.20223v1#S3.E4.m1.2.2.2.cmml" id="S3.E4.m1.2.2d.cmml" xref="S3.E4.m1.2.2"></share><ci id="S3.E4.m1.2.2.7.cmml" xref="S3.E4.m1.2.2.7">𝑅</ci></apply></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.E4.m1.2c">\displaystyle C_{r}:=\Bigl{(}\bigodot_{i=1}^{n}\bigl{(}S_{i-1}\xrightarrow{T_{%
i}}S_{i}\bigr{)}\Bigr{)}\;\odot\;\Bigl{(}S_{n}\circlearrowleft S_{0}^{\prime}%
\Bigr{)}\;\to\;R</annotation><annotation encoding="application/x-llamapun" id="S3.E4.m1.2d">italic_C start_POSTSUBSCRIPT italic_r end_POSTSUBSCRIPT := ( ⨀ start_POSTSUBSCRIPT italic_i = 1 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT italic_n end_POSTSUPERSCRIPT ( italic_S start_POSTSUBSCRIPT italic_i - 1 end_POSTSUBSCRIPT start_ARROW start_OVERACCENT italic_T start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT end_OVERACCENT → end_ARROW italic_S start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT ) ) ⊙ ( italic_S start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT ↺ italic_S start_POSTSUBSCRIPT 0 end_POSTSUBSCRIPT start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT ) → italic_R</annotation></semantics></math></td>
<td class="ltx_eqn_cell ltx_eqn_center_padright"></td>
<td class="ltx_eqn_cell ltx_eqn_eqno ltx_align_middle ltx_align_right" rowspan="1"><span class="ltx_tag ltx_tag_equation ltx_align_right">(4)</span></td>
</tr></tbody>
</table>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p3">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p3.3">Among them, <math alttext="\circlearrowleft" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p3.1.m1.1"><semantics id="S3.SS1.SSS0.Px3.p3.1.m1.1a"><mo id="S3.SS1.SSS0.Px3.p3.1.m1.1.1" xref="S3.SS1.SSS0.Px3.p3.1.m1.1.1.cmml">↺</mo><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p3.1.m1.1b"><ci id="S3.SS1.SSS0.Px3.p3.1.m1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p3.1.m1.1.1">↺</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p3.1.m1.1c">\circlearrowleft</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p3.1.m1.1d">↺</annotation></semantics></math> represents the reflective feedback process. <math alttext="S^{\prime}" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p3.2.m2.1"><semantics id="S3.SS1.SSS0.Px3.p3.2.m2.1a"><msup id="S3.SS1.SSS0.Px3.p3.2.m2.1.1" xref="S3.SS1.SSS0.Px3.p3.2.m2.1.1.cmml"><mi id="S3.SS1.SSS0.Px3.p3.2.m2.1.1.2" xref="S3.SS1.SSS0.Px3.p3.2.m2.1.1.2.cmml">S</mi><mo id="S3.SS1.SSS0.Px3.p3.2.m2.1.1.3" xref="S3.SS1.SSS0.Px3.p3.2.m2.1.1.3.cmml">′</mo></msup><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p3.2.m2.1b"><apply id="S3.SS1.SSS0.Px3.p3.2.m2.1.1.cmml" xref="S3.SS1.SSS0.Px3.p3.2.m2.1.1"><csymbol cd="ambiguous" id="S3.SS1.SSS0.Px3.p3.2.m2.1.1.1.cmml" xref="S3.SS1.SSS0.Px3.p3.2.m2.1.1">superscript</csymbol><ci id="S3.SS1.SSS0.Px3.p3.2.m2.1.1.2.cmml" xref="S3.SS1.SSS0.Px3.p3.2.m2.1.1.2">𝑆</ci><ci id="S3.SS1.SSS0.Px3.p3.2.m2.1.1.3.cmml" xref="S3.SS1.SSS0.Px3.p3.2.m2.1.1.3">′</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p3.2.m2.1c">S^{\prime}</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p3.2.m2.1d">italic_S start_POSTSUPERSCRIPT ′ end_POSTSUPERSCRIPT</annotation></semantics></math> is the reflective information generated by the trajectory function <math alttext="\tau" class="ltx_Math" display="inline" id="S3.SS1.SSS0.Px3.p3.3.m3.1"><semantics id="S3.SS1.SSS0.Px3.p3.3.m3.1a"><mi id="S3.SS1.SSS0.Px3.p3.3.m3.1.1" xref="S3.SS1.SSS0.Px3.p3.3.m3.1.1.cmml">τ</mi><annotation-xml encoding="MathML-Content" id="S3.SS1.SSS0.Px3.p3.3.m3.1b"><ci id="S3.SS1.SSS0.Px3.p3.3.m3.1.1.cmml" xref="S3.SS1.SSS0.Px3.p3.3.m3.1.1">𝜏</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.SS1.SSS0.Px3.p3.3.m3.1c">\tau</annotation><annotation encoding="application/x-llamapun" id="S3.SS1.SSS0.Px3.p3.3.m3.1d">italic_τ</annotation></semantics></math>, which includes an evaluation of historical states and thought transition processes.</p>
</div>
<div class="ltx_para" id="S3.SS1.SSS0.Px3.p4">
<p class="ltx_p" id="S3.SS1.SSS0.Px3.p4.1">The pipeline of Reflective Driving CoT possesses continual learning and long-term optimization capabilities, as exemplified by the typical system Dilu <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib41" title="">41</a>]</cite>. By analyzing recorded decision sequences, the reflection module identifies potentially unsafe or inaccurate decisions. Subsequently, LLM is employed to correct these erroneous decisions, and the revised reasoning process along with the correct decision outcomes is stored in the memory module. The memory module utilizes a vector database to store past driving scenario experiences, including decision prompts, reasoning processes, and other metadata, in vectorized form for future retrieval. Corrected driving decision information is also updated in the memory module. This pipeline achieves human-like reflective reasoning, enabling the system to continuously self-learn and evolve, thereby progressively improving driving performance.</p>
</div>
<figure class="ltx_figure" id="S3.F5"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="519" id="S3.F5.g1" src="x5.png" width="886"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 5: </span>Dilu employs Reflective Driving CoT, integrating memory recall and introspection into the reasoning process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib41" title="">41</a>]</cite>. The reflection module combines real-time cognitive data from the current scenario with selective past experiences retrieved from the memory bank, jointly feeding them into the LLMs for response decoding and action execution.</figcaption>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S3.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S3.SS2.5.1.1">III-B</span> </span><span class="ltx_text ltx_font_italic" id="S3.SS2.6.2">Task-Oriented Domains</span>
</h3>
<div class="ltx_para" id="S3.SS2.p1">
<p class="ltx_p" id="S3.SS2.p1.1">In the diverse applications of autonomous driving, the CoT methods can be divided into four main areas based on the different task domains: perception and understanding, prediction and planning, decision-making and control, and end-to-end systems. The previous investigation of CoT methods in autonomous driving systems is grounded in the pipeline paradigm. The following content will offer a more comprehensive investigation within the task domain framework.</p>
</div>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px1">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Perception and Understanding</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px1.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px1.p1.1">Complex environmental interpretation forms the foundation of autonomous driving. CoT reasoning offers a novel perspective for multi-source data fusion and dynamic environment comprehension in perception systems. Dolphins proposed a Grounded Chain-of-Thought (GCoT)-based multimodal conversational driving assistant, trained on driving-specific instruction data <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib71" title="">71</a>]</cite>. It begins with summarizing the core content of the image, followed by accurately identifying the objects within the scene and describing their spatial relationships. The process concludes with a thorough analysis of the driving perception task. By integrating these stages, a standardized GCoT response format can be established, delivering a more flexible and intelligent perception solution. RIV-CoT employs a Retrieval-Based Interleaved Visual Chain-of-Thought method<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib72" title="">72</a>]</cite>. Different from other methods that rely solely on text CoT, RIV-CoT embeds visual evidence directly into the multi-step reasoning chain through real-time image cropping, effectively mitigating hallucination phenomena in large models . DriveAgent employs a structured four-module sequential framework, comprising descriptive analysis, vehicle reasoning, environmental reasoning, and response generation, to address both environment-level and vehicle-level tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib73" title="">73</a>]</cite>. The framework employs multi-agent collaboration to fuse LiDAR-camera data through cross-modal consistency validation, enabling robust scene understanding via structured reasoning and iterative refinement.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px2">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Prediction and Planning</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p1.1">Trajectory prediction and planning are central to autonomous driving, using sensory and historical data to forecast and generate motion. CoT reasoning offers a novel framework for hierarchical inference, markedly improving performance.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p2.1">In the domain of prediction using CoT methods, LC-LLM stands out as the first to frame trajectory forecasting as a language modeling problem <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib76" title="">76</a>]</cite>. It innovatively integrates three types of domain knowledge: traffic rules, traditional lane-changing models, and driving scene characteristics. Leveraging CoT reasoning, it establishes a dual-layer reasoning framework where feature annotation and latent behavior prediction operate concurrently, enhancing both prediction accuracy and interpretability. CoT-Drive adopts a teacher-student architecture, where the teacher LLM’s reasoning process is optimized via chain-of-thought and then distilled into a lightweight student language model, achieving a balance between coherent reasoning and edge-device deployment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib77" title="">77</a>]</cite>. Specifically, in the teacher model’s CoT process, reasoning proceeds through four fundamental stages, beginning with background and statistical analysis, progressing through interactive behavior examination and risk evaluation, and culminating in prediction formulation.This thought-guided approach produces semantic descriptions that comply with traffic rules and human driving logic, ensuring precision and explainability. Unlike methods reliant on predefined tool libraries or template queries, SenseRAG combines CoT with multimodal Retrieval Augmented Generation(RAG), dynamically acquiring external knowledge to improve prediction performance <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib78" title="">78</a>]</cite>. Meanwhile, Reason2Drive enhances LLMs’ reasoning capabilities by integrating object-level perception, enabling predictive analysis of accident-prone scenarios or hazardous situations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib25" title="">25</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px2.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px2.p3.1">In the planning section, DriveVLM, building upon Qwen-VL and incorporating CoT reasoning, proposes a progressive planning framework of ”scene description – scene analysis – hierarchical planning”<cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib24" title="">24</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib106" title="">106</a>]</cite>. This framework initially evaluates environmental factors during the scene description phase and obtains target objects of various categories along with their bounding box information through key object detection. Subsequently, in the scene analysis phase, it extracts static attributes, motion states, and special behaviors of the objects. The system then transmits the scene summary generated from prior reasoning to the hierarchical planning module. During the hierarchical planning phase, leveraging the preceding reasoning results, the system generates meta-actions, decision descriptions, and trajectory nodes based on prompt information, optimizing the trajectory to achieve precise path planning. GPT-Driver enhances path planning accuracy and efficiency by linguistically formulating the motion planning problem through thought annotations of future collision probabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib79" title="">79</a>]</cite>. RDA-Driver introduces a reasoning-decision alignment mechanism, designing consistency constraint loss and contrastive function loss to help the model comprehend the involved logical reasoning, ensuring logical consistency between the model’s explanations and conclusions while correcting inconsistencies between CoT reasoning information and planning results <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib81" title="">81</a>]</cite>. CoT establishes causal relationships to predict and analyze potential event sequences, thereby adjusting behaviors accordingly to effectively plan driving routes and actions. PlanAgent employs hierarchical multi-round CoT reasoning to generate planner code for the Intelligent Driver Model (IDM), reasoning about the current scenario across three levels: scene understanding, motion commands, and code generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib80" title="">80</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib107" title="">107</a>]</cite>. CALMM-Drive flexibly modifies strategies based on scene classification to achieve multi-step reasoning, deriving Top-K decisions with attached confidence levels <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib82" title="">82</a>]</cite>. The multi-decision output mechanism effectively reduces uncertainty in the CoT reasoning process while significantly mitigating risks posed by single erroneous decisions through the integration of multiple logical strategies.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px3">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">Decision-Making and Control</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p1.1">Decision-making and control represent another core challenge in autonomous driving, responsible for translating high-level intentions into concrete operations. CoT enables autonomous vehicles to generate optimal control commands and offer human-interpretable decision justifications, aligning with logical human reasoning processes.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p2.1">LanguageMPC utilizes three predefined tool libraries: scenario encoding, action guidance, and confidence adjustment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib83" title="">83</a>]</cite>. It transforms linguistic decisions from LLMs into structured representations compatible with model predictive control (MPC) requirements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib108" title="">108</a>]</cite>. These include observation matrices, action bias vectors, and dynamic weight matrices, enabling adaptive attention allocation and contextual understanding. DRIVEMLM combines the chain of thought with a traffic knowledge base, emphasizing consistency in decision-making and planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib84" title="">84</a>]</cite>. Receive-Reason-React employs the chain of thought as a guiding signal to ensure alignment with human-like reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib85" title="">85</a>]</cite>. It integrates in-cabin monitoring, driver verbal commands, and contextual reasoning to enhance safety and robustness while delivering a personalized driving experience. CoT-VLM4Tar proposes a four-stage chain of thought for traffic anomaly detection, guiding the VLM through a precise reasoning process—from phenomenon-level scene classification to analytical reasoning, high-dimensional intervention measures, and finally, structured formatting of key vehicle movements translating VLM-generated solutions into executable commands <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib91" title="">91</a>]</cite>. SafeDrive combines natural language scene descriptions, a dynamic Driver Risk Field (DRF), and historical memory to drive LLM-based three-stage reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib86" title="">86</a>]</cite>. It forces the LLM to generate actions within the hard safety thresholds of the risk module, with each decision accompanied by a risk traceability report to ensure system safety.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px3.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px3.p3.1">Specifically, the dual-decision process is one of the unique reasoning systems within the thinking architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib109" title="">109</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib110" title="">110</a>]</cite>. LeapAD constructs a parallel fast-slow dual-decision process, simulating the coexistence of rationality and intuition in the human brain <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib49" title="">49</a>]</cite>. The slow Analytic Process (System-II) mimics human rationality to generate reliable driving decisions and enables post-incident reflection. Meanwhile, the fast Heuristic Process (System-I) employs a lightweight language model to retrieve reference knowledge from a memory bank, enabling real-time reasoning and supporting cloud-edge collaborative deployment. Building upon this, LeapVAD introduces multi-frame scene summarization and ”steering-braking” contrastive learning to enhance similar sample retrieval and scene comprehension <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib88" title="">88</a>]</cite>. In cooperative driving automation deployments, the Actor-Reasoner framework similarly adopts a dual-track cognitive structure—combining retrieval-based fast reasoning and systematic slow reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib90" title="">90</a>]</cite>. The Reasoner performs procedural predictions based on scene descriptions and experiential summaries while refining driving style elements. The Actor integrates the Reasoner’s outputs with dual-layer memory retrieval to achieve rapid, lightweight reflexes. Driving style and expressive Human-Machine Interface (eHMI) information serve as core intermediate outputs of the CoT, enabling efficient and safe decision-making in multi-vehicle coordination.</p>
</div>
</section>
<section class="ltx_paragraph" id="S3.SS2.SSS0.Px4">
<h4 class="ltx_title ltx_font_bold ltx_title_paragraph">End-to-End Autonomous Driving</h4>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p1">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p1.1">End-to-end autonomous driving simplifies system architecture through unified modeling from perception to control, but it lacks interpretability. Introducing CoT reasoning helps construct a unified cognitive representation and transparent reasoning path, thereby enhancing the system’s verifiability and explainability.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p2">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p2.1">PKRD-CoT leverages Zero-shot CoT to enable MLLMs to perform autonomous driving tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib93" title="">93</a>]</cite>. The system implements four fundamental capabilities processing architecture tailored to autonomous driving, consisting of perception, knowledge, reasoning, and decision-making. It allows for system deployment without requiring pretraining, while simultaneously ensuring interpretable decision processes. Similarly, Waymo’s EMMA explicitly prompts models to perform step-by-step visual information reasoning, achieving synergy between explicit rationale generation and data-driven models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib96" title="">96</a>]</cite>. Reproducible and enhanced follow-up work, such as OpenEMMA and LightEMMA incorporates explicit CoT reasoning. This includes components like high-level intent command, driving scene understanding, and major object detection, which work together to accurately identify objects, plan trajectories, and support rational driving decisions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib97" title="">97</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib98" title="">98</a>]</cite>. DriveCoT-Agent decomposes end-to-end driving tasks into sequential logical steps—such as collision prediction, traffic sign recognition, relation to ahead vehicle, and road recognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib18" title="">18</a>]</cite>. Within this reasoning chain, the system first evaluates prediction outputs from the network to determine whether potential hazards triggering emergency braking exist. If none are detected, it proceeds to analyze the relative dynamics between the ego vehicle and preceding vehicles, ultimately generating the final driving decisions. PRIMEDrive-CoT specifically integrates uncertainty and risk-aware reasoning for interactive object learning, generating human-aligned decision explanations through risk prediction and uncertainty quantification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib102" title="">102</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.SS2.SSS0.Px4.p3">
<p class="ltx_p" id="S3.SS2.SSS0.Px4.p3.1">Beyond these, DriveLM implements graph-structured reasoning via Graph Visual Question Answering (GVQA), organizing QA pairs into directed acyclic graphs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib100" title="">100</a>]</cite>. Its decision-making pipeline follows a chained workflow, Perception →
Prediction → Planning → Behavior → Motion, where each QA pair incorporates parent-node context to explicitly propagate reasoning-chain information, mimicking human cognitive processes. What’s more, WiseAD and LMDrive adopt an implicit CoT design, which systematically integrates structured CoT reasoning traces into LLMs training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib42" title="">42</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib94" title="">94</a>]</cite>. ORION introduces a vision-language instructed action generation paradigm, utilizing QT-Former for scene modeling and employing a generative planner to map semantic information from LLMs into executable trajectories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib99" title="">99</a>]</cite>. It bridges semantic and numerical spaces via generative models. Senna combines the commonsense reasoning of VLMs with motion intention prediction and meta-action planning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib95" title="">95</a>]</cite>. Leveraging meta-action features as high-level guidance, it facilitates accurate reasoning. See2DriveX reconstructs the inherent implicit cognitive chain of human driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib101" title="">101</a>]</cite>. It enables step-by-step deduction from scene understanding, meta-action reasoning, behavior interpretation analysis, motion planning and control, narrowing the gap between autonomous driving and human thought processes. LangCoop uses language as an intermediate representation for collaborative driving, enabling step-by-step sharing of reasoning states across vehicles <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib103" title="">103</a>]</cite>. Each vehicle derives preliminary driving intentions via CoT reasoning. The LangPack module then packages and shares knowledge, ultimately feeding back to individual vehicles’ motion planning.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">IV </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Datasets</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">Open-source datasets serve as essential resources that fuel the innovation in autonomous driving, furnishing extensive data for system training, testing, and validation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib111" title="">111</a>]</cite>. With technological evolution, dataset construction has progressively evolved beyond focusing solely on perception and behavior to integrate semantic cognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib112" title="">112</a>]</cite>. As a fundamental medium of human intelligence, linguistic elements are increasingly integrated to enhance models’ reasoning capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib113" title="">113</a>]</cite>. Training on large-scale datasets, enriched with semantic reasoning information, provides a transformative pathway for developing autonomous agents with human-like cognitive capabilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib114" title="">114</a>]</cite>. Recent advancements demonstrate the growing implementation of explicit reasoning representations, particularly CoT mechanisms, which show promising potential in modeling complex decision-making processes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib112" title="">112</a>]</cite>. This section provides a systematic review of foundational autonomous driving datasets, cognition-augmented datasets, and relevant evaluation metrics. It is followed by a comparative analysis of their characteristics, summarized in the table below.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table">TABLE II: </span>Autonomous Driving Datasets Incorporating Cognitive Intelligence</figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T2.1">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T2.1.1.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.1.1">Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_th_row ltx_border_tt" id="S4.T2.1.1.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.2.1">Year</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.3.1">Source</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.4.1">Cognitive Data Type</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.5.1">Size</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T2.1.1.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.1.1.6.1">Tasks</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.1.2.1">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.2.1.1.1">Talk2Car <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib115" title="">115</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_t" id="S4.T2.1.2.1.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2019</th>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">nuScenes</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">VQA</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">850 videos, 11,959 commands</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.1.2.1.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.3.2">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.3.2.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.3.2.1.1">nuScenes-QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib116" title="">116</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.3.2.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">nuScenes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">VQA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">28K frames, 459,941 QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.3.2.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.4.3">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.4.3.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.4.3.1.1">Talk2BEV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib117" title="">117</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.4.3.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">nuScenes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">VQA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">1K BEV scenarios, 20K QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.4.3.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.5.4">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.5.4.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.5.4.1.1">NuScenes-MQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib118" title="">118</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.5.4.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">nuScenes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Markup-QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">34,149 scenarios, 1,459,933 QA pairs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.5.4.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.6.5">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.6.5.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.6.5.1.1">Reason2Drive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib25" title="">25</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.6.5.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">nuScenes, Waymo, ONCE</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">420K frames, 420K QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.6.5.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception, Prediction</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.7.6">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.7.6.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.7.6.1.1">DriveMLLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib119" title="">119</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.7.6.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">nuScenes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">880 frames, 4,666 QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.7.6.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.8.7">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.8.7.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.8.7.1.1">nuPrompt <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib120" title="">120</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.8.7.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2025</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">nuScenes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Language prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">850 videos, 35,367 prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.8.7.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.9.8">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.9.8.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.9.8.1.1">DriveLMM-o1 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib121" title="">121</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.9.8.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2025</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">nuScenes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Reasoning process</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">1,962 frames, 18K QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.9.8.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception, Planning</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.10.9">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.10.9.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.10.9.1.1">BDD-OIA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib122" title="">122</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.10.9.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2020</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">BDD100K</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Explanation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">22,924 clips, 35,366 explanations</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.10.9.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.11.10">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.11.10.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.11.10.1.1">BDD-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib123" title="">123</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.11.10.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2018</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.11.10.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">BDD100K</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.11.10.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Description, Explanation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.11.10.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">6,984 clips, 50,298 description</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.11.10.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Planning</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.12.11">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.12.11.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.12.11.1.1">DriveGPT4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib124" title="">124</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.12.11.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.12.11.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">BDD-X</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.12.11.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.12.11.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">16K QAs, 40K conversations</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.12.11.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Planning</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.13.12">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.13.12.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.13.12.1.1">Refer-KITTI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib125" title="">125</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.13.12.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.13.12.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">KITTI</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.13.12.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Language prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.13.12.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">18 videos, 818 expressions</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.13.12.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.14.13">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.14.13.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.14.13.1.1">WOMD-Reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib75" title="">75</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.14.13.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.14.13.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">WOMD</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.14.13.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">QA, Prediction</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.14.13.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">63k scenes, 3M QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.14.13.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Prediction</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.15.14">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.15.14.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.15.14.1.1">CityFlow-NL <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib126" title="">126</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.15.14.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2021</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.15.14.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">CityFlow</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.15.14.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Language prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.15.14.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">3,028 tracks, 5,289 prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.15.14.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.16.15">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.16.15.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.16.15.1.1">DRIVINGVQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib127" title="">127</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.16.15.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2025</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.16.15.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">Code de la Route</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.16.15.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">VQA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.16.15.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">3,142 frames, 3142 QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.16.15.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.17.16">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.17.16.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.17.16.1.1">Highway-Text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib77" title="">77</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.17.16.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2025</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.17.16.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">NGSIM, HighD</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.17.16.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Language prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.17.16.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">6,606 scenarios</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.17.16.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Prediction</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.18.17">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.18.17.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.18.17.1.1">Urban-Text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib77" title="">77</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.18.17.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2025</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.18.17.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">MoCAD, ApolloScape</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.18.17.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Language prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.18.17.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">5,431 samples</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.18.17.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Prediction</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.19.18">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.19.18.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.19.18.1.1">MAPLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib128" title="">128</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.19.18.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.19.18.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">THMA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.19.18.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Language prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.19.18.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">2M scenarios, 2M prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.19.18.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.20.19">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.20.19.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.20.19.1.1">MAPLM-QA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib128" title="">128</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.20.19.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.20.19.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">THMA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.20.19.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.20.19.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">14K scenarios, 61K QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.20.19.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.21.20">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.21.20.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.21.20.1.1">Rank2Tell <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib129" title="">129</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.21.20.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.21.20.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">Self-collected</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.21.20.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Chain VQA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.21.20.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">116 20-second clips</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.21.20.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.22.21">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.22.21.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.22.21.1.1">DRAMA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib130" title="">130</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.22.21.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.22.21.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">Self-collected</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.22.21.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Chain QA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.22.21.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">17,785 scenarios, 103K QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.22.21.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Planning</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.23.22">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.23.22.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.23.22.1.1">SUP-AD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib24" title="">24</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.23.22.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.23.22.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">Self-collected</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.23.22.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Language prompts</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.23.22.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">1,000 clips, 40+ categories</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.23.22.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Planning</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.24.23">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.24.23.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.24.23.1.1">LingoQA <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib131" title="">131</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.24.23.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.24.23.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">Self-collected</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.24.23.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">VQA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.24.23.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">28K videos, 419K annotations</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.24.23.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">Perception</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.25.24">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.25.24.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.25.24.1.1">DriveMLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib84" title="">84</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.25.24.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2023</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.25.24.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">CARLA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.25.24.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Decision, Explanation</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.25.24.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">50K routes, 30 scenarios</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.25.24.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">End-to-end</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.26.25">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.26.25.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.26.25.1.1">LMDrive <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib94" title="">94</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.26.25.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.26.25.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">CARLA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.26.25.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Navigation instructions</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.26.25.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">64K clips, 464K instructions</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.26.25.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">End-to-end</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.27.26">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.27.26.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.27.26.1.1">DriveLM <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib100" title="">100</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.27.26.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.27.26.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">CARLA, nuScenes</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.27.26.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Graph VQA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.27.26.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">4,063 frames, 377K QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.27.26.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">End-to-end</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.28.27">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row" id="S4.T2.1.28.27.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.28.27.1.1">DriveBench <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib114" title="">114</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row" id="S4.T2.1.28.27.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2025</th>
<td class="ltx_td ltx_align_center" id="S4.T2.1.28.27.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">DriveLM</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.28.27.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Graph VQA</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.28.27.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">19,200 frames, 20,498 QAs</td>
<td class="ltx_td ltx_align_center" id="S4.T2.1.28.27.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">End-to-end</td>
</tr>
<tr class="ltx_tr" id="S4.T2.1.29.28">
<th class="ltx_td ltx_align_left ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.29.28.1" style="padding-top:2.5pt;padding-bottom:2.5pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.1.29.28.1.1">DriveCoT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib18" title="">18</a>]</cite></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_row ltx_border_bb" id="S4.T2.1.29.28.2" style="padding-top:2.5pt;padding-bottom:2.5pt;">2024</th>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.29.28.3" style="padding-top:2.5pt;padding-bottom:2.5pt;">CARLA</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.29.28.4" style="padding-top:2.5pt;padding-bottom:2.5pt;">Reasoning process</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.29.28.5" style="padding-top:2.5pt;padding-bottom:2.5pt;">1,058 scenarios, 36K samples</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T2.1.29.28.6" style="padding-top:2.5pt;padding-bottom:2.5pt;">End-to-end</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS1.5.1.1">IV-A</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS1.6.2">Cognition-augmented autonomous driving datasets</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The mainstream autonomous driving datasets typically combine real-world images, videos, and complementary sensor inputs from LiDAR, Radar, GPS, and IMU, providing annotations for various driving scenarios. These fundamental autonomous driving datasets primarily establish the mapping relationship between sensor perception data and core autonomous driving tasks, spanning perception, prediction, and planning. KITTI stands as a pioneering dataset that offers annotations for fundamental tasks such as object detection, traffic flow estimation, depth prediction, and object tracking <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib132" title="">132</a>]</cite>. BDD100K contains more than 10,000 hours of driving video data, including image-level labels, object bounding boxes, drivable areas, lane markings, and full-frame instance segmentation annotations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib133" title="">133</a>]</cite>. As one of the most widely used public datasets, nuScenes provides 1,000 carefully curated urban driving scenes from Boston and Singapore, with a strong focus on addressing real-world long-tail distribution challenges <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib134" title="">134</a>]</cite>. The Waymo Open Dataset (WOMD) includes two sub-datasets: the perception dataset and the motion dataset, offering high-resolution sensor data, annotations, object trajectories, and corresponding 3D map information <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib135" title="">135</a>]</cite>. HighD is a commonly adopted highway dataset for trajectory prediction. The data is smoothed, has low noise, and is of high quality <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib136" title="">136</a>]</cite>. NGSIM specializes in traffic flow analysis, with a focus on congested urban scenarios characterized by low vehicle speeds and intricate inter-vehicle interactions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib137" title="">137</a>]</cite>. CityFlow is used to solve vehicle re-identification and pedestrian detection problems <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib138" title="">138</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">With the development of autonomous driving technology, there is an increasing demand for a deeper understanding of complex environments and improved model interpretability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib139" title="">139</a>]</cite>. This has driven researchers to urgently build new datasets and benchmarks to support the development and validation of relevant systems. Concurrently, the widespread application of LLMs and CoT has introduced new dimensions to autonomous driving datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib112" title="">112</a>]</cite>. Enhanced datasets are increasingly embedding cognition-augmented information, reflecting human-like decision-making processes, into existing multimodal foundational datasets. This integration further supports comprehensive scene understanding. Such datasets provide a solid conceptual basis for generating more coherent and human-like reasoning chains.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">Research has demonstrated that incorporating specialized reasoning knowledge into the training data of LLMs can substantially improve their CoT reasoning ability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib100" title="">100</a>]</cite>. Smaller models finetuned on datasets with reasoning data can elevate their accuracy to levels comparable to larger models, which have 1-2 times bigger parameter counts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib140" title="">140</a>]</cite>. Building upon this understanding of existing foundational datasets, this section aims to systematically analyze the current progress of cognition-augmented autonomous driving datasets.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">To incorporate cognitive knowledge into datasets, a typical technical approach involves incremental annotation based on existing datasets. For instance, Talk2Car, as the first autonomous driving dataset based on natural language prompts, was built upon the nuScenes dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib115" title="">115</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib134" title="">134</a>]</cite>. Its annotations include selected keyframes focusing on individual specific objects within single images. To overcome the single-object annotation constraint, NuPrompt further expanded to associate multiple targets by providing textual descriptions of the positions and states of surrounding objects, thereby offering scene understanding information and achieving improvements in 3D driving scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib120" title="">120</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">Beyond semantic prompt annotation, the construction of driving datasets incorporating QA pairs has become a key paradigm for embedding reasoning information. Serving as a critical connection between perception and cognition, Visual Question Answering (VQA) benchmarks validate and optimize the system’s understanding and reasoning in complex scenarios, and improve decision-making interpretability <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib141" title="">141</a>]</cite>. NuScenes-QA established the first VQA benchmark for autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib116" title="">116</a>]</cite>. The annotations include key objects that may impact driving decisions or summarize the motivations behind specific driving behaviors. Additionally, the dataset also includes multimodal, multi-frame temporal information encompassing dynamic objects and static environments. However, NuScenes-QA emphasizes partial perception reasoning with limited single-word responses, lacking comprehensive scene-level analysis and annotations required for complex reasoning. NuScenesMQA advances answer quality through structured full-sentence responses, offering richer semantic hierarchies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib118" title="">118</a>]</cite>. Single-step VQA cannot adequately model the multi-stage reasoning process of human drivers. To bridge this gap, Reason2Drive constructs a chain of question-answer pairs covering perception, prediction, and reasoning, combining nuScenes, Waymo, and ONCE datasets, to support explainable complex decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib25" title="">25</a>]</cite>. Talk2BEV, by integrating LLMs, VLMs, and Bird’s Eye View (BEV) representations, significantly improves reasoning performance without relying on additional training <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib117" title="">117</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib142" title="">142</a>]</cite>. To address the limitations of autonomous driving systems in complex spatial understanding, DriveMLLM introduces spatial reasoning question-answer tasks, including absolute spatial reasoning (e.g., object positioning) and relative position reasoning (e.g., relative distances or front-back relationships between objects) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib119" title="">119</a>]</cite>. These tasks are specifically designed to evaluate and optimize the spatial understanding capabilities of MLLMs in driving scenarios. DriveLMM-o1 overcomes the limitations of focusing solely on answer accuracy by synthesizing multimodal inputs and providing manually revised chains of thought, ensuring logical consistency in the reasoning process and transparency of intermediate stages <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib121" title="">121</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">In addition to extending the nuScenes dataset, researchers have also developed cognitively enhanced data on other foundational datasets. BDD-OIA is based on BDD100K and includes numerous specialized annotations for object detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib122" title="">122</a>]</cite>. BDD-X supplements driving conditions and decision-making rationale through textual descriptions, with a particular emphasis on extreme weather conditions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib123" title="">123</a>]</cite>. To alleviate the manual burden while capturing meaningful data, some studies have adopted large language models for preliminary annotation. DriveGPT4 combines object detection models with GPT technology to extend recognition tasks and text description functions on top of BDD-X <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib124" title="">124</a>]</cite>. Refer-KITTI expands the original KITTI dataset, focusing on multi-object tracking to adapt to time-varying scenes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib125" title="">125</a>]</cite>. The WOMD dataset adopts a fully automated data curation pipeline, first developing a rule-based program to interpret trajectory and high-definition map data, then using GPT-4 to generate interaction analysis and organizing the results into question-answer format <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib75" title="">75</a>]</cite>. To address potential quality issues with LLM-generated question-answer pairs, DRIVINGVQA extracts from authentic French driving theory exam samples, accompanied by expert annotations of relevant entities to answer questions and provide explanations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib127" title="">127</a>]</cite>. This results in a high-quality visual reasoning dataset grounded in real-world driving knowledge.</p>
</div>
<div class="ltx_para" id="S4.SS1.p7">
<p class="ltx_p" id="S4.SS1.p7.1">Apart from the aforementioned extensions of mainstream datasets, several research institutions and companies have collected and developed their own cognitive-enhanced datasets. The Honda DRAMA dataset innovatively presents a dual challenge of risk object localization and risk explanation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib130" title="">130</a>]</cite>. It uses the logical format of ”what-which-where-how-why” to understand the vehicle’s reaction to a single object as a question-answer chain, achieving object-level VQA. Rank2Tell provides object-level question answering for the perception module <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib129" title="">129</a>]</cite>. LingoQA adopts a free-form question-answer mechanism, expanding the scope of reasoning in autonomous driving to include nine distinct abilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib131" title="">131</a>]</cite>. To address safety-critical challenges, the Li Auto SUP-AD dataset tackles safety-critical challenges by mining long-tail data and applying precise annotations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib24" title="">24</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS1.p8">
<p class="ltx_p" id="S4.SS1.p8.1">Another significant strategy for constructing cognitive-enhanced datasets involves leveraging simulation environments for data collection, integrating cognitive enhancement elements during the data generation phase <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib143" title="">143</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib144" title="">144</a>]</cite>. DriveLM combines the nuScenes dataset and CARLA collected data, introducing GVQA to emulate the human-like multi-step reasoning process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib100" title="">100</a>]</cite>. However, it is limited to normal normal scenarios in nuScenes and CARLA. LMDrive uses expert agents with access to CARLA’s privileged information to collect data from more challenging cases, such as high-speed driving and lane changes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib94" title="">94</a>]</cite>. DriveMLM collects data from challenging driving scenarios featured in the CARLA Leaderboard 2.0 benchmark <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib84" title="">84</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib145" title="">145</a>]</cite>. Nevertheless, it only offers single-step reasoning explanations for driving decisions, lacking complete thought chain reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib84" title="">84</a>]</cite>. The DriveCoT dataset is specifically designed to support CoT reasoning, which not only provides sensor data and decision control information but also annotates the full reasoning process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib18" title="">18</a>]</cite>. It establishes a robust benchmark for evaluating the accuracy and interpretability of multi-step reasoning models in autonomous driving.</p>
</div>
<div class="ltx_para" id="S4.SS1.p9">
<p class="ltx_p" id="S4.SS1.p9.1">In summary, the development of current cognitive-enhanced autonomous driving datasets shows an evolutionary trend from single-object to multi-object, from short answers to complete sentence structures, and from single-step reasoning to multi-step chained reasoning. Through the construction of these diversified and targeted datasets, CoT reasoning models can acquire richer training data, enhancing the autonomous driving system’s semantic understanding of complex scenarios, revealing the causal logic behind driving decisions, and advancing autonomous driving technology from perceptual intelligence to cognitive intelligence.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S4.SS2.5.1.1">IV-B</span> </span><span class="ltx_text ltx_font_italic" id="S4.SS2.6.2">Evaluation Metrics</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">A comprehensive evaluation of Chain-of-Thought in autonomous driving necessitates multi-dimensional quantitative metrics, encompassing both reasoning quality and task performance. These complementary metrics collectively assess not only how CoT enhances model capabilities but also whether such theoretical enhancements materialize in operational scenarios. Reasoning metrics capture the model’s evolving cognitive processes under CoT, while performance metrics validate the real-world efficacy of these cognitive advancements in driving scenarios <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib146" title="">146</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">Reasoning metrics focus on quantifying the optimization effect of CoT on the upstream reasoning process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib147" title="">147</a>]</cite>. They examine whether the model can generate logically coherent and causally correct decision-making grounds through step-by-step reasoning. Works like Nuscenes-QA, Drama, and Talk2bev directly applied NLP metrics such as BLEU, CIDEr, and METEOR scores to evaluate reasoning outputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib148" title="">148</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib149" title="">149</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib150" title="">150</a>]</cite>. However, these metrics primarily assess text generation quality from a global perspective, failing to account for causal relationships between reasoning steps or their alignment with final decisions in driving scenarios. As a result, their effectiveness in evaluating reasoning remains limited.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1">RDA-Driver, on the other hand, explicitly examines the alignment between reasoning and driving decisions, with some examples of CoT reasoning inconsistencies with subsequent decisions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib81" title="">81</a>]</cite>. For instance, analyses of LLaVA’s CoT scores (higher is better) versus trajectory prediction errors (lower is better) demonstrate mismatches between reasoning and planning outcomes <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib151" title="">151</a>]</cite>. Although the model correctly reasoned the current state of the scene, such as recognizing that a vehicle ahead is stationary, the decision-making process followed an incorrect plan, directing the car to move forward. Furthermore, LingoQA introduced Lingo-Judge, a new evaluation metric inspired by GPT-Judge. This learning-based text classifier evaluates answer correctness and correlates model outputs with human preferences, offering a more nuanced measure of reasoning quality in autonomous driving <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib131" title="">131</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S4.F6"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="206" id="S4.F6.g1" src="x6.png" width="885"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure">Figure 6: </span>Current autonomous driving research primarily employs Direct CoT and Imitation CoT for basic reasoning tasks, while emerging Reinforcement CoT approaches enable self-evolving reasoning capabilities that could lead to breakthrough ”Aha Moments” in driving intelligence.</figcaption>
</figure>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1">To better address the unique requirements of autonomous driving tasks and provide a more comprehensive evaluation framework, Reason2Drive introduces ADRScore <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib25" title="">25</a>]</cite>. This benchmark is specifically designed to assess the quality of CoT reasoning in autonomous driving systems. It comprehensively evaluates autonomous driving reasoning alignment, redundancy, and missing steps, addressing the reasoning ambiguity issues present in existing metrics such as BLEU and CIDEr. Reasoning alignment (RA) measures the semantic similarity between the hypothesized reasoning and reference steps, redundancy (RD) detects unnecessary steps in the reasoning chain, and missing steps (MS) identifies critical missing reasoning steps. Additionally, ADRScore is extended to ADRScore-S, which adapts to tasks that include visual elements and uses the mean squared error of perception elements to rigorously assess the quality of visual reasoning. Overall, ADRScore provides a comprehensive and accurate method to evaluate the quality of reasoning chains in autonomous driving, particularly when handling complex spatial reasoning and decision-making <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib152" title="">152</a>]</cite>. This is a newly designed aggregation evaluation metric. DriveLMM-o1, inspired by VRC-Bench, systematically evaluates the accuracy of logical explanations generated by the model and the final predictions from seven dimensions: risk assessment accuracy, traffic rule adherence, scene awareness and object understanding, relevance, and missing details <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib121" title="">121</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1">Performance metrics quantify the ultimate enhancement in driving performance attributable to CoT. In prediction tasks, average displacement error (ADE) and final displacement error (FDE) are typically used to evaluate the enhancement of CoT in prediction accuracy <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib77" title="">77</a>]</cite>. For comprehensive closed-loop evaluation in CARLA simulations, three key metrics are employed: the Driving Score (DS = RC/(1-IP)), which combines route completion rate (RC) with weighted infraction penalties (IP); Route Completion (RC) measuring the percentage of the predefined route successfully navigated (0-100%); and Time Consumption (TC) recording total operation time until completion or failure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib100" title="">100</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib143" title="">143</a>]</cite>. In open-loop testing scenarios, performance is additionally assessed through L2 trajectory error, which quantifies the deviation from ground truth trajectories at 1s, 2s, and 3s intervals, and collision rate (CR), which measures the frequency of incidents <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib116" title="">116</a>]</cite>. Together, these metrics form a rigorous framework for evaluating the impact of CoT on autonomous driving performance.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1">This integrated evaluation framework, combining both reasoning metrics and performance metrics, enables a comprehensive assessment of CoT-guided autonomous driving systems. By simultaneously examining the system’s cognitive reasoning capability and its operational effectiveness in driving scenarios, this approach establishes a robust scientific basis for the systematic optimization of autonomous driving.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">V </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Discussion</span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">In this section, we propose that the application of Chain-of-Thought (CoT) in autonomous driving has evolved through three distinct stages.
The first stage is Direct CoT, which leverages the intrinsic reasoning capabilities of foundational large models without requiring additional training or fine-tuning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib97" title="">97</a>]</cite>. By designing specific prompts, a fixed reasoning workflow is established for driving tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib41" title="">41</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib87" title="">87</a>]</cite>. At each step of reasoning, domain-specific knowledge and typical examples are systematically incorporated, enabling the system to reason step-by-step before generating the final command.</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1">The next two stages involve fine-tuning parameters to endow the model with deeper reasoning abilities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib140" title="">140</a>]</cite>. The second stage, Imitation CoT, involves supervised fine-tuning (SFT) of pre-trained models on driving datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib18" title="">18</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib42" title="">42</a>]</cite>. First, human thought semantics or the explicit reasoning chains from the first method are recorded, preserving the thought trajectories during driving to form an enhanced autonomous driving dataset, as described in Chapter IV <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib84" title="">84</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib94" title="">94</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib100" title="">100</a>]</cite>. Triplets such as ¡driving scenario, reasoning chain, driving action¿ are used as labeled data to fine-tune the large model, enabling it to replicate high-quality reasoning processes in similar scenarios.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">The third stage is Reinforcement CoT, which draws inspiration from the technical approach of the language model DeepSeek-R1, combining reinforcement learning (RL) with reasoning techniques. During GRPO training, DeepSeek-R1 experienced an ”Aha Moment”, achieving the emergence of long reasoning chains while significantly reducing data requirements <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib16" title="">16</a>]</cite>. This demonstrates the potential of RL in transitioning models from imitation intelligence to emergent intelligence. Reinforcement CoT adopts a two-step training process <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib57" title="">57</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib74" title="">74</a>]</cite>. In Step One, it resembles Imitation CoT, employing supervised fine-tuning for knowledge transfer and establishing a preliminary reasoning foundation. In Step Two, reinforcement learning is introduced to enable policy self-evolution, building upon the supervised fine-tuning. Through trial-and-error and reward-based guidance, the model’s outputs are refined, mitigating instability from Step One. The system autonomously identifies reasoning flaws and seeks optimized pathways, forming a self-reflective reasoning chain that may trigger an ”Aha Moment” in autonomous driving.</p>
</div>
<div class="ltx_para" id="S5.p4">
<p class="ltx_p" id="S5.p4.1">Currently, most research relies on the first two stages, Direct CoT and Imitation CoT. These methods primarily leverage pre-trained models and simple SFT, with limited exploration into further optimization of reasoning strategies. A few emerging studies have introduced Reinforcement CoT, combining SFT and RL to develop autonomous driving reasoning frameworks that promote model self-evolution.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VI </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Challenges and Future Directions</span>
</h2>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS1.5.1.1">VI-A</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS1.6.2">Challenges for CoT in autonomous driving</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">While the application of CoT reasoning in autonomous driving is promising, it still faces several critical challenges in practice. Based on the review of current research, these challenges can be summarized across four key dimensions:</p>
</div>
<div class="ltx_para" id="S6.SS1.p2">
<p class="ltx_p" id="S6.SS1.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p2.1.1">Cross-modal gap:</span>
Under the CoT framework, autonomous driving systems typically rely on multiple modalities such as vision, language, and motion. However, discrepancies exist between visual and semantic features at the perceptual level, leading to inter-modal semantic deviations in scene understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib81" title="">81</a>]</cite>. In complex driving scenarios, current models struggle to accurately map key image details to high-quality linguistic descriptions. Furthermore, in the reasoning chain from semantic comprehension to action execution, multi-step reasoning errors accumulate as CoT unfolds, resulting in significant deviations in final actions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib153" title="">153</a>]</cite>. Bridging the cross-modal gap represents a fundamental requirement for developing trustworthy autonomous systems.</p>
</div>
<div class="ltx_para" id="S6.SS1.p3">
<p class="ltx_p" id="S6.SS1.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.1">Alignment with human cognition:</span>
CoT attempts to simulate human thought processes to progressively accomplish reasoning tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib154" title="">154</a>]</cite>. However, human reasoning relies on commonsense knowledge, domain expertise, and contextual understanding. These factors are difficult to model through simple rules or purely data-driven methods, leading to misalignment between generated reasoning chains and human cognition <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib155" title="">155</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib156" title="">156</a>]</cite>. Ensuring that CoT reasoning aligns with human thought processes to enhance system interpretability and reliability remains an urgent problem to address.
<span class="ltx_text ltx_font_bold" id="S6.SS1.p3.1.2">Balancing reasoning depth and speed:</span>
CoT reasoning depends on a series of intermediate steps, which are at odds with the stringent real-time requirements of autonomous driving. Current large-scale pre-trained models suffer from high latency and substantial computational costs, rendering them impractical for deployment on embedded, resource-constrained platforms. Compressing the reasoning chain and computational load, without compromising decision quality, coheres as a persistent obstacle for CoT deployment <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib157" title="">157</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS1.p4">
<p class="ltx_p" id="S6.SS1.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.p4.1.1">Safety verification:</span>
Despite its enhanced interpretability, CoT reasoning introduces uncertainty through extended reasoning chains. In corner cases, systems may produce logically coherent but factually incorrect outputs, known as large model hallucination, resulting in misleading decisions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib158" title="">158</a>]</cite>. In safety-critical tasks like autonomous driving, even minor errors can have severe consequences. The absence of robust mechanisms for dynamic risk evaluation and intervention within the CoT process further complicates reliable deployment.</p>
</div>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection"><span class="ltx_text" id="S6.SS2.5.1.1">VI-B</span> </span><span class="ltx_text ltx_font_italic" id="S6.SS2.6.2">Future Directions</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p1.1.1">Interference Mechanism:</span>
In complex reasoning tasks for autonomous driving, the lack of self-correction mechanisms may lead to the gradual amplification of single-step errors in chained reasoning structures. It is advisable to introduce ”interference mechanisms” during both training and testing phases. During training, ”adversarial QA interference samples” can be incorporated by systematically constructing training cases with prompt biases, semantic ambiguities, and noise interference to enhance the model’s ability to recognize and handle abnormal inputs. Concurrently, dynamic interference testing should be conducted during the validation phase to evaluate the model’s anti-interference performance in simulated driving scenarios.</p>
</div>
<div class="ltx_para" id="S6.SS2.p2">
<p class="ltx_p" id="S6.SS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p2.1.1">Advanced CoT:</span>
Currently, the autonomous driving field primarily relies on Chain-of-Thought for multi-step reasoning, but its computational overhead is substantial. Recent studies have proposed explicitly compact CoT variants, such as Chain-of-Draft, which significantly improve reasoning efficiency <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib157" title="">157</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib17" title="">17</a>]</cite>. Additionally, implicitly latent CoT variants achieve low-latency responses while maintaining decision-making accuracy by substantially reducing textual volume and employing latent states to compress high-level semantic information, making them suitable for autonomous driving applications <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib159" title="">159</a>]</cite> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib160" title="">160</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S6.SS2.p3">
<p class="ltx_p" id="S6.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.p3.1.1">Collaborative Fast and Slow Thinking:</span>
Inspired by research on human neural mechanisms, some studies adopt Collaborative Slow and Fast-thinking Systems, which combine rapid intuitive reasoning with in-depth logical analysis <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2505.20223v1#bib.bib110" title="">110</a>]</cite>. This approach dynamically allocates computational resources in autonomous driving systems to achieve a balance between reasoning depth and real-time performance.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">VII </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Conclusion</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">This survey provides a comprehensive overview of Chain-of-Thought reasoning in autonomous driving. It encompasses the background, method classification, relevant datasets, key challenges, and future directions. The article thoroughly reviews and analyzes the application of existing CoT in various tasks of autonomous driving, including perception understanding, prediction planning, decision-making control, and end-to-end autonomous driving. The paper further categorizes CoT-based reasoning in autonomous driving according to model structures into modular CoT, logical CoT, and reflective CoT. It summarizes the available reasoning driving datasets in the field so far and elaborates on the advantages of incorporating CoT into the domain. Based on current research, the paper also discusses existing and future challenges. Some potential solutions and future exploration directions are also proposed, such as combining CoT with self-learning methods. The aim is to spotlight CoT in autonomous driving and foster deeper research in this field.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock">
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” <em class="ltx_emph ltx_font_italic" id="bib.bib1.1.1">Advances in neural information processing systems</em>, vol. 30, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock">
T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell <em class="ltx_emph ltx_font_italic" id="bib.bib2.1.1">et al.</em>, “Language models are few-shot learners,” <em class="ltx_emph ltx_font_italic" id="bib.bib2.2.2">Advances in neural information processing systems</em>, vol. 33, pp. 1877–1901, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock">
P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, “Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing,” <em class="ltx_emph ltx_font_italic" id="bib.bib3.1.1">ACM computing surveys</em>, vol. 55, no. 9, pp. 1–35, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock">
H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E. Hambro, F. Azhar <em class="ltx_emph ltx_font_italic" id="bib.bib4.1.1">et al.</em>, “Llama: Open and efficient foundation language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib4.2.2">arXiv preprint arXiv:2302.13971</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock">
L. Zhao, L. Zhang, Z. Wu, Y. Chen, H. Dai, X. Yu, Z. Liu, T. Zhang, X. Hu, X. Jiang <em class="ltx_emph ltx_font_italic" id="bib.bib5.1.1">et al.</em>, “When brain-inspired ai meets agi,” <em class="ltx_emph ltx_font_italic" id="bib.bib5.2.2">Meta-Radiology</em>, vol. 1, no. 1, p. 100005, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock">
B. R. Kiran, I. Sobh, V. Talpaert, P. Mannion, A. A. Al Sallab, S. Yogamani, and P. Pérez, “Deep reinforcement learning for autonomous driving: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib6.1.1">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 23, no. 6, pp. 4909–4926, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock">
J. Wu, B. Gao, J. Gao, J. Yu, H. Chu, Q. Yu, X. Gong, Y. Chang, H. E. Tseng, H. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib7.1.1">et al.</em>, “Prospective role of foundation models in advancing autonomous vehicles,” <em class="ltx_emph ltx_font_italic" id="bib.bib7.2.2">Research</em>, vol. 7, p. 0399, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock">
C. Cui, Y. Ma, Z. Yang, Y. Zhou, P. Liu, J. Lu, L. Li, Y. Chen, J. H. Panchal, A. Abdelraouf <em class="ltx_emph ltx_font_italic" id="bib.bib8.1.1">et al.</em>, “Large language models for autonomous driving (llm4ad): Concept, benchmark, simulation, and real-vehicle experiment,” <em class="ltx_emph ltx_font_italic" id="bib.bib8.2.2">arXiv preprint arXiv:2410.15281</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock">
C. Cui, Y. Ma, X. Cao, W. Ye, Y. Zhou, K. Liang, J. Chen, J. Lu, Z. Yang, K.-D. Liao <em class="ltx_emph ltx_font_italic" id="bib.bib9.1.1">et al.</em>, “A survey on multimodal large language models for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib9.2.2">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2024, pp. 958–979.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock">
R. Zhao, Y. Li, Y. Fan, F. Gao, M. Tsukada, and Z. Gao, “A survey on recent advancements in autonomous driving using deep reinforcement learning: Applications, challenges, and solutions,” <em class="ltx_emph ltx_font_italic" id="bib.bib10.1.1">IEEE Transactions on Intelligent Transportation Systems</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock">
L. Chen, P. Wu, K. Chitta, B. Jaeger, A. Geiger, and H. Li, “End-to-end autonomous driving: Challenges and frontiers,” <em class="ltx_emph ltx_font_italic" id="bib.bib11.1.1">IEEE Transactions on Pattern Analysis and Machine Intelligence</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock">
J. Huang and K. C.-C. Chang, “Towards reasoning in large language models: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib12.1.1">arXiv preprint arXiv:2212.10403</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock">
J. Wei, X. Wang, D. Schuurmans, M. Bosma, F. Xia, E. Chi, Q. V. Le, D. Zhou <em class="ltx_emph ltx_font_italic" id="bib.bib13.1.1">et al.</em>, “Chain-of-thought prompting elicits reasoning in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib13.2.2">Advances in neural information processing systems</em>, vol. 35, pp. 24 824–24 837, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock">
V. Xiang, C. Snell, K. Gandhi, A. Albalak, A. Singh, C. Blagden, D. Phung, R. Rafailov, N. Lile, D. Mahan <em class="ltx_emph ltx_font_italic" id="bib.bib14.1.1">et al.</em>, “Towards system 2 reasoning in llms: Learning how to think with meta chain-of-though,” <em class="ltx_emph ltx_font_italic" id="bib.bib14.2.2">arXiv preprint arXiv:2501.04682</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock">
A. Jaech, A. Kalai, A. Lerer, A. Richardson, A. El-Kishky, A. Low, A. Helyar, A. Madry, A. Beutel, A. Carney <em class="ltx_emph ltx_font_italic" id="bib.bib15.1.1">et al.</em>, “Openai o1 system card,” <em class="ltx_emph ltx_font_italic" id="bib.bib15.2.2">arXiv preprint arXiv:2412.16720</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock">
D. Guo, D. Yang, H. Zhang, J. Song, R. Zhang, R. Xu, Q. Zhu, S. Ma, P. Wang, X. Bi <em class="ltx_emph ltx_font_italic" id="bib.bib16.1.1">et al.</em>, “Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib16.2.2">arXiv preprint arXiv:2501.12948</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock">
K. Manas, S. Zwicklbauer, and A. Paschke, “Cot-tl: Low-resource temporal knowledge representation of planning instructions using chain-of-thought reasoning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib17.1.1">2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>.   IEEE, 2024, pp. 9636–9643.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock">
T. Wang, E. Xie, R. Chu, Z. Li, and P. Luo, “Drivecot: Integrating chain-of-thought reasoning with end-to-end driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib18.1.1">arXiv preprint arXiv:2403.16996</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock">
J. Liu, Y. Wang, J. Du, J. T. Zhou, and Z. Liu, “Medcot: Medical chain of thought via hierarchical expert,” <em class="ltx_emph ltx_font_italic" id="bib.bib19.1.1">arXiv preprint arXiv:2412.13736</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock">
Y. Deng, X. Zhang, D. Zhou, D. Zhang, and B. Huang, “Leveraging nlp in finance: A synergistic approach using large language models and chain-of-thought reasoning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib20.1.1">Proceedings of the 5th International Conference on Artificial Intelligence and Computer Engineering</em>, 2024, pp. 494–500.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock">
Q. Chen, L. Qin, J. Zhang, Z. Chen, X. Xu, and W. Che, “M<sup class="ltx_sup" id="bib.bib21.2.1"><span class="ltx_text ltx_font_italic" id="bib.bib21.2.1.1">3</span></sup> cot: A novel benchmark for multi-domain multi-step multi-modal chain-of-thought,” <em class="ltx_emph ltx_font_italic" id="bib.bib21.3.2">arXiv preprint arXiv:2405.16473</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock">
D. Fu, X. Li, L. Wen, M. Dou, P. Cai, B. Shi, and Y. Qiao, “Drive like a human: Rethinking autonomous driving with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib22.1.1">2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW)</em>.   IEEE, 2024, pp. 910–919.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock">
Y. Ma, X. Cao, W. Ye, C. Cui, K. Mei, and Z. Wang, “Learning autonomous driving tasks via human feedbacks with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib23.1.1">Findings of the Association for Computational Linguistics: EMNLP 2024</em>, 2024, pp. 4985–4995.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock">
X. Tian, J. Gu, B. Li, Y. Liu, Y. Wang, Z. Zhao, K. Zhan, P. Jia, X. Lang, and H. Zhao, “Drivevlm: The convergence of autonomous driving and large vision-language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib24.1.1">arXiv preprint arXiv:2402.12289</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock">
M. Nie, R. Peng, C. Wang, X. Cai, J. Han, H. Xu, and L. Zhang, “Reason2drive: Towards interpretable and chain-based reasoning for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib25.1.1">European Conference on Computer Vision</em>.   Springer, 2024, pp. 292–308.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock">
Z. Yang, X. Jia, H. Li, and J. Yan, “Llm4drive: A survey of large language models for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib26.1.1">arXiv preprint arXiv:2311.01043</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock">
H. Gao, Z. Wang, Y. Li, K. Long, M. Yang, and Y. Shen, “A survey for foundation models in autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib27.1.1">arXiv preprint arXiv:2402.01105</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock">
X. Zhou, M. Liu, E. Yurtsever, B. L. Zagar, W. Zimmer, H. Cao, and A. C. Knoll, “Vision language models in autonomous driving: A survey and outlook,” <em class="ltx_emph ltx_font_italic" id="bib.bib28.1.1">IEEE Transactions on Intelligent Vehicles</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock">
Q. Chen, L. Qin, J. Liu, D. Peng, J. Guan, P. Wang, M. Hu, Y. Zhou, T. Gao, and W. Che, “Towards reasoning era: A survey of long chain-of-thought for reasoning large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib29.1.1">arXiv preprint arXiv:2503.09567</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock">
Z. Yu, L. He, Z. Wu, X. Dai, and J. Chen, “Towards better chain-of-thought prompting strategies: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib30.1.1">arXiv preprint arXiv:2310.04959</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock">
M. O’Kelly, A. Sinha, H. Namkoong, R. Tedrake, and J. C. Duchi, “Scalable end-to-end autonomous vehicle testing via rare-event simulation,” <em class="ltx_emph ltx_font_italic" id="bib.bib31.1.1">Advances in neural information processing systems</em>, vol. 31, 2018.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock">
Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib32.1.1">et al.</em>, “Planning-oriented autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib32.2.2">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2023, pp. 17 853–17 862.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock">
Z. Zhang, A. Liniger, D. Dai, F. Yu, and L. Van Gool, “End-to-end urban driving by imitating a reinforcement learning coach,” in <em class="ltx_emph ltx_font_italic" id="bib.bib33.1.1">Proceedings of the IEEE/CVF international conference on computer vision</em>, 2021, pp. 15 222–15 232.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock">
Y. Pan, C.-A. Cheng, K. Saigol, K. Lee, X. Yan, E. A. Theodorou, and B. Boots, “Imitation learning for agile autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib34.1.1">The International Journal of Robotics Research</em>, vol. 39, no. 2-3, pp. 286–302, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock">
Y. Cui, S. Yang, C. Wan, X. Li, J. Xing, Y. Zhang, Y. Huang, and H. Chen, “Sustainable adaptation for autonomous driving with the mixture of progressive experts networ,” <em class="ltx_emph ltx_font_italic" id="bib.bib35.1.1">arXiv preprint arXiv:2502.05943</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock">
X. Li, Y. Bai, P. Cai, L. Wen, D. Fu, B. Zhang, X. Yang, X. Cai, T. Ma, J. Guo <em class="ltx_emph ltx_font_italic" id="bib.bib36.1.1">et al.</em>, “Towards knowledge-driven autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib36.2.2">arXiv preprint arXiv:2312.04316</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock">
M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in empirical observations and microscopic simulations,” <em class="ltx_emph ltx_font_italic" id="bib.bib37.1.1">Physical review E</em>, vol. 62, no. 2, p. 1805, 2000.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock">
W. Xiao, N. Mehdipour, A. Collin, A. Y. Bin-Nun, E. Frazzoli, R. D. Tebbens, and C. Belta, “Rule-based optimal control for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib38.1.1">Proceedings of the ACM/IEEE 12th International Conference on Cyber-Physical Systems</em>, 2021, pp. 143–154.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock">
Y. Shan, B. Zheng, L. Chen, L. Chen, and D. Chen, “A reinforcement learning-based adaptive path tracking approach for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib39.1.1">IEEE Transactions on Vehicular Technology</em>, vol. 69, no. 10, pp. 10 581–10 595, 2020.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock">
L. Wang, C. Fernandez, and C. Stiller, “High-level decision making for automated highway driving via behavior cloning,” <em class="ltx_emph ltx_font_italic" id="bib.bib40.1.1">IEEE Transactions on Intelligent Vehicles</em>, vol. 8, no. 1, pp. 923–935, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock">
L. Wen, D. Fu, X. Li, X. Cai, T. Ma, P. Cai, M. Dou, B. Shi, L. He, and Y. Qiao, “Dilu: A knowledge-driven approach to autonomous driving with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib41.1.1">arXiv preprint arXiv:2309.16292</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock">
S. Zhang, W. Huang, Z. Gao, H. Chen, and C. Lv, “Wisead: Knowledge augmented end-to-end autonomous driving with vision-language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib42.1.1">arXiv preprint arXiv:2412.09951</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock">
R. Brachman and H. Levesque, <em class="ltx_emph ltx_font_italic" id="bib.bib43.1.1">Knowledge representation and reasoning</em>.   Elsevier, 2004.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock">
B. Wang, S. Min, X. Deng, J. Shen, Y. Wu, L. Zettlemoyer, and H. Sun, “Towards understanding chain-of-thought prompting: An empirical study of what matters,” <em class="ltx_emph ltx_font_italic" id="bib.bib44.1.1">arXiv preprint arXiv:2212.10001</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock">
J. Xing, D. Wei, S. Zhou, T. Wang, Y. Huang, and H. Chen, “A comprehensive study on self-learning methods and implications to autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib45.1.1">IEEE Transactions on Neural Networks and Learning Systems</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock">
S. Tu, X. Zhou, D. Liang, X. Jiang, Y. Zhang, X. Li, and X. Bai, “The role of world models in shaping autonomous driving: A comprehensive survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib46.1.1">arXiv preprint arXiv:2502.10498</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock">
D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell, “Curiosity-driven exploration by self-supervised prediction,” in <em class="ltx_emph ltx_font_italic" id="bib.bib47.1.1">International conference on machine learning</em>.   PMLR, 2017, pp. 2778–2787.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock">
X. Zhang, K. Wang, T. Hu, and H. Ma, “Enhancing autonomous driving through dual-process learning with behavior and reflection integration,” in <em class="ltx_emph ltx_font_italic" id="bib.bib48.1.1">ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</em>.   IEEE, 2025, pp. 1–5.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock">
J. Mei, Y. Ma, X. Yang, L. Wen, X. Cai, X. Li, D. Fu, B. Zhang, P. Cai, M. Dou <em class="ltx_emph ltx_font_italic" id="bib.bib49.1.1">et al.</em>, “Continuously learning, adapting, and improving: A dual-process approach to autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib49.2.2">arXiv preprint arXiv:2405.15324</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock">
G. Team, R. Anil, S. Borgeaud, J.-B. Alayrac, J. Yu, R. Soricut, J. Schalkwyk, A. M. Dai, A. Hauth, K. Millican <em class="ltx_emph ltx_font_italic" id="bib.bib50.1.1">et al.</em>, “Gemini: a family of highly capable multimodal models,” <em class="ltx_emph ltx_font_italic" id="bib.bib50.2.2">arXiv preprint arXiv:2312.11805</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock">
A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford <em class="ltx_emph ltx_font_italic" id="bib.bib51.1.1">et al.</em>, “Gpt-4o system card,” <em class="ltx_emph ltx_font_italic" id="bib.bib51.2.2">arXiv preprint arXiv:2410.21276</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock">
A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan <em class="ltx_emph ltx_font_italic" id="bib.bib52.1.1">et al.</em>, “Deepseek-v3 technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib52.2.2">arXiv preprint arXiv:2412.19437</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock">
Z. Li, D. Liu, C. Zhang, H. Wang, T. Xue, and W. Cai, “Enhancing advanced visual reasoning ability of large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib53.1.1">arXiv preprint arXiv:2409.13980</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock">
D. Zhang, Y. Yu, J. Dong, C. Li, D. Su, C. Chu, and D. Yu, “Mm-llms: Recent advances in multimodal large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib54.1.1">arXiv preprint arXiv:2401.13601</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock">
Y. Li, Z. Lai, W. Bao, Z. Tan, A. Dao, K. Sui, J. Shen, D. Liu, H. Liu, and Y. Kong, “Visual large language models for generalized and specialized applications,” <em class="ltx_emph ltx_font_italic" id="bib.bib55.1.1">arXiv preprint arXiv:2501.02765</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock">
X. Zhou, X. Han, F. Yang, Y. Ma, and A. C. Knoll, “Opendrivevla: Towards end-to-end autonomous driving with large vision language action model,” <em class="ltx_emph ltx_font_italic" id="bib.bib56.1.1">arXiv preprint arXiv:2503.23463</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock">
B. Jiang, S. Chen, Q. Zhang, W. Liu, and X. Wang, “Alphadrive: Unleashing the power of vlms in autonomous driving via reinforcement learning and reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib57.1.1">arXiv preprint arXiv:2503.07608</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock">
X. Qu, Y. Li, Z. Su, W. Sun, J. Yan, D. Liu, G. Cui, D. Liu, S. Liang, J. He <em class="ltx_emph ltx_font_italic" id="bib.bib58.1.1">et al.</em>, “A survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond,” <em class="ltx_emph ltx_font_italic" id="bib.bib58.2.2">arXiv preprint arXiv:2503.21614</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock">
I. Dasgupta, A. K. Lampinen, S. C. Chan, H. R. Sheahan, A. Creswell, D. Kumaran, J. L. McClelland, and F. Hill, “Language models show human-like content effects on reasoning tasks,” <em class="ltx_emph ltx_font_italic" id="bib.bib59.1.1">arXiv preprint arXiv:2207.07051</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock">
Z. Sprague, F. Yin, J. D. Rodriguez, D. Jiang, M. Wadhwa, P. Singhal, X. Zhao, X. Ye, K. Mahowald, and G. Durrett, “To cot or not to cot? chain-of-thought helps mainly on math and symbolic reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib60.1.1">arXiv preprint arXiv:2409.12183</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock">
H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I. Sutskever, and K. Cobbe, “Let’s verify step by step,” in <em class="ltx_emph ltx_font_italic" id="bib.bib61.1.1">The Twelfth International Conference on Learning Representations</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock">
T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa, “Large language models are zero-shot reasoners,” <em class="ltx_emph ltx_font_italic" id="bib.bib62.1.1">Advances in neural information processing systems</em>, vol. 35, pp. 22 199–22 213, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock">
O. Shaikh, H. Zhang, W. Held, M. Bernstein, and D. Yang, “On second thought, let’s not think step by step! bias and toxicity in zero-shot reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib63.1.1">arXiv preprint arXiv:2212.08061</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock">
X. Zhang and D. Ding, “Supervised chain of thought,” <em class="ltx_emph ltx_font_italic" id="bib.bib64.1.1">arXiv preprint arXiv:2410.14198</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock">
X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou, “Self-consistency improves chain of thought reasoning in language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib65.1.1">arXiv preprint arXiv:2203.11171</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock">
S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan, “Tree of thoughts: Deliberate problem solving with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib66.1.1">Advances in neural information processing systems</em>, vol. 36, pp. 11 809–11 822, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock">
M. Besta, N. Blach, A. Kubicek, R. Gerstenberger, M. Podstawski, L. Gianinazzi, J. Gajda, T. Lehmann, H. Niewiadomski, P. Nyczyk <em class="ltx_emph ltx_font_italic" id="bib.bib67.1.1">et al.</em>, “Graph of thoughts: Solving elaborate problems with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib67.2.2">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 16, 2024, pp. 17 682–17 690.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock">
H. Shao, S. Qian, H. Xiao, G. Song, Z. Zong, L. Wang, Y. Liu, and H. Li, “Visual cot: Advancing multi-modal language models with a comprehensive dataset and benchmark for chain-of-thought reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib68.1.1">Advances in Neural Information Processing Systems</em>, vol. 37, pp. 8612–8642, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock">
Z. Zhang, A. Zhang, M. Li, H. Zhao, G. Karypis, and A. Smola, “Multimodal chain-of-thought reasoning in language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib69.1.1">arXiv preprint arXiv:2302.00923</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock">
J. Mao, J. Ye, Y. Qian, M. Pavone, and Y. Wang, “A language agent for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib70.1.1">arXiv preprint arXiv:2311.10813</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock">
Y. Ma, Y. Cao, J. Sun, M. Pavone, and C. Xiao, “Dolphins: Multimodal language model for driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib71.1.1">European Conference on Computer Vision</em>.   Springer, 2024, pp. 403–420.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock">
C. Corbière, S. Roburin, S. Montariol, A. Bosselut, and A. Alahi, “Retrieval-based interleaved visual chain-of-thought in real-world driving scenarios,” 2025. [Online]. Available: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://arxiv.org/abs/2501.04671" title="">https://arxiv.org/abs/2501.04671</a>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock">
X. Hou, W. Wang, L. Yang, H. Lin, J. Feng, H. Min, and X. Zhao, “Driveagent: Multi-agent structured reasoning with llm and multimodal sensor fusion for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib73.1.1">arXiv preprint arXiv:2505.02123</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock">
K. Qian, S. Jiang, Y. Zhong, Z. Luo, Z. Huang, T. Zhu, K. Jiang, M. Yang, Z. Fu, J. Miao <em class="ltx_emph ltx_font_italic" id="bib.bib74.1.1">et al.</em>, “Agentthink: A unified framework for tool-augmented chain-of-thought reasoning in vision-language models for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib74.2.2">arXiv preprint arXiv:2505.15298</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock">
Y. Li, C. Ge, C. Li, C. Xu, M. Tomizuka, C. Tang, M. Ding, and W. Zhan, “Womd-reasoning: A large-scale language dataset for interaction and driving intentions reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib75.1.1">arXiv preprint arXiv:2407.04281</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock">
M. Peng, X. Guo, X. Chen, K. Chen, M. Zhu, L. Chen, and F.-Y. Wang, “Lc-llm: Explainable lane-change intention and trajectory predictions with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib76.1.1">Communications in Transportation Research</em>, vol. 5, p. 100170, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock">
H. Liao, H. Kong, B. Wang, C. Wang, W. Ye, Z. He, C. Xu, and Z. Li, “Cot-drive: Efficient motion forecasting for autonomous driving with llms and chain-of-thought prompting,” <em class="ltx_emph ltx_font_italic" id="bib.bib77.1.1">arXiv preprint arXiv:2503.07234</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock">
X. Luo, C. Liu, F. Ding, F. Yang, Y. Zhou, J. Loo, and H. H. Tew, “Senserag: Constructing environmental knowledge bases with proactive querying for llm-based autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib78.1.1">Proceedings of the Winter Conference on Applications of Computer Vision</em>, 2025, pp. 989–996.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock">
J. Mao, Y. Qian, J. Ye, H. Zhao, and Y. Wang, “Gpt-driver: Learning to drive with gpt,” <em class="ltx_emph ltx_font_italic" id="bib.bib79.1.1">arXiv preprint arXiv:2310.01415</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock">
Y. Zheng, Z. Xing, Q. Zhang, B. Jin, P. Li, Y. Zheng, Z. Xia, K. Zhan, X. Lang, Y. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib80.1.1">et al.</em>, “Planagent: A multi-modal large language agent for closed-loop vehicle motion planning,” <em class="ltx_emph ltx_font_italic" id="bib.bib80.2.2">arXiv preprint arXiv:2406.01587</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock">
Z. Huang, T. Tang, S. Chen, S. Lin, Z. Jie, L. Ma, G. Wang, and X. Liang, “Making large language models better planners with reasoning-decision alignment,” in <em class="ltx_emph ltx_font_italic" id="bib.bib81.1.1">European Conference on Computer Vision</em>.   Springer, 2024, pp. 73–90.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock">
R. Yao, Y. Wang, H. Liu, R. Yang, Z. Peng, L. Zhu, and J. Ma, “Calmm-drive: Confidence-aware autonomous driving with large multimodal model,” <em class="ltx_emph ltx_font_italic" id="bib.bib82.1.1">arXiv preprint arXiv:2412.04209</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock">
H. Sha, Y. Mu, Y. Jiang, L. Chen, C. Xu, P. Luo, S. Li, M. Tomizuka, W. Zhan, and M. Ding, “Languagempc: Large language models as decision makers for autonomous driving. arxiv 2023,” <em class="ltx_emph ltx_font_italic" id="bib.bib83.1.1">arXiv preprint arXiv:2310.03026</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock">
W. Wang, J. Xie, C. Hu, H. Zou, J. Fan, W. Tong, Y. Wen, S. Wu, H. Deng, Z. Li <em class="ltx_emph ltx_font_italic" id="bib.bib84.1.1">et al.</em>, “Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib84.2.2">arXiv preprint arXiv:2312.09245</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock">
C. Cui, Y. Ma, X. Cao, W. Ye, and Z. Wang, “Receive, reason, and react: Drive as you say, with large language models in autonomous vehicles,” <em class="ltx_emph ltx_font_italic" id="bib.bib85.1.1">IEEE Intelligent Transportation Systems Magazine</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock">
Z. Zhou, H. Huang, B. Li, S. Zhao, Y. Mu, and J. Wang, “Safedrive: Knowledge-and data-driven risk-sensitive decision-making for autonomous vehicles with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib86.1.1">arXiv preprint arXiv:2412.13238</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock">
K. Jiang, X. Cai, Z. Cui, A. Li, Y. Ren, H. Yu, H. Yang, D. Fu, L. Wen, and P. Cai, “Koma: Knowledge-driven multi-agent framework for autonomous driving with large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib87.1.1">IEEE Transactions on Intelligent Vehicles</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock">
Y. Ma, T. Wei, N. Zhong, J. Mei, T. Hu, L. Wen, X. Yang, B. Shi, and Y. Liu, “Leapvad: A leap in autonomous driving via cognitive perception and dual-process thinking,” <em class="ltx_emph ltx_font_italic" id="bib.bib88.1.1">arXiv preprint arXiv:2501.08168</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock">
S. Fang, J. Liu, M. Ding, Y. Cui, C. Lv, P. Hang, and J. Sun, “Towards interactive and learnable cooperative driving automation: a large language model-driven decision-making framework,” <em class="ltx_emph ltx_font_italic" id="bib.bib89.1.1">IEEE Transactions on Vehicular Technology</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock">
S. Fang, J. Liu, C. Xu, C. Lv, P. Hang, and J. Sun, “Interact, instruct to improve: A llm-driven parallel actor-reasoner framework for enhancing autonomous vehicle interactions,” <em class="ltx_emph ltx_font_italic" id="bib.bib90.1.1">arXiv preprint arXiv:2503.00502</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock">
T. Ren, H. Hu, J. Zuo, X. Chen, J. Wang, C. J. Xue, J.-M. Wu, and N. Guan, “Cot-vlm4tar: Chain-of-thought guided vision-language models for traffic anomaly resolution,” <em class="ltx_emph ltx_font_italic" id="bib.bib91.1.1">arXiv preprint arXiv:2503.01632</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock">
L. Chen, O. Sinavski, J. Hünermann, A. Karnsund, A. J. Willmott, D. Birch, D. Maund, and J. Shotton, “Driving with llms: Fusing object-level vector modality for explainable autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib92.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 14 093–14 100.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock">
X. Luo, F. Ding, Y. Song, X. Zhang, and J. Loo, “Pkrd-cot: A unified chain-of-thought prompting for multi-modal large language models in autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib93.1.1">arXiv preprint arXiv:2412.02025</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock">
H. Shao, Y. Hu, L. Wang, G. Song, S. L. Waslander, Y. Liu, and H. Li, “Lmdrive: Closed-loop end-to-end driving with large language models,” in <em class="ltx_emph ltx_font_italic" id="bib.bib94.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 15 120–15 130.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock">
B. Jiang, S. Chen, B. Liao, X. Zhang, W. Yin, Q. Zhang, C. Huang, W. Liu, and X. Wang, “Senna: Bridging large vision-language models and end-to-end autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib95.1.1">arXiv preprint arXiv:2410.22313</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock">
J.-J. Hwang, R. Xu, H. Lin, W.-C. Hung, J. Ji, K. Choi, D. Huang, T. He, P. Covington, B. Sapp <em class="ltx_emph ltx_font_italic" id="bib.bib96.1.1">et al.</em>, “Emma: End-to-end multimodal model for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib96.2.2">arXiv preprint arXiv:2410.23262</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock">
S. Xing, C. Qian, Y. Wang, H. Hua, K. Tian, Y. Zhou, and Z. Tu, “Openemma: Open-source multimodal model for end-to-end autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib97.1.1">Proceedings of the Winter Conference on Applications of Computer Vision</em>, 2025, pp. 1001–1009.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock">
Z. Qiao, H. Li, Z. Cao, and H. X. Liu, “Lightemma: Lightweight end-to-end multimodal model for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib98.1.1">arXiv preprint arXiv:2505.00284</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock">
H. Fu, D. Zhang, Z. Zhao, J. Cui, D. Liang, C. Zhang, D. Zhang, H. Xie, B. Wang, and X. Bai, “Orion: A holistic end-to-end autonomous driving framework by vision-language instructed action generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib99.1.1">arXiv preprint arXiv:2503.19755</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock">
C. Sima, K. Renz, K. Chitta, L. Chen, H. Zhang, C. Xie, J. Beißwenger, P. Luo, A. Geiger, and H. Li, “Drivelm: Driving with graph visual question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib100.1.1">European Conference on Computer Vision</em>.   Springer, 2024, pp. 256–274.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock">
R. Zhao, Q. Yuan, J. Li, H. Hu, Y. Li, C. Zheng, and F. Gao, “Sce2drivex: A generalized mllm framework for scene-to-drive learning,” <em class="ltx_emph ltx_font_italic" id="bib.bib101.1.1">arXiv preprint arXiv:2502.14917</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock">
S. Mandalika, A. Nambiar <em class="ltx_emph ltx_font_italic" id="bib.bib102.1.1">et al.</em>, “Primedrive-cot: A precognitive chain-of-thought framework for uncertainty-aware object interaction in driving scene scenario,” <em class="ltx_emph ltx_font_italic" id="bib.bib102.2.2">arXiv preprint arXiv:2504.05908</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock">
X. Gao, Y. Wu, R. Wang, C. Liu, Y. Zhou, and Z. Tu, “Langcoop: Collaborative driving with language,” <em class="ltx_emph ltx_font_italic" id="bib.bib103.1.1">arXiv preprint arXiv:2504.13406</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock">
W. Liu, J. Zhang, B. Zheng, Y. Hu, Y. Lin, and Z. Zeng, “X-driver: Explainable autonomous driving with vision-language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib104.1.1">arXiv preprint arXiv:2505.05098</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock">
D. Zhou, N. Schärli, L. Hou, J. Wei, N. Scales, X. Wang, D. Schuurmans, C. Cui, O. Bousquet, Q. Le <em class="ltx_emph ltx_font_italic" id="bib.bib105.1.1">et al.</em>, “Least-to-most prompting enables complex reasoning in large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib105.2.2">arXiv preprint arXiv:2205.10625</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock">
J. Bai, S. Bai, Y. Chu, Z. Cui, K. Dang, X. Deng, Y. Fan, W. Ge, Y. Han, F. Huang <em class="ltx_emph ltx_font_italic" id="bib.bib106.1.1">et al.</em>, “Qwen technical report,” <em class="ltx_emph ltx_font_italic" id="bib.bib106.2.2">arXiv preprint arXiv:2309.16609</em>, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock">
J. Jiang, F. Wang, J. Shen, S. Kim, and S. Kim, “A survey on large language models for code generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib107.1.1">arXiv preprint arXiv:2406.00515</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock">
B. Kouvaritakis and M. Cannon, “Model predictive control,” <em class="ltx_emph ltx_font_italic" id="bib.bib108.1.1">Switzerland: Springer International Publishing</em>, vol. 38, no. 13-56, p. 7, 2016.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock">
J. S. B. Evans, “Heuristic and analytic processes in reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib109.1.1">British Journal of Psychology</em>, vol. 75, no. 4, pp. 451–468, 1984.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock">
Z.-Z. Li, D. Zhang, M.-L. Zhang, J. Zhang, Z. Liu, Y. Yao, H. Xu, J. Zheng, P.-J. Wang, X. Chen <em class="ltx_emph ltx_font_italic" id="bib.bib110.1.1">et al.</em>, “From system 1 to system 2: A survey of reasoning large language models,” <em class="ltx_emph ltx_font_italic" id="bib.bib110.2.2">arXiv preprint arXiv:2502.17419</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock">
W. Liu, Q. Dong, P. Wang, G. Yang, L. Meng, Y. Song, Y. Shi, and Y. Xue, “A survey on autonomous driving datasets,” in <em class="ltx_emph ltx_font_italic" id="bib.bib111.1.1">2021 8th International Conference on Dependable Systems and Their Applications (DSA)</em>.   IEEE, 2021, pp. 399–407.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock">
M. Liu, E. Yurtsever, J. Fossaert, X. Zhou, W. Zimmer, Y. Cui, B. L. Zagar, and A. C. Knoll, “A survey on autonomous driving datasets: Statistics, annotation quality, and a future outlook,” <em class="ltx_emph ltx_font_italic" id="bib.bib112.1.1">IEEE Transactions on Intelligent Vehicles</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock">
W. Li, C. Pan, R. Zhang, J. Ren, Y. Ma, J. Fang, F. Yan, Q. Geng, X. Huang, H. Gong <em class="ltx_emph ltx_font_italic" id="bib.bib113.1.1">et al.</em>, “Aads: Augmented autonomous driving simulation using data-driven algorithms,” <em class="ltx_emph ltx_font_italic" id="bib.bib113.2.2">Science robotics</em>, vol. 4, no. 28, p. eaaw0863, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock">
S. Xie, L. Kong, Y. Dong, C. Sima, W. Zhang, Q. A. Chen, Z. Liu, and L. Pan, “Are vlms ready for autonomous driving? an empirical study from the reliability, data, and metric perspectives,” <em class="ltx_emph ltx_font_italic" id="bib.bib114.1.1">arXiv preprint arXiv:2501.04003</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock">
T. Deruyttere, S. Vandenhende, D. Grujicic, L. Van Gool, and M.-F. Moens, “Talk2car: Taking control of your self-driving car,” <em class="ltx_emph ltx_font_italic" id="bib.bib115.1.1">arXiv preprint arXiv:1909.10838</em>, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock">
T. Qian, J. Chen, L. Zhuo, Y. Jiao, and Y.-G. Jiang, “Nuscenes-qa: A multi-modal visual question answering benchmark for autonomous driving scenario,” in <em class="ltx_emph ltx_font_italic" id="bib.bib116.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 5, 2024, pp. 4542–4550.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock">
T. Choudhary, V. Dewangan, S. Chandhok, S. Priyadarshan, A. Jain, A. K. Singh, S. Srivastava, K. M. Jatavallabhula, and K. M. Krishna, “Talk2bev: Language-enhanced bird’s-eye view maps for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib117.1.1">2024 IEEE International Conference on Robotics and Automation (ICRA)</em>.   IEEE, 2024, pp. 16 345–16 352.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock">
Y. Inoue, Y. Yada, K. Tanahashi, and Y. Yamaguchi, “Nuscenes-mqa: Integrated evaluation of captions and qa for autonomous driving datasets using markup annotations,” in <em class="ltx_emph ltx_font_italic" id="bib.bib118.1.1">Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision</em>, 2024, pp. 930–938.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock">
X. Guo, R. Zhang, Y. Duan, Y. He, C. Zhang, S. Liu, and L. Chen, “Drivemllm: A benchmark for spatial understanding with multimodal large language models in autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib119.1.1">arXiv preprint arXiv:2411.13112</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock">
D. Wu, W. Han, Y. Liu, T. Wang, C.-z. Xu, X. Zhang, and J. Shen, “Language prompt for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib120.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 39, no. 8, 2025, pp. 8359–8367.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock">
A. Ishaq, J. Lahoud, K. More, O. Thawakar, R. Thawkar, D. Dissanayake, N. Ahsan, Y. Li, F. S. Khan, H. Cholakkal <em class="ltx_emph ltx_font_italic" id="bib.bib121.1.1">et al.</em>, “Drivelmm-o1: A step-by-step reasoning dataset and large multimodal model for driving scenario understanding,” <em class="ltx_emph ltx_font_italic" id="bib.bib121.2.2">arXiv preprint arXiv:2503.10621</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock">
Y. Xu, X. Yang, L. Gong, H.-C. Lin, T.-Y. Wu, Y. Li, and N. Vasconcelos, “Explainable object-induced action decision for autonomous vehicles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib122.1.1">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2020, pp. 9523–9532.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock">
J. Kim, A. Rohrbach, T. Darrell, J. Canny, and Z. Akata, “Textual explanations for self-driving vehicles,” in <em class="ltx_emph ltx_font_italic" id="bib.bib123.1.1">Proceedings of the European conference on computer vision (ECCV)</em>, 2018, pp. 563–578.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock">
Z. Xu, Y. Zhang, E. Xie, Z. Zhao, Y. Guo, K.-Y. K. Wong, Z. Li, and H. Zhao, “Drivegpt4: Interpretable end-to-end autonomous driving via large language model,” <em class="ltx_emph ltx_font_italic" id="bib.bib124.1.1">IEEE Robotics and Automation Letters</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock">
D. Wu, W. Han, T. Wang, X. Dong, X. Zhang, and J. Shen, “Referring multi-object tracking,” in <em class="ltx_emph ltx_font_italic" id="bib.bib125.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2023, pp. 14 633–14 642.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock">
Q. Feng, V. Ablavsky, and S. Sclaroff, “Cityflow-nl: Tracking and retrieval of vehicles at city scale by natural language descriptions,” <em class="ltx_emph ltx_font_italic" id="bib.bib126.1.1">arXiv preprint arXiv:2101.04741</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock">
C. Corbière, S. Roburin, S. Montariol, A. Bosselut, and A. Alahi, “Drivingvqa: Analyzing visual chain-of-thought reasoning of vision language models in real-world scenarios with driving theory tests,” <em class="ltx_emph ltx_font_italic" id="bib.bib127.1.1">arXiv preprint arXiv:2501.04671</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock">
X. Cao, T. Zhou, Y. Ma, W. Ye, C. Cui, K. Tang, Z. Cao, K. Liang, Z. Wang, J. M. Rehg <em class="ltx_emph ltx_font_italic" id="bib.bib128.1.1">et al.</em>, “Maplm: A real-world large-scale vision-language benchmark for map and traffic scene understanding,” in <em class="ltx_emph ltx_font_italic" id="bib.bib128.2.2">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition</em>, 2024, pp. 21 819–21 830.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock">
E. Sachdeva, N. Agarwal, S. Chundi, S. Roelofs, J. Li, M. Kochenderfer, C. Choi, and B. Dariush, “Rank2tell: A multimodal driving dataset for joint importance ranking and reasoning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib129.1.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, 2024, pp. 7513–7522.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock">
S. Malla, C. Choi, I. Dwivedi, J. H. Choi, and J. Li, “Drama: Joint risk localization and captioning in driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib130.1.1">Proceedings of the IEEE/CVF winter conference on applications of computer vision</em>, 2023, pp. 1043–1052.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock">
A.-M. Marcu, L. Chen, J. Hünermann, A. Karnsund, B. Hanotte, P. Chidananda, S. Nair, V. Badrinarayanan, A. Kendall, J. Shotton <em class="ltx_emph ltx_font_italic" id="bib.bib131.1.1">et al.</em>, “Lingoqa: Visual question answering for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib131.2.2">European Conference on Computer Vision</em>.   Springer, 2024, pp. 252–269.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock">
A. Geiger, P. Lenz, C. Stiller, and R. Urtasun, “Vision meets robotics: The kitti dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib132.1.1">The international journal of robotics research</em>, vol. 32, no. 11, pp. 1231–1237, 2013.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock">
F. Yu, H. Chen, X. Wang, W. Xian, Y. Chen, F. Liu, V. Madhavan, and T. Darrell, “Bdd100k: A diverse driving dataset for heterogeneous multitask learning,” in <em class="ltx_emph ltx_font_italic" id="bib.bib133.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 2636–2645.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock">
H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous driving,” in <em class="ltx_emph ltx_font_italic" id="bib.bib134.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 11 621–11 631.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock">
P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine <em class="ltx_emph ltx_font_italic" id="bib.bib135.1.1">et al.</em>, “Scalability in perception for autonomous driving: Waymo open dataset,” in <em class="ltx_emph ltx_font_italic" id="bib.bib135.2.2">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2020, pp. 2446–2454.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock">
R. Krajewski, J. Bock, L. Kloeker, and L. Eckstein, “The highd dataset: A drone dataset of naturalistic vehicle trajectories on german highways for validation of highly automated driving systems,” in <em class="ltx_emph ltx_font_italic" id="bib.bib136.1.1">2018 21st international conference on intelligent transportation systems (ITSC)</em>.   IEEE, 2018, pp. 2118–2125.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock">
B. Coifman and L. Li, “A critical evaluation of the next generation simulation (ngsim) vehicle trajectory dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib137.1.1">Transportation Research Part B: Methodological</em>, vol. 105, pp. 362–377, 2017.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock">
Z. Tang, M. Naphade, M.-Y. Liu, X. Yang, S. Birchfield, S. Wang, R. Kumar, D. Anastasiu, and J.-N. Hwang, “Cityflow: A city-scale benchmark for multi-target multi-camera vehicle tracking and re-identification,” in <em class="ltx_emph ltx_font_italic" id="bib.bib138.1.1">Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2019, pp. 8797–8806.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock">
M. Li, Z. Cui, Y. Wang, Y. Huang, and H. Chen, “An explainable <math alttext="q" class="ltx_Math" display="inline" id="bib.bib139.1.m1.1"><semantics id="bib.bib139.1.m1.1a"><mi id="bib.bib139.1.m1.1.1" xref="bib.bib139.1.m1.1.1.cmml">q</mi><annotation-xml encoding="MathML-Content" id="bib.bib139.1.m1.1b"><ci id="bib.bib139.1.m1.1.1.cmml" xref="bib.bib139.1.m1.1.1">𝑞</ci></annotation-xml><annotation encoding="application/x-tex" id="bib.bib139.1.m1.1c">q</annotation><annotation encoding="application/x-llamapun" id="bib.bib139.1.m1.1d">italic_q</annotation></semantics></math>-learning method for longitudinal control of autonomous vehicles,” <em class="ltx_emph ltx_font_italic" id="bib.bib139.2.1">IEEE Transactions on Intelligent Transportation Systems</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock">
E. Lobo, C. Agarwal, and H. Lakkaraju, “On the impact of fine-tuning on chain-of-thought reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib140.1.1">arXiv preprint arXiv:2411.15382</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock">
S. Atakishiyev, M. Salameh, H. Babiker, and R. Goebel, “Explaining autonomous driving actions with visual question answering,” in <em class="ltx_emph ltx_font_italic" id="bib.bib141.1.1">2023 IEEE 26th International Conference on Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2023, pp. 1207–1214.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock">
J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C. Ye, W. Zhang, Z. Li <em class="ltx_emph ltx_font_italic" id="bib.bib142.1.1">et al.</em>, “One million scenes for autonomous driving: Once dataset,” <em class="ltx_emph ltx_font_italic" id="bib.bib142.2.2">arXiv preprint arXiv:2106.11037</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock">
A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “Carla: An open urban driving simulator,” in <em class="ltx_emph ltx_font_italic" id="bib.bib143.1.1">Conference on robot learning</em>.   PMLR, 2017, pp. 1–16.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock">
Z. Song, Z. He, X. Li, Q. Ma, R. Ming, Z. Mao, H. Pei, L. Peng, J. Hu, D. Yao <em class="ltx_emph ltx_font_italic" id="bib.bib144.1.1">et al.</em>, “Synthetic datasets for autonomous driving: A survey,” <em class="ltx_emph ltx_font_italic" id="bib.bib144.2.2">IEEE Transactions on Intelligent Vehicles</em>, vol. 9, no. 1, pp. 1847–1864, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock">
Q. Li, X. Jia, S. Wang, and J. Yan, “Think2drive: Efficient reinforcement learning by thinking with latent world model for autonomous driving (in carla-v2),” in <em class="ltx_emph ltx_font_italic" id="bib.bib145.1.1">European Conference on Computer Vision</em>.   Springer, 2024, pp. 142–158.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock">
J. Guo, U. Kurup, and M. Shah, “Is it safe to drive? an overview of factors, metrics, and datasets for driveability assessment in autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib146.1.1">IEEE Transactions on Intelligent Transportation Systems</em>, vol. 21, no. 8, pp. 3135–3151, 2019.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock">
O. Golovneva, M. Chen, S. Poff, M. Corredor, L. Zettlemoyer, M. Fazel-Zarandi, and A. Celikyilmaz, “Roscoe: A suite of metrics for scoring step-by-step reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib147.1.1">arXiv preprint arXiv:2212.07919</em>, 2022.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock">
K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic evaluation of machine translation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib148.1.1">Proceedings of the 40th annual meeting of the Association for Computational Linguistics</em>, 2002, pp. 311–318.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock">
R. Vedantam, C. Lawrence Zitnick, and D. Parikh, “Cider: Consensus-based image description evaluation,” in <em class="ltx_emph ltx_font_italic" id="bib.bib149.1.1">Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 2015, pp. 4566–4575.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock">
S. Banerjee and A. Lavie, “Meteor: An automatic metric for mt evaluation with improved correlation with human judgments,” in <em class="ltx_emph ltx_font_italic" id="bib.bib150.1.1">Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization</em>, 2005, pp. 65–72.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock">
H. Liu, C. Li, Q. Wu, and Y. J. Lee, “Visual instruction tuning,” <em class="ltx_emph ltx_font_italic" id="bib.bib151.1.1">Advances in neural information processing systems</em>, vol. 36, pp. 34 892–34 916, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock">
S. Xing, H. Hua, X. Gao, S. Zhu, R. Li, K. Tian, X. Li, H. Huang, T. Yang, Z. Wang <em class="ltx_emph ltx_font_italic" id="bib.bib152.1.1">et al.</em>, “Autotrust: Benchmarking trustworthiness in large vision language models for autonomous driving,” <em class="ltx_emph ltx_font_italic" id="bib.bib152.2.2">arXiv preprint arXiv:2412.15206</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock">
M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi <em class="ltx_emph ltx_font_italic" id="bib.bib153.1.1">et al.</em>, “Openvla: An open-source vision-language-action model,” <em class="ltx_emph ltx_font_italic" id="bib.bib153.2.2">arXiv preprint arXiv:2406.09246</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock">
Y. Li, K. Katsumata, E. Javanmardi, and M. Tsukada, “Large language models for human-like autonomous driving: A survey,” in <em class="ltx_emph ltx_font_italic" id="bib.bib154.1.1">2024 IEEE 27th International Conference on Intelligent Transportation Systems (ITSC)</em>.   IEEE, 2024, pp. 439–446.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock">
F. Song, B. Yu, M. Li, H. Yu, F. Huang, Y. Li, and H. Wang, “Preference ranking optimization for human alignment,” in <em class="ltx_emph ltx_font_italic" id="bib.bib155.1.1">Proceedings of the AAAI Conference on Artificial Intelligence</em>, vol. 38, no. 17, 2024, pp. 18 990–18 998.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock">
S. Kothawade, V. Khandelwal, K. Basu, H. Wang, and G. Gupta, “Auto-discern: autonomous driving using common sense reasoning,” <em class="ltx_emph ltx_font_italic" id="bib.bib156.1.1">arXiv preprint arXiv:2110.13606</em>, 2021.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock">
S. Xu, W. Xie, L. Zhao, and P. He, “Chain of draft: Thinking faster by writing less,” <em class="ltx_emph ltx_font_italic" id="bib.bib157.1.1">arXiv preprint arXiv:2502.18600</em>, 2025.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock">
Z. Ji, N. Lee, R. Frieske, T. Yu, D. Su, Y. Xu, E. Ishii, Y. J. Bang, A. Madotto, and P. Fung, “Survey of hallucination in natural language generation,” <em class="ltx_emph ltx_font_italic" id="bib.bib158.1.1">ACM computing surveys</em>, vol. 55, no. 12, pp. 1–38, 2023.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock">
Y. Deng, Y. Choi, and S. Shieber, “From explicit cot to implicit cot: Learning to internalize cot step by step,” <em class="ltx_emph ltx_font_italic" id="bib.bib159.1.1">arXiv preprint arXiv:2405.14838</em>, 2024.

</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock">
D. Kong, M. Zhao, D. Xu, B. Pang, S. Wang, E. Honig, Z. Si, C. Li, J. Xie, S. Xie <em class="ltx_emph ltx_font_italic" id="bib.bib160.1.1">et al.</em>, “Scalable language models with posterior inference of latent thought vectors,” <em class="ltx_emph ltx_font_italic" id="bib.bib160.2.2">arXiv preprint arXiv:2502.01567</em>, 2025.

</span>
</li>
</ul>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Mon May 26 17:02:56 2025 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
